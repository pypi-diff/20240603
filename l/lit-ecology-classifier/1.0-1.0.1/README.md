# Comparing `tmp/lit_ecology_classifier-1.0-py3-none-any.whl.zip` & `tmp/lit_ecology_classifier-1.0.1-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,23 +1,23 @@
-Zip file size: 24817 bytes, number of entries: 21
+Zip file size: 25986 bytes, number of entries: 21
 -rw-r--r--  2.0 unx        0 b- defN 24-May-28 11:43 lit_ecology_classifier/__init__.py
--rw-r--r--  2.0 unx     3469 b- defN 24-May-29 08:00 lit_ecology_classifier/main.py
--rw-r--r--  2.0 unx     1789 b- defN 24-May-31 13:46 lit_ecology_classifier/predict.py
--rw-r--r--  2.0 unx     2327 b- defN 24-May-30 14:25 lit_ecology_classifier/test.py
+-rw-r--r--  2.0 unx     3514 b- defN 24-Jun-03 11:44 lit_ecology_classifier/main.py
+-rw-r--r--  2.0 unx     1871 b- defN 24-Jun-03 13:42 lit_ecology_classifier/predict.py
+-rw-r--r--  2.0 unx     2299 b- defN 24-Jun-03 13:11 lit_ecology_classifier/test.py
 -rw-r--r--  2.0 unx        0 b- defN 24-May-28 11:43 lit_ecology_classifier/data/__init__.py
--rw-r--r--  2.0 unx     8156 b- defN 24-May-31 13:51 lit_ecology_classifier/data/datamodule.py
--rw-r--r--  2.0 unx     7710 b- defN 24-May-31 08:50 lit_ecology_classifier/data/imagedataset.py
--rw-r--r--  2.0 unx     8016 b- defN 24-May-31 08:33 lit_ecology_classifier/data/tardataset.py
+-rw-r--r--  2.0 unx     8039 b- defN 24-Jun-03 13:05 lit_ecology_classifier/data/datamodule.py
+-rw-r--r--  2.0 unx     9379 b- defN 24-Jun-03 12:07 lit_ecology_classifier/data/imagedataset.py
+-rw-r--r--  2.0 unx    10486 b- defN 24-Jun-03 12:06 lit_ecology_classifier/data/tardataset.py
 -rw-r--r--  2.0 unx        0 b- defN 24-May-28 11:43 lit_ecology_classifier/helpers/__init__.py
--rw-r--r--  2.0 unx     2827 b- defN 24-May-31 13:43 lit_ecology_classifier/helpers/argparser.py
+-rw-r--r--  2.0 unx     5525 b- defN 24-Jun-03 13:39 lit_ecology_classifier/helpers/argparser.py
 -rw-r--r--  2.0 unx     1145 b- defN 24-May-28 11:43 lit_ecology_classifier/helpers/calc_class_weights.py
--rw-r--r--  2.0 unx    11114 b- defN 24-May-31 13:54 lit_ecology_classifier/helpers/helpers.py
+-rw-r--r--  2.0 unx    11479 b- defN 24-Jun-03 13:06 lit_ecology_classifier/helpers/helpers.py
 -rw-r--r--  2.0 unx        0 b- defN 24-May-28 11:43 lit_ecology_classifier/models/__init__.py
--rw-r--r--  2.0 unx    11575 b- defN 24-May-31 08:24 lit_ecology_classifier/models/model.py
--rw-r--r--  2.0 unx     2665 b- defN 24-May-31 09:28 lit_ecology_classifier/models/setup_model.py
--rw-r--r--  2.0 unx     1068 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/LICENSE
--rw-r--r--  2.0 unx     3704 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/WHEEL
--rw-r--r--  2.0 unx       76 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       23 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     1996 b- defN 24-May-31 14:02 lit_ecology_classifier-1.0.dist-info/RECORD
-21 files, 67752 bytes uncompressed, 21461 bytes compressed:  68.3%
+-rw-r--r--  2.0 unx    11754 b- defN 24-Jun-03 13:09 lit_ecology_classifier/models/model.py
+-rw-r--r--  2.0 unx     2680 b- defN 24-Jun-03 13:34 lit_ecology_classifier/models/setup_model.py
+-rw-r--r--  2.0 unx     1068 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/LICENSE
+-rw-r--r--  2.0 unx     3706 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/WHEEL
+-rw-r--r--  2.0 unx       76 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       23 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     2009 b- defN 24-Jun-03 13:45 lit_ecology_classifier-1.0.1.dist-info/RECORD
+21 files, 75145 bytes uncompressed, 22606 bytes compressed:  69.9%
```

## zipnote {}

```diff
@@ -39,26 +39,26 @@
 
 Filename: lit_ecology_classifier/models/model.py
 Comment: 
 
 Filename: lit_ecology_classifier/models/setup_model.py
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/LICENSE
+Filename: lit_ecology_classifier-1.0.1.dist-info/LICENSE
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/METADATA
+Filename: lit_ecology_classifier-1.0.1.dist-info/METADATA
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/WHEEL
+Filename: lit_ecology_classifier-1.0.1.dist-info/WHEEL
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/entry_points.txt
+Filename: lit_ecology_classifier-1.0.1.dist-info/entry_points.txt
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/top_level.txt
+Filename: lit_ecology_classifier-1.0.1.dist-info/top_level.txt
 Comment: 
 
-Filename: lit_ecology_classifier-1.0.dist-info/RECORD
+Filename: lit_ecology_classifier-1.0.1.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## lit_ecology_classifier/main.py

```diff
@@ -48,15 +48,17 @@
         logger = WandbLogger(
             project=args.dataset,
             log_model=False,
             save_dir=args.train_outpath,
         )
         logger.experiment.log_code("./lit_plankformer", include_fn=lambda path: path.endswith(".py"))
     else:
-        logger = CSVLogger(save_dir=args.train_outpath, name="csv_logs")
+        logger = CSVLogger(save_dir=args.train_outpath, name='csv_logs')
+
+    torch.backends.cudnn.allow_tf32 = False
 
     # Initialize the Model
     args.num_classes = len(datamodule.class_map)
     if args.balance_classes:
         args.class_weights = calculate_class_weights(datamodule)
     else:
         args.class_weights = None
```

## lit_ecology_classifier/predict.py

```diff
@@ -47,13 +47,13 @@
     data_module = DataModule(**model.hparams)
     data_module.setup("predict")
 
     model.load_datamodule(data_module)
 
     # Initialize the Trainer and Perform Predictions
     trainer = pl.Trainer(devices=[args.gpu_id] if not args.no_gpu else None, strategy= "auto",
-    enable_progress_bar=True, default_root_dir=args.outpath,)
+    enable_progress_bar=True, default_root_dir=args.outpath,limit_predict_batches=args.limit_pred_batch if args.limit_pred_batch > 0 else None)
     trainer.predict(model, datamodule=data_module)
 
     # Calculate and log the total time taken for prediction
     total_secs = -1 if time_begin is None else (time() - time_begin)
     logging.info('Time taken for prediction (in secs): {}'.format(total_secs))
```

## lit_ecology_classifier/test.py

```diff
@@ -33,15 +33,15 @@
     parser = inference_argparser()
     args = parser.parse_args()
 
     # Create Output Directory if it doesn't exist
     pathlib.Path(args.outpath).mkdir(parents=True, exist_ok=True)
 
     # Initialize the Model
-    model = LitClassifier.load_from_checkpoint(args.model_path, num_classes=87, class_weights=None)
+    model = LitClassifier.load_from_checkpoint(args.model_path)
 
     # Initialize the Data Module
     hparams = model.hparams # copy the hyperparameters from the model
     model.hparams.batch_size *= 1
     model.hparams.TTA = not args.no_TTA # set the TTA flag based on the argument
     model.hparams.outpath = args.outpath
     model.hparams.datapath = args.datapath
@@ -56,11 +56,11 @@
     trainer = pl.Trainer(devices=torch.cuda.device_count() if not args.no_gpu else 0, strategy="ddp" if torch.cuda.device_count() > 1 else "auto",
         enable_progress_bar=True, default_root_dir=args.outpath,limit_test_batches=2)
     trainer.test(model, datamodule=data_module)
 
     # Calculate and log the total time taken for prediction
     total_secs = -1 if time_begin is None else (time() - time_begin)
     logging.info('Time taken for prediction (in secs): {}'.format(total_secs))
-    model.hparams.priority_classes="config/priority.json"
-    with open(model.hparams.priority_classes, 'r') as file:
-        priority_classes = json.load(file)["priority_classes"]
-    plot_reduced_classes(model, priority_classes)
+    # model.hparams.priority_classes="config/priority.json"
+    # with open(model.hparams.priority_classes, 'r') as file:
+    #     priority_classes = json.load(file)["priority_classes"]
+    # plot_reduced_classes(model, priority_classes)
```

## lit_ecology_classifier/data/datamodule.py

```diff
@@ -19,28 +19,27 @@
     with PyTorch training routines using the PyTorch Lightning framework.
 
     Attributes:
         tarpath (str): Path to the tar file containing the dataset.
         batch_size (int): Number of images to load per batch.
         dataset (str): Identifier for the dataset being used.
         testing (bool): Flag to enable testing mode, which includes TTA (Test Time Augmentation).
-        use_multi (bool): Flag to enable multi-processing for data loading.
         priority_classes (str): Path to the JSON file containing a list of the priority classes.
         splits (Iterable): Proportions to split the dataset into training, validation, and testing.
     """
 
-    def __init__(self, datapath: str, batch_size: int, dataset: str, TTA: bool = False, use_multi: bool = True, priority_classes: list = [], splits: Iterable = [0.7, 0.15], **kwargs):
+    def __init__(self, datapath: str, batch_size: int, dataset: str, TTA: bool = False,  priority_classes: list = [], rest_classes: list=[], splits: Iterable = [0.7, 0.15], **kwargs):
         super().__init__()
         self.datapath = datapath
         self.TTA = TTA # Enable Test Time Augmentation if testing is True
         self.batch_size = batch_size
         self.dataset = dataset
-        self.use_multi = use_multi
         self.train_split, self.val_split = splits
         self.priority_classes = priority_classes
+        self.rest_classes=rest_classes
         self.class_map_path = f"./params/{dataset}/class_map.json"
         # Verify that class map exists for testing mode
 
 
     def setup(self, stage=None):
         """
         Prepares the datasets for training, validation, and testing by applying appropriate splits.
@@ -48,17 +47,17 @@
 
         Args:
             stage (Optional[str]): Current stage of the model training/testing. Not used explicitly in the method.
         """
         # Load the dataset
         if stage != "predict":
             if self.datapath.find(".tar") == -1:
-                full_dataset = ImageFolderDataset(self.datapath, self.class_map_path, self.priority_classes, TTA=self.TTA, train=True)
+                full_dataset = ImageFolderDataset(self.datapath, self.class_map_path, self.priority_classes,self.rest_classes, TTA=self.TTA, train=True)
             else:
-                full_dataset = TarImageDataset(self.datapath, self.class_map_path, self.priority_classes, TTA=self.TTA, train=True)
+                full_dataset = TarImageDataset(self.datapath, self.class_map_path, self.priority_classes,self.rest_classes, TTA=self.TTA, train=True)
 
             self.class_map_path = full_dataset.class_map_path
             self.class_map = full_dataset.class_map
             print("Number of classes:", len(self.class_map))
 
             # Calculate dataset splits
             train_size = int(self.train_split * len(full_dataset))
@@ -68,17 +67,17 @@
             # Randomly split the dataset into train, validation, and test sets
             self.train_dataset, self.val_dataset, self.test_dataset = random_split(full_dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(42))
             # Set train flag to False for validation and test datasets
             self.val_dataset.train = False
             self.test_dataset.train = False
         else:
             if self.datapath.find(".tar") == -1:
-                self.predict_dataset = ImageFolderDataset(self.datapath, self.class_map_path, self.priority_classes, TTA=self.TTA, train=False)
+                self.predict_dataset = ImageFolderDataset(self.datapath, self.class_map_path, self.priority_classes,self.rest_classes, TTA=self.TTA, train=False)
             else:
-                self.predict_dataset = TarImageDataset(self.datapath, self.class_map_path, self.priority_classes, TTA=self.TTA, train=False)
+                self.predict_dataset = TarImageDataset(self.datapath, self.class_map_path, self.priority_classes,self.rest_classes, TTA=self.TTA, train=False)
 
             self.class_map = self.predict_dataset.class_map
             self.class_map_path = self.predict_dataset.class_map_path
         if not os.path.exists(self.class_map_path) and self.TTA:
             raise FileNotFoundError(f"Class map not found at {self.class_map_path}. The class map needs to exist for inference.")
     def train_dataloader(self):
         """
@@ -130,15 +129,15 @@
 
     def test_dataloader(self):
         """
         Constructs the DataLoader for testing data.
         Returns:
             DataLoader: DataLoader object for the testing dataset.
         """
-        sampler = DistributedSampler(self.test_dataset) if torch.cuda.device_count() > 1 and self.use_multi else None
+
         if self.TTA:
             loader = DataLoader(
                 self.test_dataset,
                 batch_size=self.batch_size,
                 shuffle=False,
                 sampler=None,
                 num_workers= 4,
```

## lit_ecology_classifier/data/imagedataset.py

```diff
@@ -23,15 +23,15 @@
         image_folder_path (str): Path to the folder containing image data.
         class_map_path (str): Path to the JSON file mapping class names to labels.
         priority_classes (str): Path to a JSON file specifying priority classes for targeted training or evaluation.
         train (bool): Specifies whether the dataset will be used for training. Determines the type of transformations applied.
         TTA (bool): Indicates if Test Time Augmentation should be applied during testing.
     """
 
-    def __init__(self, image_folder_path: str, class_map_path: str, priority_classes: str, train: bool, TTA: bool = False):
+    def __init__(self, tar_path: str, class_map_path: str, priority_classes: list, rest_classes:list,train: bool, TTA: bool = False):
         """
         Initializes the ImageFolderDataset with paths and modes.
 
         Args:
             image_folder_path (str): The folder path containing the images.
             class_map_path (str): The file path to the JSON file with class mappings.
             priority_classes (str): The file path to the JSON file that contains priority classes.
@@ -39,25 +39,35 @@
             TTA (bool): A flag to enable Test Time Augmentation.
         """
         self.image_folder_path = image_folder_path
         self.TTA = TTA
         self.train = train
         self.class_map_path = class_map_path
         self.priority_classes = priority_classes
+        self.rest_classes = rest_classes
 
         # Load priority classes and adjust class map accordingly
         if self.priority_classes != []:
 
             logging.info(f"Priority classes not None. Loading priority classes from {self.priority_classes}")
             priority_postfix = "_priority"
             logging.info(f"Priority classes loaded: {self.priority_classes}")
             self.class_map_path = self.class_map_path.replace("class_map.json", f"class_map{priority_postfix}.json")
             logging.info(f"Class map path set to {self.class_map_path}")
 
-        # Load class map from JSON or create it from the folder structure if not present
+        elif self.rest_classes != []:
+
+            logging.info(f"rest classes not None. Loading rest classes from {self.rest_classes}")
+
+            rest_postfix = "_rest"
+            logging.info(f"rest classes loaded: {self.rest_classes}")
+            self.class_map_path = self.class_map_path.replace("class_map.json", f"class_map{rest_postfix}.json")
+            logging.info(f"Class map path set to {self.class_map_path}")
+
+        # Load class map from JSON or extract it from the tar file if not present
         if not os.path.exists(self.class_map_path):
             if not train:
                 raise FileNotFoundError(f"Class map not found at {self.class_map_path}. Class map needs to be present for testing.")
             logging.info(f"Class map not found at {self.class_map_path}. Extracting class map from folder structure.")
             self._create_class_map(image_folder_path)
             logging.info(f"Class map saved to {self.class_map_path}")
         else:
@@ -66,14 +76,31 @@
                 self.class_map = json.load(json_file)
             logging.info(f"Class map loaded.")
 
         # Transformation sequences for training and validation/testing
         self._define_transforms()
         # Load image information from the folder structure
         self.image_infos = self._load_image_infos()
+        if self.rest_classes:
+            self._filter_rest_classes()
+
+
+    def _filter_rest_classes(self):
+        """
+        Removes samples that are not in rest_classes from the dataset.
+        """
+        logging.info(f"Filtering dataset to keep only classes in {self.rest_classes}")
+        filtered_image_infos = []
+        for image_info in self.image_infos:
+            class_name = os.path.basename(os.path.dirname(image_info.name))
+            if class_name in self.rest_classes:
+                filtered_image_infos.append(image_info)
+        self.image_infos = filtered_image_infos
+        logging.info(f"Filtered dataset to {len(self.image_infos)} samples.")
+
 
     def _define_transforms(self):
         mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]  # ImageNet mean and std
         self.train_transforms = Compose([ToImage(), RandomHorizontalFlip(), Resize((224, 224)), ToDtype(torch.float32, scale=True), AugMix(), Normalize(mean, std)])
         self.val_transforms = Compose([ToImage(), Resize((224, 224)), ToDtype(torch.float32, scale=True), Normalize(mean, std)])
         if self.TTA:
             self.rotations = {
@@ -142,14 +169,21 @@
         if self.priority_classes != []:
 
             logging.info(f'priority_classes not set to []. Defining priority class_map')
             for key in self.priority_classes:
                 if key not in self.class_map.keys():
                     raise KeyError(f"Priority class {key} not found in class map. Keys of class map: {pprint.pformat(self.class_map.keys())}")
             self.class_map = define_priority_classes(self.priority_classes)
+        if self.rest_classes != []:
+
+            logging.info(f'rest_classes not set to []. Defining rest class_map')
+            for key in self.rest_classes:
+                if key not in self.class_map.keys():
+                    raise KeyError(f"rest class {key} not found in class map. Keys of class map: {pprint.pformat(self.class_map.keys())}")
+            self.class_map = define_rest_classes(self.rest_classes)
 
         logging.info(f"Class map created:\n{pprint.pformat(self.class_map)}")
         logging.info(f"Saving class map to {self.class_map_path}")
         os.makedirs(os.path.dirname(self.class_map_path), exist_ok=True)
         with open(self.class_map_path, "w") as json_file:
             json.dump(self.class_map, json_file, indent=4)
 
@@ -160,15 +194,18 @@
         Args:
             filename (str): The filename from which to extract the label.
 
         Returns:
             int: The label index corresponding to the class.
         """
         label = filename.split(os.sep)[-2]
-        label = self.class_map.get(label, 0)
+        if self.priority_classes != []:
+            label = self.class_map.get(label, 0)
+        else:
+            label = self.class_map[label]
         return label
 
     def shuffle(self):
         """
         Shuffles the list of image information to randomize data access, useful during training.
         """
         random.shuffle(self.image_infos)
```

## lit_ecology_classifier/data/tardataset.py

```diff
@@ -9,15 +9,15 @@
 
 import torch
 from PIL import Image
 from torch.utils.data import Dataset
 from torchvision import transforms
 from torchvision.transforms.v2 import AugMix, Compose, Normalize, RandomHorizontalFlip, RandomRotation, Resize, ToDtype, ToImage
 
-from ..helpers.helpers import define_priority_classes
+from ..helpers.helpers import define_priority_classes, define_rest_classes
 
 
 class TarImageDataset(Dataset):
     """
     A Dataset subclass for managing and accessing image data stored in tar files. This class supports optional
     image transformations, and Test Time Augmentation (TTA) for enhancing model evaluation during testing.
 
@@ -25,15 +25,15 @@
         tar_path (str): Path to the tar file containing image data.
         class_map_path (str): Path to the JSON file mapping class names to labels.
         priority_classes (str): Path to a JSON file specifying priority classes for targeted training or evaluation.
         train (bool): Specifies whether the dataset will be used for training. Determines the type of transformations applied.
         TTA (bool): Indicates if Test Time Augmentation should be applied during testing.
     """
 
-    def __init__(self, tar_path: str, class_map_path: str, priority_classes: str, train: bool, TTA: bool = False):
+    def __init__(self, tar_path: str, class_map_path: str, priority_classes: list, rest_classes:list,train: bool, TTA: bool = False):
         """
         Initializes the TarImageDataset with paths and modes.
 
         Args:
             tar_path (str): The file path to the tar archive containing the images.
             class_map_path (str): The file path to the JSON file with class mappings.
             priority_classes (str): The file path to the JSON file that contains priority classes.
@@ -41,24 +41,35 @@
             TTA (bool): A flag to enable Test Time Augmentation.
         """
         self.tar_path = tar_path
         self.TTA = TTA
         self.train = train
         self.class_map_path = class_map_path
         self.priority_classes = priority_classes
+        self.rest_classes = rest_classes
 
         # Load priority classes and adjust class map accordingly
         if self.priority_classes != []:
 
             logging.info(f"Priority classes not None. Loading priority classes from {self.priority_classes}")
+
             priority_postfix = "_priority"
             logging.info(f"Priority classes loaded: {self.priority_classes}")
             self.class_map_path = self.class_map_path.replace("class_map.json", f"class_map{priority_postfix}.json")
             logging.info(f"Class map path set to {self.class_map_path}")
 
+        elif self.rest_classes != []:
+
+            logging.info(f"rest classes not None. Loading rest classes from {self.rest_classes}")
+
+            rest_postfix = "_rest"
+            logging.info(f"rest classes loaded: {self.rest_classes}")
+            self.class_map_path = self.class_map_path.replace("class_map.json", f"class_map{rest_postfix}.json")
+            logging.info(f"Class map path set to {self.class_map_path}")
+
         # Load class map from JSON or extract it from the tar file if not present
         if not os.path.exists(self.class_map_path):
             if not train:
                 raise FileNotFoundError(f"Class map not found at {self.class_map_path}. Class map needs to be present for testing.")
             logging.info(f"Class map not found at {self.class_map_path}. Extracting class map from tar file.")
             self._extract_class_map(tar_path)
             logging.info(f"Class map saved to {self.class_map_path}")
@@ -68,14 +79,31 @@
                 self.class_map = json.load(json_file)
             logging.info(f"Class map loaded.")
 
         # Transformation sequences for training and validation/testing
         self._define_transforms()
         # Load image information from the tar file
         self.image_infos = self._load_image_infos()
+        if self.rest_classes:
+            self._filter_rest_classes()
+
+
+    def _filter_rest_classes(self):
+        """
+        Removes samples that are not in rest_classes from the dataset.
+        """
+        logging.info(f"Filtering dataset to keep only classes in {self.rest_classes}")
+        filtered_image_infos = []
+        for image_info in self.image_infos:
+            class_name = os.path.basename(os.path.dirname(image_info.name))
+            if class_name in self.rest_classes:
+                filtered_image_infos.append(image_info)
+        self.image_infos = filtered_image_infos
+        logging.info(f"Filtered dataset to {len(self.image_infos)} samples.")
+
 
     def _define_transforms(self):
         mean, std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225] # ImageNet mean and std
         self.train_transforms = Compose([ToImage(), RandomHorizontalFlip(), Resize((224, 224)), ToDtype(torch.float32, scale=True), AugMix(), Normalize(mean, std)])
         self.val_transforms = Compose([ToImage(), Resize((224, 224)), ToDtype(torch.float32, scale=True), Normalize(mean, std)])
         if self.TTA:
             self.rotations = {
@@ -111,52 +139,75 @@
             # Apply TTA transformations if enabled
             if self.TTA:
                 image = {rot: self.val_transforms(self.rotations[rot](image)) for rot in self.rotations}
             elif self.train:
                 image = self.train_transforms(image)
             else:
                 image = self.val_transforms(image)
-            label = self.get_label_from_filename(image_info)
+            label = self.get_label_from_filename(image_info.name)
             return image, label
 
     def _load_image_infos(self):
         """
         Load image information from the tar file.
         """
         image_infos = []
         with tarfile.open(self.tar_path, "r") as tar:
             for member in tar.getmembers():
                 if member.isfile() and member.name.lower().endswith(("jpg", "jpeg", "png")):
-                    image_infos.append(member.name)
+                    image_infos.append(member)
         return image_infos
 
     def _extract_class_map(self, tar_path):
         """
         Extracts the class map from the contents of the tar file and saves it to a JSON file.
         """
         logging.info("Extracting class map from tar file.")
-        class_map = defaultdict(list)
+        class_map = {}
+
         with tarfile.open(tar_path, "r") as tar:
+            # Temporary set to track folders that contain images
+            folders_with_images = set()
+
+            # First pass: Identify folders containing images
+            for member in tar.getmembers():
+                if member.isdir():
+                    continue  # Skip directories
+                if member.isfile() and member.name.lower().endswith(("jpg", "jpeg", "png")):
+                    class_name = os.path.basename(os.path.dirname(member.name))
+                    folders_with_images.add(class_name)
+
+            # Second pass: Build the class map only for folders with images
             for member in tar.getmembers():
                 if member.isdir():
                     continue  # Skip directories
                 class_name = os.path.basename(os.path.dirname(member.name))
-                class_map[class_name] = []
+                if class_name in folders_with_images:
+                    if class_name not in class_map:
+                        class_map[class_name] = []
+                    class_map[class_name].append(member.name)
 
         # Create a sorted list of class names and map them to indices
         sorted_class_names = sorted(class_map.keys())
         logging.info(f"Found {len(sorted_class_names)} classes.")
         self.class_map = {class_name: idx for idx, class_name in enumerate(sorted_class_names)}
         if self.priority_classes != []:
 
             logging.info(f'priority_classes not set to []. Defining priority class_map')
             for key in self.priority_classes:
                 if key not in self.class_map.keys():
                     raise KeyError(f"Priority class {key} not found in class map. Keys of class map: {pprint.pformat(self.class_map.keys())}")
             self.class_map = define_priority_classes(self.priority_classes)
+        if self.rest_classes != []:
+
+            logging.info(f'rest_classes not set to []. Defining rest class_map')
+            for key in self.rest_classes:
+                if key not in self.class_map.keys():
+                    raise KeyError(f"rest class {key} not found in class map. Keys of class map: {pprint.pformat(self.class_map.keys())}")
+            self.class_map = define_rest_classes(self.rest_classes)
 
         logging.info(f"Class map created:\n{pprint.pformat(self.class_map)}")
         logging.info(f"Saving class map to {self.class_map_path}")
         os.makedirs(os.path.dirname(self.class_map_path), exist_ok=True)
         with open(self.class_map_path, "w") as json_file:
             json.dump(self.class_map, json_file, indent=4)
 
@@ -167,15 +218,18 @@
         Args:
             filename (str): The filename from which to extract the label.
 
         Returns:
             int: The label index corresponding to the class.
         """
         label = filename.split("/")[1]
-        label = self.class_map.get(label, 0)
+        if self.priority_classes != []:
+            label = self.class_map.get(label, 0)
+        else:
+            label = self.class_map[label]
         return label
 
     def shuffle(self):
         """
         Shuffles the list of image information to randomize data access, useful during training.
         """
         random.shuffle(self.image_infos)
```

## lit_ecology_classifier/helpers/argparser.py

```diff
@@ -1,43 +1,104 @@
 import argparse
 import os
 
 def argparser():
+    """
+    Creates an argument parser for configuring, training, and running the machine learning model for image classification.
+
+    Arguments:
+    --datapath: str
+        Path to the tar file containing the training data. Default is "/store/empa/em09/aquascope/phyto.tar".
+    --train_outpath: str
+        Output path for training artifacts. Default is "./train_out".
+    --main_param_path: str
+        Main directory where the training parameters are saved. Default is "./params/".
+    --dataset: str
+        Name of the dataset. Default is "phyto".
+    --use_wandb: flag
+        Use Weights and Biases for logging. Default is False.
+
+    --priority_classes: str
+        Path to the JSON file specifying priority classes for training. Default is an empty string.
+    --rest_classes: str
+        Path to the JSON file specifying rest classes for training. Default is an empty string.
+    --balance_classes: flag
+        Balance the classes for training. Default is False.
+    --batch_size: int
+        Batch size for training. Default is 64.
+    --max_epochs: int
+        Number of epochs to train. Default is 20.
+    --lr: float
+        Learning rate for training. Default is 1e-2.
+    --lr_factor: float
+        Learning rate factor for training of full body. Default is 0.01.
+    --no_gpu: flag
+        Use no GPU for training. Default is False.
+    --testing: flag
+        Set this to True if in testing mode, False for training. Default is False.
+
+    Returns:
+        argparse.ArgumentParser: The argument parser with defined arguments.
+    """
     parser = argparse.ArgumentParser(description="Configure, train and run the machine learning model for image classification.")
 
     # Paths and directories
-    parser.add_argument("--datapath",  default="/store/empa/em09/aquascope/phyto.tar", help="Folder containing the tar training data")
+    parser.add_argument("--datapath",  default="/store/empa/em09/aquascope/phyto.tar", help="Path to the tar file containing the training data")
     parser.add_argument("--train_outpath", default="./train_out", help="Output path for training artifacts")
     parser.add_argument("--main_param_path", default="./params/", help="Main directory where the training parameters are saved")
     parser.add_argument("--dataset", default="phyto", help="Name of the dataset")
     parser.add_argument("--use_wandb", action="store_true", help="Use Weights and Biases for logging")
 
     # Model configuration and training options
-    parser.add_argument("--priority_classes", type=str, default="", help="Use priority classes for training, specify the path to the JSON file")
+    parser.add_argument("--priority_classes", type=str, default="", help="Path to the JSON file specifying priority classes for training")
+    parser.add_argument("--rest_classes", type=str, default="", help="Path to the JSON file specifying rest classes for training")
     parser.add_argument("--balance_classes", action="store_true", help="Balance the classes for training")
-    # Deep learning model specifics
     parser.add_argument("--batch_size", type=int, default=64, help="Batch size for training")
     parser.add_argument("--max_epochs", type=int, default=20, help="Number of epochs to train")
     parser.add_argument("--lr", type=float, default=1e-2, help="Learning rate for training")
     parser.add_argument("--lr_factor", type=float, default=0.01, help="Learning rate factor for training of full body")
     parser.add_argument("--no_gpu", action="store_true", help="Use no GPU for training, default is False")
 
     # Augmentation and training/testing specifics
     parser.add_argument("--testing", action="store_true", help="Set this to True if in testing mode, False for training")
     return parser
 
 def inference_argparser():
-    parser = argparse.ArgumentParser(description="Use Classifier on unlabelled data.")
-    parser.add_argument("--batch_size", type=int, default=32, help="Batch Size")
-    parser.add_argument("--outpath", default="./preds/", help="Directory where you want to save the predictions")
-    parser.add_argument("--model_path", default="./checkpoints/model.ckpt", help="Path to the model file")
-    parser.add_argument("--datapath",  default="/store/empa/em09/aquascope/phyto.tar", help="Path to the folder containing the data to classify as Tar file")
-    parser.add_argument("--no_gpu", action="store_true", help="Use no GPU for training, default is False")
+    """
+    Creates an argument parser for using the classifier on unlabeled data.
+
+    Arguments:
+    --batch_size: int
+        Batch size for inference. Default is 32.
+    --outpath: str
+        Directory where predictions will be saved. Default is "./preds/".
+    --model_path: str
+        Path to the model checkpoint file. Default is "./checkpoints/model.ckpt".
+    --datapath: str
+        Path to the tar file containing the data to classify. Default is "/store/empa/em09/aquascope/phyto.tar".
+    --no_gpu: flag
+        Use no GPU for inference. Default is False.
+    --no_TTA: flag
+        Disable test-time augmentation. Default is False.
+    --gpu_id: int
+        GPU ID to use for inference. Default is 0.
+    --limit_pred_batch: int
+        Limit the number of batches to predict. Default is 0, meaning no limit
+    Returns:
+        argparse.ArgumentParser: The argument parser with defined arguments.
+    """
+    parser = argparse.ArgumentParser(description="Use Classifier on unlabeled data.")
+    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for inference")
+    parser.add_argument("--outpath", default="./preds/", help="Directory where predictions will be saved")
+    parser.add_argument("--model_path", default="./checkpoints/model.ckpt", help="Path to the model checkpoint file")
+    parser.add_argument("--datapath",  default="/store/empa/em09/aquascope/phyto.tar", help="Path to the tar file containing the data to classify")
+    parser.add_argument("--no_gpu", action="store_true", help="Use no GPU for inference, default is False")
     parser.add_argument("--no_TTA", action="store_true", help="Disable test-time augmentation")
     parser.add_argument("--gpu_id", type=int, default=0, help="GPU ID to use for inference")
+    parser.add_argument("--limit_pred_batch", type=int, default=0, help="Limit the number of batches to predict")
     return parser
 
 
 # Example of using the argument parser
 if __name__ == "__main__":
     parser = argparser()
     args = parser.parse_args()
```

## lit_ecology_classifier/helpers/helpers.py

```diff
@@ -7,26 +7,26 @@
 from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint, ModelSummary, StochasticWeightAveraging
 import numpy as np
 from matplotlib.colors import LinearSegmentedColormap
 import json
 
 
 
-def output_results(outpath, im_names, labels, scores):
+def output_results(outpath, im_names, labels, scores,priority_classes=False,rest_classes=False):
     """
     Output the prediction results to a file.
 
     Args:
         outpath (str): Output directory path.
         im_names (list): List of image filenames.
         labels (list): List of predicted labels.
     """
 
     labels = labels.tolist()
-    base_filename = f"{outpath}/predictions_lit_ecology_classifier"
+    base_filename = f"{outpath}/predictions_lit_ecology_classifier"+("_priority" if priority_classes else "")+("_rest" if rest_classes else "")
     file_path = f"{base_filename}.txt"
     lines = [f"{img}------------------ {label}/{score}\n" for img, label,score in zip(im_names, labels,scores)]
     with open(file_path, "w+") as f:
         f.writelines(lines)
 
 
 def gmean(input_x, dim):
@@ -58,24 +58,27 @@
     """
 
 
     class_indices = np.arange(len(class_names))
     confusion_matrix = sklearn.metrics.confusion_matrix(all_labels.cpu(), all_preds.cpu(), labels=class_indices)
     confusion_matrix_norm = sklearn.metrics.confusion_matrix(all_labels.cpu(), all_preds.cpu(), normalize="pred", labels=class_indices)
     num_classes = confusion_matrix.shape[0]
-    fig, ax = plt.subplots(figsize=(15, 15))
-    fig2, ax2 = plt.subplots(figsize=(15, 15))
+    fig, ax = plt.subplots(figsize=(20, 20))
+    fig2, ax2 = plt.subplots(figsize=(20, 20))
+
+
     if len(class_names) != num_classes:
         print(f"Warning: Number of class names ({len(class_names)}) does not match the number of classes ({num_classes}) in confusion matrix.")
         class_names = class_names[:num_classes]
     cm_display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=class_names)
     cm_display_norm = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix_norm, display_labels=class_names)
     cmap = cvd_colormap()
     cm_display.plot(cmap=cmap, ax=ax, xticks_rotation=90)
     cm_display_norm.plot(cmap=cmap, ax=ax2, xticks_rotation=90)
+
     fig.tight_layout()
     fig2.tight_layout()
     return fig, fig2
 
 def cvd_colormap():
     """
     A color map accessible for people with color vision deficiency (CVD).
@@ -128,14 +131,18 @@
 
 
 def define_priority_classes(priority_classes):
     class_map = {class_name: i + 1 for i, class_name in enumerate(priority_classes)}
     class_map["rest"] = 0
     return class_map
 
+def define_rest_classes(priority_classes):
+    class_map = {class_name: i for i, class_name in enumerate(priority_classes)}
+    return class_map
+
 
 def plot_score_distributions(all_scores, all_preds, class_names, true_label):
     """
     Plot the distribution of prediction scores for each class in separate plots.
 
     Args:
         all_scores (torch.Tensor): Confidence scores of the predictions.
@@ -240,18 +247,18 @@
         priority_classes (list): List of priority classes to monitor for false positives.
         ckpt_name (str): The name of the checkpoint file.
 
     Returns:
         list: A list of configured callbacks including EarlyStopping, ModelCheckpoint, and ModelSummary.
     """
     callbacks = []
-    monitor = "val_acc" if priority_classes == "" else "val_false_positives"
-    mode = "max" if not priority_classes == "" else "min"
-    callbacks.append(EarlyStopping(monitor=monitor, patience=3, mode=mode))
-    callbacks.append(ModelCheckpoint(filename=ckpt_name, monitor=monitor, mode=mode))
+    ckpt_name = ckpt_name+"-{epoch:02d}-{val_acc:.2f}" if len(priority_classes )==0 else ckpt_name+"-{epoch:02d}-{val_acc:.2f}-{val_false_positives:.2f}"
+    monitor = "val_acc" if len(priority_classes )==0 else "val_false_positives"
+    mode = "max" if not len(priority_classes )==0 else "min"
+    callbacks.append(ModelCheckpoint(filename=ckpt_name, monitor=monitor, mode=mode, save_top_k=5))
     callbacks.append(ModelSummary())
     return callbacks
 
 def plot_reduced_classes(model, priority_classes):
     """
     Plots the confusion matrix for reduced classes.
```

## lit_ecology_classifier/models/model.py

```diff
@@ -21,14 +21,15 @@
         super().__init__()
         self.save_hyperparameters()
         self.model = setup_model(**self.hparams)
         if self.hparams.class_weights is not None:
             self.loss = torch.nn.CrossEntropyLoss(weight=torch.tensor(self.hparams.class_weights, dtype=torch.float32))
         else:
             self.loss = torch.nn.CrossEntropyLoss()
+
         logging.info("Model initialized with hyperparameters:\n {}".format(pprint.pformat(self.hparams)))
 
     def TTA(self, batch):
         """
         Perform Test Time Augmentation (TTA) on the input batch.
         Args:
             batch (tuple): Input batch containing images and labels.
@@ -249,15 +250,17 @@
         Saves predicted labels in text file in folder Output
         """
         filenames = self.datamodule.predict_dataset.image_infos
         max_index = torch.cat(self.probabilities).argmax(axis=1)
 
         pred_label = np.array([self.inverted_class_map[idx] for idx in max_index.numpy()], dtype=object)
         pred_score = torch.cat(self.probabilities).max(1)[0].numpy()
-        output_results(self.hparams.outpath, filenames, pred_label, pred_score)
+        priority_classes = len(self.hparams.get("priority_classes", [])) ==0
+        rest_classes = len(self.hparams.get("rest_classes", [])) ==0
+        output_results(self.hparams.outpath, filenames, pred_label, pred_score, priority_classes, rest_classes)
         plt.hist(max_index.numpy(), bins=len(self.inverted_class_map))
         plt.savefig(f"{self.hparams.outpath}/predictions_histogram.png")
         return super().on_test_epoch_end()
 
     def on_fit_end(self) -> None:
         """
         If the model is not using wandb, plot the loss and accuracy curves at the end of training
```

## lit_ecology_classifier/models/setup_model.py

```diff
@@ -1,14 +1,14 @@
 
 import numpy as np
 import timm
 import torch
 from safetensors.torch import load_file
 
-def setup_model( finetune, num_classes,checkpoint_path="checkpoints/backbone.safetensors", **kwargs):
+def setup_model( pretrained=False, num_classes=None,checkpoint_path="checkpoints/backbone.safetensors", **kwargs):
     """
     Set up and return the specified model architecture.
 
     Args:
         architecture (str): The model architecture to use.
         main_param_path (str): Path to the directory containing main parameters.
         ensemble (bool): Whether to use model ensembling.
@@ -33,15 +33,15 @@
 
     # Load the remaining state dict
     model.load_state_dict(checkpoint, strict=False)
 
     # Modify the model to match the number of classes in your dataset
     model.head = torch.nn.Linear(model.head.in_features, num_classes)
 
-    set_trainable_params(model, finetune=finetune)
+    set_trainable_params(model, finetune=pretrained)
 
     # Total parameters and trainable parameters
     total_params = sum(p.numel() for p in model.parameters())
     print(f"{total_params:,} total parameters.")
     total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
     print(f"{total_trainable_params:,} training parameters.")
```

## Comparing `lit_ecology_classifier-1.0.dist-info/LICENSE` & `lit_ecology_classifier-1.0.1.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `lit_ecology_classifier-1.0.dist-info/METADATA` & `lit_ecology_classifier-1.0.1.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: lit_ecology_classifier
-Version: 1.0
+Version: 1.0.1
 Summary: Image Classifier optimised for ecology use-cases
 Home-page: https://github.com/kaechb/lit_ecology_classifier
 Author: Benno Kaech
 Author-email: your.email@example.com
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: MIT License
 Classifier: Operating System :: OS Independent
```

## Comparing `lit_ecology_classifier-1.0.dist-info/RECORD` & `lit_ecology_classifier-1.0.1.dist-info/RECORD`

 * *Files 16% similar despite different names*

```diff
@@ -1,21 +1,21 @@
 lit_ecology_classifier/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lit_ecology_classifier/main.py,sha256=oye_MqYi8_Pnlq4E3Dy65VtXPW7JyBxecJW9cHEM6rw,3469
-lit_ecology_classifier/predict.py,sha256=SZJeR842g3JrOgiIrNdxcCcM75HJncSHMe4qYq2mGxc,1789
-lit_ecology_classifier/test.py,sha256=NCm4SGXD8Aa4ckt3bXqskH6XtDNSV2KPZ6BO938Oo0Q,2327
+lit_ecology_classifier/main.py,sha256=ysDis7VpaboSNJBNX-cC4Qize3Mi8mLm-VIY2kUi3IE,3514
+lit_ecology_classifier/predict.py,sha256=ihv6pupv0IgIp1JTxoSd2851Nut0ty2fCWPH_HK6ppA,1871
+lit_ecology_classifier/test.py,sha256=1Gi31g9OhdG-hqDbSa303fdcQY1AlcWypt5Nt4aGwzU,2299
 lit_ecology_classifier/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lit_ecology_classifier/data/datamodule.py,sha256=3VVC8TJGrT9IeZeK5rg0uIgHSBLOO2qSxpLN4O628fI,8156
-lit_ecology_classifier/data/imagedataset.py,sha256=YblIo-n5LaVBzYuMu07b11Xy_fod_oZTnwGqLoYL4hs,7710
-lit_ecology_classifier/data/tardataset.py,sha256=HDxnCiJHuKUtgLJJslulhjXsC2KGnp8s_FUiQXbtXNI,8016
+lit_ecology_classifier/data/datamodule.py,sha256=kQ689tplO5HuUWb8nCtuLya5DIv8Zz_RU8RCVb08G5M,8039
+lit_ecology_classifier/data/imagedataset.py,sha256=T8kOPUbXbVcNRdihuWem3S27grCw7vEkT9DkxhVNAj8,9379
+lit_ecology_classifier/data/tardataset.py,sha256=2nRuU5ZJgCt4DU-hY3-kt_iNZWR1TTx_6Hvzq7FQwJk,10486
 lit_ecology_classifier/helpers/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lit_ecology_classifier/helpers/argparser.py,sha256=JjWiA1cdqjOXVdx-WNUzGSJ0W8FEAIUZq3wk4dMalqI,2827
+lit_ecology_classifier/helpers/argparser.py,sha256=qHtkoku-UzkR8KDb1rF-U_h5eqgoM5mVjmW2qv0xAgU,5525
 lit_ecology_classifier/helpers/calc_class_weights.py,sha256=fMzuX0jeRNXScX9IO8veOgKsxAnWI3VfjwDwAFOO6A4,1145
-lit_ecology_classifier/helpers/helpers.py,sha256=H_Yu0S-UrJbI9RVVWz4cCnMWMYMGf7awKfu57VAoL2U,11114
+lit_ecology_classifier/helpers/helpers.py,sha256=LsZhnoqwNm4J59PMYYEgURs7BWkJXlNpW-TI6rGlwlI,11479
 lit_ecology_classifier/models/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-lit_ecology_classifier/models/model.py,sha256=HX2sC5ZEZNK2yf-bQNPj-FZGCNZaoKfDz1WOxNf-Cj4,11575
-lit_ecology_classifier/models/setup_model.py,sha256=sVULgitlYHlaVPNu3-IIMb_V5ZbA1-Vt3YPZ390ZmTs,2665
-lit_ecology_classifier-1.0.dist-info/LICENSE,sha256=otRPec6AwxKerM0zN-WiG66AnplxnmpbPaDyJd_QxE0,1068
-lit_ecology_classifier-1.0.dist-info/METADATA,sha256=JtSWhoQxvtEMdysKQr6zop4M5oKNIuREONreW0nf6ys,3704
-lit_ecology_classifier-1.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-lit_ecology_classifier-1.0.dist-info/entry_points.txt,sha256=v3ZJ9RRS0_MjXgy4y3gPb2R7ANPKKyPQrRlEO384ITQ,76
-lit_ecology_classifier-1.0.dist-info/top_level.txt,sha256=PKi3Fm6NgOu2ej8D7-MCov1FmFYRFdfaTOAxYhDYJnw,23
-lit_ecology_classifier-1.0.dist-info/RECORD,,
+lit_ecology_classifier/models/model.py,sha256=ZlKGRge5zF5k6bAPrCUUUSpZ-kv_0a-3ETmSGNZUOQQ,11754
+lit_ecology_classifier/models/setup_model.py,sha256=r670sKtkAvi4XUEuHgB28YczqKszWVeRi6pYml1cZkE,2680
+lit_ecology_classifier-1.0.1.dist-info/LICENSE,sha256=otRPec6AwxKerM0zN-WiG66AnplxnmpbPaDyJd_QxE0,1068
+lit_ecology_classifier-1.0.1.dist-info/METADATA,sha256=U6q4haNP0Hr8Z18vTpAtejaCyxrTUJl6SVVyc8SK6L0,3706
+lit_ecology_classifier-1.0.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+lit_ecology_classifier-1.0.1.dist-info/entry_points.txt,sha256=v3ZJ9RRS0_MjXgy4y3gPb2R7ANPKKyPQrRlEO384ITQ,76
+lit_ecology_classifier-1.0.1.dist-info/top_level.txt,sha256=PKi3Fm6NgOu2ej8D7-MCov1FmFYRFdfaTOAxYhDYJnw,23
+lit_ecology_classifier-1.0.1.dist-info/RECORD,,
```

