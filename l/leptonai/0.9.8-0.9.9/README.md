# Comparing `tmp/leptonai-0.9.8-py3-none-any.whl.zip` & `tmp/leptonai-0.9.9-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,62 +1,63 @@
-Zip file size: 103426 bytes, number of entries: 60
--rw-r--r--  2.0 unx      155 b- defN 23-Sep-19 17:48 leptonai/__init__.py
--rw-r--r--  2.0 unx      160 b- defN 23-Sep-19 17:48 leptonai/_version.py
--rw-r--r--  2.0 unx    22245 b- defN 23-Sep-19 17:48 leptonai/client.py
--rw-r--r--  2.0 unx     2714 b- defN 23-Sep-19 17:48 leptonai/config.py
--rw-r--r--  2.0 unx      864 b- defN 23-Sep-19 17:48 leptonai/registry.py
--rw-r--r--  2.0 unx     2126 b- defN 23-Sep-19 17:48 leptonai/util.py
--rw-r--r--  2.0 unx        0 b- defN 23-Sep-19 17:48 leptonai/_internal/__init__.py
--rw-r--r--  2.0 unx     7632 b- defN 23-Sep-19 17:48 leptonai/_internal/client_utils.py
--rw-r--r--  2.0 unx      948 b- defN 23-Sep-19 17:48 leptonai/_internal/db.py
--rw-r--r--  2.0 unx     2081 b- defN 23-Sep-19 17:48 leptonai/_internal/logging.py
--rw-r--r--  2.0 unx     1506 b- defN 23-Sep-19 17:48 leptonai/api/__init__.py
--rw-r--r--  2.0 unx     1723 b- defN 23-Sep-19 17:48 leptonai/api/connection.py
--rw-r--r--  2.0 unx     5407 b- defN 23-Sep-19 17:48 leptonai/api/deployment.py
--rw-r--r--  2.0 unx     6144 b- defN 23-Sep-19 17:48 leptonai/api/photon.py
--rw-r--r--  2.0 unx      877 b- defN 23-Sep-19 17:48 leptonai/api/secret.py
--rw-r--r--  2.0 unx     4507 b- defN 23-Sep-19 17:48 leptonai/api/storage.py
--rw-r--r--  2.0 unx     7759 b- defN 23-Sep-19 17:48 leptonai/api/types.py
--rw-r--r--  2.0 unx     2451 b- defN 23-Sep-19 17:48 leptonai/api/util.py
--rw-r--r--  2.0 unx    10157 b- defN 23-Sep-19 17:48 leptonai/api/workspace.py
--rw-r--r--  2.0 unx      326 b- defN 23-Sep-19 17:48 leptonai/bench/gpt2/client.py
--rw-r--r--  2.0 unx      394 b- defN 23-Sep-19 17:48 leptonai/cli/__init__.py
--rw-r--r--  2.0 unx     1624 b- defN 23-Sep-19 17:48 leptonai/cli/cli.py
--rw-r--r--  2.0 unx      143 b- defN 23-Sep-19 17:48 leptonai/cli/constants.py
--rw-r--r--  2.0 unx    17387 b- defN 23-Sep-19 17:48 leptonai/cli/deployment.py
--rw-r--r--  2.0 unx     3986 b- defN 23-Sep-19 17:48 leptonai/cli/in_n_out.py
--rw-r--r--  2.0 unx    25104 b- defN 23-Sep-19 17:48 leptonai/cli/photon.py
--rw-r--r--  2.0 unx     3370 b- defN 23-Sep-19 17:48 leptonai/cli/secret.py
--rw-r--r--  2.0 unx     7737 b- defN 23-Sep-19 17:48 leptonai/cli/storage.py
--rw-r--r--  2.0 unx     4881 b- defN 23-Sep-19 17:48 leptonai/cli/util.py
--rw-r--r--  2.0 unx     9728 b- defN 23-Sep-19 17:48 leptonai/cli/workspace.py
--rw-r--r--  2.0 unx      552 b- defN 23-Sep-19 17:48 leptonai/cloudrun/__init__.py
--rw-r--r--  2.0 unx    16090 b- defN 23-Sep-19 17:48 leptonai/cloudrun/remote.py
--rw-r--r--  2.0 unx      479 b- defN 23-Sep-19 17:48 leptonai/photon/__init__.py
--rw-r--r--  2.0 unx      926 b- defN 23-Sep-19 17:48 leptonai/photon/background.py
--rw-r--r--  2.0 unx     8135 b- defN 23-Sep-19 17:48 leptonai/photon/base.py
--rw-r--r--  2.0 unx     5188 b- defN 23-Sep-19 17:48 leptonai/photon/batcher.py
--rw-r--r--  2.0 unx       33 b- defN 23-Sep-19 17:48 leptonai/photon/constants.py
--rw-r--r--  2.0 unx     3019 b- defN 23-Sep-19 17:48 leptonai/photon/download.py
--rw-r--r--  2.0 unx     5937 b- defN 23-Sep-19 17:48 leptonai/photon/favicon.ico
--rw-r--r--  2.0 unx    34803 b- defN 23-Sep-19 17:48 leptonai/photon/photon.py
--rw-r--r--  2.0 unx      464 b- defN 23-Sep-19 17:48 leptonai/photon/rate_limit.py
--rw-r--r--  2.0 unx     2291 b- defN 23-Sep-19 17:48 leptonai/photon/util.py
--rw-r--r--  2.0 unx       58 b- defN 23-Sep-19 17:48 leptonai/photon/hf/__init__.py
--rw-r--r--  2.0 unx    27263 b- defN 23-Sep-19 17:48 leptonai/photon/hf/hf.py
--rw-r--r--  2.0 unx     1019 b- defN 23-Sep-19 17:48 leptonai/photon/hf/hf_dependencies.py
--rw-r--r--  2.0 unx    10540 b- defN 23-Sep-19 17:48 leptonai/photon/hf/hf_utils.py
--rw-r--r--  2.0 unx      166 b- defN 23-Sep-19 17:48 leptonai/photon/prebuilt/__init__.py
--rw-r--r--  2.0 unx     1591 b- defN 23-Sep-19 17:48 leptonai/photon/prebuilt/vllm.py
--rw-r--r--  2.0 unx      235 b- defN 23-Sep-19 17:48 leptonai/photon/types/__init__.py
--rw-r--r--  2.0 unx     1992 b- defN 23-Sep-19 17:48 leptonai/photon/types/fileparam.py
--rw-r--r--  2.0 unx     3269 b- defN 23-Sep-19 17:48 leptonai/photon/types/pickled.py
--rw-r--r--  2.0 unx      186 b- defN 23-Sep-19 17:48 leptonai/photon/types/responses.py
--rw-r--r--  2.0 unx     3190 b- defN 23-Sep-19 17:48 leptonai/photon/types/util.py
--rw-r--r--  2.0 unx    11357 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/LICENSE
--rw-r--r--  2.0 unx     6342 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/METADATA
--rw-r--r--  2.0 unx       29 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/NOTICE
--rw-r--r--  2.0 unx       92 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/WHEEL
--rw-r--r--  2.0 unx       41 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/entry_points.txt
--rw-r--r--  2.0 unx       16 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/top_level.txt
--rw-rw-r--  2.0 unx     4949 b- defN 23-Sep-19 17:48 leptonai-0.9.8.dist-info/RECORD
-60 files, 304388 bytes uncompressed, 95636 bytes compressed:  68.6%
+Zip file size: 103952 bytes, number of entries: 61
+-rw-r--r--  2.0 unx      155 b- defN 23-Sep-22 20:44 leptonai/__init__.py
+-rw-r--r--  2.0 unx      160 b- defN 23-Sep-22 20:45 leptonai/_version.py
+-rw-r--r--  2.0 unx    22245 b- defN 23-Sep-22 20:44 leptonai/client.py
+-rw-r--r--  2.0 unx     2714 b- defN 23-Sep-22 20:44 leptonai/config.py
+-rw-r--r--  2.0 unx      861 b- defN 23-Sep-22 20:44 leptonai/registry.py
+-rw-r--r--  2.0 unx        0 b- defN 23-Sep-22 20:44 leptonai/_internal/__init__.py
+-rw-r--r--  2.0 unx     7632 b- defN 23-Sep-22 20:44 leptonai/_internal/client_utils.py
+-rw-r--r--  2.0 unx      948 b- defN 23-Sep-22 20:44 leptonai/_internal/db.py
+-rw-r--r--  2.0 unx     2081 b- defN 23-Sep-22 20:44 leptonai/_internal/logging.py
+-rw-r--r--  2.0 unx     1506 b- defN 23-Sep-22 20:44 leptonai/api/__init__.py
+-rw-r--r--  2.0 unx     1723 b- defN 23-Sep-22 20:44 leptonai/api/connection.py
+-rw-r--r--  2.0 unx     5407 b- defN 23-Sep-22 20:44 leptonai/api/deployment.py
+-rw-r--r--  2.0 unx     4001 b- defN 23-Sep-22 20:44 leptonai/api/photon.py
+-rw-r--r--  2.0 unx      877 b- defN 23-Sep-22 20:44 leptonai/api/secret.py
+-rw-r--r--  2.0 unx     4507 b- defN 23-Sep-22 20:44 leptonai/api/storage.py
+-rw-r--r--  2.0 unx     7925 b- defN 23-Sep-22 20:44 leptonai/api/types.py
+-rw-r--r--  2.0 unx     2451 b- defN 23-Sep-22 20:44 leptonai/api/util.py
+-rw-r--r--  2.0 unx    10157 b- defN 23-Sep-22 20:44 leptonai/api/workspace.py
+-rw-r--r--  2.0 unx      326 b- defN 23-Sep-22 20:44 leptonai/bench/gpt2/client.py
+-rw-r--r--  2.0 unx      394 b- defN 23-Sep-22 20:44 leptonai/cli/__init__.py
+-rw-r--r--  2.0 unx     1624 b- defN 23-Sep-22 20:44 leptonai/cli/cli.py
+-rw-r--r--  2.0 unx      143 b- defN 23-Sep-22 20:44 leptonai/cli/constants.py
+-rw-r--r--  2.0 unx    18086 b- defN 23-Sep-22 20:44 leptonai/cli/deployment.py
+-rw-r--r--  2.0 unx     4020 b- defN 23-Sep-22 20:44 leptonai/cli/in_n_out.py
+-rw-r--r--  2.0 unx    25208 b- defN 23-Sep-22 20:44 leptonai/cli/photon.py
+-rw-r--r--  2.0 unx     3370 b- defN 23-Sep-22 20:44 leptonai/cli/secret.py
+-rw-r--r--  2.0 unx     7737 b- defN 23-Sep-22 20:44 leptonai/cli/storage.py
+-rw-r--r--  2.0 unx     4881 b- defN 23-Sep-22 20:44 leptonai/cli/util.py
+-rw-r--r--  2.0 unx     9762 b- defN 23-Sep-22 20:44 leptonai/cli/workspace.py
+-rw-r--r--  2.0 unx      552 b- defN 23-Sep-22 20:44 leptonai/cloudrun/__init__.py
+-rw-r--r--  2.0 unx    16143 b- defN 23-Sep-22 20:44 leptonai/cloudrun/remote.py
+-rw-r--r--  2.0 unx      479 b- defN 23-Sep-22 20:44 leptonai/photon/__init__.py
+-rw-r--r--  2.0 unx      926 b- defN 23-Sep-22 20:44 leptonai/photon/background.py
+-rw-r--r--  2.0 unx     8135 b- defN 23-Sep-22 20:44 leptonai/photon/base.py
+-rw-r--r--  2.0 unx     5188 b- defN 23-Sep-22 20:44 leptonai/photon/batcher.py
+-rw-r--r--  2.0 unx       33 b- defN 23-Sep-22 20:44 leptonai/photon/constants.py
+-rw-r--r--  2.0 unx     3019 b- defN 23-Sep-22 20:44 leptonai/photon/download.py
+-rw-r--r--  2.0 unx     5937 b- defN 23-Sep-22 20:44 leptonai/photon/favicon.ico
+-rw-r--r--  2.0 unx    34803 b- defN 23-Sep-22 20:44 leptonai/photon/photon.py
+-rw-r--r--  2.0 unx      464 b- defN 23-Sep-22 20:44 leptonai/photon/rate_limit.py
+-rw-r--r--  2.0 unx     2308 b- defN 23-Sep-22 20:44 leptonai/photon/util.py
+-rw-r--r--  2.0 unx       58 b- defN 23-Sep-22 20:44 leptonai/photon/hf/__init__.py
+-rw-r--r--  2.0 unx    29990 b- defN 23-Sep-22 20:44 leptonai/photon/hf/hf.py
+-rw-r--r--  2.0 unx     1019 b- defN 23-Sep-22 20:44 leptonai/photon/hf/hf_dependencies.py
+-rw-r--r--  2.0 unx    10568 b- defN 23-Sep-22 20:44 leptonai/photon/hf/hf_utils.py
+-rw-r--r--  2.0 unx      166 b- defN 23-Sep-22 20:44 leptonai/photon/prebuilt/__init__.py
+-rw-r--r--  2.0 unx     1591 b- defN 23-Sep-22 20:44 leptonai/photon/prebuilt/vllm.py
+-rw-r--r--  2.0 unx      235 b- defN 23-Sep-22 20:44 leptonai/photon/types/__init__.py
+-rw-r--r--  2.0 unx     1992 b- defN 23-Sep-22 20:44 leptonai/photon/types/fileparam.py
+-rw-r--r--  2.0 unx     3269 b- defN 23-Sep-22 20:44 leptonai/photon/types/pickled.py
+-rw-r--r--  2.0 unx      186 b- defN 23-Sep-22 20:44 leptonai/photon/types/responses.py
+-rw-r--r--  2.0 unx     3190 b- defN 23-Sep-22 20:44 leptonai/photon/types/util.py
+-rw-r--r--  2.0 unx      211 b- defN 23-Sep-22 20:44 leptonai/util/__init__.py
+-rw-r--r--  2.0 unx     2126 b- defN 23-Sep-22 20:44 leptonai/util/util.py
+-rw-r--r--  2.0 unx    11357 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/LICENSE
+-rw-r--r--  2.0 unx     6342 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/METADATA
+-rw-r--r--  2.0 unx       29 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/NOTICE
+-rw-r--r--  2.0 unx       92 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/WHEEL
+-rw-r--r--  2.0 unx       41 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/entry_points.txt
+-rw-r--r--  2.0 unx       16 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx     5035 b- defN 23-Sep-22 20:45 leptonai-0.9.9.dist-info/RECORD
+61 files, 306401 bytes uncompressed, 96026 bytes compressed:  68.7%
```

## zipnote {}

```diff
@@ -9,17 +9,14 @@
 
 Filename: leptonai/config.py
 Comment: 
 
 Filename: leptonai/registry.py
 Comment: 
 
-Filename: leptonai/util.py
-Comment: 
-
 Filename: leptonai/_internal/__init__.py
 Comment: 
 
 Filename: leptonai/_internal/client_utils.py
 Comment: 
 
 Filename: leptonai/_internal/db.py
@@ -153,29 +150,35 @@
 
 Filename: leptonai/photon/types/responses.py
 Comment: 
 
 Filename: leptonai/photon/types/util.py
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/LICENSE
+Filename: leptonai/util/__init__.py
+Comment: 
+
+Filename: leptonai/util/util.py
+Comment: 
+
+Filename: leptonai-0.9.9.dist-info/LICENSE
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/METADATA
+Filename: leptonai-0.9.9.dist-info/METADATA
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/NOTICE
+Filename: leptonai-0.9.9.dist-info/NOTICE
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/WHEEL
+Filename: leptonai-0.9.9.dist-info/WHEEL
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/entry_points.txt
+Filename: leptonai-0.9.9.dist-info/entry_points.txt
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/top_level.txt
+Filename: leptonai-0.9.9.dist-info/top_level.txt
 Comment: 
 
-Filename: leptonai-0.9.8.dist-info/RECORD
+Filename: leptonai-0.9.9.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## leptonai/_version.py

```diff
@@ -1,4 +1,4 @@
 # file generated by setuptools_scm
 # don't change, don't track in version control
-__version__ = version = '0.9.8'
-__version_tuple__ = version_tuple = (0, 9, 8)
+__version__ = version = '0.9.9'
+__version_tuple__ = version_tuple = (0, 9, 9)
```

## leptonai/config.py

```diff
@@ -15,15 +15,15 @@
 #
 # If you are in need of accessing the cache directory, use the functions `create_cached_dir_if_needed()` in `leptonai.utils`.
 CACHE_DIR = Path(os.environ.get("LEPTON_CACHE_DIR", Path.home() / ".cache" / "lepton"))
 DB_PATH = CACHE_DIR / "lepton.db"
 LOGS_DIR = CACHE_DIR / "logs"
 
 # Lepton's base image and image repository location.
-BASE_IMAGE_VERSION = "0.9.8"
+BASE_IMAGE_VERSION = "0.9.9"
 BASE_IMAGE_REGISTRY = "default"
 BASE_IMAGE_REPO = f"{BASE_IMAGE_REGISTRY}/lepton"
 BASE_IMAGE = f"{BASE_IMAGE_REPO}:photon-py{sys.version_info.major}.{sys.version_info.minor}-runner-{BASE_IMAGE_VERSION}"
 BASE_IMAGE_ARGS = ["--shm-size=1g"]
 
 # Default port used by the Lepton deployments.
 DEFAULT_PORT = 8080
```

## leptonai/registry.py

```diff
@@ -30,9 +30,9 @@
             self._map[key] = value
 
     def get(self, key: Any) -> Any:
         if key in self._map:
             return self._map[key]
         return None
 
-    def get_all(self):
+    def keys(self):
         return self._map.keys()
```

## leptonai/api/photon.py

```diff
@@ -1,106 +1,28 @@
 import os
-from typing import Any, Dict, List, Optional
+from typing import List, Optional
 import warnings
 
 from leptonai.config import CACHE_DIR
 
 # import leptonai.photon to register the schemas and types
 import leptonai.photon  # noqa: F401
 from leptonai.photon.base import (
-    schema_registry,
-    type_registry,
-    BasePhoton,
     add_photon,
     remove_local_photon,
     find_all_local_photons,
 )
-from leptonai.util import check_photon_name
+from leptonai.photon.util import load
 
 from .connection import Connection
 from . import types
 from .util import APIError, json_or_error
 from .workspace import version
 
 
-def create(name: str, model: Any) -> BasePhoton:
-    """
-    Create a photon from a model.
-
-    :param str name: name of the photon
-    :param Any model: model to create the photon from
-
-    :return: the created photon
-    :rtype: BasePhoton
-
-    :raises ValueError: if the model is not supported
-    """
-    check_photon_name(name)
-
-    def _find_creator(model: str):
-        model_parts = model.split(":")
-        schema = model_parts[0]
-        return schema_registry.get(schema)
-
-    if isinstance(model, str):
-        creator = _find_creator(model)
-        if creator is None:
-            model = f"py:{model}"
-            # default to Python Photon, try again with auto-filling schema
-            creator = _find_creator(model)
-        if creator is not None:
-            return creator(name, model)
-    else:
-        for type_checker in type_registry.get_all():
-            if type_checker(model):
-                creator = type_registry.get(type_checker)
-                return creator(name, model)
-
-    raise ValueError(f"Failed to find Photon creator for name={name} and model={model}")
-
-
-def save(photon: BasePhoton, path: Optional[str] = None) -> str:
-    """
-    Save a photon to a file. By default, the file is saved in the
-    cache directory (``{CACHE_DIR} / {name}.photon``)
-
-    :param BasePhoton photon: photon to save
-    :param str path: path to save the photon to
-
-    :return: path to the saved photon
-    :rtype: str
-
-    :raises FileExistsError: if the file already exists at the target path
-    """
-    return photon.save(path)
-
-
-def load(path: str) -> BasePhoton:
-    """
-    Load a photon from a file.
-    :param str path: path to the photon file
-
-    :return: the loaded photon
-    :rtype: BasePhoton
-    """
-    return BasePhoton.load(path)
-
-
-def load_metadata(path: str, unpack_extra_files: bool = False) -> Dict[Any, Any]:
-    """
-    Load the metadata of a photon from a file.
-    :param str path: path to the photon file
-    :param bool unpack_extra_files: whether to unpack extra files
-
-    :return: the metadata of the photon
-    :rtype: dict
-    """
-    return BasePhoton.load_metadata(path, unpack_extra_files)
-
-
 def push(conn: Connection, path: str):
     """
     Push a photon to a workspace.
     :param str path: path to the photon file
     """
     with open(path, "rb") as file:
         response = conn.post("/photons", files={"file": file})
@@ -170,14 +92,17 @@
     # TODO: use remote creation time
     add_photon(id, photon.name, photon.model, str(new_path))
 
     return photon
 
 
 def run_remote_with_spec(conn: Connection, deployment_spec: types.DeploymentSpec):
+    """
+    Run a photon on a workspace, with the given deployment spec.
+    """
     response = conn.post("/deployments", json=deployment_spec.dict(exclude_none=True))
     return response
 
 
 def run_remote(
     conn: Connection,
     id: str,
```

## leptonai/api/types.py

```diff
@@ -91,15 +91,17 @@
     ) -> Optional[List["TokenVar"]]:
         # Note that None is different from [] here. None means that the tokens are not
         # changed, while [] means that the tokens are cleared (aka, public deployment)
         if is_public is None and tokens is None:
             return None
         elif is_public and tokens:
             raise ValueError(
-                "Cannot specify both is_public and token at the same time."
+                "For access control, you cannot specify both is_public and token at the"
+                " same time. Please specify either is_public=True with no tokens passed"
+                " in, or is_public=False and tokens as a list."
             )
         else:
             if is_public:
                 return TokenVar.public()
             else:
                 final_tokens = [
                     TokenVar(value_from=TokenValue(token_name_ref="WORKSPACE_TOKEN"))
```

## leptonai/api/workspace.py

```diff
@@ -259,11 +259,11 @@
     info = json_or_error(response)
     assert isinstance(info, (dict, APIError))
     if isinstance(info, APIError):
         return None
     else:
         match = _semver_pattern.match(info["git_commit"])
         return (
-            (int(match.group(0)), int(match.group(1)), int(match.group(2)))
+            (int(match.group(1)), int(match.group(2)), int(match.group(3)))
             if match
             else None
         )
```

## leptonai/cli/deployment.py

```diff
@@ -307,29 +307,50 @@
         " specified, no change will be made to the access control of the deployment."
     ),
 )
 @click.option(
     "--tokens",
     help=(
         "Access tokens that can be used to access the deployment. See docs for"
-        " details on access control."
+        " details on access control. If no tokens is specified, we will not change the"
+        " tokens of the deployment. If you want to remove all additional tokens, use"
+        "--remove-tokens."
     ),
     multiple=True,
 )
 @click.option(
+    "--remove-tokens",
+    is_flag=True,
+    default=False,
+    help=(
+        "If specified, all additional tokens will be removed, and the deployment will"
+        " be either public (if --public) is specified, or only accessible with the"
+        " workspace token (if --public is not specified)."
+    ),
+)
+@click.option(
     "--no-traffic-timeout",
     type=int,
     default=None,
     help=(
         "If specified, the deployment will be scaled down to 0 replicas after the"
         " specified number of seconds without traffic. Set to 0 to explicitly change"
         " the deployment to have no timeout."
     ),
 )
-def update(name, min_replicas, resource_shape, public, tokens, id, no_traffic_timeout):
+def update(
+    name,
+    id,
+    min_replicas,
+    resource_shape,
+    public,
+    tokens,
+    remove_tokens,
+    no_traffic_timeout,
+):
     """
     Updates a deployment. Note that for all the update options, changes are made
     as replacements, and not incrementals. For example, if you specify `--tokens`,
     old tokens are replaced by the new set of tokens.
     """
     conn = get_connection_or_die()
     if id == "latest":
@@ -362,14 +383,20 @@
         records = [
             (photon["name"], photon["model"], photon["id"], photon["created_at"])
             for photon in photons
             if photon["name"] == current_photon_name
         ]
         id = sorted(records, key=lambda x: x[3])[-1][2]
         console.print(f"Updating to latest photon id [green]{id}[/].")
+    if remove_tokens:
+        # [] means removing all tokens
+        tokens = []
+    elif len(tokens) == 0:
+        # None means no change
+        tokens = None
     guard_api(
         api.update_deployment(
             conn,
             name,
             photon_id=id,
             min_replicas=min_replicas,
             resource_shape=resource_shape,
```

## leptonai/cli/in_n_out.py

```diff
@@ -72,14 +72,15 @@
         msg=(
             f"Cannot properly log into workspace [red]{workspace_id}."
             " This should usually not happen - it might be a transient"
             " network issue. Please contact us by sharing the error message above."
         ),
     )
 
+    assert isinstance(info, dict)
     console.print(f"Logged in to your workspace [green]{workspace_id}[/].")
     console.print(f"\tbuild time: {info['build_time']}")
     console.print(f"\t   version: {info['git_commit']}")
 
 
 def cloud_logout(purge=False):
     """
```

## leptonai/cli/photon.py

```diff
@@ -15,22 +15,24 @@
 import click
 
 from leptonai.api.connection import Connection
 from leptonai.api import photon as api
 from leptonai.api import types
 from leptonai.api.deployment import list_deployment
 from leptonai.api.workspace import WorkspaceInfoLocalRecord
+from leptonai.photon import util as photon_util
 from leptonai.photon import Photon
 from leptonai.photon.base import (
     find_all_local_photons,
     find_local_photon,
     remove_local_photon,
 )
 from leptonai.photon.constants import METADATA_VCS_URL_KEY
 from leptonai.photon.download import fetch_code_from_vcs
+
 from .util import (
     click_group,
     guard_api,
     check,
     get_connection_or_die,
     explain_response,
     APIError,
@@ -114,20 +116,20 @@
     Creates a new photon in the local environment.
     For specifics on the model spec, see `leptonai.photon.Photon`. To push a photon
     to the workspace, use `lep photon push`.
 
     Developer note: insert a link to the photon documentation here.
     """
     try:
-        photon = api.create(name=name, model=model)
+        photon = photon_util.create(name=name, model=model)
     except Exception as e:
         console.print(f"Failed to create photon: [red]{e}[/]")
         sys.exit(1)
     try:
-        api.save(photon)
+        photon_util.save(photon)
     except Exception as e:
         console.print(f"Failed to save photon: [red]{e}[/]")
         sys.exit(1)
     console.print(f"Photon [green]{name}[/green] created.")
 
 
 @photon.command()
@@ -173,17 +175,17 @@
         else:
             ids = [id_]
         # Actually remove the ids
         for id_to_remove in ids:  # type: ignore
             explain_response(
                 api.remove_remote(conn, id_to_remove),
                 f"Photon id [green]{id_to_remove}[/] removed.",
-                f"Photon id [red]{id_to_remove}[/] not removed. Some deployments still"
-                " using it. Remove the deployments first with `lep deployment"
-                " remove`.",
+                f"Photon id [red]{id_to_remove}[/] not removed. Some deployments"
+                " still using it. Remove the deployments first with `lep"
+                " deployment remove`.",
                 f"Photon id [red]{id_to_remove}[/] not removed. See error message"
                 " above.",
                 exit_if_4xx=True,
             )
         return
     else:
         # local mode
@@ -444,16 +446,17 @@
         # as the default behavior.
         # TODO: Support push and run if the photon does not exist on remote
         if id is None:
             # look for the latest photon with the given name.
             id = _get_most_recent_photon_id_or_none(conn, name)
             if not id:
                 console.print(
-                    f"Photon [red]{name}[/] does not exist in the workspace. Did you"
-                    " intend to run a local photon? If so, please specify --local.",
+                    f"Photon [red]{name}[/] does not exist in the workspace. Did"
+                    " you intend to run a local photon? If so, please specify"
+                    " --local.",
                 )
                 sys.exit(1)
             console.print(f"Running the most recent version of [green]{name}[/]: {id}")
         else:
             console.print(f"Running the specified version: [green]{id}[/]")
         # parse environment variables and secrets
         deployment_name = _find_deployment_name_or_die(conn, name, id, deployment_name)
@@ -520,15 +523,15 @@
                 " remote execution. They will be ignored for local execution."
             )
         path = str(path)
         check(
             os.path.exists(path),
             f"You encountered an internal error: photon [red]{path}[/] does not exist.",
         )
-        metadata = api.load_metadata(path)
+        metadata = photon_util.load_metadata(path)
 
         if metadata.get(METADATA_VCS_URL_KEY, None):
             workpath = fetch_code_from_vcs(metadata[METADATA_VCS_URL_KEY])
             os.chdir(workpath)
 
         try:
             photon = api.load(path)
@@ -582,15 +585,15 @@
 @click.pass_context
 def prepare(ctx, path):
     """
     Prepare the environment for running a photon. This is only used by the
     platform to prepare the environment inside the container and not meant to
     be used by users.
     """
-    metadata = api.load_metadata(path, unpack_extra_files=True)
+    metadata = photon_util.load_metadata(path, unpack_extra_files=True)
 
     if metadata.get(METADATA_VCS_URL_KEY, None):
         fetch_code_from_vcs(metadata[METADATA_VCS_URL_KEY])
 
     # pip install
     requirement_dependency = metadata.get("requirement_dependency", [])
     if requirement_dependency:
```

## leptonai/cli/workspace.py

```diff
@@ -226,14 +226,15 @@
         msg=(
             "Cannot properly obtain info for the current workspace."
             " This should usually not happen - it might be a transient"
             " network issue. If you encounter this persistently, please"
             " contact us by sharing the error message above."
         ),
     )
+    assert isinstance(info, dict)
     # Note: in our backend, right now the "workspace_name" item is actually
     # the workspace id in the frontend definition. If we decide to consolidate
     # naming, consider changing it.
     id = info["workspace_name"]
 
     console.print(f"id:         {id}")
     console.print(
```

## leptonai/cloudrun/remote.py

```diff
@@ -11,14 +11,15 @@
 import uuid
 import warnings
 import weakref
 
 from loguru import logger
 
 from leptonai.photon import Photon
+from leptonai.photon.util import create as create_photon
 from leptonai import api
 from leptonai.api import APIError
 from leptonai.client import Client, current
 
 
 _unique_name_pattern = re.compile(r"cldrun-[0-9a-f]{25}")
 _unique_photon_id_pattern = re.compile(r"cldrun-[0-9a-f]{25}-[0-9a-z]{8}")
@@ -102,15 +103,15 @@
         elif isinstance(photon, Photon):
             # if the photon is already a Photon object, then directly save it.
             photon_instance: Photon = photon
             photon_instance.name = self.unique_name
         elif isinstance(photon, str):
             # if the photon is a string, then create a Photon object and save it.
             try:
-                created_photon = api.photon.create(name=self.unique_name, model=photon)
+                created_photon = create_photon(name=self.unique_name, model=photon)
                 if not isinstance(created_photon, Photon):
                     raise RuntimeError(
                         "Currently we do not support non-python photons yet."
                     )
                 photon_instance: Photon = created_photon
             except Exception as e:
                 raise RuntimeError(
```

## leptonai/photon/util.py

```diff
@@ -1,8 +1,8 @@
-from typing import Any, Dict
+from typing import Any, Dict, Optional
 from leptonai.photon.base import schema_registry, type_registry, BasePhoton
 
 from leptonai.util import check_photon_name
 
 
 def create(name: str, model: Any) -> BasePhoton:
     """
@@ -28,23 +28,23 @@
         if creator is None:
             model = f"py:{model}"
             # default to Python Photon, try again with auto-filling schema
             creator = _find_creator(model)
         if creator is not None:
             return creator(name, model)
     else:
-        for type_checker in type_registry.get_all():
+        for type_checker in type_registry.keys():
             if type_checker(model):
                 creator = type_registry.get(type_checker)
                 return creator(name, model)
 
     raise ValueError(f"Failed to find Photon creator for name={name} and model={model}")
 
 
-def save(photon: BasePhoton, path: str = None) -> str:
+def save(photon: BasePhoton, path: Optional[str] = None) -> str:
     """
     Save a photon to a file. By default, the file is saved in the
     cache directory (``{CACHE_DIR} / {name}.photon``)
 
     :param BasePhoton photon: photon to save
     :param str path: path to save the photon to
```

## leptonai/photon/hf/hf.py

```diff
@@ -19,15 +19,15 @@
     hf_try_explain_run_exception,
 )
 from .hf_dependencies import hf_pipeline_dependencies
 
 task_cls_registry = Registry()
 
 
-SUPPORTED_TASKS = [
+HF_DEFINED_TASKS = [
     # diffusers
     "text-to-image",
     # transformers
     "audio-classification",
     "automatic-speech-recognition",
     "conversational",
     "depth-estimation",
@@ -59,15 +59,15 @@
 # This is a manually maintained list of mappings from model name to
 # tasks. Somehow these models are not properly annotated in the Huggingface
 # Hub.
 _MANUALLY_ANNOTATED_MODEL_TO_TASK = {
     "hf-internal-testing/tiny-stable-diffusion-torch": "text-to-image",
 }
 
-schemas = ["hf", "huggingface"]
+HUGGING_FACE_SCHEMAS = ["hf", "huggingface"]
 
 
 def _get_transformers_base_types():
     import transformers
 
     return (transformers.PreTrainedModel, transformers.Pipeline)
 
@@ -91,27 +91,40 @@
             for d in pipeline_specific_deps:
                 if d not in deps:
                     deps.append(d)
         return deps
 
     def __init_subclass__(cls, **kwargs):
         super().__init_subclass__(**kwargs)
+        if cls.hf_task not in HF_DEFINED_TASKS:
+            raise ValueError(
+                f"You made a programming error: the task {cls.hf_task} is not a"
+                " supported task defined in HuggingFace. If you believe this is an"
+                " error, please file an issue."
+            )
         task_cls_registry.register(cls.hf_task, cls)
 
     @classmethod
+    def supported_tasks(cls):
+        """
+        Returns the set of supported tasks.
+        """
+        return task_cls_registry.keys()
+
+    @classmethod
     def _parse_model_str(cls, model_str):
         model_parts = model_str.split(":")
         if len(model_parts) != 2:
             raise ValueError(
                 f'Unsupported Huggingface model: "{model_str}" (can not parse model'
                 " name). Huggingface model spec should be in the form of"
                 " hf:<model_name>[@<revision>]."
             )
         schema = model_parts[0]
-        if schema not in schemas:
+        if schema not in HUGGING_FACE_SCHEMAS:
             # In theory, this should not happen - the schema should be
             # automatically checked by the photon registry, but we'll check it
             # here just in case.
             raise ValueError(
                 f'Unsupported Huggingface model: "{model_str}" (unknown schema:'
                 f' "{schema}")'
             )
@@ -140,20 +153,21 @@
                     " creator provided any instructions on how to run the model. You"
                     " can then wrap this model into a custom Photon with relatively"
                     " easy scaffolding. Check out the documentation at"
                     " https://www.lepton.ai/docs/walkthrough/anatomy_of_a_photon for"
                     " more details."
                 )
 
-        if hf_task not in SUPPORTED_TASKS:
+        if hf_task not in HF_DEFINED_TASKS:
             raise ValueError(
                 f'Unsupported Huggingface model: "{model_str}" (task: {hf_task}). This'
                 " task is not supported by LeptonAI SDK yet. If you would like us to"
                 " add support for this task type, please let us know by opening an"
                 " issue at https://github.com/lepton/leptonai-sdk/issues/new/choose."
+                f"\nCurrently supported HF tasks are: {cls.supported_tasks()}."
             )
 
         # 8 chars should be enough to identify a commit
         hf_revision = mi.sha[:8]
         model = f"{schema}:{hf_model_id}@{hf_revision}"
 
         return model, hf_task, hf_model_id, hf_revision
@@ -183,14 +197,20 @@
         if pipeline_creator is None:
             raise ValueError(f"Could not find pipeline creator for {self.hf_task}")
         logger.info(
             f"Creating pipeline for {self.hf_task}(model={self.hf_model},"
             f" revision={self.hf_revision}).\n"
             "HuggingFace download might take a while, please be patient..."
         )
+        logger.info(
+            "Note: HuggingFace caches the downloaded models in ~/.cache/huggingface/"
+            " (or C:\\Users\\<username>\\.cache\\huggingface\\ on Windows). If you"
+            " have already downloaded the model before, the download should be much"
+            " faster. If you run out of disk space, you can delete the cache folder."
+        )
         try:
             pipeline = pipeline_creator(
                 task=self.hf_task,
                 model=self.hf_model,
                 revision=self.hf_revision,
             )
         except ImportError as e:
@@ -215,27 +235,36 @@
         return pipeline
 
     def init(self):
         super().init()
         # access pipeline here to trigger download and load
         self.pipeline
 
-    def run(self, *args, **kwargs):
+    def _run_pipeline(self, *args, **kwargs):
         import torch
 
         if torch.cuda.is_available():
             with torch.autocast(device_type="cuda"):
                 return self.pipeline(*args, **kwargs)
         else:
             return self.pipeline(*args, **kwargs)
 
     @classmethod
     def create_from_model_str(cls, name, model_str):
         _, hf_task, _, _ = cls._parse_model_str(model_str)
         task_cls = task_cls_registry.get(hf_task)
+        if task_cls is None:
+            raise ValueError(
+                f"Lepton currently does not support the specified task: {hf_task}. If"
+                " you would like us to support this task, please let us know by"
+                " opening an issue"
+                " at https://github.com/leptonai/leptonai-sdk/issues/new/choose, and"
+                " kindly include the specific model that you are trying to run for"
+                " debugging purposes: {model_str}"
+            )
         return task_cls(name, model_str)
 
     @classmethod
     def create_from_model_obj(cls, name, model):
         import transformers
 
         if not is_transformers_model(model):
@@ -296,30 +325,30 @@
             "inputs": "I enjoy walking with my cute dog",
             "max_new_tokens": 50,
             "do_sample": True,
             "top_k": 50,
             "top_p": 0.95,
         },
     )
-    def run_handler(
+    def run(
         self,
         inputs: Union[str, List[str]],
         top_k: Optional[int] = None,
         top_p: Optional[float] = None,
         temperature: Optional[float] = 1.0,
         repetition_penalty: Optional[float] = None,
         max_new_tokens: Optional[int] = None,
         max_time: Optional[float] = None,
         return_full_text: bool = True,
         num_return_sequences: int = 1,
         do_sample: bool = True,
         **kwargs,
     ) -> Union[str, List[str]]:
         try:
-            res = self.run(
+            res = self._run_pipeline(
                 inputs,
                 top_k=top_k,
                 top_p=top_p,
                 temperature=temperature,
                 repetition_penalty=repetition_penalty,
                 max_new_tokens=max_new_tokens,
                 max_time=max_time,
@@ -343,15 +372,17 @@
         prompt = f"""\
 The following is a friendly conversation between a user and an assistant. The assistant is talkative and provides lots of specific details from its context. If the assistant does not know the answer to a question, it truthfully says it does not know.
 Current conversation:
 {history_prompt}
 
 assistant:
 """
-        response = self.run(prompt, return_full_text=False)[0]["generated_text"]
+        response = self._run_pipeline(prompt, return_full_text=False)[0][
+            "generated_text"
+        ]
         history.append({"role": "assistant", "content": response})
         messages = [
             (history[i]["content"], history[i + 1]["content"])
             for i in range(0, len(history) - 1, 2)
         ]
         return messages, history
 
@@ -382,28 +413,28 @@
             "inputs": "I enjoy walking with my cute dog",
             "max_new_tokens": 50,
             "do_sample": True,
             "top_k": 50,
             "top_p": 0.95,
         },
     )
-    def run_handler(
+    def run(
         self,
         inputs: Union[str, List[str]],
         top_k: Optional[int] = None,
         top_p: Optional[float] = None,
         temperature: Optional[float] = 1.0,
         repetition_penalty: Optional[float] = None,
         max_new_tokens: Optional[int] = None,
         max_time: Optional[float] = None,
         num_return_sequences: int = 1,
         do_sample: bool = True,
         **kwargs,
     ) -> Union[str, List[str]]:
-        res = self.run(
+        res = self._run_pipeline(
             inputs,
             top_k=top_k,
             top_p=top_p,
             temperature=temperature,
             repetition_penalty=repetition_penalty,
             max_new_tokens=max_new_tokens,
             max_time=max_time,
@@ -424,15 +455,15 @@
         prompt = f"""\
 The following is a friendly conversation between a user and an assistant. The assistant is talkative and provides lots of specific details from its context. If the assistant does not know the answer to a question, it truthfully says it does not know.
 Current conversation:
 {history_prompt}
 
 assistant:
 """
-        response = self.run(prompt)[0]["generated_text"]
+        response = self._run_pipeline(prompt)[0]["generated_text"]
         history.append({"role": "assistant", "content": response})
         messages = [
             (history[i]["content"], history[i + 1]["content"])
             for i in range(0, len(history) - 1, 2)
         ]
         return messages, history
 
@@ -461,23 +492,23 @@
         "run",
         example={
             "inputs": (
                 "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac"
             )
         },
     )
-    def run_handler(self, inputs: Union[str, FileParam]) -> str:
+    def run(self, inputs: Union[str, FileParam]) -> str:
         if isinstance(inputs, FileParam):
             file = tempfile.NamedTemporaryFile()
             with open(file.name, "wb") as f:
                 f.write(inputs.file.read())
                 f.flush()
             inputs = file.name
 
-        res = self.run(inputs)
+        res = self._run_pipeline(inputs)
         return res["text"]
 
 
 class HuggingfaceTextToImagePhoton(HuggingfacePhoton):
     hf_task: str = "text-to-image"
 
     def init(self):
@@ -494,15 +525,15 @@
         "run",
         example={
             "prompt": "a photograph of an astronaut riding a horse",
             "num_inference_steps": 25,
             "seed": 42,
         },
     )
-    def run_handler(
+    def run(
         self,
         prompt: Union[str, List[str]],
         height: Optional[int] = None,
         width: Optional[int] = None,
         num_inference_steps: int = 50,
         guidance_scale: float = 7.5,
         negative_prompt: Optional[Union[str, List[str]]] = None,
@@ -516,15 +547,15 @@
                 seed = [seed]
             generator = [
                 torch.Generator(device=self._device).manual_seed(s) for s in seed
             ]
         else:
             generator = None
 
-        res = self.run(
+        res = self._run_pipeline(
             prompt,
             height=height,
             width=width,
             num_inference_steps=num_inference_steps,
             guidance_scale=guidance_scale,
             negative_prompt=negative_prompt,
             generator=generator,
@@ -541,20 +572,20 @@
 
     @Photon.handler(
         "run",
         example={
             "inputs": """The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest man-made structure in the world, a title it held for 41 years until the Chrysler Building in New York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second tallest free-standing structure in France after the Millau Viaduct."""
         },
     )
-    def run_handler(
+    def run(
         self,
         inputs: Union[str, List[str]],
         **kwargs,
     ) -> Union[str, List[str]]:
-        res = self.run(
+        res = self._run_pipeline(
             inputs,
             **kwargs,
         )
         if isinstance(res, dict):
             return res["summary_text"]
         elif len(res) == 1:
             return res[0]["summary_text"]
@@ -587,15 +618,15 @@
 
     @Photon.handler(example={"inputs": "The cat sat on the mat"})
     def embed(
         self,
         inputs: Union[str, List[str]],
         **kwargs,
     ) -> Union[List[float], List[List[float]]]:
-        res = self.run(
+        res = self._run_pipeline(
             inputs,
             **kwargs,
         )
         if isinstance(res, list):
             return [r.tolist() for r in res]
         else:
             return res.tolist()
@@ -607,41 +638,41 @@
             "sentences": [
                 "That is a happy dog",
                 "That is a very happy person",
                 "Today is a sunny day",
             ],
         },
     )
-    def run_handler(
+    def run(
         self,
         source_sentence: str,
         sentences: Union[str, List[str]],
         **kwargs,
     ) -> Union[float, List[float]]:
         from sentence_transformers import util
 
-        sentences_embs = self.run(sentences, **kwargs)
-        source_sentence_emb = self.run(source_sentence, **kwargs)
+        sentences_embs = self._run_pipeline(sentences, **kwargs)
+        source_sentence_emb = self._run_pipeline(source_sentence, **kwargs)
         res = util.cos_sim(source_sentence_emb, sentences_embs)
 
         if res.dim() != 2 or res.size(0) != 1:
             logger.error(f"Unexpected result shape: {res.shape}")
         return res[0].tolist()
 
 
 class HuggingfaceSentimentAnalysisPhoton(HuggingfacePhoton):
     hf_task: str = "sentiment-analysis"
 
     @Photon.handler(example={"inputs": ["I love you", "I hate you"]})
-    def run_handler(
+    def run(
         self,
         inputs: Union[str, List[str]],
         **kwargs,
     ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
-        res = self.run(
+        res = self._run_pipeline(
             inputs,
             **kwargs,
         )
         return res
 
 
 # text-classification is an alias of sentiment-analysis
@@ -658,15 +689,15 @@
         "run",
         example={
             "inputs": (
                 "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac"
             )
         },
     )
-    def run_handler(
+    def run(
         self,
         inputs: Union[Union[str, FileParam], List[Union[str, FileParam]]],
         **kwargs,
     ) -> Union[
         List[Dict[str, Union[float, str]]], List[List[Dict[str, Union[float, str]]]]
     ]:
         inputs_is_list = isinstance(inputs, list)
@@ -681,15 +712,15 @@
                 with open(file.name, "wb") as f:
                     f.write(inp.file.read())
                     f.flush()
                 temp_files.append(file)
                 inputs_.append(file.name)
             else:
                 inputs_.append(inp)
-        res = self.run(
+        res = self._run_pipeline(
             inputs_,
             **kwargs,
         )
         if not inputs_is_list:
             res = res[0]
         return res
 
@@ -702,26 +733,26 @@
         example={
             "images": [
                 "http://images.cocodataset.org/val2017/000000039769.jpg",
                 "https://images.unsplash.com/photo-1536396123481-991b5b636cbb?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2896&q=80",
             ]
         },
     )
-    def run_handler(
+    def run(
         self,
         images: Union[Union[str, FileParam], List[Union[str, FileParam]]],
         **kwargs,
     ) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
         inputs_is_list = isinstance(images, list)
 
         if not inputs_is_list:
             images = [images]
         images = [img_param_to_img(img) for img in images]
 
-        res = self.run(
+        res = self._run_pipeline(
             images,
             **kwargs,
         )
 
         if not isinstance(res, list):
             res = [res]
         for i, r in enumerate(res):
@@ -751,29 +782,29 @@
         with blocks:
             with gr.Row():
                 input_image = gr.Image(type="filepath")
                 output_image = gr.Image(type="pil")
             with gr.Row():
                 btn = gr.Button("Depth Estimate", variant="primary")
                 btn.click(
-                    fn=lambda img: self.run(img)["depth"],
+                    fn=lambda img: self._run_pipeline(img)["depth"],
                     inputs=input_image,
                     outputs=output_image,
                 )
         return blocks
 
 
 class HuggingfaceImageToTextPhoton(HuggingfacePhoton):
     hf_task: str = "image-to-text"
 
     @Photon.handler(
         "run",
         example={"images": "http://images.cocodataset.org/val2017/000000039769.jpg"},
     )
-    def run_handler(
+    def run(
         self,
         images: Union[Union[str, FileParam], List[Union[str, FileParam]]],
         **kwargs,
     ) -> Union[str, List[str]]:
         images_is_list = isinstance(images, list)
         if not images_is_list:
             images = [images]
@@ -786,19 +817,56 @@
                 with open(file.name, "wb") as f:
                     f.write(img.file.read())
                     f.flush()
                 temp_files.append(file)
                 images_.append(file.name)
             else:
                 images_.append(img)
-        res = self.run(
+        res = self._run_pipeline(
             images_,
             **kwargs,
         )
         return _get_generated_text(res)
 
 
+class HuggingfaceImageClassificationPhoton(HuggingfacePhoton):
+    hf_task: str = "image-classification"
+
+    @Photon.handler(
+        "run",
+        example={"images": "http://images.cocodataset.org/val2017/000000039769.jpg"},
+    )
+    def run(
+        self,
+        images: Union[Union[str, FileParam], List[Union[str, FileParam]]],
+        **kwargs,
+    ) -> Union[List[Dict], List[List[Dict]]]:
+        images_is_list = isinstance(images, list)
+        if not images_is_list:
+            images = [images]
+        # keep references to NamedTemporaryFile objects so they don't get deleted
+        images_ = []
+        temp_files = []
+        for img in images:
+            if isinstance(img, FileParam):
+                file = tempfile.NamedTemporaryFile()
+                with open(file.name, "wb") as f:
+                    f.write(img.file.read())
+                    f.flush()
+                temp_files.append(file)
+                images_.append(file.name)
+            else:
+                images_.append(img)
+        res = self._run_pipeline(
+            images_,
+            **kwargs,
+        )
+        return res if images_is_list else res[0]
+
+
 def register_hf_photon():
-    schema_registry.register(schemas, HuggingfacePhoton.create_from_model_str)
+    schema_registry.register(
+        HUGGING_FACE_SCHEMAS, HuggingfacePhoton.create_from_model_str
+    )
     type_registry.register(
         is_transformers_model, HuggingfacePhoton.create_from_model_obj
     )
```

## leptonai/photon/hf/hf_utils.py

```diff
@@ -218,14 +218,15 @@
         return _create_hf_transformers_pipeline(task, model, revision)
 
 
 for task in [
     "audio-classification",
     "automatic-speech-recognition",
     "depth-estimation",
+    "image-classification",
     "image-to-text",
     "sentiment-analysis",
     "summarization",
     "text-classification",
     "text-generation",
     "text2text-generation",
 ]:
```

## Comparing `leptonai/util.py` & `leptonai/util/util.py`

 * *Files identical despite different names*

## Comparing `leptonai-0.9.8.dist-info/LICENSE` & `leptonai-0.9.9.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `leptonai-0.9.8.dist-info/METADATA` & `leptonai-0.9.9.dist-info/METADATA`

 * *Files 0% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: leptonai
-Version: 0.9.8
+Version: 0.9.9
 Summary: Lepton AI Platform
 Author-email: "Lepton AI Inc." <dev@lepton.ai>
 Project-URL: Homepage, https://lepton.ai
 Requires-Python: >=3.8
 Description-Content-Type: text/markdown
 License-File: LICENSE
 License-File: NOTICE
```

## Comparing `leptonai-0.9.8.dist-info/RECORD` & `leptonai-0.9.9.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,60 +1,61 @@
 leptonai/__init__.py,sha256=WPXFjdfPEyUBGXJ1TKaz-a3JuJbsgC9PZoTTf8DHhi0,155
-leptonai/_version.py,sha256=ficafFYOOGXY1nkEtvgl2PmK0ugmCuRYnz1ic7lfU2I,160
+leptonai/_version.py,sha256=j98TUGxfK1WXFLP9SyTtzvASOSf2Kih8ZIF8YsTEu3U,160
 leptonai/client.py,sha256=sZFwDhuEQRFxJc5YuNGHlMAytLMgV4zhX6f1_JzEqYA,22245
-leptonai/config.py,sha256=646Dg_JVvdyrQgl3HimrpzeW9nmE0MvVmYrncGMUtHc,2714
-leptonai/registry.py,sha256=VVd9trMUkRMiX6PUMZ-NHSlxI1GBs03BbWvSGgu6Pvo,864
-leptonai/util.py,sha256=kZ1ZoFwCT1mfOA0wh2hvZ5wf2c0GoheouyOr8UNJ798,2126
+leptonai/config.py,sha256=56Dwc9pLyt9Azu0fr_ua2X4DYD2YcZMqV5oq9deKTbs,2714
+leptonai/registry.py,sha256=ay9dFPxKbeBsWhfx-URtW1MTGTYrb_vaG8DiySoDucA,861
 leptonai/_internal/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 leptonai/_internal/client_utils.py,sha256=pi7pTHTiBJj7ATO_7z43PEDu2Fce2DZm87wSCxch5dI,7632
 leptonai/_internal/db.py,sha256=CICCXggiP3SaZVn5gj5uzA7a9R_vL1KH8nnZc_pUJAk,948
 leptonai/_internal/logging.py,sha256=gE3UxGz48KDHagI_36UNZ3P4TE7fVL-nfTIxboYOoog,2081
 leptonai/api/__init__.py,sha256=R3VAsbpCsCwXgWtjVNw-ZpRZbkwBRSQLnGRyZpaYOTA,1506
 leptonai/api/connection.py,sha256=jMFrUmdNIfzMfcTRe-5O7ghSldM7uyGtgWYSgR9zY-0,1723
 leptonai/api/deployment.py,sha256=6naB8Ya9e97NSIpMeVnAUvz0U22d5YARZNWuVl0IIJs,5407
-leptonai/api/photon.py,sha256=1szDusVoh2aMiLgs6SvenLA3QfTs7vVk_pd3EeMKOQk,6144
+leptonai/api/photon.py,sha256=9tQ1yrOLxWaRdYnYz0DQbv2F0dgn2yWXsGhqByrrWgw,4001
 leptonai/api/secret.py,sha256=ZGMjjjlpvuW37MMNGIFiFFkPndXwnwTIreO7arHrp04,877
 leptonai/api/storage.py,sha256=ayIZRc_sQUzcLutKxUeLBlRrz8NLYY9dX6wWSdgccBw,4507
-leptonai/api/types.py,sha256=cz_iciwxac0QBsKqPVj4376R4fZOhvkAUbYFFD8AXsU,7759
+leptonai/api/types.py,sha256=5cZmkWgfbK2laPZvOGrdsEND5Y3-3vHY9cnlGdPZhPU,7925
 leptonai/api/util.py,sha256=Kh_Q-ZG-7L30IaEg75_Gx3nwtMtaK1i3eyB8a9Yx0d0,2451
-leptonai/api/workspace.py,sha256=CbWeJXhIrcDVAeL4ZhpHXBh801Jc5-28JB18GAdWK_g,10157
+leptonai/api/workspace.py,sha256=GYFnV8rb1bktSFNlfYU_0F1sUL4qZ0VKGyVRWTvcp5I,10157
 leptonai/bench/gpt2/client.py,sha256=we9bMXtoRXblNInHq0PStYA7ls74xQPF6Sdly67YdOs,326
 leptonai/cli/__init__.py,sha256=NMOFMTbAq884w0x0lKLQwD_Zp_yxwW7tVRKS1mSzc3I,394
 leptonai/cli/cli.py,sha256=BNNRr2ZKhJRWrUz93GCl3RORhPGzjQvJ7UFFE8jNens,1624
 leptonai/cli/constants.py,sha256=IxmSrFLv6bSAZU4GmwFgf0YTKFC3lVGGAwtKd4SSe4w,143
-leptonai/cli/deployment.py,sha256=VqqRtjBrIl644eD3iZUT2Mtv8z4LA39TjeW6a_Kmu6g,17387
-leptonai/cli/in_n_out.py,sha256=Yo0ZlFFI6HNnspMgC-Llvzc_7J9LyYa7EGri0kFnjDo,3986
-leptonai/cli/photon.py,sha256=QOY8jW1dLdTjwxZmBui7iAI-L6QN-QZGbB0cJmbuEfU,25104
+leptonai/cli/deployment.py,sha256=djYJ0H3cHiAhcq0n_yxnjtpA0VwCH9n1Ajb4-eGoQwg,18086
+leptonai/cli/in_n_out.py,sha256=O6W-u8yPADs0BOs_jS2B8Wh76XiQXXso5Mcvf6U0fqc,4020
+leptonai/cli/photon.py,sha256=opVAETa_wCNwtbBCUnyvNaSiwhuMTQbQuq8fjPBSaXo,25208
 leptonai/cli/secret.py,sha256=zKBipYD-ejlDE-iwHGT-TpXrZMn3kYm3XLKd4zc7x5w,3370
 leptonai/cli/storage.py,sha256=0DixCTImp6jTKpjzPMHqeEKU5pjnnmXoLi-lFBd8cVU,7737
 leptonai/cli/util.py,sha256=lI6FPdlwiml7zlh2Kv12CHmuN7ULbNVs_eLIPc4iwJY,4881
-leptonai/cli/workspace.py,sha256=AolK5G3K2Gq6i8xICt3FeN3sAh3E8SZytQckn58ZxnA,9728
+leptonai/cli/workspace.py,sha256=BgDA5lgXOgr9NDFUms9qqWLOBsW_TymPYkHMLq2oeUw,9762
 leptonai/cloudrun/__init__.py,sha256=vLhqfVj_pIpHb6bzoe-7qKMgTiB4KxO88wSQM3oyyps,552
-leptonai/cloudrun/remote.py,sha256=-Cme4185D-GSGa1u5oE1ssggpaI_Pgs7zeWD8TTM1QA,16090
+leptonai/cloudrun/remote.py,sha256=dHZr6tTCFw4d2Iqq9_9KoGosDzCM5euyriQ7pqagJ3E,16143
 leptonai/photon/__init__.py,sha256=YJWhjVvg1xOdXPytBi3rbG-gob26QKISg1zpktDWYis,479
 leptonai/photon/background.py,sha256=kY67Hnn3DHqwd2awDo23hoFdzF1_ElBozRIlsrSoU8c,926
 leptonai/photon/base.py,sha256=F52Su-Xr9hYbwI5wV7WsPcL4rXjVRlunhLI-rUZlzII,8135
 leptonai/photon/batcher.py,sha256=oJMbz8Lm16LaPJTPDyZmDjqytX_BsaCl0rani35SjvA,5188
 leptonai/photon/constants.py,sha256=Pie7nh0C0_mTkkj2wFifCUVM_Q8q07-N7YK8FDVbBSE,33
 leptonai/photon/download.py,sha256=z0FzZhliEkIlwG10JDystFcMMjSW8exx0iuFhOeRmtI,3019
 leptonai/photon/favicon.ico,sha256=pw3UuN95h2w3_7rwRsszrK6YPQPRsgbVdKSamRtw0N8,5937
 leptonai/photon/photon.py,sha256=sNKmOerH5YFyxaHLpYYM2RYJSmzCjrdibkeK6HyUO-o,34803
 leptonai/photon/rate_limit.py,sha256=coUjygosz5eQTBHVsdTaUI4RJruqCQKlUpOcmIWwoQA,464
-leptonai/photon/util.py,sha256=8bjgOL09HXnJ9k7vmntkTzKu5dK0HyjbdynhQfxjtKA,2291
+leptonai/photon/util.py,sha256=1hQxMN4nP3SDi1DXP7_qXk7MuIH1_FWytEHe6HexmqY,2308
 leptonai/photon/hf/__init__.py,sha256=XsyS0hSxEJXgrBh6bZ4ZTZlHGMQntCP7DJgMYoy9ffE,58
-leptonai/photon/hf/hf.py,sha256=G-YdDDtU3Usjx1zjjmS6DbpRg6J8U53QhKGS9o9kGjM,27263
+leptonai/photon/hf/hf.py,sha256=AGc7myxMRuKCQS5EHvGJ-cCqLLMCcTC3fMf4ckh-y0I,29990
 leptonai/photon/hf/hf_dependencies.py,sha256=u8MpAgVxndJftBK6uqR38B1oTN72yOrG9oLzpDsQbBY,1019
-leptonai/photon/hf/hf_utils.py,sha256=N00JH5yorLJY3y4tn9pdpHJLKQXxfDSv4wOOPDXyeSo,10540
+leptonai/photon/hf/hf_utils.py,sha256=GTxw-c27CNnd0fdxsI3GkTRyi2Y-hUg5N7ZSywfKP6Y,10568
 leptonai/photon/prebuilt/__init__.py,sha256=mzaqcvXO2YE7rAjZyad1H6go587xc4F7h-oFUQ2bLSI,166
 leptonai/photon/prebuilt/vllm.py,sha256=8cZKDIGplrtGBVk-d5ZwLYpUiVVZRDyd7fJ9TTYTKpM,1591
 leptonai/photon/types/__init__.py,sha256=bE57Z5O68Fo8P3iPDHjUl--jWPR93qBvNJ3uo2ESOgY,235
 leptonai/photon/types/fileparam.py,sha256=TqmIJiI7toie8fa9yw4cd5PPYSpVwirgZOFQKNsHaTg,1992
 leptonai/photon/types/pickled.py,sha256=lXUJ_WlHujCESNYKRILZVaSJhbHFwWhqX_y9foF-ZNQ,3269
 leptonai/photon/types/responses.py,sha256=Vy2gHISyXpNZoMW5NnSr5IS2J3c2XrvqytX5HlWrbSw,186
 leptonai/photon/types/util.py,sha256=qppDidJoXhciLmWfzq4ZZTxsLn01cKgCV7y046c17Rc,3190
-leptonai-0.9.8.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
-leptonai-0.9.8.dist-info/METADATA,sha256=W2IfLAEoxSfLKvoRnnuLQYav0039B6zW60gOoZRpSiU,6342
-leptonai-0.9.8.dist-info/NOTICE,sha256=HTpH9ZJ2-qFTS2K8V4uicqnb5N0gRRLIZvF0QUKVbdM,29
-leptonai-0.9.8.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-leptonai-0.9.8.dist-info/entry_points.txt,sha256=iSDzSAJlhlvqSDBFM8hTaueLaT13_C3ky7-mds7FkRg,41
-leptonai-0.9.8.dist-info/top_level.txt,sha256=MX_iM-WOAdmZKu41DFu3vbGCnN0ht0KCpHKpKvhJTCU,16
-leptonai-0.9.8.dist-info/RECORD,,
+leptonai/util/__init__.py,sha256=8HvhFMOhMRhB78CLzERO4uM1b5DokRy8fRO0xiw9M30,211
+leptonai/util/util.py,sha256=kZ1ZoFwCT1mfOA0wh2hvZ5wf2c0GoheouyOr8UNJ798,2126
+leptonai-0.9.9.dist-info/LICENSE,sha256=xx0jnfkXJvxRnG63LTGOxlggYnIysveWIZ6H3PNdCrQ,11357
+leptonai-0.9.9.dist-info/METADATA,sha256=-6quup8h8aH2Dt7duFLD5QAE_r4a1D7y1NzH-E8eCIk,6342
+leptonai-0.9.9.dist-info/NOTICE,sha256=HTpH9ZJ2-qFTS2K8V4uicqnb5N0gRRLIZvF0QUKVbdM,29
+leptonai-0.9.9.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+leptonai-0.9.9.dist-info/entry_points.txt,sha256=iSDzSAJlhlvqSDBFM8hTaueLaT13_C3ky7-mds7FkRg,41
+leptonai-0.9.9.dist-info/top_level.txt,sha256=MX_iM-WOAdmZKu41DFu3vbGCnN0ht0KCpHKpKvhJTCU,16
+leptonai-0.9.9.dist-info/RECORD,,
```

