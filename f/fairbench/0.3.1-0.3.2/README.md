# Comparing `tmp/fairbench-0.3.1-py3-none-any.whl.zip` & `tmp/fairbench-0.3.2-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,53 +1,53 @@
-Zip file size: 42625 bytes, number of entries: 51
+Zip file size: 44341 bytes, number of entries: 51
 -rw-rw-rw-  2.0 fat      497 b- defN 24-Apr-18 11:38 fairbench/__init__.py
 -rw-rw-rw-  2.0 fat     6985 b- defN 24-May-18 11:02 fairbench/verification.py
 -rw-rw-rw-  2.0 fat       38 b- defN 23-Jun-14 13:57 fairbench/bench/__init__.py
 -rw-rw-rw-  2.0 fat     5853 b- defN 24-May-16 19:02 fairbench/bench/demos.py
 -rw-rw-rw-  2.0 fat     1660 b- defN 23-Jul-19 12:04 fairbench/bench/loader.py
 -rw-rw-rw-  2.0 fat      179 b- defN 23-Nov-10 23:11 fairbench/blocks/__init__.py
--rw-rw-rw-  2.0 fat     3823 b- defN 24-Feb-28 07:00 fairbench/blocks/framework.py
+-rw-rw-rw-  2.0 fat     4073 b- defN 24-Jun-03 12:08 fairbench/blocks/framework.py
 -rw-rw-rw-  2.0 fat       52 b- defN 23-Nov-10 23:11 fairbench/blocks/expanders/__init__.py
 -rw-rw-rw-  2.0 fat     4309 b- defN 24-May-16 14:21 fairbench/blocks/expanders/expanders.py
 -rw-rw-rw-  2.0 fat      274 b- defN 23-Nov-10 22:56 fairbench/blocks/metrics/__init__.py
--rw-rw-rw-  2.0 fat     3911 b- defN 24-May-22 17:08 fairbench/blocks/metrics/classification.py
+-rw-rw-rw-  2.0 fat     3927 b- defN 24-Jun-03 07:29 fairbench/blocks/metrics/classification.py
 -rw-rw-rw-  2.0 fat     2769 b- defN 23-Nov-09 17:25 fairbench/blocks/metrics/disparate_impact.py
 -rw-rw-rw-  2.0 fat     1583 b- defN 23-Nov-09 17:25 fairbench/blocks/metrics/disparate_mistreatment.py
--rw-rw-rw-  2.0 fat     5162 b- defN 24-May-19 11:14 fairbench/blocks/metrics/ranking.py
+-rw-rw-rw-  2.0 fat     7082 b- defN 24-Jun-03 12:08 fairbench/blocks/metrics/ranking.py
 -rw-rw-rw-  2.0 fat     3006 b- defN 24-Feb-13 17:27 fairbench/blocks/metrics/regression.py
 -rw-rw-rw-  2.0 fat       50 b- defN 23-Nov-10 23:11 fairbench/blocks/reducers/__init__.py
--rw-rw-rw-  2.0 fat     4283 b- defN 24-May-16 14:21 fairbench/blocks/reducers/reducers.py
+-rw-rw-rw-  2.0 fat     4324 b- defN 24-Jun-03 11:28 fairbench/blocks/reducers/reducers.py
 -rw-rw-rw-  2.0 fat      157 b- defN 23-Nov-09 17:25 fairbench/core/__init__.py
 -rw-rw-rw-  2.0 fat     2463 b- defN 24-Feb-13 17:27 fairbench/core/categorical.py
--rw-rw-rw-  2.0 fat    15076 b- defN 24-May-16 14:21 fairbench/core/fork.py
+-rw-rw-rw-  2.0 fat    15202 b- defN 24-Jun-03 08:00 fairbench/core/fork.py
 -rw-rw-rw-  2.0 fat       49 b- defN 23-Nov-09 17:25 fairbench/core/compute/__init__.py
 -rw-rw-rw-  2.0 fat     3935 b- defN 23-Nov-09 20:07 fairbench/core/compute/backends.py
--rw-rw-rw-  2.0 fat     5563 b- defN 24-Apr-18 11:38 fairbench/core/compute/delegation.py
--rw-rw-rw-  2.0 fat      183 b- defN 23-Nov-09 17:25 fairbench/core/explanation/__init__.py
--rw-rw-rw-  2.0 fat     1999 b- defN 23-Nov-10 16:55 fairbench/core/explanation/base.py
+-rw-rw-rw-  2.0 fat     5858 b- defN 24-Jun-03 11:58 fairbench/core/compute/delegation.py
+-rw-rw-rw-  2.0 fat      183 b- defN 24-Jun-03 08:05 fairbench/core/explanation/__init__.py
+-rw-rw-rw-  2.0 fat     2294 b- defN 24-Jun-03 11:18 fairbench/core/explanation/base.py
 -rw-rw-rw-  2.0 fat      674 b- defN 23-Nov-09 17:25 fairbench/core/explanation/curve.py
--rw-rw-rw-  2.0 fat      388 b- defN 23-Nov-09 17:25 fairbench/core/explanation/error.py
+-rw-rw-rw-  2.0 fat     4096 b- defN 24-Jun-03 12:08 fairbench/core/explanation/error.py
 -rw-rw-rw-  2.0 fat      134 b- defN 23-Jun-28 12:59 fairbench/export/__init__.py
 -rw-rw-rw-  2.0 fat    19106 b- defN 24-May-16 08:55 fairbench/export/interactive.py
--rw-rw-rw-  2.0 fat     3307 b- defN 24-May-22 17:14 fairbench/export/native.py
+-rw-rw-rw-  2.0 fat     3375 b- defN 24-May-22 17:16 fairbench/export/native.py
 -rw-rw-rw-  2.0 fat      173 b- defN 23-Jun-28 12:05 fairbench/export/modelcards/__init__.py
 -rw-rw-rw-  2.0 fat     2978 b- defN 24-Feb-28 09:48 fairbench/export/modelcards/tohtml.py
 -rw-rw-rw-  2.0 fat     2569 b- defN 24-Feb-28 09:32 fairbench/export/modelcards/tomarkdown.py
--rw-rw-rw-  2.0 fat     1229 b- defN 24-May-18 13:17 fairbench/export/modelcards/toyaml.py
+-rw-rw-rw-  2.0 fat     1229 b- defN 24-May-28 12:51 fairbench/export/modelcards/toyaml.py
 -rw-rw-rw-  2.0 fat      164 b- defN 23-Nov-10 22:54 fairbench/reports/__init__.py
 -rw-rw-rw-  2.0 fat     1437 b- defN 24-Feb-13 17:25 fairbench/reports/accumulate.py
--rw-rw-rw-  2.0 fat     3954 b- defN 24-May-19 10:45 fairbench/reports/adhoc.py
+-rw-rw-rw-  2.0 fat     4034 b- defN 24-Jun-03 12:08 fairbench/reports/adhoc.py
 -rw-rw-rw-  2.0 fat     2969 b- defN 24-May-18 21:56 fairbench/reports/base.py
 -rw-rw-rw-  2.0 fat      912 b- defN 24-Feb-13 10:59 fairbench/reports/surrogate.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-May-26 21:15 tests/__init__.py
 -rw-rw-rw-  2.0 fat     1613 b- defN 24-Feb-11 00:25 tests/test_batching.py
 -rw-rw-rw-  2.0 fat     1370 b- defN 24-May-16 19:11 tests/test_demos.py
 -rw-rw-rw-  2.0 fat     7095 b- defN 23-Nov-10 23:01 tests/test_forks.py
 -rw-rw-rw-  2.0 fat     4391 b- defN 24-May-22 17:10 tests/test_metrics.py
 -rw-rw-rw-  2.0 fat     1101 b- defN 24-May-18 11:04 tests/test_modelcards.py
 -rw-rw-rw-  2.0 fat     2242 b- defN 23-Nov-10 16:55 tests/test_reduction.py
 -rw-rw-rw-  2.0 fat     8347 b- defN 24-May-19 11:18 tests/test_reports.py
--rw-rw-rw-  2.0 fat      901 b- defN 24-May-22 17:16 fairbench-0.3.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-May-22 17:16 fairbench-0.3.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       16 b- defN 24-May-22 17:16 fairbench-0.3.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat     4408 b- defN 24-May-22 17:16 fairbench-0.3.1.dist-info/RECORD
-51 files, 145429 bytes uncompressed, 35567 bytes compressed:  75.5%
+-rw-rw-rw-  2.0 fat      887 b- defN 24-Jun-03 12:19 fairbench-0.3.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-Jun-03 12:19 fairbench-0.3.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       16 b- defN 24-Jun-03 12:19 fairbench-0.3.2.dist-info/top_level.txt
+?rw-rw-r--  2.0 fat     4409 b- defN 24-Jun-03 12:19 fairbench-0.3.2.dist-info/RECORD
+51 files, 152215 bytes uncompressed, 37283 bytes compressed:  75.5%
```

## zipnote {}

```diff
@@ -135,20 +135,20 @@
 
 Filename: tests/test_reduction.py
 Comment: 
 
 Filename: tests/test_reports.py
 Comment: 
 
-Filename: fairbench-0.3.1.dist-info/METADATA
+Filename: fairbench-0.3.2.dist-info/METADATA
 Comment: 
 
-Filename: fairbench-0.3.1.dist-info/WHEEL
+Filename: fairbench-0.3.2.dist-info/WHEEL
 Comment: 
 
-Filename: fairbench-0.3.1.dist-info/top_level.txt
+Filename: fairbench-0.3.2.dist-info/top_level.txt
 Comment: 
 
-Filename: fairbench-0.3.1.dist-info/RECORD
+Filename: fairbench-0.3.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## fairbench/blocks/framework.py

```diff
@@ -1,14 +1,24 @@
 from fairbench.core.fork import Fork, astensor, asprimitive, role
 from fairbench.core.compute import comparator
 from fairbench.core.explanation import Explainable, ExplainableError
 from typing import Optional
 
 # from fairbench.reports.accumulate import todict as tokwargs
 
+"""
+def _stopatexplainableerror(value):
+    if isinstance(value, ExplainableError):
+        raise value.reraise()
+    if isinstance(value, list):
+        for v in value:
+            _stopatexplainableerror(v)
+    return value
+"""
+
 
 def areduce(fork: Fork, reducer, expand=None, transform=None, branches=None):
     return reduce(fork, reducer, expand, transform, branches, name=None)
 
 
 def _tryorexplainable(method, *args, **kwargs):
     for arg in args:
@@ -16,15 +26,15 @@
             return arg
     for arg in kwargs.values():
         if isinstance(arg, ExplainableError):
             return arg
     try:
         return method(*args, **kwargs)
     except ExplainableError as e:
-        return e
+        return e.caught()
 
 
 def reduce_namefinder(
     reducer, expand=None, transform=None, branches=None, base=None
 ) -> str:
     name = reducer.__name__
     if expand is not None:
```

## fairbench/blocks/metrics/classification.py

```diff
@@ -76,15 +76,15 @@
     sensitive: Optional[Tensor] = None,
     max_prediction: float = 1,
 ):
     if sensitive is None:
         sensitive = predictions.ones_like()
     false_positives = (predictions * (max_prediction - labels) * sensitive).sum()
     negatives = ((max_prediction - labels) * sensitive).sum()
-    fpr_value = 0 if negatives == 0 else false_positives / negatives
+    fpr_value = 1 if negatives == 0 else false_positives / negatives
     return Explainable(
         fpr_value,
         negatives=negatives.item(),
         false_positives=false_positives.item(),
         samples=sensitive.sum().item(),
     )
 
@@ -96,15 +96,17 @@
     predictions: Tensor,
     labels: Tensor,
     sensitive: Optional[Tensor] = None,
     max_prediction: float = 1,
 ):
     if sensitive is None:
         sensitive = predictions.ones_like()
-    true_negatives = ((max_prediction - predictions) * (max_prediction - labels) * sensitive).sum()
+    true_negatives = (
+        (max_prediction - predictions) * (max_prediction - labels) * sensitive
+    ).sum()
     negatives = ((max_prediction - labels) * sensitive).sum()
     tnr_value = 0 if negatives == 0 else true_negatives / negatives
     return Explainable(
         tnr_value,
         negatives=negatives.item(),
         true_negatives=true_negatives.item(),
         samples=sensitive.sum().item(),
@@ -120,14 +122,14 @@
     sensitive: Optional[Tensor] = None,
     max_prediction: float = 1,
 ):
     if sensitive is None:
         sensitive = predictions.ones_like()
     false_negatives = ((max_prediction - predictions) * labels * sensitive).sum()
     positives = (labels * sensitive).sum()
-    fnr_value = 0 if positives == 0 else false_negatives / positives
+    fnr_value = 1 if positives == 0 else false_negatives / positives
     return Explainable(
         fnr_value,
         positives=positives.item(),
         false_negatives=false_negatives.item(),
         samples=sensitive.sum().item(),
     )
```

## fairbench/blocks/metrics/ranking.py

```diff
@@ -1,12 +1,14 @@
 from fairbench.core import parallel, unit_bounded, role
 from fairbench.core.explanation import Explainable, ExplanationCurve
+from fairbench.core.explanation.error import verify
 from eagerpy import Tensor
 import numpy as np
 from typing import Optional
+import math
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def avgscore(scores: Tensor, sensitive: Optional[Tensor] = None, bins: int = 100):
     bins = int(bins.numpy())
@@ -42,99 +44,143 @@
     import sklearn
 
     if sensitive is None:
         sensitive = scores.ones_like()
     scores = scores[sensitive == 1]
     labels = labels[sensitive == 1]
     fpr, tpr, _ = sklearn.metrics.roc_curve(labels.numpy(), scores.numpy())
+    value = sklearn.metrics.auc(fpr, tpr)
+    verify(
+        not math.isnan(value),
+        f"Cannot compute AUC when all instances have the same label for branch",
+    )
     return Explainable(
-        sklearn.metrics.auc(fpr, tpr),
+        value,
         curve=ExplanationCurve(fpr, tpr, "ROC"),
         samples=sensitive.sum(),
     )
 
 
 @role("metric")
 @parallel
 @unit_bounded
-def tophr(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
+def tophr(
+    scores: Tensor,
+    labels: Tensor,
+    sensitive: Tensor = None,
+    top: int = 3,
+    branch: str = "branch",
+):
     k = int(top.numpy())
-    assert 0 < k <= scores.shape[0]
+    verify(
+        0 < k <= scores.shape[0],
+        f"There are only {scores.shape[0]} inputs but top={top} were requested for ranking analysis",
+    )
     if sensitive is None:
         sensitive = scores.ones_like()
     scores = scores[sensitive == 1]
     labels = labels[sensitive == 1]
     indexes = scores.argsort()
-    indexes = indexes[-k:]
+    verify(
+        k <= len(indexes),
+        f"There are only {len(indexes)} {branch} members but top={top} were requested for ranking analysis",
+    )
+    indexes = indexes[(len(indexes) - k) :]
     return Explainable(
         labels[indexes].mean(),
         top=k,
         true_top=labels[indexes].sum(),
         samples=sensitive.sum(),
     )
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def toprec(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
     k = int(top.numpy())
-    assert 0 < k <= scores.shape[0]
+    verify(
+        0 < k <= scores.shape[0],
+        f"There are only {scores.shape[0]} inputs but top={top} were requested for ranking analysis",
+    )
     if sensitive is None:
         sensitive = scores.ones_like()
     scores = scores[sensitive == 1]
     labels = labels[sensitive == 1]
     indexes = scores.argsort()
-    indexes = indexes[-k:]
+    verify(
+        k <= len(indexes),
+        f"There are only {len(indexes)} members but top={top} were requested for ranking analysis",
+    )
+    indexes = indexes[(len(indexes) - k) :]
+    denom = labels.sum()
     return Explainable(
-        labels[indexes].sum() / labels.sum(),
+        0 if denom == 0 else labels[indexes].sum() / denom,
         top=k,
         true_top=labels[indexes].sum(),
+        true_all=denom,
         samples=sensitive.sum(),
     )
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def topf1(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
     k = int(top.numpy())
-    assert 0 < k <= scores.shape[0]
+    verify(
+        0 < k <= scores.shape[0],
+        f"There are only {scores.shape[0]} inputs but top={top} were requested for ranking analysis",
+    )
     if sensitive is None:
         sensitive = scores.ones_like()
     scores = scores[sensitive == 1]
     labels = labels[sensitive == 1]
     indexes = scores.argsort()
-    indexes = indexes[-k:]
+    verify(
+        k <= len(indexes),
+        f"There are only {len(indexes)} members but top={top} were requested for ranking analysis",
+    )
+    indexes = indexes[(len(indexes) - k) :]
     prec = labels[indexes].mean()
-    rec = labels[indexes].sum() / labels.sum()
+    denom_rec = labels.sum()
+    rec = 0 if denom_rec == 0 else labels[indexes].sum() / denom_rec
+    denom = prec + rec
     return Explainable(
-        2 * prec * rec / (prec + rec),
+        0 if denom == 0 else 2 * prec * rec / denom,
         top=k,
         true_top=labels[indexes].sum(),
+        true_all=denom_rec,
         samples=sensitive.sum(),
     )
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def avghr(scores: Tensor, labels: Tensor, sensitive: Tensor = None, top: int = 3):
     k = int(top.numpy())
-    assert 0 < k <= scores.shape[0]
+    verify(
+        0 < k <= scores.shape[0],
+        f"There are only {scores.shape[0]} inputs but top={top} were requested for ranking analysis",
+    )
     if sensitive is None:
         sensitive = scores.ones_like()
     scores = scores[sensitive == 1]
     labels = labels[sensitive == 1]
     indexes = scores.argsort()
+    verify(
+        k <= len(indexes),
+        f"There are only {len(indexes)} members but top={top} were requested for ranking analysis",
+    )
     curve = list()
     accum = 0
     for num in range(1, k + 1):
-        accum += labels[indexes[-num]].numpy()
-        curve.append(accum / num * labels[indexes[-num]].numpy())
+        accum += labels[indexes[len(indexes) - num]].numpy()
+        curve.append(accum / num * labels[indexes[len(indexes) - num]].numpy())
     curve = [v for v in curve]
     return Explainable(
         sum(curve) / len(curve),
         top=k,
         curve=ExplanationCurve(
             np.array(list(range(len(curve))), dtype=float) + 1,
             np.array(curve, dtype=float),
@@ -145,27 +191,34 @@
 
 
 @role("metric")
 @parallel
 @unit_bounded
 def avgrepr(scores: Tensor, sensitive: Tensor = None, top: int = 3):
     k = int(top.numpy())
-    assert 0 < k <= scores.shape[0]
+    verify(
+        0 < k <= scores.shape[0],
+        f"There are only {scores.shape[0]} inputs but top={top} were requested for ranking analysis",
+    )
     if sensitive is None:
         sensitive = scores.ones_like()
     expected = float(sensitive.mean().numpy())
     indexes = scores.argsort()
+    verify(
+        k <= len(indexes),
+        f"There are only {len(indexes)} members but top={top} were requested for ranking analysis",
+    )
     curve = list()
     accum = 0
     for num in range(1, k + 1):
         accum += sensitive[indexes[-num]].numpy()
         curve.append(accum / num / expected)
     curve = [v for v in curve]
     return Explainable(
-        sum(curve) / len(curve),
+        0 if len(curve) == 0 else sum(curve) / len(curve),
         top=k,
         curve=ExplanationCurve(
             np.array(list(range(len(curve))), dtype=float) + 1,
             np.array(curve, dtype=float),
             "hks",
         ),
         samples=sensitive.sum(),
```

## fairbench/blocks/reducers/reducers.py

```diff
@@ -1,10 +1,10 @@
 import eagerpy as ep
-from typing import Iterable
 from fairbench.core import Explainable, ExplainableError
+from fairbench.core.explanation.error import verify
 
 
 def abs(value):
     if value < 0:
         return -value
     return value
 
@@ -12,123 +12,128 @@
 def _sum(values):
     ret = 0
     for value in values:
         ret = value + ret
     return ret
 
 
-def sum(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(
-        values, list
-    ), "fairbench.sum can only reduce lists. Maybe you meant to use eagerpy.sum?"
+def sum(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(
+        isinstance(values, list),
+        "fairbench.sum can only reduce lists. Maybe you meant to use eagerpy.sum?",
+    )
     ret = 0
     for value in values:
         ret = ret + value
     return ret
 
 
-def mean(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(
-        values, list
-    ), "fairbench.mean can only reduce lists. Maybe you meant to use eagerpy.mean?"
+def mean(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(
+        isinstance(values, list),
+        "fairbench.mean can only reduce lists. Maybe you meant to use eagerpy.mean?",
+    )
     return _sum(values) / len(values)
 
 
-def wmean(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(values, list), "fairbench.wmean can only reduce lists."
+def wmean(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(isinstance(values, list), "fairbench.wmean can only reduce lists.")
     for value in values:
         if (
             not isinstance(value, Explainable)
             or "samples" not in value.explain.branches()
         ):
             raise ExplainableError("Explanation absent or does not store `samples`")
     nom = _sum([value * value.explain.samples for value in values])
     denom = _sum([value.explain.samples for value in values])
     return nom if denom == 0 else nom / denom
 
 
-def identical(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(
-        values, list
-    ), "Can only reduce lists with fairbench.identical. Maybe you meant to use an eagerpy method?"
+def identical(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(
+        isinstance(values, list),
+        "Can only reduce lists with fairbench.identical. Maybe you meant to use an eagerpy method?",
+    )
     for value in values:
         if (value - values[0]).abs().sum() != 0:
             raise ExplainableError(
                 "The same value should reside in all branches for identical reducers."
             )
     return values[0]
 
 
-def gm(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(values, list), "fairbench.gm can only reduce lists."
+def gm(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(isinstance(values, list), "fairbench.gm can only reduce lists.")
     ret = 1
     for value in values:
         ret = ret * value
     return ret ** (1.0 / len(values))
 
 
-def max(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(
-        values, list
-    ), "fairbench.max can only reduce lists. Maybe you meant to use eagerpy.maximum?"
+def max(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(
+        isinstance(values, list),
+        "fairbench.max can only reduce lists. Maybe you meant to use eagerpy.maximum?",
+    )
     ret = float("-inf")
     for value in values:
         if value > ret:
             ret = value
     return ret
 
 
-def budget(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(values, list), "fairbench.budget can only reduce lists."
+def budget(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(isinstance(values, list), "fairbench.budget can only reduce lists.")
     from math import log  # TODO: make this compatible with backpropagation
 
     # "An Intersectional Definition of Fairness"
     return max(values).log()
 
 
-def min(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(
-        values, list
-    ), "fairbench.min can only reduce lists. Maybe you meant to use eagerpy.minimum?"
+def min(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(
+        isinstance(values, list),
+        "fairbench.min can only reduce lists. Maybe you meant to use eagerpy.minimum?",
+    )
     ret = float("inf")
     for value in values:
         if value < ret:
             ret = value
     return ret
 
 
-def std(values: Iterable[ep.Tensor]) -> ep.Tensor:
-    assert isinstance(values, list), "fairbench.std can only reduce lists."
+def std(values: list[ep.Tensor]) -> ep.Tensor:
+    verify(isinstance(values, list), "fairbench.std can only reduce lists.")
     n = len(values)
     s = 0
     ss = 0
     for val in values:
         s = val + s
         ss = val * val + ss
     variance = (ss - (s * s) / n) / n
     return variance**0.5
 
 
-def coefvar(values: Iterable[ep.Tensor]) -> ep.Tensor:
+def coefvar(values: list[ep.Tensor]) -> ep.Tensor:
     # coefficient of variation
     # adhered to requirements by Campano, F., & Salvatore, D. (2006). Income Distribution: Includes CD. Oxford University Press.
-    assert isinstance(values, list), "fairbench.std can only reduce lists."
+    verify(isinstance(values, list), "fairbench.std can only reduce lists.")
     n = len(values)
     s = 0
     ss = 0
     for val in values:
         s = val + s
         ss = val * val + ss
     variance = (ss - (s * s) / n) / n
     return variance**0.5 * n / s
 
 
-def gini(values: Iterable[ep.Tensor]) -> ep.Tensor:
+def gini(values: list[ep.Tensor]) -> ep.Tensor:
     # coefficient of variation
-    assert isinstance(values, list), "fairbench.std can only reduce lists."
+    verify(isinstance(values, list), "fairbench.std can only reduce lists.")
     n = len(values)
 
     # Mean of the values
     mean = _sum(values) / n
 
     # Calculate the Gini numerator
     gini_sum = 0
```

## fairbench/core/fork.py

```diff
@@ -449,22 +449,27 @@
         return method(*args, **kwargs)
 
     return wrapper
 
 
 @parallel_primitive
 def call(obj, method, *args, **kwargs):
+    from fairbench import ExplainableError
+
     if method == "__getattribute__":
         obj = _result(obj)
     if (
         method == "__getattribute__"
         and isinstance(obj, dict)
         and len(args) == 1
         and len(kwargs) == 0
     ):
         return obj[args[0]]
-    if callable(method):
-        return method(obj, *args, **kwargs)
+    try:
+        if callable(method):
+            return method(obj, *args, **kwargs)
+    except ExplainableError as e:
+        return e.caught()
     attr = getattr(obj, method)
     if not callable(attr):
         return attr
     return attr(*args, **kwargs)
```

## fairbench/core/compute/delegation.py

```diff
@@ -1,9 +1,10 @@
 import inspect
 from fairbench.core.compute.backends import *
+from fairbench.core.explanation.error import ExplainableError
 from makefun import wraps
 
 
 def _call_on_branch(_wrapped_method, args, kwargs, branch, transform_args):
     return asprimitive(
         _wrapped_method(
             *(transform_args(arg._branches[branch]) for arg in args),
@@ -37,43 +38,48 @@
 
 
 def parallel(_wrapped_method):
     @wraps(_wrapped_method)
     def wrapper(*args, **kwargs):
         from fairbench.core.fork import Fork
 
-        if len(args) == 1 and not kwargs:
-            argnames = inspect.getfullargspec(_wrapped_method)[0]
-            arg = args[0]
-            kwargs = {k: getattr(arg, k) for k in argnames if hasattr(arg, k)}
-            args = []
-        branches = set(
-            [
-                branch
-                for arg in list(args) + list(kwargs.values())
-                if isinstance(arg, Fork)
-                for branch in arg._branches
-            ]
-        )
-        if not branches:
-            return asprimitive(
-                _wrapped_method(
-                    *(astensor(arg) for arg in args),
-                    **{key: astensor(arg) for key, arg in kwargs.items()},
-                )
+        try:
+            if len(args) == 1 and not kwargs:
+                argnames = inspect.getfullargspec(_wrapped_method)[0]
+                arg = args[0]
+                kwargs = {k: getattr(arg, k) for k in argnames if hasattr(arg, k)}
+                args = []
+            branches = set(
+                [
+                    branch
+                    for arg in list(args) + list(kwargs.values())
+                    if isinstance(arg, Fork)
+                    for branch in arg._branches
+                ]
             )
-        args, kwargs = _align_branches(_wrapped_method, args, kwargs, Fork, branches)
-        return Fork(
-            **{
-                branch: asprimitive(
-                    _call_on_branch(_wrapped_method, args, kwargs, branch, astensor)
+            if not branches:
+                return asprimitive(
+                    _wrapped_method(
+                        *(astensor(arg) for arg in args),
+                        **{key: astensor(arg) for key, arg in kwargs.items()},
+                    )
                 )
-                for branch in branches
-            }
-        )
+            args, kwargs = _align_branches(
+                _wrapped_method, args, kwargs, Fork, branches
+            )
+            return Fork(
+                **{
+                    branch: asprimitive(
+                        _call_on_branch(_wrapped_method, args, kwargs, branch, astensor)
+                    )
+                    for branch in branches
+                }
+            )
+        except ExplainableError as e:
+            return e.caught()
 
     return wrapper
 
 
 def comparator(_wrapped_method):
     @wraps(_wrapped_method)
     def wrapper(*args, **kwargs):
```

## fairbench/core/explanation/base.py

```diff
@@ -1,11 +1,12 @@
 from typing import Any
 from objwrap import ClosedWrapper
 import numpy as np
 import eagerpy as ep
+from fairbench.core.explanation.error import ExplainableError
 
 
 def tofloat(value: Any) -> float:
     if isinstance(value, ep.Tensor):
         return float(value.raw)
     return float(value)
 
@@ -57,7 +58,13 @@
         return self.__value__()
 
     def numpy(self):
         return self.value.numpy()
 
     def __after__(self, obj):
         return obj
+
+    def __wrapcall__(self, obj, name, *args, **kwargs):
+        for arg in args:
+            if isinstance(arg, ExplainableError):
+                arg.reraise()
+        return super().__wrapcall__(obj, name, *args, **kwargs)
```

## fairbench/core/explanation/error.py

```diff
@@ -1,16 +1,129 @@
+import traceback
+import sys
+import logging
+import os
+
+logging.basicConfig(
+    level=logging.ERROR,
+    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
+    handlers=[logging.FileHandler("fairbench.log")],
+)
+from objwrap import Wrapper
+
+
 class ExplainableError(Exception):
     def __init__(self, message):
         super().__init__(message)
         self.explain = message
         self.value = float("NaN")
         self.distribution = None
         self.desc = None
+        self.__printed = False
 
     def __float__(self):
         return self.value
 
     def __int__(self):
         return self.value
 
     def __str__(self):
         return "---"
+
+    def __repr__(self):
+        return f"{type(self).__name__}: {self.explain}"
+
+    def format_traceback(self):
+        tb = traceback.extract_tb(self.__traceback__)
+        tb = [frame for frame in traceback.extract_stack()[:-1]] + tb
+        filtered_tb = [
+            frame
+            for frame in tb
+            if not frame.filename.endswith(r"fairbench\core\compute\delegation.py")
+            and not frame.filename.endswith(r"fairbench\core\fork.py")
+            and not frame.filename.endswith(r"fairbench\core\explanation\error.py")
+            and not frame.filename.endswith(r"fairbench\blocks\framework.py")
+            and not frame.filename.startswith("<makefun-gen-")
+        ]
+        formatted_tb = traceback.format_list(filtered_tb)
+        return (
+            "".join(formatted_tb) + f"{type(self).__name__}: {self.explain}"
+            f"\nThis error only appears because you requested dependent computations."
+            f"\nOtherwise, it is normal for reports or other FairBench data to hold ExplainableError."
+            f"\nYou need to check your data first for any errors (you will see --- when printing them)."
+            f"\n- Issue tracker https://github.com/mever-team/FairBench/issues"
+            f"\n- Full trace in ./{os.path.relpath('fairbench.log')}"
+        )
+
+    def caught(self):
+        # this is what to return when catching explainable errors (e.g., raised within metric execution)
+        return self
+
+    def __add__(self, other):
+        self.reraise()
+
+    def __radd__(self, other):
+        self.reraise()
+
+    def __sub__(self, other):
+        self.reraise()
+
+    def __rsub__(self, other):
+        self.reraise()
+
+    def __mul__(self, other):
+        self.reraise()
+
+    def __rmul__(self, other):
+        self.reraise()
+
+    def __le__(self, other):
+        self.reraise()
+
+    def __ge__(self, other):
+        self.reraise()
+
+    def __lt__(self, other):
+        self.reraise()
+
+    def __gt__(self, other):
+        self.reraise()
+
+    def __eq__(self, other):
+        self.reraise()
+
+    def __neg__(self):
+        self.reraise()
+
+    def __ne__(self, other):
+        self.reraise()
+
+    def __pow__(self, power, modulo=None):
+        self.reraise()
+
+    def __rpow__(self, other):
+        self.reraise()
+
+    def reraise(self):
+        if self.__printed:
+            print("Repeated usage of " + repr(self), file=sys.stderr)
+            raise self
+        self.__printed = True
+        stack_trace = "".join(traceback.format_stack())
+        logging.error(
+            "There was an attempt to call computations that encountered an ExplainabeError"
+            "\nThis is the full stack trace of the ExplainableError that includes FairBench internals",
+            exc_info=self,
+        )
+        logging.error(
+            "This is the full stack trace of the computations where the ExplainableError was encountered"
+            "\nTraceback (most recent call last):" + stack_trace + "-" * 120
+        )
+        # this is what happens when the explainable error creates another issue down the line
+        print(self.format_traceback(), file=sys.stderr)
+        # remove this to print full errors for fairbench's internal debugging
+        raise self
+
+
+def verify(condition, message):
+    if not condition:
+        raise ExplainableError(message)
```

## fairbench/export/native.py

```diff
@@ -36,19 +36,28 @@
                     }
                 )
             else:
                 data[metric].append(tofloat(value[metric]))
     return json.dumps(data)
 
 
-def describe(report: Fork, spacing: int = 15, show: bool = True, separator: str = " ", newline="\n"):
+def describe(
+    report: Fork,
+    spacing: int = 15,
+    show: bool = True,
+    separator: str = " ",
+    newline="\n",
+):
     report = json.loads(tojson(report))
     ret = ""
     if report["header"]:
-        ret += separator.join([entry.ljust(spacing) for entry in report["header"]]) + newline
+        ret += (
+            separator.join([entry.ljust(spacing) for entry in report["header"]])
+            + newline
+        )
     for metric in report:
         if metric != "header":
             ret += (
                 separator.join(
                     [metric.ljust(spacing)]
                     + [f"{entry:.3f}".ljust(spacing) for entry in report[metric]]
                 )
```

## fairbench/reports/adhoc.py

```diff
@@ -1,14 +1,15 @@
 from fairbench.reports.base import report
 from fairbench.blocks import framework, reducers, expanders, metrics
 from fairbench.reports.accumulate import todict as tokwargs
 from fairbench.core.fork import combine, merge, role, tobackend, Fork
 from fairbench.reports.surrogate import surrogate_positives
 from fairbench.blocks.reducers import identical
 import numpy as np
+import math
 from makefun import wraps
 
 # TODO: create a differentiable report (and maybe separate metrics into subpackages of diffs vs normal)
 # TODO: have to check which values are differentiable
 
 common_adhoc_metrics = (
     metrics.accuracy,
@@ -80,15 +81,17 @@
 ):
     def modify_kwargs(kwargs):
         # adds an additional branch for the whole population called Any
         if "sensitive" in kwargs and "Any" not in kwargs["sensitive"]._branches:
             length = framework.areduce(
                 kwargs["sensitive"].shape[0], identical
             )  # asserts that everything has the same identical shape and returns it
-            length = int(length.value)  # retrieve int value from the explainable
+            length = int(
+                0 if math.isnan(length.value) else length.value
+            )  # retrieve int value from the explainable
             kwargs["sensitive"]._branches["Any"] = tobackend(np.ones((length,)))
         return kwargs
 
     base = report(*args, metrics=metrics, modify_kwargs=modify_kwargs, **kwargs)
     # perform the reduction while accounting for the
     return combine(
         *[
```

## Comparing `fairbench-0.3.1.dist-info/METADATA` & `fairbench-0.3.2.dist-info/METADATA`

 * *Files 17% similar despite different names*

```diff
@@ -1,12 +1,12 @@
 Metadata-Version: 2.1
 Name: fairbench
-Version: 0.3.1
-Summary: Fairness model assessment framework
-Home-page: https://github.com/mever-team/FairBench
+Version: 0.3.2
+Summary: A fairness assessment framework
+Home-page: https://fairbench.readthedocs.io
 Author: Emmanouil (Manios) Krasanakis
 Author-email: maniospas@hotmail.com
 License: UNKNOWN
 Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3
 Classifier: License :: OSI Approved :: Apache Software License
 Classifier: Operating System :: OS Independent
@@ -20,9 +20,9 @@
 Requires-Dist: scikit-learn
 Requires-Dist: pandas
 Requires-Dist: objwrap
 Requires-Dist: bokeh
 Requires-Dist: pyyaml
 Requires-Dist: requests
 
-A comprehensive AI fairness exploration framework.<br>**Homepage:** https://github.com/mever-team/FairBench<br>**Documentation:** https://fairbench.readthedocs.io
+A comprehensive AI fairness exploration framework.<br>**Homepage:** https://fairbench.readthedocs.io<br>**Repository:** https://github.com/mever-team/FairBench
```

## Comparing `fairbench-0.3.1.dist-info/RECORD` & `fairbench-0.3.2.dist-info/RECORD`

 * *Files 11% similar despite different names*

```diff
@@ -1,51 +1,51 @@
 fairbench/__init__.py,sha256=vx1onc6vHPsAv_M78DUG9Segdq4tJGc8TcRGOrTMBrM,497
 fairbench/verification.py,sha256=hpHtLM17cJ1AnG83wMGX0D8I6UtYPEtWCaPiEm7wZ1Q,6985
 fairbench/bench/__init__.py,sha256=piA_S0SAZ96_-MxdcGO_V7pCKngOFBeAv8KGj9rkGRA,38
 fairbench/bench/demos.py,sha256=hLWxupPPaEB-7fF2DlOP3u_h0lfJAmpx1gSHPsnbju8,5853
 fairbench/bench/loader.py,sha256=tg9GvdxhsxUE_7fX-Yj7IprvExmSRRyesAibz6fFyjY,1660
 fairbench/blocks/__init__.py,sha256=DmnOAEuZscW_w_fzywqzDMxB5pnrLWMg5gDOqfEBAmY,179
-fairbench/blocks/framework.py,sha256=-HbNcC9ASWKW8zd2OUqhhr6qVscorrAqxtiZ0teYI_A,3823
+fairbench/blocks/framework.py,sha256=VFrF-9qaH3bJ-ZLiXroF_8up51430qQv1s_b7qnAP1k,4073
 fairbench/blocks/expanders/__init__.py,sha256=voqD22YoinY2UiNnlmPSUDCNqgzI6nyJSbMM-mHJ3Hg,52
 fairbench/blocks/expanders/expanders.py,sha256=Es0blfm-N1fazVPHOTUGspljepK0E-JYyGZizBYl5ys,4309
 fairbench/blocks/metrics/__init__.py,sha256=cwCdbJ1bSSbEJKxb546lLhH-eNtfRth0onev3NYmjPY,274
-fairbench/blocks/metrics/classification.py,sha256=Y0-n9iZTyfNPlWvOeuG1T7rzzdFPFZjjUGoz7ehHGl8,3911
+fairbench/blocks/metrics/classification.py,sha256=YcZNStp7-LLi5txgobeYuDBdJCAIHTieqj8uAyFKTRk,3927
 fairbench/blocks/metrics/disparate_impact.py,sha256=IU6D_edmcrNv8G4a77GGlEgTcVXn-g6-HCnaRQIfyM8,2769
 fairbench/blocks/metrics/disparate_mistreatment.py,sha256=I6wvfnGQ5ojJv1mgdQMDHQyhhCiClzmU15OK1Ijxni0,1583
-fairbench/blocks/metrics/ranking.py,sha256=0stoNAhI9LSQmG7HWy_pPAgOeY_JOvQ59kpd1DRDzqA,5162
+fairbench/blocks/metrics/ranking.py,sha256=fIrx8h5N_DUPzOuGy3h1nLY0k23TRBrXozMKitXRLEo,7082
 fairbench/blocks/metrics/regression.py,sha256=7XFu1pwbJAs3plmzkUuvHOmubbQDVxXum0Bax2aQfYw,3006
 fairbench/blocks/reducers/__init__.py,sha256=8Vnpfirx3LzEXotYq816X8NdG-E3o8u7dF_ZgE42afg,50
-fairbench/blocks/reducers/reducers.py,sha256=o-1MrykqK7U889sSQAgjhiYSzwLxac3BFlgeTJbr45w,4283
+fairbench/blocks/reducers/reducers.py,sha256=iyHHhzeWYKzD0_fFhYR-T3V9ugV_s4b1Ko85IK9R7ws,4324
 fairbench/core/__init__.py,sha256=CwHCbN-2c3-GKF-OwxcOg9jYSmPFtWnQ206AtiCXkyQ,157
 fairbench/core/categorical.py,sha256=3JAYlGGXPLKVJH3djWGgn3YsfEjdd7gl3Rka2ZaPvbs,2463
-fairbench/core/fork.py,sha256=M8sYC_yekygM3HtMp10fh9DT6Q-Dm-yLMctjgzcmjSo,15076
+fairbench/core/fork.py,sha256=qsQIIXh7Ew66gBUT4aQgDktr0THSNIcoFM_rh_KKZDo,15202
 fairbench/core/compute/__init__.py,sha256=Weh6I7npSLreu6G4Ntk4SfDwEzVsRDtvIbBt3ixar9Q,49
 fairbench/core/compute/backends.py,sha256=4TVEjIlDXHrpTfI1gowUd3wCaywXGVGUyC9USW8AvJs,3935
-fairbench/core/compute/delegation.py,sha256=hkFpCeB6D9Bczx-cB0twaQ3wbK9yM6H-ZS6vwNwMb9w,5563
+fairbench/core/compute/delegation.py,sha256=87Udpce2DQ36JoYrQZmklfxGRfWmFPFYgvb_BznTThk,5858
 fairbench/core/explanation/__init__.py,sha256=fdnZ5GT5oZmriea9k-dVQ4j9bg_JoT1evhnnAj_5J5E,183
-fairbench/core/explanation/base.py,sha256=bBBFc9RLPMuYovo8MhGI4RqcgXMZPbmeSgIYXmvc6ms,1999
+fairbench/core/explanation/base.py,sha256=To86KUrSPZbwfmk7BPbkOXTo16K1ab0vgGu3DUc6j-0,2294
 fairbench/core/explanation/curve.py,sha256=daHB7H13USy3Qvj2n-IzVLAT_0BgXZ2yBSJX8UMNzyo,674
-fairbench/core/explanation/error.py,sha256=wYCIFAewcL5bvV1WYfcBY8ks7hA8GwaP3yEyjnPPYeI,388
+fairbench/core/explanation/error.py,sha256=Rp4OS6_x9WHrfuKpXl2DFbpgcOqwdaf83xktx9kurgc,4096
 fairbench/export/__init__.py,sha256=ZQ3m4piX_o-z5mlmG_H7O26SGuhKo7b38Nc64Bs8pik,134
 fairbench/export/interactive.py,sha256=OWGinEoGgvGL7bjWfxubJYRdLu0F9oZIw3ylDfI7yxw,19106
-fairbench/export/native.py,sha256=jMWmn4d0FoZ1qWLf5dLqs3hEg-sIYNK3GfD2_LCDn2A,3307
+fairbench/export/native.py,sha256=OiwNCupXNVSHQHdfxAMQE5ZVmP7Cnh8XNGQLWvQH2Xg,3375
 fairbench/export/modelcards/__init__.py,sha256=B9gsTh-sWzIkY5pIPSPJ8ZdurlBsr2MS4o95n3WXXj0,173
 fairbench/export/modelcards/tohtml.py,sha256=Yvex2msBltytQpvoSE4whASH0C6lO5yEgDRMpdTEgnc,2978
 fairbench/export/modelcards/tomarkdown.py,sha256=p62rCtRiIhhIHOyLv4MArD6aoSIUi33YXeZ4fPcT-R0,2569
 fairbench/export/modelcards/toyaml.py,sha256=tfSnr27ECkvF7rMUdTOz3cw0iKQEJSAM92CpYXzP0OQ,1229
 fairbench/reports/__init__.py,sha256=Cq6gbXBY3DfzP66Bt1NojxwIzRRA5hbfWjGI4yW89-E,164
 fairbench/reports/accumulate.py,sha256=iZGNeAvVqbc-FQvXpJGhKhntVMDt18BmehLSrR4_yYY,1437
-fairbench/reports/adhoc.py,sha256=JVxPag5IwUeDSOiw2MSY5T_vQSwnfvt81XTq99W_YP4,3954
+fairbench/reports/adhoc.py,sha256=o81m-1HLmgK7wOUMAeBhlG6ZsoPyRU9inPfmMFYmtew,4034
 fairbench/reports/base.py,sha256=wNHs-inLNKivoF4zTGDaDifivgErBhGEoSAXAKzBHXg,2969
 fairbench/reports/surrogate.py,sha256=cCyYSk6LJl7RtiFyHiouCXzV6XfA7mAr-sGzS5bUriY,912
 tests/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 tests/test_batching.py,sha256=vPwN0ZjgIcCh5inIAfxPmzHyCxbATLyt35WTxiIzdfQ,1613
 tests/test_demos.py,sha256=5q5NeAsbsDuz5ymtmi1mMFD-WKtuCGaebcFStRFXhwg,1370
 tests/test_forks.py,sha256=stYpDWDmzyJrqweHU8f1c888HNwzeij7k8FUtEBttQE,7095
 tests/test_metrics.py,sha256=NIPivYFJ0bnMy4BeHM4niCn5u1LF_-lQ3WL-zLfkTqY,4391
 tests/test_modelcards.py,sha256=5glZjRh8M7fmq20KS16ZB3VfsreI0kqMKjrvULHI8bQ,1101
 tests/test_reduction.py,sha256=t-HOX6m-zJlFVQGyrRQh_NRaguRB2BUdAoCqlg-yVuw,2242
 tests/test_reports.py,sha256=wKSl-bKXqwuraDxauofgNsRQiDuSDdTW4U5kAsI4Mv8,8347
-fairbench-0.3.1.dist-info/METADATA,sha256=qPa9jqplBDRO6tmrPTxMeVJpMpwtLuzB2s_R4_W3tz8,901
-fairbench-0.3.1.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
-fairbench-0.3.1.dist-info/top_level.txt,sha256=lrkG910bN_2UdVUqCXaR6aeRjjXfOQX2-wSeVjhhFnM,16
-fairbench-0.3.1.dist-info/RECORD,,
+fairbench-0.3.2.dist-info/METADATA,sha256=UgIlAb4T1SXgmZR-PcXU-MyId9vHtwTWEjBNM1cQlWw,887
+fairbench-0.3.2.dist-info/WHEEL,sha256=OqRkF0eY5GHssMorFjlbTIq072vpHpF60fIQA6lS9xA,92
+fairbench-0.3.2.dist-info/top_level.txt,sha256=lrkG910bN_2UdVUqCXaR6aeRjjXfOQX2-wSeVjhhFnM,16
+fairbench-0.3.2.dist-info/RECORD,,
```

