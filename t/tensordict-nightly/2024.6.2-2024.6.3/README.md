# Comparing `tmp/tensordict_nightly-2024.6.2-cp39-cp39-win_amd64.whl.zip` & `tmp/tensordict_nightly-2024.6.3-cp39-cp39-manylinux1_x86_64.whl.zip`

## zipinfo {}

```diff
@@ -1,39 +1,39 @@
-Zip file size: 298408 bytes, number of entries: 37
--rw-rw-rw-  2.0 fat     1459 b- defN 24-Jun-02 13:49 tensordict/__init__.py
--rw-rw-rw-  2.0 fat     6156 b- defN 24-Jun-02 13:49 tensordict/_contextlib.py
--rw-rw-rw-  2.0 fat   137304 b- defN 24-Jun-02 13:49 tensordict/_lazy.py
--rw-rw-rw-  2.0 fat     7844 b- defN 24-Jun-02 13:49 tensordict/_pytree.py
--rw-rw-rw-  2.0 fat   154656 b- defN 24-Jun-02 13:49 tensordict/_td.py
--rw-rw-rw-  2.0 fat   113664 b- defN 24-Jun-02 13:51 tensordict/_tensordict.pyd
--rw-rw-rw-  2.0 fat    23597 b- defN 24-Jun-02 13:49 tensordict/_torch_func.py
--rw-rw-rw-  2.0 fat   295790 b- defN 24-Jun-02 13:49 tensordict/base.py
--rw-rw-rw-  2.0 fat    17864 b- defN 24-Jun-02 13:49 tensordict/functional.py
--rw-rw-rw-  2.0 fat    40454 b- defN 24-Jun-02 13:49 tensordict/memmap.py
--rw-rw-rw-  2.0 fat    50308 b- defN 24-Jun-02 13:49 tensordict/persistent.py
--rw-rw-rw-  2.0 fat   102288 b- defN 24-Jun-02 13:49 tensordict/tensorclass.py
--rw-rw-rw-  2.0 fat     1002 b- defN 24-Jun-02 13:49 tensordict/tensordict.py
--rw-rw-rw-  2.0 fat    76842 b- defN 24-Jun-02 13:49 tensordict/utils.py
--rw-rw-rw-  2.0 fat       86 b- defN 24-Jun-02 13:50 tensordict/version.py
--rw-rw-rw-  2.0 fat     1634 b- defN 24-Jun-02 13:49 tensordict/nn/__init__.py
--rw-rw-rw-  2.0 fat    54882 b- defN 24-Jun-02 13:49 tensordict/nn/common.py
--rw-rw-rw-  2.0 fat     5940 b- defN 24-Jun-02 13:49 tensordict/nn/ensemble.py
--rw-rw-rw-  2.0 fat    25976 b- defN 24-Jun-02 13:49 tensordict/nn/functional_modules.py
--rw-rw-rw-  2.0 fat    38863 b- defN 24-Jun-02 13:49 tensordict/nn/params.py
--rw-rw-rw-  2.0 fat    26168 b- defN 24-Jun-02 13:49 tensordict/nn/probabilistic.py
--rw-rw-rw-  2.0 fat    19947 b- defN 24-Jun-02 13:49 tensordict/nn/sequence.py
--rw-rw-rw-  2.0 fat    13231 b- defN 24-Jun-02 13:49 tensordict/nn/utils.py
--rw-rw-rw-  2.0 fat      795 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/__init__.py
--rw-rw-rw-  2.0 fat     6629 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/composite.py
--rw-rw-rw-  2.0 fat     9924 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/continuous.py
--rw-rw-rw-  2.0 fat     2667 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/discrete.py
--rw-rw-rw-  2.0 fat     6694 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/truncated_normal.py
--rw-rw-rw-  2.0 fat     1266 b- defN 24-Jun-02 13:49 tensordict/nn/distributions/utils.py
--rw-rw-rw-  2.0 fat      393 b- defN 24-Jun-02 13:49 tensordict/prototype/__init__.py
--rw-rw-rw-  2.0 fat     7889 b- defN 24-Jun-02 13:49 tensordict/prototype/fx.py
--rw-rw-rw-  2.0 fat      796 b- defN 24-Jun-02 13:49 tensordict/prototype/tensorclass.py
--rw-rw-rw-  2.0 fat     1119 b- defN 24-Jun-02 13:51 tensordict_nightly-2024.6.2.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    22933 b- defN 24-Jun-02 13:51 tensordict_nightly-2024.6.2.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 24-Jun-02 13:51 tensordict_nightly-2024.6.2.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       11 b- defN 24-Jun-02 13:51 tensordict_nightly-2024.6.2.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     3171 b- defN 24-Jun-02 13:51 tensordict_nightly-2024.6.2.dist-info/RECORD
-37 files, 1280342 bytes uncompressed, 293370 bytes compressed:  77.1%
+Zip file size: 1082673 bytes, number of entries: 37
+-rw-r--r--  2.0 unx     1406 b- defN 24-Jun-03 13:51 tensordict/__init__.py
+-rw-r--r--  2.0 unx     6000 b- defN 24-Jun-03 13:51 tensordict/_contextlib.py
+-rw-r--r--  2.0 unx   133661 b- defN 24-Jun-03 13:51 tensordict/_lazy.py
+-rw-r--r--  2.0 unx     7568 b- defN 24-Jun-03 13:51 tensordict/_pytree.py
+-rw-r--r--  2.0 unx   150542 b- defN 24-Jun-03 13:51 tensordict/_td.py
+-rwxr-xr-x  2.0 unx  3286432 b- defN 24-Jun-03 13:51 tensordict/_tensordict.so
+-rw-r--r--  2.0 unx    22955 b- defN 24-Jun-03 13:51 tensordict/_torch_func.py
+-rw-r--r--  2.0 unx   288174 b- defN 24-Jun-03 13:51 tensordict/base.py
+-rw-r--r--  2.0 unx    17459 b- defN 24-Jun-03 13:51 tensordict/functional.py
+-rw-r--r--  2.0 unx    39377 b- defN 24-Jun-03 13:51 tensordict/memmap.py
+-rw-r--r--  2.0 unx    48928 b- defN 24-Jun-03 13:51 tensordict/persistent.py
+-rw-r--r--  2.0 unx    99433 b- defN 24-Jun-03 13:51 tensordict/tensorclass.py
+-rw-r--r--  2.0 unx      965 b- defN 24-Jun-03 13:51 tensordict/tensordict.py
+-rw-r--r--  2.0 unx    74581 b- defN 24-Jun-03 13:51 tensordict/utils.py
+-rw-r--r--  2.0 unx       51 b- defN 24-Jun-03 13:51 tensordict/version.py
+-rw-r--r--  2.0 unx     1572 b- defN 24-Jun-03 13:51 tensordict/nn/__init__.py
+-rw-r--r--  2.0 unx    53582 b- defN 24-Jun-03 13:51 tensordict/nn/common.py
+-rw-r--r--  2.0 unx     5811 b- defN 24-Jun-03 13:51 tensordict/nn/ensemble.py
+-rw-r--r--  2.0 unx    25318 b- defN 24-Jun-03 13:51 tensordict/nn/functional_modules.py
+-rw-r--r--  2.0 unx    37581 b- defN 24-Jun-03 13:51 tensordict/nn/params.py
+-rw-r--r--  2.0 unx    25591 b- defN 24-Jun-03 13:51 tensordict/nn/probabilistic.py
+-rw-r--r--  2.0 unx    19496 b- defN 24-Jun-03 13:51 tensordict/nn/sequence.py
+-rw-r--r--  2.0 unx    12884 b- defN 24-Jun-03 13:51 tensordict/nn/utils.py
+-rw-r--r--  2.0 unx      774 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/__init__.py
+-rw-r--r--  2.0 unx     6469 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/composite.py
+-rw-r--r--  2.0 unx     9658 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/continuous.py
+-rw-r--r--  2.0 unx     2580 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/discrete.py
+-rw-r--r--  2.0 unx     6504 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/truncated_normal.py
+-rw-r--r--  2.0 unx     1226 b- defN 24-Jun-03 13:51 tensordict/nn/distributions/utils.py
+-rw-r--r--  2.0 unx      381 b- defN 24-Jun-03 13:51 tensordict/prototype/__init__.py
+-rw-r--r--  2.0 unx     7690 b- defN 24-Jun-03 13:51 tensordict/prototype/fx.py
+-rw-r--r--  2.0 unx      773 b- defN 24-Jun-03 13:51 tensordict/prototype/tensorclass.py
+-rw-r--r--  2.0 unx     1098 b- defN 24-Jun-03 13:51 tensordict_nightly-2024.6.3.dist-info/LICENSE
+-rw-r--r--  2.0 unx    22390 b- defN 24-Jun-03 13:51 tensordict_nightly-2024.6.3.dist-info/METADATA
+-rw-r--r--  2.0 unx      103 b- defN 24-Jun-03 13:51 tensordict_nightly-2024.6.3.dist-info/WHEEL
+-rw-r--r--  2.0 unx       11 b- defN 24-Jun-03 13:51 tensordict_nightly-2024.6.3.dist-info/top_level.txt
+?rw-rw-r--  2.0 unx     3169 b- defN 24-Jun-03 13:51 tensordict_nightly-2024.6.3.dist-info/RECORD
+37 files, 4422193 bytes uncompressed, 1077637 bytes compressed:  75.6%
```

## zipnote {}

```diff
@@ -9,15 +9,15 @@
 
 Filename: tensordict/_pytree.py
 Comment: 
 
 Filename: tensordict/_td.py
 Comment: 
 
-Filename: tensordict/_tensordict.pyd
+Filename: tensordict/_tensordict.so
 Comment: 
 
 Filename: tensordict/_torch_func.py
 Comment: 
 
 Filename: tensordict/base.py
 Comment: 
@@ -90,23 +90,23 @@
 
 Filename: tensordict/prototype/fx.py
 Comment: 
 
 Filename: tensordict/prototype/tensorclass.py
 Comment: 
 
-Filename: tensordict_nightly-2024.6.2.dist-info/LICENSE
+Filename: tensordict_nightly-2024.6.3.dist-info/LICENSE
 Comment: 
 
-Filename: tensordict_nightly-2024.6.2.dist-info/METADATA
+Filename: tensordict_nightly-2024.6.3.dist-info/METADATA
 Comment: 
 
-Filename: tensordict_nightly-2024.6.2.dist-info/WHEEL
+Filename: tensordict_nightly-2024.6.3.dist-info/WHEEL
 Comment: 
 
-Filename: tensordict_nightly-2024.6.2.dist-info/top_level.txt
+Filename: tensordict_nightly-2024.6.3.dist-info/top_level.txt
 Comment: 
 
-Filename: tensordict_nightly-2024.6.2.dist-info/RECORD
+Filename: tensordict_nightly-2024.6.3.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## tensordict/__init__.py

 * *Ordering differences only*

```diff
@@ -1,53 +1,53 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict._lazy import LazyStackedTensorDict
-from tensordict._td import is_tensor_collection, TensorDict
-from tensordict.base import TensorDictBase
-from tensordict.functional import (
-    dense_stack_tds,
-    make_tensordict,
-    merge_tensordicts,
-    pad,
-    pad_sequence,
-)
-from tensordict.memmap import MemoryMappedTensor
-from tensordict.persistent import PersistentTensorDict
-from tensordict.tensorclass import NonTensorData, NonTensorStack, tensorclass
-from tensordict.utils import (
-    assert_allclose_td,
-    is_batchedtensor,
-    is_tensorclass,
-    lazy_legacy,
-    NestedKey,
-    set_lazy_legacy,
-)
-from tensordict._pytree import *
-from tensordict._tensordict import unravel_key, unravel_key_list
-from tensordict.nn import TensorDictParams
-
-try:
-    from tensordict.version import __version__
-except ImportError:
-    __version__ = None
-
-__all__ = [
-    "LazyStackedTensorDict",
-    "NestedKey",
-    "NonTensorData",
-    "NonTensorStack",
-    "PersistentTensorDict",
-    "TensorDict",
-    "TensorDictBase",
-    "assert_allclose_td",
-    "dense_stack_tds",
-    "is_batchedtensor",
-    "is_tensor_collection",
-    "make_tensordict",
-    "merge_tensordicts",
-    "pad",
-    "pad_sequence",
-    "tensorclass",
-]
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict._lazy import LazyStackedTensorDict
+from tensordict._td import is_tensor_collection, TensorDict
+from tensordict.base import TensorDictBase
+from tensordict.functional import (
+    dense_stack_tds,
+    make_tensordict,
+    merge_tensordicts,
+    pad,
+    pad_sequence,
+)
+from tensordict.memmap import MemoryMappedTensor
+from tensordict.persistent import PersistentTensorDict
+from tensordict.tensorclass import NonTensorData, NonTensorStack, tensorclass
+from tensordict.utils import (
+    assert_allclose_td,
+    is_batchedtensor,
+    is_tensorclass,
+    lazy_legacy,
+    NestedKey,
+    set_lazy_legacy,
+)
+from tensordict._pytree import *
+from tensordict._tensordict import unravel_key, unravel_key_list
+from tensordict.nn import TensorDictParams
+
+try:
+    from tensordict.version import __version__
+except ImportError:
+    __version__ = None
+
+__all__ = [
+    "LazyStackedTensorDict",
+    "NestedKey",
+    "NonTensorData",
+    "NonTensorStack",
+    "PersistentTensorDict",
+    "TensorDict",
+    "TensorDictBase",
+    "assert_allclose_td",
+    "dense_stack_tds",
+    "is_batchedtensor",
+    "is_tensor_collection",
+    "make_tensordict",
+    "merge_tensordicts",
+    "pad",
+    "pad_sequence",
+    "tensorclass",
+]
```

## tensordict/_contextlib.py

 * *Ordering differences only*

```diff
@@ -1,156 +1,156 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-# This is a copy from https://github.com/pytorch/pytorch/blob/main/torch/utils/_contextlib.py#L120
-# We use it for compatibility with torch >= 1.10 where the implementation fails
-# for some tests in torchrl.
-
-# Extra utilities for working with context managers that should have been
-# in the standard library but are not
-
-import functools
-import inspect
-import sys
-import warnings
-from typing import Any, Callable, cast, TypeVar
-
-# Used for annotating the decorator usage of _DecoratorContextManager (e.g.,
-# 'no_grad' and 'enable_grad').
-# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators
-FuncType = Callable[..., Any]
-F = TypeVar("F", bound=FuncType)
-
-
-def _wrap_generator(ctx_factory, func):
-    """Wrap each generator invocation with the context manager factory.
-
-    The input should be a function that returns a context manager,
-    not a context manager itself, to handle one-shot context managers.
-    """
-
-    @functools.wraps(func)
-    def generator_context(*args, **kwargs):
-        gen = func(*args, **kwargs)
-
-        # Generators are suspended and unsuspended at `yield`, hence we
-        # make sure the grad mode is properly set every time the execution
-        # flow returns into the wrapped generator and restored when it
-        # returns through our `yield` to our caller (see PR #49017).
-        try:
-            # Issuing `None` to a generator fires it up
-            with ctx_factory():
-                response = gen.send(None)
-
-            while True:
-                try:
-                    # Forward the response to our caller and get its next request
-                    request = yield response
-
-                except GeneratorExit:
-                    # Inform the still active generator about its imminent closure
-                    with ctx_factory():
-                        gen.close()
-                    raise
-
-                except BaseException:
-                    # Propagate the exception thrown at us by the caller
-                    with ctx_factory():
-                        response = gen.throw(*sys.exc_info())
-
-                else:
-                    # Pass the last request to the generator and get its response
-                    with ctx_factory():
-                        response = gen.send(request)
-
-        # We let the exceptions raised above by the generator's `.throw` or
-        # `.send` methods bubble up to our caller, except for StopIteration
-        except StopIteration as e:
-            # The generator informed us that it is done: take whatever its
-            # returned value (if any) was and indicate that we're done too
-            # by returning it (see docs for python's return-statement).
-            return e.value
-
-    return generator_context
-
-
-def context_decorator(ctx, func):
-    """Like contextlib.ContextDecorator.
-
-    Except:
-
-    1. Is done by wrapping, rather than inheritance, so it works with context
-       managers that are implemented from C and thus cannot easily inherit from
-       Python classes
-    2. Wraps generators in the intuitive way (c.f. https://bugs.python.org/issue37743)
-    3. Errors out if you try to wrap a class, because it is ambiguous whether
-       or not you intended to wrap only the constructor
-
-    The input argument can either be a context manager (in which case it must
-    be a multi-shot context manager that can be directly invoked multiple times)
-    or a callable that produces a context manager.
-    """
-    assert not (callable(ctx) and hasattr(ctx, "__enter__")), (
-        f"Passed in {ctx} is both callable and also a valid context manager "
-        "(has __enter__), making it ambiguous which interface to use.  If you "
-        "intended to pass a context manager factory, rewrite your call as "
-        "context_decorator(lambda: ctx()); if you intended to pass a context "
-        "manager directly, rewrite your call as context_decorator(lambda: ctx)"
-    )
-
-    if not callable(ctx):
-
-        def ctx_factory():
-            return ctx
-
-    else:
-        ctx_factory = ctx
-
-    if inspect.isclass(func):
-        raise RuntimeError(
-            "Cannot decorate classes; it is ambiguous whether or not only the "
-            "constructor or all methods should have the context manager applied; "
-            "additionally, decorating a class at definition-site will prevent "
-            "use of the identifier as a conventional type.  "
-            "To specify which methods to decorate, decorate each of them "
-            "individually."
-        )
-
-    if inspect.isgeneratorfunction(func):
-        return _wrap_generator(ctx_factory, func)
-
-    @functools.wraps(func)
-    def decorate_context(*args, **kwargs):
-        with ctx_factory():
-            return func(*args, **kwargs)
-
-    return decorate_context
-
-
-class _DecoratorContextManager:
-    """Allows a context manager to be used as a decorator."""
-
-    def __call__(self, orig_func: F) -> F:
-        if inspect.isclass(orig_func):
-            warnings.warn(
-                "Decorating classes is deprecated and will be disabled in "
-                "future versions. You should only decorate functions or methods. "
-                "To preserve the current behavior of class decoration, you can "
-                "directly decorate the `__init__` method and nothing else."
-            )
-            func = cast(F, lambda *args, **kwargs: orig_func(*args, **kwargs))
-        else:
-            func = orig_func
-
-        return cast(F, context_decorator(self.clone, func))
-
-    def __enter__(self) -> None:
-        raise NotImplementedError
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        raise NotImplementedError
-
-    def clone(self):
-        # override this method if your children class takes __init__ parameters
-        return self.__class__()
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+# This is a copy from https://github.com/pytorch/pytorch/blob/main/torch/utils/_contextlib.py#L120
+# We use it for compatibility with torch >= 1.10 where the implementation fails
+# for some tests in torchrl.
+
+# Extra utilities for working with context managers that should have been
+# in the standard library but are not
+
+import functools
+import inspect
+import sys
+import warnings
+from typing import Any, Callable, cast, TypeVar
+
+# Used for annotating the decorator usage of _DecoratorContextManager (e.g.,
+# 'no_grad' and 'enable_grad').
+# See https://mypy.readthedocs.io/en/latest/generics.html#declaring-decorators
+FuncType = Callable[..., Any]
+F = TypeVar("F", bound=FuncType)
+
+
+def _wrap_generator(ctx_factory, func):
+    """Wrap each generator invocation with the context manager factory.
+
+    The input should be a function that returns a context manager,
+    not a context manager itself, to handle one-shot context managers.
+    """
+
+    @functools.wraps(func)
+    def generator_context(*args, **kwargs):
+        gen = func(*args, **kwargs)
+
+        # Generators are suspended and unsuspended at `yield`, hence we
+        # make sure the grad mode is properly set every time the execution
+        # flow returns into the wrapped generator and restored when it
+        # returns through our `yield` to our caller (see PR #49017).
+        try:
+            # Issuing `None` to a generator fires it up
+            with ctx_factory():
+                response = gen.send(None)
+
+            while True:
+                try:
+                    # Forward the response to our caller and get its next request
+                    request = yield response
+
+                except GeneratorExit:
+                    # Inform the still active generator about its imminent closure
+                    with ctx_factory():
+                        gen.close()
+                    raise
+
+                except BaseException:
+                    # Propagate the exception thrown at us by the caller
+                    with ctx_factory():
+                        response = gen.throw(*sys.exc_info())
+
+                else:
+                    # Pass the last request to the generator and get its response
+                    with ctx_factory():
+                        response = gen.send(request)
+
+        # We let the exceptions raised above by the generator's `.throw` or
+        # `.send` methods bubble up to our caller, except for StopIteration
+        except StopIteration as e:
+            # The generator informed us that it is done: take whatever its
+            # returned value (if any) was and indicate that we're done too
+            # by returning it (see docs for python's return-statement).
+            return e.value
+
+    return generator_context
+
+
+def context_decorator(ctx, func):
+    """Like contextlib.ContextDecorator.
+
+    Except:
+
+    1. Is done by wrapping, rather than inheritance, so it works with context
+       managers that are implemented from C and thus cannot easily inherit from
+       Python classes
+    2. Wraps generators in the intuitive way (c.f. https://bugs.python.org/issue37743)
+    3. Errors out if you try to wrap a class, because it is ambiguous whether
+       or not you intended to wrap only the constructor
+
+    The input argument can either be a context manager (in which case it must
+    be a multi-shot context manager that can be directly invoked multiple times)
+    or a callable that produces a context manager.
+    """
+    assert not (callable(ctx) and hasattr(ctx, "__enter__")), (
+        f"Passed in {ctx} is both callable and also a valid context manager "
+        "(has __enter__), making it ambiguous which interface to use.  If you "
+        "intended to pass a context manager factory, rewrite your call as "
+        "context_decorator(lambda: ctx()); if you intended to pass a context "
+        "manager directly, rewrite your call as context_decorator(lambda: ctx)"
+    )
+
+    if not callable(ctx):
+
+        def ctx_factory():
+            return ctx
+
+    else:
+        ctx_factory = ctx
+
+    if inspect.isclass(func):
+        raise RuntimeError(
+            "Cannot decorate classes; it is ambiguous whether or not only the "
+            "constructor or all methods should have the context manager applied; "
+            "additionally, decorating a class at definition-site will prevent "
+            "use of the identifier as a conventional type.  "
+            "To specify which methods to decorate, decorate each of them "
+            "individually."
+        )
+
+    if inspect.isgeneratorfunction(func):
+        return _wrap_generator(ctx_factory, func)
+
+    @functools.wraps(func)
+    def decorate_context(*args, **kwargs):
+        with ctx_factory():
+            return func(*args, **kwargs)
+
+    return decorate_context
+
+
+class _DecoratorContextManager:
+    """Allows a context manager to be used as a decorator."""
+
+    def __call__(self, orig_func: F) -> F:
+        if inspect.isclass(orig_func):
+            warnings.warn(
+                "Decorating classes is deprecated and will be disabled in "
+                "future versions. You should only decorate functions or methods. "
+                "To preserve the current behavior of class decoration, you can "
+                "directly decorate the `__init__` method and nothing else."
+            )
+            func = cast(F, lambda *args, **kwargs: orig_func(*args, **kwargs))
+        else:
+            func = orig_func
+
+        return cast(F, context_decorator(self.clone, func))
+
+    def __enter__(self) -> None:
+        raise NotImplementedError
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        raise NotImplementedError
+
+    def clone(self):
+        # override this method if your children class takes __init__ parameters
+        return self.__class__()
```

## tensordict/_lazy.py

 * *Ordering differences only*

```diff
@@ -1,3643 +1,3643 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import json
-import numbers
-import os
-import re
-import textwrap
-import weakref
-from collections import defaultdict
-from copy import copy, deepcopy
-from functools import wraps
-from pathlib import Path
-from textwrap import indent
-from typing import Any, Callable, Iterator, OrderedDict, Sequence, Tuple, Type
-
-import numpy as np
-import torch
-import torch.distributed as dist
-
-from tensordict.memmap import MemoryMappedTensor
-
-try:
-    from functorch import dim as ftdim
-
-    _has_funcdim = True
-except ImportError:
-    from tensordict.utils import _ftdim_mock as ftdim
-
-    _has_funcdim = False
-from tensordict._td import _SubTensorDict, _TensorDictKeysView, TensorDict
-from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
-from tensordict.base import (
-    _is_tensor_collection,
-    _NESTED_TENSORS_AS_LISTS,
-    _register_tensor_class,
-    BEST_ATTEMPT_INPLACE,
-    CompatibleType,
-    is_tensor_collection,
-    NO_DEFAULT,
-    T,
-    TensorDictBase,
-)
-from tensordict.utils import (
-    _broadcast_tensors,
-    _get_shape_from_args,
-    _getitem_batch_size,
-    _is_number,
-    _parse_to,
-    _renamed_inplace_method,
-    _shape,
-    _td_fields,
-    as_decorator,
-    cache,
-    convert_ellipsis_to_idx,
-    DeviceType,
-    erase_cache,
-    expand_right,
-    IndexType,
-    infer_size_impl,
-    is_non_tensor,
-    is_tensorclass,
-    KeyedJaggedTensor,
-    lock_blocked,
-    NestedKey,
-)
-from torch import Tensor
-
-
-_has_functorch = False
-try:
-    try:
-        from torch._C._functorch import (
-            _add_batch_dim,
-            _remove_batch_dim,
-            is_batchedtensor,
-        )
-    except ImportError:
-        from functorch._C import is_batchedtensor
-
-    _has_functorch = True
-except ImportError:
-    _has_functorch = False
-
-    def is_batchedtensor(tensor: Tensor) -> bool:
-        """Placeholder for the functorch function."""
-        return False
-
-
-class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
-    tensordict: LazyStackedTensorDict
-
-    def __len__(self) -> int:
-        return len(self._keys())
-
-    def _keys(self) -> list[str]:
-        result = self.tensordict._key_list()
-        if self.is_leaf is _NESTED_TENSORS_AS_LISTS:
-            return [
-                (key, str(i))
-                for key in result
-                for i in range(len(self.tensordict.tensordicts))
-            ]
-        return result
-
-    def __contains__(self, item):
-        item = _unravel_key_to_tuple(item)
-        if item[0] in self.tensordict._iterate_over_keys():
-            if self.leaves_only:
-                return not _is_tensor_collection(self.tensordict.entry_class(item[0]))
-            has_first_key = True
-        else:
-            has_first_key = False
-        if not has_first_key or len(item) == 1:
-            return has_first_key
-        # otherwise take the long way
-        return all(
-            item[1:]
-            in tensordict.get(item[0]).keys(self.include_nested, self.leaves_only)
-            for tensordict in self.tensordict.tensordicts
-        )
-
-
-def _fails_exclusive_keys(func):
-    @wraps(func)
-    def newfunc(self, *args, **kwargs):
-        if self._has_exclusive_keys:
-            raise RuntimeError(
-                f"the method {func.__name__} cannot complete when there are exclusive keys."
-            )
-        parent_func = getattr(TensorDictBase, func.__name__, None)
-        if parent_func is None:
-            parent_func = getattr(TensorDict, func.__name__)
-        return parent_func(self, *args, **kwargs)
-
-    return newfunc
-
-
-class LazyStackedTensorDict(TensorDictBase):
-    """A Lazy stack of TensorDicts.
-
-    When stacking TensorDicts together, the default behaviour is to put them
-    in a stack that is not instantiated.
-    This allows to seamlessly work with stacks of tensordicts with operations
-    that will affect the original tensordicts.
-
-    Args:
-         *tensordicts (TensorDict instances): a list of tensordict with
-            same batch size.
-         stack_dim (int): a dimension (between `-td.ndimension()` and
-            `td.ndimension()-1` along which the stack should be performed.
-         hook_out (callable, optional): a callable to execute after :meth:`~.get`.
-         hook_in (callable, optional): a callable to execute before :meth:`~.set`.
-         stack_dim_name (str, optional): the name of the stack dimension.
-            Defaults to ``None``.
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
-        ...     for _ in range(10)]
-        >>> td_stack = torch.stack(tds, -1)
-        >>> print(td_stack.shape)
-        torch.Size([3, 10])
-        >>> print(td_stack.get("a").shape)
-        torch.Size([3, 10, 4])
-        >>> print(td_stack[:, 0] is tds[0])
-        True
-
-    """
-
-    _is_vmapped: bool = False
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        from tensordict._torch_func import LAZY_TD_HANDLED_FUNCTIONS
-
-        if func in LAZY_TD_HANDLED_FUNCTIONS:
-            if kwargs is None:
-                kwargs = {}
-            if func not in LAZY_TD_HANDLED_FUNCTIONS or not all(
-                issubclass(t, (Tensor, TensorDictBase)) for t in types
-            ):
-                return NotImplemented
-            return LAZY_TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-        else:
-            return super().__torch_function__(func, types, args, kwargs)
-
-    _td_dim_name = None
-    _safe = False
-    _lazy = True
-
-    def __init__(
-        self,
-        *tensordicts: T,
-        stack_dim: int = 0,
-        hook_out: callable | None = None,
-        hook_in: callable | None = None,
-        batch_size: Sequence[int] | None = None,  # TODO: remove
-        stack_dim_name: str | None = None,
-    ) -> None:
-        self._is_locked = None
-
-        # sanity check
-        N = len(tensordicts)
-        if not N:
-            raise RuntimeError(
-                "at least one tensordict must be provided to "
-                "StackedTensorDict to be instantiated"
-            )
-        if stack_dim < 0:
-            raise RuntimeError(
-                f"stack_dim must be non negative, got stack_dim={stack_dim}"
-            )
-        _batch_size = tensordicts[0].batch_size
-        device = tensordicts[0].device
-        if stack_dim > len(_batch_size):
-            raise RuntimeError(
-                f"Stack dim {stack_dim} is too big for batch size {_batch_size}."
-            )
-
-        for td in tensordicts[1:]:
-            if not is_tensor_collection(td):
-                raise TypeError(
-                    "Expected all inputs to be TensorDictBase instances but got "
-                    f"{type(td)} instead."
-                )
-            _bs = td.batch_size
-            _device = td.device
-            if device != _device:
-                raise RuntimeError(f"devices differ, got {device} and {_device}")
-            if _bs != _batch_size:
-                raise RuntimeError(
-                    f"batch sizes in tensordicts differs, StackedTensorDict "
-                    f"cannot be created. Got td[0].batch_size={_batch_size} "
-                    f"and td[i].batch_size={_bs} "
-                )
-        self.tensordicts: list[TensorDictBase] = list(tensordicts)
-        self.stack_dim = stack_dim
-        self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
-        self.hook_out = hook_out
-        self.hook_in = hook_in
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-        if stack_dim_name is not None:
-            self._td_dim_name = stack_dim_name
-
-    # These attributes should never be set
-    @property
-    def _is_shared(self):
-        return all(td._is_shared for td in self.tensordicts)
-
-    @property
-    def _is_memmap(self):
-        return all(td._is_memmap for td in self.tensordicts)
-
-    @property
-    @cache  # noqa: B019
-    def _has_exclusive_keys(self):
-        keys = None
-        for td in self.tensordicts:
-            _keys = set(td.keys(True, True))
-            if keys is None:
-                keys = _keys
-            else:
-                if keys != _keys:
-                    return True
-        else:
-            return False
-
-    @_fails_exclusive_keys
-    def to_dict(self) -> dict[str, Any]:
-        ...
-
-    @_fails_exclusive_keys
-    def state_dict(
-        self,
-        destination=None,
-        prefix="",
-        keep_vars=False,
-        flatten=False,
-    ) -> OrderedDict[str, Any]:
-        ...
-
-    @_fails_exclusive_keys
-    def flatten_keys(
-        self,
-        separator: str = ".",
-        inplace: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> T:
-        ...
-
-    @_fails_exclusive_keys
-    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
-        ...
-
-    @property
-    def device(self) -> torch.device | None:
-        # devices might have changed, so we check that they're all the same
-        device_set = {td.device for td in self.tensordicts}
-        if len(device_set) != 1:
-            return None
-        device = self.tensordicts[0].device
-        return device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        for t in self.tensordicts:
-            t.device = value
-
-    def clear_device_(self) -> T:
-        for td in self.tensordicts:
-            td.clear_device_()
-        return self
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        return self._batch_size_setter(new_size)
-
-    @property
-    @cache  # noqa
-    def names(self):
-        names = list(self.tensordicts[0].names)
-        for td in self.tensordicts[1:]:
-            if names != td.names:
-                raise ValueError(
-                    f"Not all dim names match, got {names} and {td.names}."
-                )
-        names.insert(self.stack_dim, self._td_dim_name)
-        return names
-
-    @names.setter
-    @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
-    def names(self, value):
-        if value is None:
-            for td in self.tensordicts:
-                td.names = None
-            self._td_dim_name = None
-        else:
-            names_c = list(value)
-            name = names_c[self.stack_dim]
-            self._td_dim_name = name
-            del names_c[self.stack_dim]
-            for td in self.tensordicts:
-                if td._check_dim_name(name):
-                    # TODO: should reset names here
-                    raise ValueError(f"The dimension name {name} is already taken.")
-                td.rename_(*names_c)
-
-    def _rename_subtds(self, names):
-        # remove the name of the stack dim
-        names = list(names)
-        del names[self.stack_dim]
-        for td in self.tensordicts:
-            td.names = names
-
-    def _has_names(self):
-        return all(td._has_names() for td in self.tensordicts)
-
-    def _erase_names(self):
-        self._td_dim_name = None
-        for td in self.tensordicts:
-            td._erase_names()
-
-    def get_item_shape(self, key):
-        """Gets the shape of an item in the lazy stack.
-
-        Heterogeneous dimensions are returned as -1.
-
-        This implementation is inefficient as it will attempt to stack the items
-        to compute their shape, and should only be used for printing.
-        """
-        try:
-            item = self.get(key)
-            return item.shape
-        except RuntimeError as err:
-            if re.match(
-                r"Found more than one unique shape in the tensors|Could not run 'aten::stack' with arguments from the",
-                str(err),
-            ):
-                shape = None
-                for td in self.tensordicts:
-                    if shape is None:
-                        shape = list(td.get_item_shape(key))
-                    else:
-                        _shape = td.get_item_shape(key)
-                        if len(shape) != len(_shape):
-                            shape = [-1]
-                            return torch.Size(shape)
-                        shape = [
-                            s1 if s1 == s2 else -1 for (s1, s2) in zip(shape, _shape)
-                        ]
-                shape.insert(self.stack_dim, len(self.tensordicts))
-                return torch.Size(shape)
-            else:
-                raise err
-
-    def is_shared(self) -> bool:
-        are_shared = [td.is_shared() for td in self.tensordicts]
-        are_shared = [value for value in are_shared if value is not None]
-        if not len(are_shared):
-            return None
-        if any(are_shared) and not all(are_shared):
-            raise RuntimeError(
-                f"tensordicts shared status mismatch, got {sum(are_shared)} "
-                f"shared tensordicts and "
-                f"{len(are_shared) - sum(are_shared)} non shared tensordict "
-            )
-        return all(are_shared)
-
-    def is_memmap(self) -> bool:
-        are_memmap = [td.is_memmap() for td in self.tensordicts]
-        if any(are_memmap) and not all(are_memmap):
-            raise RuntimeError(
-                f"tensordicts memmap status mismatch, got {sum(are_memmap)} "
-                f"memmap tensordicts and "
-                f"{len(are_memmap) - sum(are_memmap)} non memmap tensordict "
-            )
-        return all(are_memmap)
-
-    @staticmethod
-    def _compute_batch_size(
-        batch_size: torch.Size, stack_dim: int, N: int
-    ) -> torch.Size:
-        s = list(batch_size)
-        s.insert(stack_dim, N)
-        return torch.Size(s)
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ) -> T:
-        try:
-            inplace = self._convert_inplace(inplace, key)
-        except KeyError as e:
-            raise KeyError(
-                "setting a value in-place on a stack of TensorDict is only "
-                "permitted if all members of the stack have this key in "
-                "their register."
-            ) from e
-        if not validated:
-            value = self._validate_value(value)
-            validated = True
-        if self._is_vmapped:
-            value = self.hook_in(value)
-        values = value.unbind(self.stack_dim)
-        for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_str(
-                key,
-                item,
-                inplace=inplace,
-                validated=validated,
-                ignore_lock=ignore_lock,
-                non_blocking=non_blocking,
-            )
-        return self
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        non_blocking: bool = False,
-    ) -> T:
-        if len(key) == 1:
-            return self._set_str(
-                key[0],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        # if inplace is not False:  # inplace could be None
-        #     # we don't want to end up in the situation where one tensordict has
-        #     # inplace=True and another one inplace=False because inplace was loose.
-        #     # Worse could be writing with inplace=True up until some level then to
-        #     # realize the key is missing in one td, raising an exception and having
-        #     # messed up the data. Hence we must start by checking if the key
-        #     # is present.
-        #     has_key = key in self.keys(True)
-        #     if inplace is True and not has_key:  # inplace could be None
-        #         raise KeyError(
-        #             TensorDictBase.KEY_ERROR.format(
-        #                 key, self.__class__.__name__, sorted(self.keys())
-        #             )
-        #         )
-        #     inplace = has_key
-        if not validated:
-            value = self._validate_value(value)
-            validated = True
-        if self._is_vmapped:
-            value = self.hook_in(value)
-        values = value.unbind(self.stack_dim)
-        for tensordict, item in zip(self.tensordicts, values):
-            tensordict._set_tuple(
-                key,
-                item,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        return self
-
-    def _split_index(self, index):
-        """Given a tuple index, split it in as many indices as the number of tensordicts.
-
-        Returns:
-            a dictionary with {index-of-td: index-within-td}
-            the number of single dim indices until stack dim
-            a boolean indicating if the index along the stack dim is an integer
-        """
-        if not isinstance(index, tuple):
-            index = (index,)
-        index = convert_ellipsis_to_idx(index, self.batch_size)
-        index = _broadcast_tensors(index)
-        out = []
-        num_single = 0
-        num_none = 0
-        isinteger = False
-        is_nd_tensor = False
-        cursor = 0  # the dimension cursor
-        selected_td_idx = torch.arange(len(self.tensordicts))
-        has_bool = False
-        num_squash = 0
-        encountered_tensor = False
-        for i, idx in enumerate(index):  # noqa: B007
-            cursor_incr = 1
-            if idx is None:
-                out.append(None)
-                num_none += cursor <= self.stack_dim
-                continue
-            if cursor == self.stack_dim:
-                # we need to check which tds need to be indexed
-                if isinstance(idx, ftdim.Dim):
-                    raise ValueError(
-                        "Cannot index a lazy stacked tensordict along the stack dimension with "
-                        "a first-class dimension index. Consider consolidating the tensordict first "
-                        "using `tensordict.contiguous()`."
-                    )
-                elif isinstance(idx, slice) or _is_number(idx):
-                    selected_td_idx = range(len(self.tensordicts))[idx]
-                    if not isinstance(selected_td_idx, range):
-                        isinteger = True
-                        selected_td_idx = [selected_td_idx]
-                elif isinstance(idx, torch.Tensor):
-                    if idx.dtype == torch.bool:
-                        # we mark that we need to dispatch the indices across stack idx
-                        has_bool = True
-                        # split mask along dim
-                        individual_masks = idx = idx.unbind(0)
-                        selected_td_idx = range(len(self.tensordicts))
-                        out.append(idx)
-                        split_dim = self.stack_dim - num_single
-                        mask_loc = i
-                    else:
-                        is_nd_tensor = True
-                        if not encountered_tensor:
-                            # num_single -= idx.ndim - 1
-                            encountered_tensor = True
-                        else:
-                            num_single += 1
-                        selected_td_idx = idx
-                        # out.append(idx.unbind(0))
-                else:
-                    raise TypeError(f"Invalid index type: {type(idx)}.")
-            else:
-                if _is_number(idx) and cursor < self.stack_dim:
-                    num_single += 1
-                if _is_number(idx) or isinstance(
-                    idx,
-                    (
-                        ftdim.Dim,
-                        slice,
-                    ),
-                ):
-                    out.append(idx)
-                elif isinstance(idx, torch.Tensor):
-                    if idx.dtype == torch.bool:
-                        cursor_incr = idx.ndim
-                        if cursor < self.stack_dim:
-                            num_squash += cursor_incr - 1
-                        if (
-                            cursor < self.stack_dim
-                            and cursor + cursor_incr > self.stack_dim
-                        ):
-                            # we mark that we need to dispatch the indices across stack idx
-                            has_bool = True
-                            # split mask along dim
-                            # relative_stack_dim = self.stack_dim - cursor - cursor_incr
-                            individual_masks = idx = idx.unbind(0)
-                            selected_td_idx = range(self.shape[i])
-                            split_dim = cursor - num_single
-                            mask_loc = i
-                    elif cursor < self.stack_dim:
-                        # we know idx is not a single integer, so it must have
-                        # a dimension. We play with num_single, reducing it
-                        # by the number of dims of idx: if idx has 3 dims, our
-                        # indexed tensor will have 2 more dimensions, going in
-                        # the opposite direction of indexing with a single integer,
-                        # smth[torch.tensor(1)].ndim = smth.ndim-1
-                        # smth[torch.tensor([1])].ndim = smth.ndim
-                        # smth[torch.tensor([[1]])].ndim = smth.ndim+1
-                        if not encountered_tensor:
-                            num_single -= idx.ndim - 1
-                            encountered_tensor = True
-                        else:
-                            num_single += 1
-                    out.append(idx)
-                else:
-                    raise TypeError(f"Invalid index type: {type(idx)}.")
-            cursor += cursor_incr
-        if has_bool:
-            out = tuple(
-                tuple(idx if not isinstance(idx, tuple) else idx[i] for idx in out)
-                for i in selected_td_idx
-            )
-            return {
-                "index_dict": {i: out[i] for i in selected_td_idx},
-                "num_single": num_single,
-                "isinteger": isinteger,
-                "has_bool": has_bool,
-                "individual_masks": individual_masks,
-                "split_dim": split_dim,
-                "mask_loc": mask_loc,
-                "is_nd_tensor": is_nd_tensor,
-                "num_none": num_none,
-                "num_squash": num_squash,
-            }
-        elif is_nd_tensor:
-
-            def isindexable(idx):
-                if isinstance(idx, torch.Tensor):
-                    if idx.dtype == torch.bool:
-                        return False
-                    return True
-                if isinstance(idx, (tuple, list, range)):
-                    return True
-                return False
-
-            def outer_list(tensor_index, tuple_index):
-                """Converts a tensor and a tuple to a nested list where each leaf is a (int, index) tuple where the index only points to one element."""
-                if isinstance(tensor_index, torch.Tensor):
-                    list_index = tensor_index.tolist()
-                else:
-                    list_index = tensor_index
-                list_result = []
-
-                def index_tuple_index(i, convert=False):
-                    for idx in tuple_index:
-                        if isindexable(idx):
-                            if convert:
-                                yield int(idx[i])
-                            else:
-                                yield idx[i]
-                        else:
-                            yield idx
-
-                for i, idx in enumerate(list_index):
-                    if isinstance(idx, int):
-                        list_result.append(
-                            (idx, tuple(index_tuple_index(i, convert=True)))
-                        )
-                    elif isinstance(idx, list):
-                        list_result.append(outer_list(idx, tuple(index_tuple_index(i))))
-                    else:
-                        raise NotImplementedError
-                return list_result
-
-            return {
-                "index_dict": outer_list(selected_td_idx, out),
-                "num_single": num_single,
-                "isinteger": isinteger,
-                "has_bool": has_bool,
-                "is_nd_tensor": is_nd_tensor,
-                "num_none": num_none,
-                "num_squash": num_squash,
-            }
-        return {
-            "index_dict": {i: tuple(out) for i in selected_td_idx},
-            "num_single": num_single,
-            "isinteger": isinteger,
-            "has_bool": has_bool,
-            "is_nd_tensor": is_nd_tensor,
-            "num_none": num_none,
-            "num_squash": num_squash,
-        }
-
-    def _set_at_str(self, key, value, index, *, validated, non_blocking: bool):
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if self._is_vmapped:
-            value = self.hook_in(value)
-        split_index = self._split_index(index)
-        converted_idx = split_index["index_dict"]
-        num_single = split_index["num_single"]
-        isinteger = split_index["isinteger"]
-        has_bool = split_index["has_bool"]
-        num_squash = split_index.get("num_squash", 0)
-        num_none = split_index.get("num_none", 0)
-        is_nd_tensor = split_index.get("is_nd_tensor", False)
-        if isinteger:
-            # this will break if the index along the stack dim is [0] or :1 or smth
-            for i, _idx in converted_idx.items():
-                self.tensordicts[i]._set_at_str(
-                    key, value, _idx, validated=validated, non_blocking=non_blocking
-                )
-            return self
-        if is_nd_tensor:
-            unbind_dim = self.stack_dim - num_single + num_none - num_squash
-            value_unbind = value.unbind(unbind_dim)
-
-            def set_at_str(converted_idx):
-                for i, item in enumerate(converted_idx):
-                    if isinstance(item, list):
-                        set_at_str(item)
-                    else:
-                        _value = value_unbind[i]
-                        stack_idx, idx = item
-                        self.tensordicts[stack_idx]._set_at_str(
-                            key,
-                            _value,
-                            idx,
-                            validated=validated,
-                            non_blocking=non_blocking,
-                        )
-
-            set_at_str(converted_idx)
-            return self
-        elif not has_bool:
-            unbind_dim = self.stack_dim - num_single + num_none - num_squash
-            value_unbind = value.unbind(unbind_dim)
-            for (i, _idx), _value in zip(
-                converted_idx.items(),
-                value_unbind,
-            ):
-                self.tensordicts[i]._set_at_str(
-                    key, _value, _idx, validated=validated, non_blocking=non_blocking
-                )
-        else:
-            # we must split, not unbind
-            mask_unbind = split_index["individual_masks"]
-            split_dim = split_index["split_dim"]
-            splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
-            value_unbind = value.split(splits, split_dim)
-            if mask_unbind[0].ndim == 0:
-                # we can return a stack
-                for (i, _idx), mask, _value in zip(
-                    converted_idx.items(),
-                    mask_unbind,
-                    value_unbind,
-                ):
-                    if mask.any():
-                        self.tensordicts[i]._set_at_str(
-                            key,
-                            _value,
-                            _idx,
-                            validated=validated,
-                            non_blocking=non_blocking,
-                        )
-            else:
-                for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
-                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                    self[self_idx]._set_at_str(
-                        key,
-                        _value,
-                        _idx,
-                        validated=validated,
-                        non_blocking=non_blocking,
-                    )
-
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
-        if len(key) == 1:
-            return self._set_at_str(
-                key[0], value, idx, validated=validated, non_blocking=non_blocking
-            )
-        # get the "last" tds
-        tds = []
-        for td in self.tensordicts:
-            tds.append(td.get(key[:-1]))
-        # build only a single lazy stack from it
-        # (if the stack is a stack of stacks this won't be awesomely efficient
-        # but then we'd need to splut the value (which we can do) and recompute
-        # the sub-index for each td, which is a different story!
-        td = LazyStackedTensorDict(
-            *tds, stack_dim=self.stack_dim, hook_out=self.hook_out, hook_in=self.hook_in
-        )
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if self._is_vmapped:
-            value = self.hook_in(value)
-        item = td._get_str(key, NO_DEFAULT)
-        item[idx] = value
-        td._set_str(key, item, inplace=True, validated=True, non_blocking=non_blocking)
-        return self
-
-    def _legacy_unsqueeze(self, dim: int) -> T:
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-
-        if (dim > self.batch_dims) or (dim < 0):
-            raise RuntimeError(
-                f"unsqueezing is allowed for dims comprised between "
-                f"`-td.batch_dims` and `td.batch_dims` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        if dim <= self.stack_dim:
-            stack_dim = self.stack_dim + 1
-        else:
-            dim = dim - 1
-            stack_dim = self.stack_dim
-        return type(self)(
-            *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
-            stack_dim=stack_dim,
-            stack_dim_name=self._td_dim_name,
-        )
-
-    def _legacy_squeeze(self, dim: int | None = None) -> T:
-        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
-
-        Args:
-            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
-
-        """
-        if dim is None:
-            size = self.size()
-            if len(self.size()) == 1 or size.count(1) == 0:
-                return self
-            first_singleton_dim = size.index(1)
-            return self.squeeze(first_singleton_dim).squeeze()
-
-        if dim < 0:
-            dim = self.batch_dims + dim
-
-        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
-            raise RuntimeError(
-                f"squeezing is allowed for dims comprised between 0 and "
-                f"td.batch_dims only. Got dim={dim} and batch_size"
-                f"={self.batch_size}."
-            )
-
-        if dim >= self.batch_dims or self.batch_size[dim] != 1:
-            return self
-        if dim == self.stack_dim:
-            return self.tensordicts[0]
-        elif dim < self.stack_dim:
-            stack_dim = self.stack_dim - 1
-        else:
-            dim = dim - 1
-            stack_dim = self.stack_dim
-        return type(self)(
-            *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
-            stack_dim=stack_dim,
-            stack_dim_name=self._td_dim_name,
-        )
-
-    def _unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
-        if dim == self.stack_dim:
-            return tuple(self.tensordicts)
-        else:
-            # return a stack of unbound tensordicts
-            out = []
-            new_dim = dim if dim < self.stack_dim else dim - 1
-            new_stack_dim = (
-                self.stack_dim if dim > self.stack_dim else self.stack_dim - 1
-            )
-            for td in self.tensordicts:
-                out.append(td._unbind(new_dim))
-            return tuple(self.lazy_stack(vals, new_stack_dim) for vals in zip(*out))
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        if dim == self.stack_dim:
-            for source, tensordict_dest in zip(list_item, self.tensordicts):
-                tensordict_dest.update_(source)
-        else:
-            for i, td in enumerate(list_item):
-                idx = (slice(None),) * dim + (i,)
-                self.update_at_(td, idx)
-        return self
-
-    def _maybe_get_list(self, key):
-        vals = []
-        for td in self.tensordicts:
-            if isinstance(td, LazyStackedTensorDict):
-                val = td._maybe_get_list(key)
-            else:
-                val = td._get_str(key, None)
-                if _is_tensor_collection(type(val)):
-                    return self._get_str(key, NO_DEFAULT)
-                elif val is None:
-                    return None
-            vals.append(val)
-        return vals
-
-    @cache  # noqa: B019
-    def _get_str(
-        self,
-        key: NestedKey,
-        default: Any = NO_DEFAULT,
-    ) -> CompatibleType:
-        # we can handle the case where the key is a tuple of length 1
-        tensors = []
-        for td in self.tensordicts:
-            tensors.append(td._get_str(key, default=default))
-            if (
-                tensors[-1] is default
-                and not isinstance(default, (KeyedJaggedTensor, torch.Tensor))
-                and not is_tensor_collection(default)
-            ):
-                # then we consider this default as non-stackable and return prematurly
-                return default
-        try:
-            out = self.lazy_stack(
-                tensors, self.stack_dim, stack_dim_name=self._td_dim_name
-            )
-            if _is_tensor_collection(out.__class__):
-                if isinstance(out, LazyStackedTensorDict):
-                    # then it's a LazyStackedTD
-                    out.hook_out = self.hook_out
-                    out.hook_in = self.hook_in
-                    out._is_vmapped = self._is_vmapped
-                    incr = 0 if not self._is_vmapped else 1
-                    out._batch_size = (
-                        self._batch_size
-                        + out.batch_size[(len(self._batch_size) + incr) :]
-                    )
-                elif is_tensorclass(out):
-                    # then it's a tensorclass
-                    out._tensordict.hook_out = self.hook_out
-                    out._tensordict.hook_in = self.hook_in
-                    out._tensordict._is_vmapped = self._is_vmapped
-                    incr = 0 if not self._is_vmapped else 1
-                    out._tensordict._batch_size = (
-                        self._batch_size
-                        + out._tensordict.batch_size[(len(self._batch_size) + incr) :]
-                    )
-                else:
-                    raise RuntimeError
-            elif self.hook_out is not None:
-                out = self.hook_out(out)
-            return out
-        except RuntimeError as err:
-            if "stack expects each tensor to be equal size" in str(err):
-                shapes = {_shape(tensor) for tensor in tensors}
-                raise RuntimeError(
-                    f"Found more than one unique shape in the tensors to be "
-                    f"stacked ({shapes}). This is likely due to a modification "
-                    f"of one of the stacked TensorDicts, where a key has been "
-                    f"updated/created with an uncompatible shape. If the entries "
-                    f"are intended to have a different shape, use the get_nestedtensor "
-                    f"method instead."
-                )
-            else:
-                raise err
-
-    def _get_tuple(self, key, default):
-        first = self._get_str(key[0], None)
-        if first is None:
-            return self._default_get(key[0], default)
-        if len(key) == 1:
-            return first
-        try:
-            if isinstance(first, KeyedJaggedTensor):
-                if len(key) != 2:
-                    raise ValueError(f"Got too many keys for a KJT: {key}.")
-                return first[key[-1]]
-            else:
-                return first._get_tuple(key[1:], default=default)
-        except AttributeError as err:
-            if "has no attribute" in str(err):
-                raise ValueError(
-                    f"Expected a TensorDictBase instance but got {type(first)} instead"
-                    f" for key '{key[1:]}' in tensordict:\n{self}."
-                )
-
-    @classmethod
-    def lazy_stack(
-        cls,
-        items: Sequence[TensorDictBase],
-        dim: int = 0,
-        *,
-        device: DeviceType | None = None,
-        out: T | None = None,
-        stack_dim_name: str | None = None,
-    ) -> T:
-        """Stacks tensordicts in a LazyStackedTensorDict."""
-        if not items:
-            raise RuntimeError("items cannot be empty")
-
-        if all(isinstance(item, torch.Tensor) for item in items):
-            return torch.stack(items, dim=dim, out=out)
-        if all(is_non_tensor(tensordict) for tensordict in items):
-            # Non-tensor data (Data or Stack) are stacked using NonTensorStack
-            # If the content is identical (not equal but same id) this does not
-            # require additional memory.
-            from .tensorclass import NonTensorStack
-
-            return NonTensorStack(*items, stack_dim=dim)
-        if all(
-            is_tensorclass(item) and type(item) == type(items[0])  # noqa: E721
-            for item in items
-        ):
-            lazy_stack = cls.lazy_stack(
-                [item._tensordict for item in items],
-                dim=dim,
-                out=out,
-                stack_dim_name=stack_dim_name,
-            )
-            # we take the first non_tensordict by convention
-            return type(items[0])._from_tensordict(
-                tensordict=lazy_stack, non_tensordict=items[0]._non_tensordict
-            )
-
-        batch_size = items[0].batch_size
-        if dim < 0:
-            dim = len(batch_size) + dim + 1
-
-        for td in items[1:]:
-            if td.batch_size != items[0].batch_size:
-                raise RuntimeError(
-                    "stacking tensordicts requires them to have congruent batch sizes, "
-                    f"got td1.batch_size={td.batch_size} and td2.batch_size="
-                    f"{items[0].batch_size}"
-                )
-
-        if out is None:
-            # We need to handle tensordicts with exclusive keys and tensordicts with
-            # mismatching shapes.
-            # The first case is handled within _check_keys which fails if keys
-            # don't match exactly.
-            # The second requires a check over the tensor shapes.
-            return LazyStackedTensorDict(
-                *items, stack_dim=dim, stack_dim_name=stack_dim_name
-            )
-        else:
-            batch_size = list(batch_size)
-            batch_size.insert(dim, len(items))
-            batch_size = torch.Size(batch_size)
-
-            if out.batch_size != batch_size:
-                raise RuntimeError(
-                    "out.batch_size and stacked batch size must match, "
-                    f"got out.batch_size={out.batch_size} and batch_size"
-                    f"={batch_size}"
-                )
-
-            try:
-                out._stack_onto_(items, dim)
-            except KeyError as err:
-                raise err
-        return out
-
-    @classmethod
-    def maybe_dense_stack(
-        cls,
-        items: Sequence[TensorDictBase],
-        dim: int = 0,
-        out: T | None = None,
-        strict: bool = False,
-    ) -> T:
-        """Stacks tensors or tensordicts densly if possible, or onto a LazyStackedTensorDict otherwise.
-
-        Examples:
-            >>> td0 = TensorDict({"a": 0}, [])
-            >>> td1 = TensorDict({"b": 0}, [])
-            >>> LazyStackedTensorDict.maybe_dense_stack([td0, td0])  # returns a TensorDict with shape [2]
-            >>> LazyStackedTensorDict.maybe_dense_stack([td0, td1])  # returns a LazyStackedTensorDict with shape [2]
-            >>> LazyStackedTensorDict.maybe_dense_stack(list(torch.randn(2)))  # returns a torch.Tensor with shape [2]
-        """
-        from ._torch_func import _stack
-
-        return _stack(items, dim=dim, out=out, strict=strict, maybe_dense_stack=True)
-
-    @cache  # noqa: B019
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        if self.is_memmap():
-            td = LazyStackedTensorDict.lazy_stack(
-                [td.cpu().as_tensor() for td in self.tensordicts], 0
-            )
-        else:
-            td = self
-        if in_dim < 0:
-            in_dim = self.ndim + in_dim
-        if in_dim == self.stack_dim:
-            result = self._cached_add_batch_dims(
-                td, in_dim=in_dim, vmap_level=vmap_level
-            )
-        else:
-            if in_dim < td.stack_dim:
-                # then we'll stack along a dim before
-                stack_dim = td.stack_dim - 1
-            else:
-                in_dim = in_dim - 1
-                stack_dim = td.stack_dim
-            tds = [
-                td._fast_apply(
-                    lambda _arg: _add_batch_dim(_arg, in_dim, vmap_level),
-                    batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
-                    names=[name for i, name in enumerate(td.names) if i != in_dim],
-                )
-                for td in td.tensordicts
-            ]
-            result = LazyStackedTensorDict(*tds, stack_dim=stack_dim)
-        if self.is_locked:
-            result.lock_()
-        return result
-
-    @classmethod
-    def _cached_add_batch_dims(cls, td, in_dim, vmap_level):
-        # we return a stack with hook_out, and hack the batch_size and names
-        # Per se it is still a LazyStack but the stacking dim is "hidden" from
-        # the outside
-        out = td.copy()
-
-        def hook_out(tensor, in_dim=in_dim, vmap_level=vmap_level):
-            if _is_tensor_collection(type(tensor)):
-                return tensor._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
-            return _add_batch_dim(tensor, in_dim, vmap_level)
-
-        n = len(td.tensordicts)
-
-        def hook_in(
-            tensor,
-            out_dim=in_dim,
-            batch_size=n,
-            vmap_level=vmap_level,
-        ):
-            if _is_tensor_collection(type(tensor)):
-                return tensor._remove_batch_dim(vmap_level, batch_size, out_dim)
-            return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
-
-        out.hook_out = hook_out
-        out.hook_in = hook_in
-        out._is_vmapped = True
-        out._batch_size = torch.Size(
-            [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
-        )
-        return out
-
-    @cache  # noqa: B019
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        if self.hook_out is not None:
-            # this is the hacked version. We just need to remove the hook_out and
-            # reset a proper batch size
-            result = LazyStackedTensorDict(
-                *self.tensordicts,
-                stack_dim=out_dim,
-            )
-            # return self._cache_remove_batch_dim(vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim)
-        else:
-            # we must call _remove_batch_dim on all tensordicts
-            # batch_size: size of the batch when we unhide it.
-            # out_dim: dimension where the output will be found
-            new_batch_size = list(self.batch_size)
-            new_batch_size.insert(out_dim, batch_size)
-            new_names = list(self.names)
-            new_names.insert(out_dim, None)
-            # rebuild the lazy stack
-            # the stack dim is the same if the out_dim is past it, but it
-            # must be incremented by one otherwise.
-            # In the first case, the out_dim must be decremented by one
-            if out_dim > self.stack_dim:
-                stack_dim = self.stack_dim
-                out_dim = out_dim - 1
-            else:
-                stack_dim = self.stack_dim + 1
-            result = LazyStackedTensorDict(
-                *[
-                    td._remove_batch_dim(
-                        vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                    )
-                    for td in self.tensordicts
-                ],
-                stack_dim=stack_dim,
-            )
-        if self.is_locked:
-            result.lock_()
-        return result
-
-    def get_nestedtensor(
-        self,
-        key: NestedKey,
-        default: Any = NO_DEFAULT,
-    ) -> CompatibleType:
-        """Returns a nested tensor when stacking cannot be achieved.
-
-        Args:
-            key (NestedKey): the entry to nest.
-            default (Any, optiona): the default value to return in case the key
-                isn't in all sub-tensordicts.
-
-                .. note:: In case the default is a tensor, this method will attempt
-                  the construction of a nestedtensor with it. Otherwise, the default
-                  value will be returned.
-
-        Examples:
-            >>> td0 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(4)}, [])
-            >>> td1 = TensorDict({"a": torch.ones(5)}, [])
-            >>> td = torch.stack([td0, td1], 0)
-            >>> a = td.get_nestedtensor("a")
-            >>> # using a tensor as default uses this default to build the nested tensor
-            >>> b = td.get_nestedtensor("b", default=torch.ones(4))
-            >>> assert (a == b).all()
-            >>> # using anything else as default returns the default
-            >>> b2 = td.get_nestedtensor("b", None)
-            >>> assert b2 is None
-
-        """
-        # disallow getting nested tensor if the stacking dimension is not 0
-        if self.stack_dim != 0:
-            raise RuntimeError(
-                "Because nested tensors can only be stacked along their first "
-                "dimension, LazyStackedTensorDict.get_nestedtensor can only be called "
-                "when the stack_dim is 0."
-            )
-
-        # we can handle the case where the key is a tuple of length 1
-        key = _unravel_key_to_tuple(key)
-        subkey = key[0]
-        if len(key) > 1:
-            tensordict = self.get(subkey, default)
-            if tensordict is default:
-                return default
-            return tensordict.get_nestedtensor(key[1:], default=default)
-        tensors = [td.get(subkey, default=default) for td in self.tensordicts]
-        if not isinstance(default, torch.Tensor) and any(
-            tensor is default for tensor in tensors
-        ):
-            # we don't stack but return the default
-            return default
-        return torch.nested.nested_tensor(tensors)
-
-    def is_contiguous(self) -> bool:
-        return False
-
-    def contiguous(self) -> T:
-        source = {key: value.contiguous() for key, value in self.items()}
-        batch_size = self.batch_size
-        device = self.device
-        out = TensorDict(
-            source=source,
-            batch_size=batch_size,
-            device=device,
-            names=self.names,
-            _run_checks=False,
-            lock=self.is_locked,
-        )
-        return out
-
-    def empty(
-        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
-    ) -> T:
-        name = None
-        if batch_size is not None:
-            return TensorDict.empty(
-                self,
-                recurse=recurse,
-                batch_size=batch_size,
-                device=device if device is not NO_DEFAULT else self.device,
-                names=names if names is not None else None,
-            )
-        if names is not None:
-            if len(names) > self.stack_dim:
-                name = names[self.stack_dim]
-            names = [name for i, name in enumerate(names) if i != self.stack_dim]
-        return type(self)(
-            *[
-                td.empty(
-                    recurse=recurse, batch_size=batch_size, device=device, names=names
-                )
-                for td in self.tensordicts
-            ],
-            stack_dim=self.stack_dim,
-            stack_dim_name=name,
-        )
-
-    def _clone(self, recurse: bool = True) -> T:
-        if recurse:
-            # This could be optimized using copy but we must be careful with
-            # metadata (_is_shared etc)
-            result = type(self)(
-                *[td._clone() for td in self.tensordicts],
-                stack_dim=self.stack_dim,
-                stack_dim_name=self._td_dim_name,
-            )
-        else:
-            result = type(self)(
-                *[td._clone(recurse=False) for td in self.tensordicts],
-                stack_dim=self.stack_dim,
-                stack_dim_name=self._td_dim_name,
-            )
-        return result
-
-    def pin_memory(self) -> T:
-        for td in self.tensordicts:
-            td.pin_memory()
-        return self
-
-    def to(self, *args, **kwargs) -> T:
-        non_blocking = kwargs.pop("non_blocking", None)
-        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
-        if batch_size is not None:
-            raise TypeError("Cannot pass batch-size to a LazyStackedTensorDict.")
-        result = self
-
-        if device is not None and dtype is None and device == self.device:
-            return result
-
-        if non_blocking in (None, True):
-            kwargs["non_blocking"] = True
-        else:
-            kwargs["non_blocking"] = False
-        non_blocking = bool(non_blocking)
-        result = type(self)(
-            *[td.to(*args, **kwargs) for td in self.tensordicts],
-            stack_dim=self.stack_dim,
-            hook_out=self.hook_out,
-            hook_in=self.hook_in,
-            stack_dim_name=self._td_dim_name,
-        )
-        if device is not None and not non_blocking:
-            self._sync_all()
-        if self.is_locked:
-            result.lock_()
-        return result
-
-    def _check_new_batch_size(self, new_size: torch.Size) -> None:
-        if len(new_size) <= self.stack_dim:
-            raise RuntimeError(
-                "Changing the batch_size of a LazyStackedTensorDicts can only "
-                "be done with sizes that are at least as long as the "
-                "stacking dimension."
-            )
-        super()._check_new_batch_size(new_size)
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _LazyStackedTensorDictKeysView:
-        keys = _LazyStackedTensorDictKeysView(
-            self,
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-            is_leaf=is_leaf,
-        )
-        return keys
-
-    def values(self, include_nested=False, leaves_only=False, is_leaf=None):
-        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
-            yield from super().values(
-                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-            )
-        else:
-            for td in self.tensordicts:
-                yield from td.values(
-                    include_nested=include_nested,
-                    leaves_only=leaves_only,
-                    is_leaf=is_leaf,
-                )
-
-    def items(self, include_nested=False, leaves_only=False, is_leaf=None):
-        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
-            yield from super().items(
-                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-            )
-        else:
-            for i, td in enumerate(self.tensordicts):
-                for key, val in td.items(
-                    include_nested=include_nested,
-                    leaves_only=leaves_only,
-                    is_leaf=is_leaf,
-                ):
-                    if isinstance(key, str):
-                        key = (str(i), key)
-                    else:
-                        key = (str(i), *key)
-                    yield key, val
-
-    valid_keys = keys
-
-    def non_tensor_items(self, include_nested: bool = False):
-        """Returns all non-tensor leaves, maybe recursively."""
-        items = self.tensordicts[0].non_tensor_items(include_nested=include_nested)
-        return tuple(
-            (
-                key,
-                torch.stack(
-                    [val0, *[td.get(key) for td in self.tensordicts[1:]]],
-                    self.stack_dim,
-                ),
-            )
-            for (key, val0) in items
-        )
-
-    def _iterate_over_keys(self) -> None:
-        # this is about 20x faster than the version above
-        yield from self._key_list()
-
-    @cache  # noqa: B019
-    def _key_list(self):
-        keys = set(self.tensordicts[0].keys())
-        for td in self.tensordicts[1:]:
-            keys = keys.intersection(td.keys())
-        return sorted(keys, key=str)
-
-    @lock_blocked
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        key, val = self.tensordicts[0].popitem()
-        vals = [val]
-        for i, td in enumerate(self.tensordicts[1:]):
-            val = td.pop(key, None)
-            if val is not None:
-                vals.append(val)
-            else:
-                for j in range(i + 1):
-                    self.tensordicts[j].set(key, vals[j])
-                raise RuntimeError(f"Could not find key {key} in all tensordicts.")
-        return key, torch.stack(vals, dim=self.stack_dim)
-
-    def entry_class(self, key: NestedKey) -> type:
-        data_type = type(self.tensordicts[0].get(key))
-        if _is_tensor_collection(data_type):
-            return LazyStackedTensorDict
-        return data_type
-
-    def apply_(self, fn: Callable, *others, **kwargs):
-        others = (other.unbind(self.stack_dim) for other in others)
-        for td, *_others in zip(self.tensordicts, *others):
-            td._fast_apply(fn, *_others, inplace=True, propagate_lock=True, **kwargs)
-        return self
-
-    def _apply_nest(
-        self,
-        fn: Callable,
-        *others: T,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        checked: bool = False,
-        call_on_nested: bool = False,
-        default: Any = NO_DEFAULT,
-        named: bool = False,
-        nested_keys: bool = False,
-        prefix: tuple = (),
-        filter_empty: bool | None = None,
-        is_leaf: Callable | None = None,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        if inplace and any(
-            arg for arg in (batch_size, device, names, constructor_kwargs)
-        ):
-            raise ValueError(
-                "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
-            )
-        if out is not None:
-            if not isinstance(out, LazyStackedTensorDict):
-                raise ValueError(
-                    "out must be a LazyStackedTensorDict instance in lazy_stack.apply(..., out=out)."
-                )
-            out = out.tensordicts
-        elif batch_size is not None:
-            # any op that modifies the batch-size will result in a regular TensorDict
-            return TensorDict._apply_nest(
-                self,
-                fn,
-                *others,
-                batch_size=batch_size,
-                device=device,
-                names=names,
-                checked=checked,
-                call_on_nested=call_on_nested,
-                default=default,
-                named=named,
-                nested_keys=nested_keys,
-                prefix=prefix,
-                inplace=inplace,
-                filter_empty=filter_empty,
-                is_leaf=is_leaf,
-                **constructor_kwargs,
-            )
-
-        others = (other.unbind(self.stack_dim) for other in others)
-        results = [
-            td._apply_nest(
-                fn,
-                *oth,
-                checked=checked,
-                device=device,
-                call_on_nested=call_on_nested,
-                default=default,
-                named=named,
-                nested_keys=nested_keys,
-                prefix=prefix + (str(i),)
-                if is_leaf is _NESTED_TENSORS_AS_LISTS
-                else prefix,
-                inplace=inplace,
-                filter_empty=filter_empty,
-                is_leaf=is_leaf,
-                out=out[i] if out is not None else None,
-            )
-            for i, (td, *oth) in enumerate(zip(self.tensordicts, *others))
-        ]
-        if filter_empty and all(r is None for r in results):
-            return
-        if not inplace:
-            out = type(self)(
-                *results,
-                stack_dim=self.stack_dim,
-                stack_dim_name=self._td_dim_name,
-            )
-        else:
-            out = self
-        if names is not None:
-            out.names = names
-        return out
-
-    def _select(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        strict: bool = False,
-        set_shared: bool = True,
-    ) -> LazyStackedTensorDict:
-        # the following implementation keeps the hidden keys in the tensordicts
-        tensordicts = [
-            td._select(*keys, inplace=inplace, strict=strict, set_shared=set_shared)
-            for td in self.tensordicts
-        ]
-        if inplace:
-            return self
-        result = type(self)(
-            *tensordicts, stack_dim=self.stack_dim, stack_dim_name=self._td_dim_name
-        )
-        return result
-
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> LazyStackedTensorDict:
-        tensordicts = [
-            tensordict._exclude(*keys, inplace=inplace, set_shared=set_shared)
-            for tensordict in self.tensordicts
-        ]
-        if inplace:
-            self.tensordicts = tensordicts
-            return self
-        result = type(self)(
-            *tensordicts, stack_dim=self.stack_dim, stack_dim_name=self._td_dim_name
-        )
-        return result
-
-    def __setitem__(self, index: IndexType, value: T) -> T:
-        if isinstance(index, (tuple, str)):
-            # try:
-            index_unravel = _unravel_key_to_tuple(index)
-            if index_unravel:
-                self._set_tuple(
-                    index_unravel,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE
-                    if isinstance(self, _SubTensorDict)
-                    else False,
-                    validated=False,
-                    non_blocking=False,
-                )
-                return
-
-            if any(
-                isinstance(sub_index, (list, range, np.ndarray)) for sub_index in index
-            ):
-                index = tuple(
-                    torch.as_tensor(sub_index, device=self.device)
-                    if isinstance(sub_index, (list, range, np.ndarray))
-                    else sub_index
-                    for sub_index in index
-                )
-
-        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-        elif isinstance(index, (list, range)):
-            index = torch.as_tensor(index, device=self.device)
-
-        if is_tensor_collection(value) or isinstance(value, dict):
-            indexed_bs = _getitem_batch_size(self.batch_size, index)
-            if isinstance(value, dict):
-                value = TensorDict(
-                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
-                )
-            if value.batch_size != indexed_bs:
-                # try to expand
-                try:
-                    value = value.expand(indexed_bs)
-                except RuntimeError as err:
-                    raise RuntimeError(
-                        f"indexed destination TensorDict batch size is {indexed_bs} "
-                        f"(batch_size = {self.batch_size}, index={index}), "
-                        f"which differs from the source batch size {value.batch_size}"
-                    ) from err
-            split_index = self._split_index(index)
-            converted_idx = split_index["index_dict"]
-            num_single = split_index["num_single"]
-            isinteger = split_index["isinteger"]
-            has_bool = split_index["has_bool"]
-            num_squash = split_index.get("num_squash", 0)
-            num_none = split_index.get("num_none", 0)
-            is_nd_tensor = split_index.get("is_nd_tensor", False)
-            if isinteger:
-                # this will break if the index along the stack dim is [0] or :1 or smth
-                for i, _idx in converted_idx.items():
-                    if _idx == ():
-                        self.tensordicts[i].update(value, inplace=True)
-                    else:
-                        self.tensordicts[i][_idx] = value
-                return self
-            if is_nd_tensor:
-                unbind_dim = self.stack_dim - num_single + num_none - num_squash
-
-                # converted_idx is a nested list with (int, index) items
-                def assign(converted_idx, value=value):
-                    value = value.unbind(unbind_dim)
-                    for i, item in enumerate(converted_idx):
-                        if isinstance(item, list):
-                            assign(item)
-                        else:
-                            stack_item, idx = item
-                            self.tensordicts[stack_item][idx] = value[i]
-
-                assign(converted_idx)
-                return self
-            if not has_bool:
-                unbind_dim = self.stack_dim - num_single + num_none - num_squash
-                value_unbind = value.unbind(unbind_dim)
-                for (i, _idx), _value in zip(
-                    converted_idx.items(),
-                    value_unbind,
-                ):
-                    if _idx == ():
-                        self.tensordicts[i].update(_value, inplace=True)
-                    else:
-                        self.tensordicts[i][_idx] = _value
-            else:
-                # we must split, not unbind
-                mask_unbind = split_index["individual_masks"]
-                split_dim = split_index["split_dim"]
-                splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
-                value_unbind = value.split(splits, split_dim)
-                if mask_unbind[0].ndim == 0:
-                    # we can return a stack
-                    for (i, _idx), mask, _value in zip(
-                        converted_idx.items(),
-                        mask_unbind,
-                        value_unbind,
-                    ):
-                        if mask.any():
-                            self.tensordicts[i][_idx] = _value
-                else:
-                    for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
-                        self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                        self[self_idx][_idx] = _value
-        else:
-            for key in self.keys():
-                self.set_at_(key, value, index)
-
-    def __contains__(self, item: IndexType) -> bool:
-        if isinstance(item, TensorDictBase):
-            return any(item is td for td in self.tensordicts)
-        return super().__contains__(item)
-
-    def __getitem__(self, index: IndexType) -> T:
-        if isinstance(index, (tuple, str)):
-            index_key = _unravel_key_to_tuple(index)
-            if index_key:
-                leaf = self._get_tuple(index_key, NO_DEFAULT)
-                if is_non_tensor(leaf):
-                    result = getattr(leaf, "data", NO_DEFAULT)
-                    if result is NO_DEFAULT:
-                        return leaf.tolist()
-                    return result
-                return leaf
-        split_index = self._split_index(index)
-        converted_idx = split_index["index_dict"]
-        isinteger = split_index["isinteger"]
-        has_bool = split_index["has_bool"]
-        is_nd_tensor = split_index["is_nd_tensor"]
-        num_single = split_index.get("num_single", 0)
-        num_none = split_index.get("num_none", 0)
-        num_squash = split_index.get("num_squash", 0)
-        if has_bool:
-            mask_unbind = split_index["individual_masks"]
-            cat_dim = split_index["mask_loc"] - num_single
-            result = []
-            if mask_unbind[0].ndim == 0:
-                # we can return a stack
-                for (i, _idx), mask in zip(converted_idx.items(), mask_unbind):
-                    if mask.any():
-                        if mask.all() and self.tensordicts[i].ndim == 0:
-                            result.append(self.tensordicts[i])
-                        else:
-                            result.append(self.tensordicts[i][_idx])
-                            result[-1] = result[-1].squeeze(cat_dim)
-                return LazyStackedTensorDict.lazy_stack(result, cat_dim)
-            else:
-                for i, _idx in converted_idx.items():
-                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
-                    result.append(self[self_idx][_idx])
-                return torch.cat(result, cat_dim)
-        elif is_nd_tensor:
-            new_stack_dim = self.stack_dim - num_single + num_none
-
-            def recompose(converted_idx, stack_dim=new_stack_dim):
-                stack = []
-                for item in converted_idx:
-                    if isinstance(item, list):
-                        stack.append(recompose(item, stack_dim=stack_dim))
-                    else:
-                        stack_elt, idx = item
-                        if idx != ():
-                            stack.append(self.tensordicts[stack_elt][idx])
-                        else:
-                            stack.append(self.tensordicts[stack_elt])
-
-                # TODO: this produces multiple dims with the same name
-                result = LazyStackedTensorDict.lazy_stack(
-                    stack, stack_dim, stack_dim_name=self._td_dim_name
-                )
-                if self.is_locked:
-                    result.lock_()
-                return result
-
-            return recompose(converted_idx)
-        else:
-            if isinteger:
-                for (
-                    i,
-                    _idx,
-                ) in (
-                    converted_idx.items()
-                ):  # for convenience but there's only one element
-                    result = self.tensordicts[i]
-                    if _idx is not None and _idx != ():
-                        result = result[_idx]
-                    return result
-            else:
-                result = []
-                new_stack_dim = self.stack_dim - num_single + num_none - num_squash
-                for i, _idx in converted_idx.items():
-                    if _idx == ():
-                        result.append(self.tensordicts[i])
-                    else:
-                        result.append(self.tensordicts[i][_idx])
-                result = LazyStackedTensorDict.lazy_stack(
-                    result, new_stack_dim, stack_dim_name=self._td_dim_name
-                )
-                if self.is_locked:
-                    result.lock_()
-                return result
-
-    def __eq__(self, other):
-        return self._dispatch_comparison(other, "__eq__", "__eq__", default=False)
-
-    def __ne__(self, other):
-        return self._dispatch_comparison(other, "__ne__", "__ne__", default=True)
-
-    def __or__(self, other):
-        return self._dispatch_comparison(other, "__or__", "__or__", default=NO_DEFAULT)
-
-    def __xor__(self, other):
-        return self._dispatch_comparison(
-            other, "__xor__", "__xor__", default=NO_DEFAULT
-        )
-
-    def __ge__(self, other):
-        return self._dispatch_comparison(other, "__ge__", "__le__", default=NO_DEFAULT)
-
-    def __gt__(self, other):
-        return self._dispatch_comparison(other, "__gt__", "__lt__", default=NO_DEFAULT)
-
-    def __le__(self, other):
-        return self._dispatch_comparison(other, "__le__", "__ge__", default=NO_DEFAULT)
-
-    def __lt__(self, other):
-        return self._dispatch_comparison(other, "__lt__", "__gt__", default=NO_DEFAULT)
-
-    def _dispatch_comparison(self, other, comparison_str, inverse_str, default):
-        if is_tensorclass(other):
-            return getattr(other, inverse_str)(self)
-        if isinstance(other, (dict,)):
-            # we may want to broadcast it instead
-            other = TensorDict.from_dict(other, batch_size=self.batch_size)
-        if _is_tensor_collection(other.__class__):
-            if other.batch_size != self.batch_size:
-                if self.ndim < other.ndim:
-                    self_expand = self.expand(other.batch_size)
-                elif self.ndim > other.ndim:
-                    other = other.expand(self.batch_size)
-                    self_expand = self
-                else:
-                    raise RuntimeError(
-                        f"Could not compare tensordicts with shapes {self.shape} and {other.shape}"
-                    )
-            else:
-                self_expand = self
-            out = []
-            for td0, td1 in zip(
-                self_expand.tensordicts, other.unbind(self_expand.stack_dim)
-            ):
-                out.append(getattr(td0, comparison_str)(td1))
-            return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return LazyStackedTensorDict.lazy_stack(
-                [getattr(td, comparison_str)(other) for td in self.tensordicts],
-                self.stack_dim,
-            )
-        if default is NO_DEFAULT:
-            raise ValueError(
-                f"Incompatible value {type(other)} for op {comparison_str}."
-            )
-        return default
-
-    def _cast_reduction(
-        self,
-        *,
-        reduction_name,
-        dim=NO_DEFAULT,
-        keepdim=NO_DEFAULT,
-        tuple_ok=True,
-        **kwargs,
-    ):
-        try:
-            td = self.to_tensordict()
-        except Exception:
-            raise RuntimeError(
-                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
-                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
-                f" on github!"
-            )
-        return td._cast_reduction(
-            reduction_name=reduction_name,
-            dim=dim,
-            keepdim=keepdim,
-            tuple_ok=tuple_ok,
-            **kwargs,
-        )
-
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            # TODO: we need to adapt this to LazyStackedTensorDict too
-            if dim < 0:
-                dim = self.batch_dims + dim
-            return TensorDict(
-                source={key: value.all(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-            )
-        return all(value.all() for value in self.tensordicts)
-
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            # TODO: we need to adapt this to LazyStackedTensorDict too
-            if dim < 0:
-                dim = self.batch_dims + dim
-            return TensorDict(
-                source={key: value.any(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-            )
-        return any(value.any() for value in self.tensordicts)
-
-    def _send(
-        self,
-        dst: int,
-        _tag: int = -1,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> int:
-        for td in self.tensordicts:
-            _tag = td._send(dst, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
-        return _tag
-
-    def _isend(
-        self,
-        dst: int,
-        _tag: int = -1,
-        _futures: list[torch.Future] | None = None,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> int:
-        if _futures is None:
-            is_root = True
-            _futures = []
-        else:
-            is_root = False
-        for td in self.tensordicts:
-            _tag = td._isend(
-                dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures, group=group
-            )
-        if is_root:
-            for future in _futures:
-                future.wait()
-        return _tag
-
-    def _recv(
-        self,
-        src: int,
-        _tag: int = -1,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> int:
-        for td in self.tensordicts:
-            _tag = td._recv(src, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
-        return _tag
-
-    def _irecv(
-        self,
-        src: int,
-        return_premature: bool = False,
-        _tag: int = -1,
-        _future_list: list[torch.Future] = None,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-        for td in self.tensordicts:
-            _tag, _future_list = td._irecv(
-                src=src,
-                return_premature=return_premature,
-                _tag=_tag,
-                _future_list=_future_list,
-                pseudo_rand=pseudo_rand,
-                group=group,
-            )
-
-        if not root:
-            return _tag, _future_list
-        elif return_premature:
-            return _future_list
-        else:
-            for future in _future_list:
-                future.wait()
-            return
-
-    @lock_blocked
-    def del_(self, key: NestedKey, **kwargs: Any) -> T:
-        ids = set()
-        cur_len = len(ids)
-        is_deleted = False
-        error = None
-        for td in self.tensordicts:
-            # checking that the td has not been processed yet.
-            # It could be that not all sub-tensordicts have the appropriate
-            # entry but one must have it (or an error is thrown).
-            tdid = id(td)
-            ids.add(tdid)
-            new_cur_len = len(ids)
-            if new_cur_len == cur_len:
-                continue
-            cur_len = new_cur_len
-            try:
-                td.del_(key, **kwargs)
-                is_deleted = True
-            except KeyError as err:
-                error = err
-                continue
-        if not is_deleted:
-            # we know err is defined because LazyStackedTensorDict cannot be empty
-            raise error
-        return self
-
-    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        # using try/except for get/del is suboptimal, but
-        # this is faster that checkink if key in self keys
-        key = _unravel_key_to_tuple(key)
-        if len(key) == 1:
-            key = key[0]
-        present = False
-        if isinstance(key, tuple):
-            if key in self.keys(True):
-                present = True
-                value = self._get_tuple(key, NO_DEFAULT)
-        elif key in self.keys():
-            present = True
-            value = self._get_str(key, NO_DEFAULT)
-        if present:
-            self.del_(key)
-        elif default is not NO_DEFAULT:
-            value = default
-        else:
-            raise KeyError(
-                f"You are trying to pop key `{key}` which is not in dict "
-                f"without providing default value."
-            )
-        return value
-
-    def share_memory_(self) -> T:
-        for td in self.tensordicts:
-            td.share_memory_()
-        self.lock_()
-        return self
-
-    def detach_(self) -> T:
-        for td in self.tensordicts:
-            td.detach_()
-        return self
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        executor=None,
-        futures=None,
-        inplace=True,
-        like=False,
-        share_non_tensor,
-    ) -> T:
-        if prefix is not None:
-            prefix = Path(prefix)
-
-            def save_metadata(prefix=prefix, self=self):
-                prefix = Path(prefix)
-                if not prefix.exists():
-                    os.makedirs(prefix, exist_ok=True)
-                with open(prefix / "meta.json", "w") as f:
-                    json.dump(
-                        {"_type": str(self.__class__), "stack_dim": self.stack_dim}, f
-                    )
-
-            if executor is None:
-                save_metadata()
-            else:
-                futures.append(executor.submit(save_metadata))
-
-        results = []
-        for i, td in enumerate(self.tensordicts):
-            results.append(
-                td._memmap_(
-                    prefix=(prefix / str(i)) if prefix is not None else None,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=inplace,
-                    like=like,
-                    share_non_tensor=share_non_tensor,
-                )
-            )
-        if not inplace:
-            results = LazyStackedTensorDict.lazy_stack(results, dim=self.stack_dim)
-        else:
-            results = self
-        results._device = torch.device("cpu")
-        return results
-
-    @classmethod
-    def _load_memmap(
-        cls,
-        prefix: str,
-        metadata: dict,
-        device: torch.device | None = None,
-        *,
-        out=None,
-        **kwargs,
-    ) -> LazyStackedTensorDict:
-        tensordicts = []
-        i = 0
-        stack_dim = metadata["stack_dim"]
-        if out is not None:
-            out = out.unbind(stack_dim)
-        while (prefix / str(i)).exists():
-            tensordicts.append(
-                TensorDict.load_memmap(
-                    prefix / str(i),
-                    device=device,
-                    **kwargs,
-                    non_blocking=True,
-                    out=out[i] if out is not None else None,
-                )
-            )
-            i += 1
-        return cls(*tensordicts, stack_dim=stack_dim, **kwargs)
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
-            "it can't return a contiguous view of the lazy stacked tensors. "
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
-            "it can't return a contiguous view of the lazy stacked tensors. "
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
-            "it can't return a contiguous view of the lazy stacked tensors. "
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def expand(self, *args: int, inplace: bool = False) -> T:
-        if len(args) == 1 and isinstance(args[0], Sequence):
-            shape = tuple(args[0])
-        else:
-            shape = args
-        stack_dim = len(shape) + self.stack_dim - self.ndimension()
-        new_shape_tensordicts = [v for i, v in enumerate(shape) if i != stack_dim]
-        tensordicts = [td.expand(new_shape_tensordicts) for td in self.tensordicts]
-        if inplace:
-            self.tensordicts = tensordicts
-            self.stack_dim = stack_dim
-            return self
-        return LazyStackedTensorDict.maybe_dense_stack(tensordicts, dim=stack_dim)
-
-    def update(
-        self,
-        input_dict_or_td: T,
-        clone: bool = False,
-        *,
-        keys_to_update: Sequence[NestedKey] | None = None,
-        non_blocking: bool = False,
-        **kwargs: Any,
-    ) -> T:
-        # This implementation of update is compatible with exclusive keys
-        # as well as vmapped lazy stacks.
-        # We iterate over the tensordicts rather than iterating over the keys,
-        # which requires stacking and unbinding but is also not robust to missing keys.
-        if input_dict_or_td is self:
-            # no op
-            return self
-        if isinstance(input_dict_or_td, dict):
-            input_dict_or_td = TensorDict.from_dict(
-                input_dict_or_td, batch_size=self.batch_size
-            )
-
-        if keys_to_update is not None:
-            keys_to_update = unravel_key_list(keys_to_update)
-            if len(keys_to_update) == 0:
-                return self
-
-        if (
-            isinstance(input_dict_or_td, LazyStackedTensorDict)
-            and input_dict_or_td.stack_dim == self.stack_dim
-        ):
-            if len(input_dict_or_td.tensordicts) != len(self.tensordicts):
-                raise ValueError(
-                    "cannot update stacked tensordicts with different shapes."
-                )
-            for td_dest, td_source in zip(
-                self.tensordicts, input_dict_or_td.tensordicts
-            ):
-                td_dest.update(
-                    td_source,
-                    clone=clone,
-                    keys_to_update=keys_to_update,
-                    non_blocking=non_blocking,
-                    **kwargs,
-                )
-            return self
-
-        if self.hook_in is not None:
-            self_upd = self.hook_in(self)
-            input_dict_or_td = self.hook_in(input_dict_or_td)
-        else:
-            self_upd = self
-        # Then we can decompose the tensordict along its stack dim
-        if input_dict_or_td.ndim <= self_upd.stack_dim or input_dict_or_td.batch_size[
-            self_upd.stack_dim
-        ] != len(self_upd.tensordicts):
-            try:
-                # if the batch-size does not permit unbinding, let's first try to reset the batch-size.
-                input_dict_or_td = input_dict_or_td.copy()
-                batch_size = self_upd.batch_size
-                if self_upd.hook_out is not None:
-                    batch_size = list(batch_size)
-                    batch_size.insert(self_upd.stack_dim, len(self_upd.tensordicts))
-                input_dict_or_td.batch_size = batch_size
-            except RuntimeError as err:
-                raise ValueError(
-                    "cannot update stacked tensordicts with different shapes."
-                ) from err
-        for td_dest, td_source in zip(
-            self_upd.tensordicts, input_dict_or_td.unbind(self_upd.stack_dim)
-        ):
-            td_dest.update(
-                td_source, clone=clone, keys_to_update=keys_to_update, **kwargs
-            )
-        if self.hook_out is not None:
-            self_upd = self.hook_out(self_upd)
-        else:
-            self_upd = self
-        return self_upd
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        **kwargs: Any,
-    ) -> T:
-        if input_dict_or_td is self:
-            # no op
-            return self
-        if not is_tensor_collection(input_dict_or_td):
-            input_dict_or_td = TensorDict.from_dict(
-                input_dict_or_td, batch_dims=self.batch_dims
-            )
-            if input_dict_or_td.batch_dims <= self.stack_dim:
-                raise RuntimeError(
-                    f"Built tensordict with ndim={input_dict_or_td.ndim} does not have enough dims."
-                )
-        if input_dict_or_td.batch_size[self.stack_dim] != len(self.tensordicts):
-            raise ValueError("cannot update stacked tensordicts with different shapes.")
-        for td_dest, td_source in zip(
-            self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
-        ):
-            td_dest.update_(td_source, clone=clone, non_blocking=non_blocking, **kwargs)
-        return self
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        index: IndexType,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-    ) -> T:
-        if not _is_tensor_collection(type(input_dict_or_td)):
-            input_dict_or_td = TensorDict.from_dict(
-                input_dict_or_td, batch_size=self.batch_size
-            )
-        split_index = self._split_index(index)
-        converted_idx = split_index["index_dict"]
-        num_single = split_index["num_single"]
-        isinteger = split_index["isinteger"]
-        if isinteger:
-            # this will break if the index along the stack dim is [0] or :1 or smth
-            for i, _idx in converted_idx.items():
-                self.tensordicts[i].update_at_(
-                    input_dict_or_td,
-                    _idx,
-                    non_blocking=non_blocking,
-                )
-            return self
-        unbind_dim = self.stack_dim - num_single
-        for (i, _idx), _value in zip(
-            converted_idx.items(),
-            input_dict_or_td.unbind(unbind_dim),
-        ):
-            self.tensordicts[i].update_at_(
-                _value,
-                _idx,
-                non_blocking=non_blocking,
-            )
-        return self
-
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> T:
-        for td in self.tensordicts:
-            td.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    def where(self, condition, other, *, out=None, pad=None):
-        if condition.ndim < self.ndim:
-            condition = expand_right(condition, self.batch_size)
-        condition = condition.unbind(self.stack_dim)
-        if _is_tensor_collection(other.__class__) or (
-            isinstance(other, Tensor)
-            and other.shape[: self.stack_dim] == self.shape[: self.stack_dim]
-        ):
-            other = other.unbind(self.stack_dim)
-
-            def where(td, cond, other, pad):
-                if cond.numel() > 1:
-                    return td.where(cond, other, pad=pad)
-                return other if not cond else td
-
-            result = LazyStackedTensorDict.maybe_dense_stack(
-                [
-                    where(td, cond, _other, pad=pad)
-                    for td, cond, _other in zip(self.tensordicts, condition, other)
-                ],
-                self.stack_dim,
-            )
-        else:
-            result = LazyStackedTensorDict.maybe_dense_stack(
-                [
-                    td.where(cond, other, pad=pad)
-                    for td, cond in zip(self.tensordicts, condition)
-                ],
-                self.stack_dim,
-            )
-        # We should not pass out to stack because this will overwrite the tensors in-place, but
-        # we don't want that
-        if out is not None:
-            out.update(result)
-            return out
-        return result
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
-        mask_unbind = mask.unbind(dim=self.stack_dim)
-        for _mask, td in zip(mask_unbind, self.tensordicts):
-            td.masked_fill_(_mask, value)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    @lock_blocked
-    def insert(self, index: int, tensordict: T) -> None:
-        """Insert a TensorDict into the stack at the specified index.
-
-        Analogous to list.insert. The inserted TensorDict must have compatible
-        batch_size and device. Insertion is in-place, nothing is returned.
-
-        Args:
-            index (int): The index at which the new TensorDict should be inserted.
-            tensordict (TensorDictBase): The TensorDict to be inserted into the stack.
-
-        """
-        if not isinstance(tensordict, TensorDictBase):
-            raise TypeError(
-                "Expected new value to be TensorDictBase instance but got "
-                f"{type(tensordict)} instead."
-            )
-
-        batch_size = self.tensordicts[0].batch_size
-        device = self.tensordicts[0].device
-
-        _batch_size = tensordict.batch_size
-        _device = tensordict.device
-
-        if device != _device:
-            raise ValueError(
-                f"Devices differ: stack has device={device}, new value has "
-                f"device={_device}."
-            )
-        if _batch_size != batch_size:
-            raise ValueError(
-                f"Batch sizes in tensordicts differs: stack has "
-                f"batch_size={batch_size}, new_value has batch_size={_batch_size}."
-            )
-
-        self.tensordicts.insert(index, tensordict)
-
-        N = len(self.tensordicts)
-        self._batch_size = self._compute_batch_size(batch_size, self.stack_dim, N)
-
-    @lock_blocked
-    def append(self, tensordict: T) -> None:
-        """Append a TensorDict onto the stack.
-
-        Analogous to list.append. The appended TensorDict must have compatible
-        batch_size and device. The append operation is in-place, nothing is returned.
-
-        Args:
-            tensordict (TensorDictBase): The TensorDict to be appended onto the stack.
-
-        """
-        self.insert(len(self.tensordicts), tensordict)
-
-    @property
-    def is_locked(self) -> bool:
-        if self._is_locked is not None:
-            # if tensordicts have been locked through this Lazy stack, then we can
-            # trust this lazy stack to contain the info.
-            # In all other cases we must check
-            return self._is_locked
-        # If any of the tensordicts is not locked, we assume that the lazy stack
-        # is not locked either. Caching is then disabled and
-        for td in self.tensordicts:
-            if not td.is_locked:
-                return False
-        else:
-            # In this case, all tensordicts were locked before the lazy stack
-            # was created and they were not locked through the lazy stack.
-            # This means we cannot cache the value because this lazy stack
-            # if not part of the graph. We don't want it to be part of the graph
-            # because this object being locked is only a side-effect.
-            # Calling self.lock_() here could however speed things up.
-            return True
-
-    @is_locked.setter
-    def is_locked(self, value: bool) -> None:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @property
-    def _lock_parents_weakrefs(self):
-        """Weakrefs of all tensordicts that need to be unlocked for this to be unlocked."""
-        _lock_parents_weakrefs = []
-        for tensordict in self.tensordicts:
-            _lock_parents_weakrefs = (
-                _lock_parents_weakrefs + tensordict._lock_parents_weakrefs
-            )
-        _lock_parents_weakrefs = [
-            item for item in _lock_parents_weakrefs if item is not weakref.ref(self)
-        ]
-        return _lock_parents_weakrefs
-
-    def _propagate_lock(self, lock_parents_weakrefs=None):
-        """Registers the parent tensordict that handles the lock."""
-        self._is_locked = True
-        is_root = lock_parents_weakrefs is None
-        if is_root:
-            lock_parents_weakrefs = []
-
-        lock_parents_weakrefs = copy(lock_parents_weakrefs) + [weakref.ref(self)]
-        for dest in self.tensordicts:
-            dest._propagate_lock(lock_parents_weakrefs)
-
-    @erase_cache
-    def _propagate_unlock(self):
-        # we can't set _is_locked to False because after it's unlocked, anything
-        # can happen to a child tensordict.
-        self._is_locked = None
-        sub_tds = defaultdict()
-        for child in self.tensordicts:
-            # we want to make sure that if the same child is present twice in the
-            # stack we won't iterate multiple times over it
-            sub_tds[id(child)] = child._propagate_unlock() + [child]
-        sub_tds = [item for value in sub_tds.values() for item in value]
-        return sub_tds
-
-    def __repr__(self):
-        fields = _td_fields(self)
-        field_str = indent(f"fields={{{fields}}}", 4 * " ")
-        exclusive_fields_str = indent(
-            f"exclusive_fields={{{self._repr_exclusive_fields()}}}", 4 * " "
-        )
-        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-        device_str = indent(f"device={self.device}", 4 * " ")
-        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-        stack_dim = indent(f"stack_dim={self.stack_dim}", 4 * " ")
-        string = ",\n".join(
-            [
-                field_str,
-                exclusive_fields_str,
-                batch_size_str,
-                device_str,
-                is_shared_str,
-                stack_dim,
-            ]
-        )
-        return f"{type(self).__name__}(\n{string})"
-
-    def _repr_exclusive_fields(self):
-        keys = set(self.keys())
-        exclusive_keys = [
-            _td_fields(td, [k for k in td.keys() if k not in keys])
-            for td in self.tensordicts
-        ]
-        exclusive_key_str = ",\n".join(
-            [
-                indent(f"{i} ->{line}", 4 * " ")
-                for i, line in enumerate(exclusive_keys)
-                if line != "\n"
-            ]
-        )
-
-        return "\n" + exclusive_key_str
-
-    def _view(self, *args, **kwargs):
-        raise RuntimeError(
-            "Cannot call `view` on a lazy stacked tensordict. Call `reshape` instead."
-        )
-
-    def _transpose(self, dim0, dim1):
-        if self._is_vmapped:
-            raise RuntimeError("cannot call transpose within vmap.")
-        if dim0 == self.stack_dim:
-            # we know dim0 and dim1 are sorted so dim1 comes after dim0
-            # example: shape = [5, 4, 3, 2, 1], stack_dim=1, dim0=1, dim1=4
-            # resulting shape: [5, 1, 3, 2, 4]
-            if dim1 == dim0 + 1:
-                result = type(self)(
-                    *self.tensordicts, stack_dim=dim1, stack_dim_name=self._td_dim_name
-                )
-            else:
-                result = type(self)(
-                    *(td.transpose(dim0, dim1 - 1) for td in self.tensordicts),
-                    stack_dim=dim1,
-                    stack_dim_name=self._td_dim_name,
-                )
-        elif dim1 == self.stack_dim:
-            # example: shape = [5, 4, 3, 2, 1], stack_dim=3, dim0=1, dim1=3
-            # resulting shape: [5, 2, 3, 4, 1]
-            if dim0 + 1 == dim1:
-                result = type(self)(
-                    *self.tensordicts, stack_dim=dim0, stack_dim_name=self._td_dim_name
-                )
-            else:
-                result = type(self)(
-                    *(td.transpose(dim0 + 1, dim1) for td in self.tensordicts),
-                    stack_dim=dim0,
-                    stack_dim_name=self._td_dim_name,
-                )
-        else:
-            dim0 = dim0 if dim0 < self.stack_dim else dim0 - 1
-            dim1 = dim1 if dim1 < self.stack_dim else dim1 - 1
-            result = type(self)(
-                *(td.transpose(dim0, dim1) for td in self.tensordicts),
-                stack_dim=self.stack_dim,
-                stack_dim_name=self._td_dim_name,
-            )
-        return result
-
-    def _permute(
-        self,
-        *args,
-        **kwargs,
-    ):
-        dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
-        dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
-        dims_list_sort = np.argsort(dims_list)
-        # find the new stack dim
-        stack_dim = dims_list_sort[self.stack_dim]
-        # remove that dim from the dims_list
-        dims_list = [
-            d if d < self.stack_dim else d - 1 for d in dims_list if d != self.stack_dim
-        ]
-        result = LazyStackedTensorDict.lazy_stack(
-            [td.permute(dims_list) for td in self.tensordicts],
-            stack_dim,
-            stack_dim_name=self._td_dim_name,
-        )
-        return result
-
-    def _squeeze(self, dim=None):
-        if dim is not None:
-            new_dim = dim
-            if new_dim < 0:
-                new_dim = self.batch_dims + new_dim
-            if new_dim > self.batch_dims - 1 or new_dim < 0:
-                raise RuntimeError(
-                    f"The dim provided to squeeze is incompatible with the tensordict shape: dim={dim} and batch_size={self.batch_size}."
-                )
-            dim = new_dim
-            if self.batch_size[dim] != 1:
-                return self
-            if dim == self.stack_dim:
-                return self.tensordicts[0]
-            if dim > self.stack_dim:
-                dim = dim - 1
-                stack_dim = self.stack_dim
-            else:
-                stack_dim = self.stack_dim - 1
-            result = LazyStackedTensorDict.lazy_stack(
-                [td.squeeze(dim) for td in self.tensordicts],
-                stack_dim,
-                stack_dim_name=self._td_dim_name,
-            )
-        else:
-            result = self
-            for dim in range(self.batch_dims - 1, -1, -1):
-                if self.batch_size[dim] == 1:
-                    result = result.squeeze(dim)
-        return result
-
-    def _unsqueeze(self, dim):
-        new_dim = dim
-        if new_dim < 0:
-            new_dim = self.batch_dims + new_dim + 1
-        if new_dim > self.batch_dims or new_dim < 0:
-            raise RuntimeError(
-                f"The dim provided to unsqueeze is incompatible with the tensordict shape: dim={dim} and batch_size={self.batch_size}."
-            )
-        dim = new_dim
-        if dim > self.stack_dim:
-            dim = dim - 1
-            stack_dim = self.stack_dim
-        else:
-            stack_dim = self.stack_dim + 1
-        result = LazyStackedTensorDict.lazy_stack(
-            [td.unsqueeze(dim) for td in self.tensordicts],
-            stack_dim,
-            stack_dim_name=self._td_dim_name,
-        )
-        return result
-
-    lock_ = TensorDictBase.lock_
-    lock = _renamed_inplace_method(lock_)
-
-    unlock_ = TensorDictBase.unlock_
-    unlock = _renamed_inplace_method(unlock_)
-
-    _check_device = TensorDict._check_device
-    _check_is_shared = TensorDict._check_is_shared
-    _convert_to_tensordict = TensorDict._convert_to_tensordict
-    _index_tensordict = TensorDict._index_tensordict
-    masked_select = TensorDict.masked_select
-    reshape = TensorDict.reshape
-    split = TensorDict.split
-    _to_module = TensorDict._to_module
-    from_dict_instance = TensorDict.from_dict_instance
-
-
-class _CustomOpTensorDict(TensorDictBase):
-    """Encodes lazy operations on tensors contained in a TensorDict."""
-
-    _safe = False
-    _lazy = True
-
-    def __init__(
-        self,
-        source: T,
-        custom_op: str,
-        inv_op: str | None = None,
-        custom_op_kwargs: dict | None = None,
-        inv_op_kwargs: dict | None = None,
-        batch_size: Sequence[int] | None = None,
-    ) -> None:
-
-        if not isinstance(source, TensorDictBase):
-            raise TypeError(
-                f"Expected source to be a TensorDictBase isntance, "
-                f"but got {type(source)} instead."
-            )
-        self._source = source
-        self.custom_op = custom_op
-        self.inv_op = inv_op
-        self.custom_op_kwargs = custom_op_kwargs if custom_op_kwargs is not None else {}
-        self.inv_op_kwargs = inv_op_kwargs if inv_op_kwargs is not None else {}
-        self._batch_size = None
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-
-    # These attributes should never be set
-    @property
-    @cache  # noqa
-    def _is_shared(self):
-        return self._source._is_shared
-
-    @property
-    @cache  # noqa
-    def _is_memmap(self):
-        return self._source._is_memmap
-
-    def is_empty(self) -> bool:
-        return self._source.is_empty()
-
-    def is_memmap(self) -> bool:
-        return self._source.is_memmap()
-
-    def is_shared(self) -> bool:
-        return self._source.is_shared()
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        """Allows for a transformation to be customized for a certain shape, device or dtype.
-
-        By default, this is a no-op on self.custom_op_kwargs
-
-        Args:
-            source_tensor: corresponding Tensor
-
-        Returns:
-            a dictionary with the kwargs of the operation to execute
-            for the tensor
-
-        """
-        return self.custom_op_kwargs
-
-    def _update_inv_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        """Allows for an inverse transformation to be customized for a certain shape, device or dtype.
-
-        By default, this is a no-op on self.inv_op_kwargs
-
-        Args:
-            source_tensor: corresponding tensor
-
-        Returns:
-            a dictionary with the kwargs of the operation to execute for
-            the tensor
-
-        """
-        return self.inv_op_kwargs
-
-    def entry_class(self, key: NestedKey) -> type:
-        return type(self._source.get(key))
-
-    @property
-    def device(self) -> torch.device | None:
-        return self._source.device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        self._source.device = value
-
-    @property
-    def batch_size(self) -> torch.Size:
-        if self._batch_size is None:
-            self._batch_size = getattr(
-                torch.zeros(self._source.batch_size, device="meta"), self.custom_op
-            )(**self.custom_op_kwargs).shape
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    def _has_names(self):
-        return self._source._has_names()
-
-    def _erase_names(self):
-        raise RuntimeError(
-            f"Cannot erase names of a {type(self)}. "
-            f"Erase source TensorDict's names instead."
-        )
-
-    def _rename_subtds(self, names):
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                raise RuntimeError(
-                    "Cannot rename dimensions of a lazy TensorDict with "
-                    "nested collections. Convert the instance to a regular "
-                    "tensordict by using the `to_tensordict()` method first."
-                )
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def _get_str(self, key, default):
-        tensor = self._source._get_str(key, default)
-        if tensor is default:
-            return tensor
-        return self._transform_value(tensor)
-
-    def _get_tuple(self, key, default):
-        tensor = self._source._get_tuple(key, default)
-        if tensor is default:
-            return tensor
-        return self._transform_value(tensor)
-
-    def _transform_value(self, item):
-        return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
-
-    def _set_str(
-        self,
-        key,
-        value,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ):
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-            validated = True
-        value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
-        self._source._set_str(
-            key,
-            value,
-            inplace=inplace,
-            validated=validated,
-            ignore_lock=ignore_lock,
-            non_blocking=non_blocking,
-        )
-        return self
-
-    def _set_tuple(
-        self, key, value, *, inplace: bool, validated: bool, non_blocking: bool
-    ):
-        if len(key) == 1:
-            return self._set_str(
-                key[0],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        source = self._source._get_str(key[0], None)
-        if source is None:
-            source = self._source._create_nested_str(key[0])
-        nested = type(self)(
-            source,
-            custom_op=self.custom_op,
-            inv_op=self.inv_op,
-            custom_op_kwargs=self._update_custom_op_kwargs(source),
-            inv_op_kwargs=self._update_inv_op_kwargs(source),
-        )
-        nested._set_tuple(
-            key[1:],
-            value,
-            inplace=inplace,
-            validated=validated,
-            non_blocking=non_blocking,
-        )
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
-        transformed_tensor, original_tensor = self._get_str(
-            key, NO_DEFAULT
-        ), self._source._get_str(key, NO_DEFAULT)
-        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
-            raise RuntimeError(
-                f"{self} original tensor and transformed_in do not point to the "
-                f"same storage. Setting values in place is not currently "
-                f"supported in this setting, consider calling "
-                f"`td.clone()` before `td.set_at_(...)`"
-            )
-        transformed_tensor[idx] = value
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
-        transformed_tensor, original_tensor = self._get_tuple(
-            key, NO_DEFAULT
-        ), self._source._get_tuple(key, NO_DEFAULT)
-        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
-            raise RuntimeError(
-                f"{self} original tensor and transformed_in do not point to the "
-                f"same storage. Setting values in place is not currently "
-                f"supported in this setting, consider calling "
-                f"`td.clone()` before `td.set_at_(...)`"
-            )
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-
-        transformed_tensor[idx] = value
-        return self
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        raise RuntimeError(
-            f"stacking tensordicts is not allowed for type {type(self)}"
-            f"consider calling 'to_tensordict()` first"
-        )
-
-    def __repr__(self) -> str:
-        custom_op_kwargs_str = ", ".join(
-            [f"{key}={value}" for key, value in self.custom_op_kwargs.items()]
-        )
-        indented_source = textwrap.indent(f"source={self._source}", "\t")
-        return (
-            f"{self.__class__.__name__}(\n{indented_source}, "
-            f"\n\top={self.custom_op}({custom_op_kwargs_str}))"
-        )
-
-    # @cache  # noqa: B019
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _TensorDictKeysView:
-        return self._source.keys(
-            include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-        )
-
-    def _select(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        strict: bool = True,
-        set_shared: bool = True,
-    ) -> _CustomOpTensorDict:
-        if inplace:
-            raise RuntimeError("Cannot call select inplace on a lazy tensordict.")
-        return self.to_tensordict()._select(
-            *keys, inplace=False, strict=strict, set_shared=set_shared
-        )
-
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> _CustomOpTensorDict:
-        if inplace:
-            raise RuntimeError("Cannot call exclude inplace on a lazy tensordict.")
-        return self.to_tensordict()._exclude(
-            *keys, inplace=False, set_shared=set_shared
-        )
-
-    def _clone(self, recurse: bool = True) -> T:
-        """Clones the Lazy TensorDict.
-
-        Args:
-            recurse (bool, optional): if ``True`` (default), a regular
-                :class:`~.tensordict.TensorDict` instance will be returned.
-                Otherwise, another :class:`~.tensordict.SubTensorDict` with identical content
-                will be returned.
-        """
-        if not recurse:
-            return type(self)(
-                source=self._source.clone(False),
-                custom_op=self.custom_op,
-                inv_op=self.inv_op,
-                custom_op_kwargs=self.custom_op_kwargs,
-                inv_op_kwargs=self.inv_op_kwargs,
-                batch_size=self.batch_size,
-            )
-        return self.to_tensordict()
-
-    def is_contiguous(self) -> bool:
-        return all([value.is_contiguous() for _, value in self.items()])
-
-    def contiguous(self) -> T:
-        return self._fast_apply(lambda x: x.contiguous(), propagate_lock=True)
-
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> _CustomOpTensorDict:
-        self._source.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    rename_key = _renamed_inplace_method(rename_key_)
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> _CustomOpTensorDict:
-        self._source = self._source.del_(key)
-        return self
-
-    def to(self, *args, **kwargs) -> T:
-        non_blocking = kwargs.pop("non_blocking", None)
-        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
-        if batch_size is not None:
-            raise TypeError(f"Cannot pass batch-size to a {type(self)}.")
-        result = self
-
-        if device is not None and dtype is None and device == self.device:
-            return result
-
-        td = self._source.to(*args, non_blocking=non_blocking, **kwargs)
-        self_copy = copy(self)
-        self_copy._source = td
-        return self_copy
-
-    def pin_memory(self) -> _CustomOpTensorDict:
-        self._source.pin_memory()
-        return self
-
-    @lock_blocked
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        key, val = self._source.popitem()
-        return key, self._transform_value(val)
-
-    def detach_(self) -> _CustomOpTensorDict:
-        self._source.detach_()
-        return self
-
-    def where(self, condition, other, *, out=None, pad=None):
-        return self.to_tensordict().where(
-            condition=condition, other=other, out=out, pad=pad
-        )
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> _CustomOpTensorDict:
-        for key, item in self.items():
-            val = self._source.get(key)
-            mask_exp = expand_right(
-                mask, list(mask.shape) + list(val.shape[self._source.batch_dims :])
-            )
-            mask_proc_inv = getattr(mask_exp, self.inv_op)(
-                **self._update_inv_op_kwargs(item)
-            )
-            val[mask_proc_inv] = value
-            self._source.set(key, val)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None,
-        copy_existing: bool,
-        executor,
-        futures,
-        inplace,
-        like,
-        share_non_tensor,
-    ) -> T:
-        def save_metadata(data: TensorDictBase, filepath, metadata=None):
-            if metadata is None:
-                metadata = {}
-            metadata.update(
-                {
-                    "shape": list(data.shape),
-                    "device": str(data.device),
-                    "_type": str(data.__class__),
-                    "custom_op": data.custom_op,
-                    "inv_op": data.inv_op,
-                    "custom_op_kwargs": data.custom_op_kwargs,
-                    "inv_op_kwargs": data.inv_op_kwargs,
-                }
-            )
-            with open(filepath, "w") as json_metadata:
-                json.dump(metadata, json_metadata)
-
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            metadata = {}
-
-        dest_source = self._source._memmap_(
-            prefix=None if prefix is None else prefix / "_source",
-            copy_existing=copy_existing,
-            executor=executor,
-            futures=futures,
-            inplace=inplace,
-            like=like,
-            share_non_tensor=share_non_tensor,
-        )
-        if not inplace:
-            dest = type(self)(
-                dest_source,
-                custom_op=self.custom_op,
-                inv_op=self.inv_op,
-                custom_op_kwargs=self.custom_op_kwargs,
-                inv_op_kwargs=self.inv_op_kwargs,
-                batch_size=self.batch_size,
-            )
-        else:
-            dest = self
-
-        if prefix is not None:
-            if executor is None:
-                save_metadata(
-                    dest,
-                    prefix / "meta.json",
-                    metadata=metadata,
-                )
-            else:
-                futures.append(
-                    executor.submit(save_metadata, dest, prefix / "meta.json", metadata)
-                )
-        return dest
-
-    @classmethod
-    def _load_memmap(cls, prefix: str, metadata: dict, **kwargs) -> _CustomOpTensorDict:
-        custom_op = metadata.pop("custom_op")
-        inv_op = metadata.pop("inv_op")
-        custom_op_kwargs = metadata.pop("custom_op_kwargs")
-        inv_op_kwargs = metadata.pop("inv_op_kwargs")
-
-        source = TensorDict.load_memmap(prefix / "_source", **kwargs, non_blocking=True)
-
-        return cls(
-            source,
-            custom_op=custom_op,
-            inv_op=inv_op,
-            custom_op_kwargs=custom_op_kwargs,
-            inv_op_kwargs=inv_op_kwargs,
-        )
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def share_memory_(self) -> _CustomOpTensorDict:
-        self._source.share_memory_()
-        self.lock_()
-        return self
-
-    @property
-    def _td_dim_names(self):
-        # we also want for _td_dim_names to be accurate
-        if self._source._td_dim_names is None:
-            return None
-        return self.names
-
-    @property
-    def is_locked(self) -> bool:
-        return self._source.is_locked
-
-    @is_locked.setter
-    def is_locked(self, value) -> bool:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @as_decorator("is_locked")
-    def lock_(self) -> T:
-        self._source.lock_()
-        return self
-
-    @erase_cache
-    @as_decorator("is_locked")
-    def unlock_(self) -> T:
-        self._source.unlock_()
-        return self
-
-    def _remove_lock(self, lock_id):
-        return self._source._remove_lock(lock_id)
-
-    @erase_cache
-    def _propagate_lock(self, lock_ids):
-        return self._source._propagate_lock(lock_ids)
-
-    @erase_cache
-    def _propagate_unlock(self):
-        return self._source._propagate_unlock()
-
-    lock = _renamed_inplace_method(lock_)
-    unlock = _renamed_inplace_method(unlock_)
-
-    def __del__(self):
-        pass
-
-    @property
-    def sorted_keys(self):
-        return self._source.sorted_keys
-
-    def _view(self, *args, **kwargs):
-        raise RuntimeError(
-            "Cannot call `view` on a lazy tensordict. Call `reshape` instead."
-        )
-
-    def _transpose(self, dim0, dim1):
-        raise RuntimeError(
-            "Cannot call `transpose` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _permute(
-        self,
-        *args,
-        **kwargs,
-    ):
-        raise RuntimeError(
-            "Cannot call `permute` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _squeeze(self, dim=None):
-        raise RuntimeError(
-            "Cannot call `squeeze` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _unsqueeze(self, dim):
-        raise RuntimeError(
-            "Cannot call `unsqueeze` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _cast_reduction(
-        self,
-        *,
-        reduction_name,
-        dim=NO_DEFAULT,
-        keepdim=NO_DEFAULT,
-        tuple_ok=True,
-        **kwargs,
-    ):
-        try:
-            td = self.to_tensordict()
-        except Exception:
-            raise RuntimeError(
-                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
-                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
-                f" on github!"
-            )
-        return td._cast_reduction(
-            reduction_name=reduction_name,
-            dim=dim,
-            keepdim=keepdim,
-            tuple_ok=tuple_ok,
-            **kwargs,
-        )
-
-    __xor__ = TensorDict.__xor__
-    __or__ = TensorDict.__or__
-    __eq__ = TensorDict.__eq__
-    __ne__ = TensorDict.__ne__
-    __ge__ = TensorDict.__ge__
-    __gt__ = TensorDict.__gt__
-    __le__ = TensorDict.__le__
-    __lt__ = TensorDict.__lt__
-    __setitem__ = TensorDict.__setitem__
-    _add_batch_dim = TensorDict._add_batch_dim
-    _check_device = TensorDict._check_device
-    _check_is_shared = TensorDict._check_is_shared
-    _convert_to_tensordict = TensorDict._convert_to_tensordict
-    _index_tensordict = TensorDict._index_tensordict
-    masked_select = TensorDict.masked_select
-    reshape = TensorDict.reshape
-    split = TensorDict.split
-    _to_module = TensorDict._to_module
-    _apply_nest = TensorDict._apply_nest
-    _remove_batch_dim = TensorDict._remove_batch_dim
-    all = TensorDict.all
-    any = TensorDict.any
-    expand = TensorDict.expand
-    _unbind = TensorDict._unbind
-    _get_names_idx = TensorDict._get_names_idx
-    from_dict_instance = TensorDict.from_dict_instance
-
-
-class _UnsqueezedTensorDict(_CustomOpTensorDict):
-    """A lazy view on an unsqueezed TensorDict.
-
-    When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.unsqueeze(dim).squeeze(dim) is tensordict
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> td = TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
-        >>> td_unsqueeze = td.unsqueeze(-1)
-        >>> print(td_unsqueeze.shape)
-        torch.Size([3, 1])
-        >>> print(td_unsqueeze.squeeze(-1) is td)
-        True
-    """
-
-    def _legacy_squeeze(self, dim: int | None) -> T:
-        if dim is not None and dim < 0:
-            dim = self.batch_dims + dim
-        if dim == self.custom_op_kwargs.get("dim"):
-            return self._source
-        return super()._legacy_squeeze(dim)
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        unsqueezed_dim = self.custom_op_kwargs["dim"]
-        diff_to_apply = 1 if dim < unsqueezed_dim else 0
-        list_item_unsqueeze = [
-            item.squeeze(unsqueezed_dim - diff_to_apply) for item in list_item
-        ]
-        return self._source._stack_onto_(list_item_unsqueeze, dim)
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim = self.custom_op_kwargs.get("dim")
-        names.insert(dim, None)
-        return names
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _SqueezedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a squeezed TensorDict.
-
-    See the `UnsqueezedTensorDict` class documentation for more information.
-
-    """
-
-    def _legacy_unsqueeze(self, dim: int) -> T:
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-        inv_op_dim = self.inv_op_kwargs.get("dim")
-        if inv_op_dim < 0:
-            inv_op_dim = self.batch_dims + inv_op_dim + 1
-        if dim == inv_op_dim:
-            return self._source
-        return super()._legacy_unsqueeze(dim)
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        squeezed_dim = self.custom_op_kwargs["dim"]
-        # dim=0, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[4, 5], [4, 5], [4, 5]] => unsq 1
-        # dim=1, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 5], [3, 5], [3, 5], [3, 4]] => unsq 1
-        # dim=2, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 4], [3, 4], ...] => unsq 2
-        diff_to_apply = 1 if dim < squeezed_dim else 0
-        list_item_unsqueeze = [
-            item.unsqueeze(squeezed_dim - diff_to_apply) for item in list_item
-        ]
-        return self._source._stack_onto_(list_item_unsqueeze, dim)
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim = self.custom_op_kwargs["dim"]
-        if self._source.batch_size[dim] == 1:
-            del names[dim]
-        return names
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _ViewedTensorDict(_CustomOpTensorDict):
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        new_dim_list = list(self.custom_op_kwargs.get("size"))
-        new_dim_list += list(source_tensor.shape[self._source.batch_dims :])
-        new_dim = torch.Size(new_dim_list)
-        new_dict = deepcopy(self.custom_op_kwargs)
-        new_dict.update({"size": new_dim})
-        return new_dict
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict:
-        size = list(self.inv_op_kwargs.get("size"))
-        size += list(_shape(tensor)[self.batch_dims :])
-        new_dim = torch.Size(size)
-        new_dict = deepcopy(self.inv_op_kwargs)
-        new_dict.update({"size": new_dim})
-        return new_dict
-
-    def _legacy_view(
-        self, *shape: int, size: list | tuple | torch.Size | None = None
-    ) -> T:
-        if len(shape) == 0 and size is not None:
-            return self._legacy_view(*size)
-        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
-            return self._legacy_view(*shape[0])
-        elif not isinstance(shape, torch.Size):
-            shape = infer_size_impl(shape, self.numel())
-            shape = torch.Size(shape)
-        if shape == self._source.batch_size:
-            return self._source
-        return super()._legacy_view(*shape)
-
-    @property
-    def names(self):
-        return [None] * self.ndim
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _TransposedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a TensorDict with two batch dimensions transposed.
-
-    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.transpose(dims_list, dim).transpose(dims_list, dim) is tensordict
-
-    """
-
-    def _legacy_transpose(self, dim0, dim1) -> T:
-        if dim0 < 0:
-            dim0 = self.ndim + dim0
-        if dim1 < 0:
-            dim1 = self.ndim + dim1
-        if any((dim0 < 0, dim1 < 0)):
-            raise ValueError(
-                "The provided dimensions are incompatible with the tensordict batch-size."
-            )
-        if dim0 == dim1:
-            return self
-        dims = (self.inv_op_kwargs.get("dim0"), self.inv_op_kwargs.get("dim1"))
-        if dim0 in dims and dim1 in dims:
-            return self._source
-        return super()._legacy_transpose(dim0, dim1)
-
-    def add_missing_dims(
-        self, num_dims: int, batch_dims: tuple[int, ...]
-    ) -> tuple[int, ...]:
-        dim_diff = num_dims - len(batch_dims)
-        all_dims = list(range(num_dims))
-        for i, x in enumerate(batch_dims):
-            if x < 0:
-                x = x - dim_diff
-            all_dims[i] = x
-        return tuple(all_dims)
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        return self.custom_op_kwargs
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
-        return self.custom_op_kwargs
-
-    def _stack_onto_(
-        self,
-        # key: str,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        trsp = self.custom_op_kwargs["dim0"], self.custom_op_kwargs["dim1"]
-        if dim == trsp[0]:
-            dim = trsp[1]
-        elif dim == trsp[1]:
-            dim = trsp[0]
-
-        list_permuted_items = []
-        for item in list_item:
-            list_permuted_items.append(item.transpose(*trsp))
-        self._source._stack_onto_(list_permuted_items, dim)
-        return self
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        dim0 = self.custom_op_kwargs["dim0"]
-        dim1 = self.custom_op_kwargs["dim1"]
-        names = [
-            names[dim0] if i == dim1 else names[dim1] if i == dim0 else name
-            for i, name in enumerate(names)
-        ]
-        return names
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-class _PermutedTensorDict(_CustomOpTensorDict):
-    """A lazy view on a TensorDict with the batch dimensions permuted.
-
-    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
-    returned such that the following code snippet works without raising an
-    exception:
-
-        >>> assert tensordict.permute(dims_list, dim).permute(dims_list, dim) is tensordict
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> import torch
-        >>> td = TensorDict({'a': torch.randn(4, 5, 6, 9)}, batch_size=[3])
-        >>> td_permute = td.permute(dims=(2, 1, 0))
-        >>> print(td_permute.shape)
-        torch.Size([6, 5, 4])
-        >>> print(td_permute.permute(dims=(2, 1, 0)) is td)
-        True
-
-    """
-
-    def _legacy_permute(
-        self,
-        *dims_list: int,
-        dims: Sequence[int] | None = None,
-    ) -> T:
-        if len(dims_list) == 0:
-            dims_list = dims
-        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
-            dims_list = dims_list[0]
-        if len(dims_list) != len(self.shape):
-            raise RuntimeError(
-                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
-            )
-        if not len(dims_list) and not self.batch_dims:
-            return self
-        if np.array_equal(dims_list, range(self.batch_dims)):
-            return self
-        if np.array_equal(np.argsort(dims_list), self.inv_op_kwargs.get("dims")):
-            return self._source
-        return super()._legacy_permute(*dims_list)
-
-    def add_missing_dims(
-        self, num_dims: int, batch_dims: tuple[int, ...]
-    ) -> tuple[int, ...]:
-        # Adds the feature dimensions to the permute dims
-        dim_diff = num_dims - len(batch_dims)
-        all_dims = list(range(num_dims))
-        for i, x in enumerate(batch_dims):
-            if x < 0:
-                x = x - dim_diff
-            all_dims[i] = x
-        return tuple(all_dims)
-
-    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
-        new_dims = self.add_missing_dims(
-            len(source_tensor.shape), self.custom_op_kwargs["dims"]
-        )
-        kwargs = deepcopy(self.custom_op_kwargs)
-        kwargs.update({"dims": new_dims})
-        return kwargs
-
-    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
-        new_dims = self.add_missing_dims(
-            self._source.batch_dims + len(_shape(tensor)[self.batch_dims :]),
-            self.custom_op_kwargs["dims"],
-        )
-        kwargs = deepcopy(self.custom_op_kwargs)
-        kwargs.update({"dims": tuple(np.argsort(new_dims))})
-        return kwargs
-
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        permute_dims = self.custom_op_kwargs["dims"]
-        inv_permute_dims = np.argsort(permute_dims)
-        new_dim = [i for i, v in enumerate(inv_permute_dims) if v == dim][0]
-        inv_permute_dims = [p for p in inv_permute_dims if p != dim]
-        inv_permute_dims = np.argsort(np.argsort(inv_permute_dims))
-
-        list_permuted_items = []
-        for item in list_item:
-            perm = list(inv_permute_dims) + list(
-                range(self.batch_dims - 1, item.ndimension())
-            )
-            list_permuted_items.append(item.permute(*perm))
-        self._source._stack_onto_(list_permuted_items, new_dim)
-        return self
-
-    @property
-    def names(self):
-        names = copy(self._source.names)
-        return [names[i] for i in self.custom_op_kwargs["dims"]]
-
-    @names.setter
-    def names(self, value):
-        if value[: self.batch_dims] == self.names:
-            return
-        raise RuntimeError(
-            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
-        )
-
-
-def _iter_items_lazystack(
-    tensordict: LazyStackedTensorDict, return_none_for_het_values: bool = False
-) -> Iterator[tuple[str, CompatibleType]]:
-    for key in tensordict.tensordicts[0].keys():
-        values = tensordict._maybe_get_list(key)
-        if values is not None:
-            yield key, values
-
-
-_register_tensor_class(LazyStackedTensorDict)
-_register_tensor_class(_CustomOpTensorDict)
-_register_tensor_class(_PermutedTensorDict)
-_register_tensor_class(_SqueezedTensorDict)
-_register_tensor_class(_UnsqueezedTensorDict)
-_register_tensor_class(_TransposedTensorDict)
-_register_tensor_class(_ViewedTensorDict)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import json
+import numbers
+import os
+import re
+import textwrap
+import weakref
+from collections import defaultdict
+from copy import copy, deepcopy
+from functools import wraps
+from pathlib import Path
+from textwrap import indent
+from typing import Any, Callable, Iterator, OrderedDict, Sequence, Tuple, Type
+
+import numpy as np
+import torch
+import torch.distributed as dist
+
+from tensordict.memmap import MemoryMappedTensor
+
+try:
+    from functorch import dim as ftdim
+
+    _has_funcdim = True
+except ImportError:
+    from tensordict.utils import _ftdim_mock as ftdim
+
+    _has_funcdim = False
+from tensordict._td import _SubTensorDict, _TensorDictKeysView, TensorDict
+from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
+from tensordict.base import (
+    _is_tensor_collection,
+    _NESTED_TENSORS_AS_LISTS,
+    _register_tensor_class,
+    BEST_ATTEMPT_INPLACE,
+    CompatibleType,
+    is_tensor_collection,
+    NO_DEFAULT,
+    T,
+    TensorDictBase,
+)
+from tensordict.utils import (
+    _broadcast_tensors,
+    _get_shape_from_args,
+    _getitem_batch_size,
+    _is_number,
+    _parse_to,
+    _renamed_inplace_method,
+    _shape,
+    _td_fields,
+    as_decorator,
+    cache,
+    convert_ellipsis_to_idx,
+    DeviceType,
+    erase_cache,
+    expand_right,
+    IndexType,
+    infer_size_impl,
+    is_non_tensor,
+    is_tensorclass,
+    KeyedJaggedTensor,
+    lock_blocked,
+    NestedKey,
+)
+from torch import Tensor
+
+
+_has_functorch = False
+try:
+    try:
+        from torch._C._functorch import (
+            _add_batch_dim,
+            _remove_batch_dim,
+            is_batchedtensor,
+        )
+    except ImportError:
+        from functorch._C import is_batchedtensor
+
+    _has_functorch = True
+except ImportError:
+    _has_functorch = False
+
+    def is_batchedtensor(tensor: Tensor) -> bool:
+        """Placeholder for the functorch function."""
+        return False
+
+
+class _LazyStackedTensorDictKeysView(_TensorDictKeysView):
+    tensordict: LazyStackedTensorDict
+
+    def __len__(self) -> int:
+        return len(self._keys())
+
+    def _keys(self) -> list[str]:
+        result = self.tensordict._key_list()
+        if self.is_leaf is _NESTED_TENSORS_AS_LISTS:
+            return [
+                (key, str(i))
+                for key in result
+                for i in range(len(self.tensordict.tensordicts))
+            ]
+        return result
+
+    def __contains__(self, item):
+        item = _unravel_key_to_tuple(item)
+        if item[0] in self.tensordict._iterate_over_keys():
+            if self.leaves_only:
+                return not _is_tensor_collection(self.tensordict.entry_class(item[0]))
+            has_first_key = True
+        else:
+            has_first_key = False
+        if not has_first_key or len(item) == 1:
+            return has_first_key
+        # otherwise take the long way
+        return all(
+            item[1:]
+            in tensordict.get(item[0]).keys(self.include_nested, self.leaves_only)
+            for tensordict in self.tensordict.tensordicts
+        )
+
+
+def _fails_exclusive_keys(func):
+    @wraps(func)
+    def newfunc(self, *args, **kwargs):
+        if self._has_exclusive_keys:
+            raise RuntimeError(
+                f"the method {func.__name__} cannot complete when there are exclusive keys."
+            )
+        parent_func = getattr(TensorDictBase, func.__name__, None)
+        if parent_func is None:
+            parent_func = getattr(TensorDict, func.__name__)
+        return parent_func(self, *args, **kwargs)
+
+    return newfunc
+
+
+class LazyStackedTensorDict(TensorDictBase):
+    """A Lazy stack of TensorDicts.
+
+    When stacking TensorDicts together, the default behaviour is to put them
+    in a stack that is not instantiated.
+    This allows to seamlessly work with stacks of tensordicts with operations
+    that will affect the original tensordicts.
+
+    Args:
+         *tensordicts (TensorDict instances): a list of tensordict with
+            same batch size.
+         stack_dim (int): a dimension (between `-td.ndimension()` and
+            `td.ndimension()-1` along which the stack should be performed.
+         hook_out (callable, optional): a callable to execute after :meth:`~.get`.
+         hook_in (callable, optional): a callable to execute before :meth:`~.set`.
+         stack_dim_name (str, optional): the name of the stack dimension.
+            Defaults to ``None``.
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> tds = [TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
+        ...     for _ in range(10)]
+        >>> td_stack = torch.stack(tds, -1)
+        >>> print(td_stack.shape)
+        torch.Size([3, 10])
+        >>> print(td_stack.get("a").shape)
+        torch.Size([3, 10, 4])
+        >>> print(td_stack[:, 0] is tds[0])
+        True
+
+    """
+
+    _is_vmapped: bool = False
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        from tensordict._torch_func import LAZY_TD_HANDLED_FUNCTIONS
+
+        if func in LAZY_TD_HANDLED_FUNCTIONS:
+            if kwargs is None:
+                kwargs = {}
+            if func not in LAZY_TD_HANDLED_FUNCTIONS or not all(
+                issubclass(t, (Tensor, TensorDictBase)) for t in types
+            ):
+                return NotImplemented
+            return LAZY_TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+        else:
+            return super().__torch_function__(func, types, args, kwargs)
+
+    _td_dim_name = None
+    _safe = False
+    _lazy = True
+
+    def __init__(
+        self,
+        *tensordicts: T,
+        stack_dim: int = 0,
+        hook_out: callable | None = None,
+        hook_in: callable | None = None,
+        batch_size: Sequence[int] | None = None,  # TODO: remove
+        stack_dim_name: str | None = None,
+    ) -> None:
+        self._is_locked = None
+
+        # sanity check
+        N = len(tensordicts)
+        if not N:
+            raise RuntimeError(
+                "at least one tensordict must be provided to "
+                "StackedTensorDict to be instantiated"
+            )
+        if stack_dim < 0:
+            raise RuntimeError(
+                f"stack_dim must be non negative, got stack_dim={stack_dim}"
+            )
+        _batch_size = tensordicts[0].batch_size
+        device = tensordicts[0].device
+        if stack_dim > len(_batch_size):
+            raise RuntimeError(
+                f"Stack dim {stack_dim} is too big for batch size {_batch_size}."
+            )
+
+        for td in tensordicts[1:]:
+            if not is_tensor_collection(td):
+                raise TypeError(
+                    "Expected all inputs to be TensorDictBase instances but got "
+                    f"{type(td)} instead."
+                )
+            _bs = td.batch_size
+            _device = td.device
+            if device != _device:
+                raise RuntimeError(f"devices differ, got {device} and {_device}")
+            if _bs != _batch_size:
+                raise RuntimeError(
+                    f"batch sizes in tensordicts differs, StackedTensorDict "
+                    f"cannot be created. Got td[0].batch_size={_batch_size} "
+                    f"and td[i].batch_size={_bs} "
+                )
+        self.tensordicts: list[TensorDictBase] = list(tensordicts)
+        self.stack_dim = stack_dim
+        self._batch_size = self._compute_batch_size(_batch_size, stack_dim, N)
+        self.hook_out = hook_out
+        self.hook_in = hook_in
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+        if stack_dim_name is not None:
+            self._td_dim_name = stack_dim_name
+
+    # These attributes should never be set
+    @property
+    def _is_shared(self):
+        return all(td._is_shared for td in self.tensordicts)
+
+    @property
+    def _is_memmap(self):
+        return all(td._is_memmap for td in self.tensordicts)
+
+    @property
+    @cache  # noqa: B019
+    def _has_exclusive_keys(self):
+        keys = None
+        for td in self.tensordicts:
+            _keys = set(td.keys(True, True))
+            if keys is None:
+                keys = _keys
+            else:
+                if keys != _keys:
+                    return True
+        else:
+            return False
+
+    @_fails_exclusive_keys
+    def to_dict(self) -> dict[str, Any]:
+        ...
+
+    @_fails_exclusive_keys
+    def state_dict(
+        self,
+        destination=None,
+        prefix="",
+        keep_vars=False,
+        flatten=False,
+    ) -> OrderedDict[str, Any]:
+        ...
+
+    @_fails_exclusive_keys
+    def flatten_keys(
+        self,
+        separator: str = ".",
+        inplace: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> T:
+        ...
+
+    @_fails_exclusive_keys
+    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
+        ...
+
+    @property
+    def device(self) -> torch.device | None:
+        # devices might have changed, so we check that they're all the same
+        device_set = {td.device for td in self.tensordicts}
+        if len(device_set) != 1:
+            return None
+        device = self.tensordicts[0].device
+        return device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        for t in self.tensordicts:
+            t.device = value
+
+    def clear_device_(self) -> T:
+        for td in self.tensordicts:
+            td.clear_device_()
+        return self
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        return self._batch_size_setter(new_size)
+
+    @property
+    @cache  # noqa
+    def names(self):
+        names = list(self.tensordicts[0].names)
+        for td in self.tensordicts[1:]:
+            if names != td.names:
+                raise ValueError(
+                    f"Not all dim names match, got {names} and {td.names}."
+                )
+        names.insert(self.stack_dim, self._td_dim_name)
+        return names
+
+    @names.setter
+    @erase_cache  # a nested lazy stacked tensordict is not apparent to the root
+    def names(self, value):
+        if value is None:
+            for td in self.tensordicts:
+                td.names = None
+            self._td_dim_name = None
+        else:
+            names_c = list(value)
+            name = names_c[self.stack_dim]
+            self._td_dim_name = name
+            del names_c[self.stack_dim]
+            for td in self.tensordicts:
+                if td._check_dim_name(name):
+                    # TODO: should reset names here
+                    raise ValueError(f"The dimension name {name} is already taken.")
+                td.rename_(*names_c)
+
+    def _rename_subtds(self, names):
+        # remove the name of the stack dim
+        names = list(names)
+        del names[self.stack_dim]
+        for td in self.tensordicts:
+            td.names = names
+
+    def _has_names(self):
+        return all(td._has_names() for td in self.tensordicts)
+
+    def _erase_names(self):
+        self._td_dim_name = None
+        for td in self.tensordicts:
+            td._erase_names()
+
+    def get_item_shape(self, key):
+        """Gets the shape of an item in the lazy stack.
+
+        Heterogeneous dimensions are returned as -1.
+
+        This implementation is inefficient as it will attempt to stack the items
+        to compute their shape, and should only be used for printing.
+        """
+        try:
+            item = self.get(key)
+            return item.shape
+        except RuntimeError as err:
+            if re.match(
+                r"Found more than one unique shape in the tensors|Could not run 'aten::stack' with arguments from the",
+                str(err),
+            ):
+                shape = None
+                for td in self.tensordicts:
+                    if shape is None:
+                        shape = list(td.get_item_shape(key))
+                    else:
+                        _shape = td.get_item_shape(key)
+                        if len(shape) != len(_shape):
+                            shape = [-1]
+                            return torch.Size(shape)
+                        shape = [
+                            s1 if s1 == s2 else -1 for (s1, s2) in zip(shape, _shape)
+                        ]
+                shape.insert(self.stack_dim, len(self.tensordicts))
+                return torch.Size(shape)
+            else:
+                raise err
+
+    def is_shared(self) -> bool:
+        are_shared = [td.is_shared() for td in self.tensordicts]
+        are_shared = [value for value in are_shared if value is not None]
+        if not len(are_shared):
+            return None
+        if any(are_shared) and not all(are_shared):
+            raise RuntimeError(
+                f"tensordicts shared status mismatch, got {sum(are_shared)} "
+                f"shared tensordicts and "
+                f"{len(are_shared) - sum(are_shared)} non shared tensordict "
+            )
+        return all(are_shared)
+
+    def is_memmap(self) -> bool:
+        are_memmap = [td.is_memmap() for td in self.tensordicts]
+        if any(are_memmap) and not all(are_memmap):
+            raise RuntimeError(
+                f"tensordicts memmap status mismatch, got {sum(are_memmap)} "
+                f"memmap tensordicts and "
+                f"{len(are_memmap) - sum(are_memmap)} non memmap tensordict "
+            )
+        return all(are_memmap)
+
+    @staticmethod
+    def _compute_batch_size(
+        batch_size: torch.Size, stack_dim: int, N: int
+    ) -> torch.Size:
+        s = list(batch_size)
+        s.insert(stack_dim, N)
+        return torch.Size(s)
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ) -> T:
+        try:
+            inplace = self._convert_inplace(inplace, key)
+        except KeyError as e:
+            raise KeyError(
+                "setting a value in-place on a stack of TensorDict is only "
+                "permitted if all members of the stack have this key in "
+                "their register."
+            ) from e
+        if not validated:
+            value = self._validate_value(value)
+            validated = True
+        if self._is_vmapped:
+            value = self.hook_in(value)
+        values = value.unbind(self.stack_dim)
+        for tensordict, item in zip(self.tensordicts, values):
+            tensordict._set_str(
+                key,
+                item,
+                inplace=inplace,
+                validated=validated,
+                ignore_lock=ignore_lock,
+                non_blocking=non_blocking,
+            )
+        return self
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        non_blocking: bool = False,
+    ) -> T:
+        if len(key) == 1:
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        # if inplace is not False:  # inplace could be None
+        #     # we don't want to end up in the situation where one tensordict has
+        #     # inplace=True and another one inplace=False because inplace was loose.
+        #     # Worse could be writing with inplace=True up until some level then to
+        #     # realize the key is missing in one td, raising an exception and having
+        #     # messed up the data. Hence we must start by checking if the key
+        #     # is present.
+        #     has_key = key in self.keys(True)
+        #     if inplace is True and not has_key:  # inplace could be None
+        #         raise KeyError(
+        #             TensorDictBase.KEY_ERROR.format(
+        #                 key, self.__class__.__name__, sorted(self.keys())
+        #             )
+        #         )
+        #     inplace = has_key
+        if not validated:
+            value = self._validate_value(value)
+            validated = True
+        if self._is_vmapped:
+            value = self.hook_in(value)
+        values = value.unbind(self.stack_dim)
+        for tensordict, item in zip(self.tensordicts, values):
+            tensordict._set_tuple(
+                key,
+                item,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        return self
+
+    def _split_index(self, index):
+        """Given a tuple index, split it in as many indices as the number of tensordicts.
+
+        Returns:
+            a dictionary with {index-of-td: index-within-td}
+            the number of single dim indices until stack dim
+            a boolean indicating if the index along the stack dim is an integer
+        """
+        if not isinstance(index, tuple):
+            index = (index,)
+        index = convert_ellipsis_to_idx(index, self.batch_size)
+        index = _broadcast_tensors(index)
+        out = []
+        num_single = 0
+        num_none = 0
+        isinteger = False
+        is_nd_tensor = False
+        cursor = 0  # the dimension cursor
+        selected_td_idx = torch.arange(len(self.tensordicts))
+        has_bool = False
+        num_squash = 0
+        encountered_tensor = False
+        for i, idx in enumerate(index):  # noqa: B007
+            cursor_incr = 1
+            if idx is None:
+                out.append(None)
+                num_none += cursor <= self.stack_dim
+                continue
+            if cursor == self.stack_dim:
+                # we need to check which tds need to be indexed
+                if isinstance(idx, ftdim.Dim):
+                    raise ValueError(
+                        "Cannot index a lazy stacked tensordict along the stack dimension with "
+                        "a first-class dimension index. Consider consolidating the tensordict first "
+                        "using `tensordict.contiguous()`."
+                    )
+                elif isinstance(idx, slice) or _is_number(idx):
+                    selected_td_idx = range(len(self.tensordicts))[idx]
+                    if not isinstance(selected_td_idx, range):
+                        isinteger = True
+                        selected_td_idx = [selected_td_idx]
+                elif isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
+                        # we mark that we need to dispatch the indices across stack idx
+                        has_bool = True
+                        # split mask along dim
+                        individual_masks = idx = idx.unbind(0)
+                        selected_td_idx = range(len(self.tensordicts))
+                        out.append(idx)
+                        split_dim = self.stack_dim - num_single
+                        mask_loc = i
+                    else:
+                        is_nd_tensor = True
+                        if not encountered_tensor:
+                            # num_single -= idx.ndim - 1
+                            encountered_tensor = True
+                        else:
+                            num_single += 1
+                        selected_td_idx = idx
+                        # out.append(idx.unbind(0))
+                else:
+                    raise TypeError(f"Invalid index type: {type(idx)}.")
+            else:
+                if _is_number(idx) and cursor < self.stack_dim:
+                    num_single += 1
+                if _is_number(idx) or isinstance(
+                    idx,
+                    (
+                        ftdim.Dim,
+                        slice,
+                    ),
+                ):
+                    out.append(idx)
+                elif isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
+                        cursor_incr = idx.ndim
+                        if cursor < self.stack_dim:
+                            num_squash += cursor_incr - 1
+                        if (
+                            cursor < self.stack_dim
+                            and cursor + cursor_incr > self.stack_dim
+                        ):
+                            # we mark that we need to dispatch the indices across stack idx
+                            has_bool = True
+                            # split mask along dim
+                            # relative_stack_dim = self.stack_dim - cursor - cursor_incr
+                            individual_masks = idx = idx.unbind(0)
+                            selected_td_idx = range(self.shape[i])
+                            split_dim = cursor - num_single
+                            mask_loc = i
+                    elif cursor < self.stack_dim:
+                        # we know idx is not a single integer, so it must have
+                        # a dimension. We play with num_single, reducing it
+                        # by the number of dims of idx: if idx has 3 dims, our
+                        # indexed tensor will have 2 more dimensions, going in
+                        # the opposite direction of indexing with a single integer,
+                        # smth[torch.tensor(1)].ndim = smth.ndim-1
+                        # smth[torch.tensor([1])].ndim = smth.ndim
+                        # smth[torch.tensor([[1]])].ndim = smth.ndim+1
+                        if not encountered_tensor:
+                            num_single -= idx.ndim - 1
+                            encountered_tensor = True
+                        else:
+                            num_single += 1
+                    out.append(idx)
+                else:
+                    raise TypeError(f"Invalid index type: {type(idx)}.")
+            cursor += cursor_incr
+        if has_bool:
+            out = tuple(
+                tuple(idx if not isinstance(idx, tuple) else idx[i] for idx in out)
+                for i in selected_td_idx
+            )
+            return {
+                "index_dict": {i: out[i] for i in selected_td_idx},
+                "num_single": num_single,
+                "isinteger": isinteger,
+                "has_bool": has_bool,
+                "individual_masks": individual_masks,
+                "split_dim": split_dim,
+                "mask_loc": mask_loc,
+                "is_nd_tensor": is_nd_tensor,
+                "num_none": num_none,
+                "num_squash": num_squash,
+            }
+        elif is_nd_tensor:
+
+            def isindexable(idx):
+                if isinstance(idx, torch.Tensor):
+                    if idx.dtype == torch.bool:
+                        return False
+                    return True
+                if isinstance(idx, (tuple, list, range)):
+                    return True
+                return False
+
+            def outer_list(tensor_index, tuple_index):
+                """Converts a tensor and a tuple to a nested list where each leaf is a (int, index) tuple where the index only points to one element."""
+                if isinstance(tensor_index, torch.Tensor):
+                    list_index = tensor_index.tolist()
+                else:
+                    list_index = tensor_index
+                list_result = []
+
+                def index_tuple_index(i, convert=False):
+                    for idx in tuple_index:
+                        if isindexable(idx):
+                            if convert:
+                                yield int(idx[i])
+                            else:
+                                yield idx[i]
+                        else:
+                            yield idx
+
+                for i, idx in enumerate(list_index):
+                    if isinstance(idx, int):
+                        list_result.append(
+                            (idx, tuple(index_tuple_index(i, convert=True)))
+                        )
+                    elif isinstance(idx, list):
+                        list_result.append(outer_list(idx, tuple(index_tuple_index(i))))
+                    else:
+                        raise NotImplementedError
+                return list_result
+
+            return {
+                "index_dict": outer_list(selected_td_idx, out),
+                "num_single": num_single,
+                "isinteger": isinteger,
+                "has_bool": has_bool,
+                "is_nd_tensor": is_nd_tensor,
+                "num_none": num_none,
+                "num_squash": num_squash,
+            }
+        return {
+            "index_dict": {i: tuple(out) for i in selected_td_idx},
+            "num_single": num_single,
+            "isinteger": isinteger,
+            "has_bool": has_bool,
+            "is_nd_tensor": is_nd_tensor,
+            "num_none": num_none,
+            "num_squash": num_squash,
+        }
+
+    def _set_at_str(self, key, value, index, *, validated, non_blocking: bool):
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if self._is_vmapped:
+            value = self.hook_in(value)
+        split_index = self._split_index(index)
+        converted_idx = split_index["index_dict"]
+        num_single = split_index["num_single"]
+        isinteger = split_index["isinteger"]
+        has_bool = split_index["has_bool"]
+        num_squash = split_index.get("num_squash", 0)
+        num_none = split_index.get("num_none", 0)
+        is_nd_tensor = split_index.get("is_nd_tensor", False)
+        if isinteger:
+            # this will break if the index along the stack dim is [0] or :1 or smth
+            for i, _idx in converted_idx.items():
+                self.tensordicts[i]._set_at_str(
+                    key, value, _idx, validated=validated, non_blocking=non_blocking
+                )
+            return self
+        if is_nd_tensor:
+            unbind_dim = self.stack_dim - num_single + num_none - num_squash
+            value_unbind = value.unbind(unbind_dim)
+
+            def set_at_str(converted_idx):
+                for i, item in enumerate(converted_idx):
+                    if isinstance(item, list):
+                        set_at_str(item)
+                    else:
+                        _value = value_unbind[i]
+                        stack_idx, idx = item
+                        self.tensordicts[stack_idx]._set_at_str(
+                            key,
+                            _value,
+                            idx,
+                            validated=validated,
+                            non_blocking=non_blocking,
+                        )
+
+            set_at_str(converted_idx)
+            return self
+        elif not has_bool:
+            unbind_dim = self.stack_dim - num_single + num_none - num_squash
+            value_unbind = value.unbind(unbind_dim)
+            for (i, _idx), _value in zip(
+                converted_idx.items(),
+                value_unbind,
+            ):
+                self.tensordicts[i]._set_at_str(
+                    key, _value, _idx, validated=validated, non_blocking=non_blocking
+                )
+        else:
+            # we must split, not unbind
+            mask_unbind = split_index["individual_masks"]
+            split_dim = split_index["split_dim"]
+            splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
+            value_unbind = value.split(splits, split_dim)
+            if mask_unbind[0].ndim == 0:
+                # we can return a stack
+                for (i, _idx), mask, _value in zip(
+                    converted_idx.items(),
+                    mask_unbind,
+                    value_unbind,
+                ):
+                    if mask.any():
+                        self.tensordicts[i]._set_at_str(
+                            key,
+                            _value,
+                            _idx,
+                            validated=validated,
+                            non_blocking=non_blocking,
+                        )
+            else:
+                for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
+                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                    self[self_idx]._set_at_str(
+                        key,
+                        _value,
+                        _idx,
+                        validated=validated,
+                        non_blocking=non_blocking,
+                    )
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
+        if len(key) == 1:
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
+        # get the "last" tds
+        tds = []
+        for td in self.tensordicts:
+            tds.append(td.get(key[:-1]))
+        # build only a single lazy stack from it
+        # (if the stack is a stack of stacks this won't be awesomely efficient
+        # but then we'd need to splut the value (which we can do) and recompute
+        # the sub-index for each td, which is a different story!
+        td = LazyStackedTensorDict(
+            *tds, stack_dim=self.stack_dim, hook_out=self.hook_out, hook_in=self.hook_in
+        )
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if self._is_vmapped:
+            value = self.hook_in(value)
+        item = td._get_str(key, NO_DEFAULT)
+        item[idx] = value
+        td._set_str(key, item, inplace=True, validated=True, non_blocking=non_blocking)
+        return self
+
+    def _legacy_unsqueeze(self, dim: int) -> T:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+
+        if (dim > self.batch_dims) or (dim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        if dim <= self.stack_dim:
+            stack_dim = self.stack_dim + 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return type(self)(
+            *(tensordict.unsqueeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+            stack_dim_name=self._td_dim_name,
+        )
+
+    def _legacy_squeeze(self, dim: int | None = None) -> T:
+        """Squeezes all tensors for a dimension comprised in between `-td.batch_dims+1` and `td.batch_dims-1` and returns them in a new tensordict.
+
+        Args:
+            dim (Optional[int]): dimension along which to squeeze. If dim is None, all singleton dimensions will be squeezed. dim is None by default.
+
+        """
+        if dim is None:
+            size = self.size()
+            if len(self.size()) == 1 or size.count(1) == 0:
+                return self
+            first_singleton_dim = size.index(1)
+            return self.squeeze(first_singleton_dim).squeeze()
+
+        if dim < 0:
+            dim = self.batch_dims + dim
+
+        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between 0 and "
+                f"td.batch_dims only. Got dim={dim} and batch_size"
+                f"={self.batch_size}."
+            )
+
+        if dim >= self.batch_dims or self.batch_size[dim] != 1:
+            return self
+        if dim == self.stack_dim:
+            return self.tensordicts[0]
+        elif dim < self.stack_dim:
+            stack_dim = self.stack_dim - 1
+        else:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        return type(self)(
+            *(tensordict.squeeze(dim) for tensordict in self.tensordicts),
+            stack_dim=stack_dim,
+            stack_dim_name=self._td_dim_name,
+        )
+
+    def _unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
+        if dim == self.stack_dim:
+            return tuple(self.tensordicts)
+        else:
+            # return a stack of unbound tensordicts
+            out = []
+            new_dim = dim if dim < self.stack_dim else dim - 1
+            new_stack_dim = (
+                self.stack_dim if dim > self.stack_dim else self.stack_dim - 1
+            )
+            for td in self.tensordicts:
+                out.append(td._unbind(new_dim))
+            return tuple(self.lazy_stack(vals, new_stack_dim) for vals in zip(*out))
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        if dim == self.stack_dim:
+            for source, tensordict_dest in zip(list_item, self.tensordicts):
+                tensordict_dest.update_(source)
+        else:
+            for i, td in enumerate(list_item):
+                idx = (slice(None),) * dim + (i,)
+                self.update_at_(td, idx)
+        return self
+
+    def _maybe_get_list(self, key):
+        vals = []
+        for td in self.tensordicts:
+            if isinstance(td, LazyStackedTensorDict):
+                val = td._maybe_get_list(key)
+            else:
+                val = td._get_str(key, None)
+                if _is_tensor_collection(type(val)):
+                    return self._get_str(key, NO_DEFAULT)
+                elif val is None:
+                    return None
+            vals.append(val)
+        return vals
+
+    @cache  # noqa: B019
+    def _get_str(
+        self,
+        key: NestedKey,
+        default: Any = NO_DEFAULT,
+    ) -> CompatibleType:
+        # we can handle the case where the key is a tuple of length 1
+        tensors = []
+        for td in self.tensordicts:
+            tensors.append(td._get_str(key, default=default))
+            if (
+                tensors[-1] is default
+                and not isinstance(default, (KeyedJaggedTensor, torch.Tensor))
+                and not is_tensor_collection(default)
+            ):
+                # then we consider this default as non-stackable and return prematurly
+                return default
+        try:
+            out = self.lazy_stack(
+                tensors, self.stack_dim, stack_dim_name=self._td_dim_name
+            )
+            if _is_tensor_collection(out.__class__):
+                if isinstance(out, LazyStackedTensorDict):
+                    # then it's a LazyStackedTD
+                    out.hook_out = self.hook_out
+                    out.hook_in = self.hook_in
+                    out._is_vmapped = self._is_vmapped
+                    incr = 0 if not self._is_vmapped else 1
+                    out._batch_size = (
+                        self._batch_size
+                        + out.batch_size[(len(self._batch_size) + incr) :]
+                    )
+                elif is_tensorclass(out):
+                    # then it's a tensorclass
+                    out._tensordict.hook_out = self.hook_out
+                    out._tensordict.hook_in = self.hook_in
+                    out._tensordict._is_vmapped = self._is_vmapped
+                    incr = 0 if not self._is_vmapped else 1
+                    out._tensordict._batch_size = (
+                        self._batch_size
+                        + out._tensordict.batch_size[(len(self._batch_size) + incr) :]
+                    )
+                else:
+                    raise RuntimeError
+            elif self.hook_out is not None:
+                out = self.hook_out(out)
+            return out
+        except RuntimeError as err:
+            if "stack expects each tensor to be equal size" in str(err):
+                shapes = {_shape(tensor) for tensor in tensors}
+                raise RuntimeError(
+                    f"Found more than one unique shape in the tensors to be "
+                    f"stacked ({shapes}). This is likely due to a modification "
+                    f"of one of the stacked TensorDicts, where a key has been "
+                    f"updated/created with an uncompatible shape. If the entries "
+                    f"are intended to have a different shape, use the get_nestedtensor "
+                    f"method instead."
+                )
+            else:
+                raise err
+
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], None)
+        if first is None:
+            return self._default_get(key[0], default)
+        if len(key) == 1:
+            return first
+        try:
+            if isinstance(first, KeyedJaggedTensor):
+                if len(key) != 2:
+                    raise ValueError(f"Got too many keys for a KJT: {key}.")
+                return first[key[-1]]
+            else:
+                return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
+
+    @classmethod
+    def lazy_stack(
+        cls,
+        items: Sequence[TensorDictBase],
+        dim: int = 0,
+        *,
+        device: DeviceType | None = None,
+        out: T | None = None,
+        stack_dim_name: str | None = None,
+    ) -> T:
+        """Stacks tensordicts in a LazyStackedTensorDict."""
+        if not items:
+            raise RuntimeError("items cannot be empty")
+
+        if all(isinstance(item, torch.Tensor) for item in items):
+            return torch.stack(items, dim=dim, out=out)
+        if all(is_non_tensor(tensordict) for tensordict in items):
+            # Non-tensor data (Data or Stack) are stacked using NonTensorStack
+            # If the content is identical (not equal but same id) this does not
+            # require additional memory.
+            from .tensorclass import NonTensorStack
+
+            return NonTensorStack(*items, stack_dim=dim)
+        if all(
+            is_tensorclass(item) and type(item) == type(items[0])  # noqa: E721
+            for item in items
+        ):
+            lazy_stack = cls.lazy_stack(
+                [item._tensordict for item in items],
+                dim=dim,
+                out=out,
+                stack_dim_name=stack_dim_name,
+            )
+            # we take the first non_tensordict by convention
+            return type(items[0])._from_tensordict(
+                tensordict=lazy_stack, non_tensordict=items[0]._non_tensordict
+            )
+
+        batch_size = items[0].batch_size
+        if dim < 0:
+            dim = len(batch_size) + dim + 1
+
+        for td in items[1:]:
+            if td.batch_size != items[0].batch_size:
+                raise RuntimeError(
+                    "stacking tensordicts requires them to have congruent batch sizes, "
+                    f"got td1.batch_size={td.batch_size} and td2.batch_size="
+                    f"{items[0].batch_size}"
+                )
+
+        if out is None:
+            # We need to handle tensordicts with exclusive keys and tensordicts with
+            # mismatching shapes.
+            # The first case is handled within _check_keys which fails if keys
+            # don't match exactly.
+            # The second requires a check over the tensor shapes.
+            return LazyStackedTensorDict(
+                *items, stack_dim=dim, stack_dim_name=stack_dim_name
+            )
+        else:
+            batch_size = list(batch_size)
+            batch_size.insert(dim, len(items))
+            batch_size = torch.Size(batch_size)
+
+            if out.batch_size != batch_size:
+                raise RuntimeError(
+                    "out.batch_size and stacked batch size must match, "
+                    f"got out.batch_size={out.batch_size} and batch_size"
+                    f"={batch_size}"
+                )
+
+            try:
+                out._stack_onto_(items, dim)
+            except KeyError as err:
+                raise err
+        return out
+
+    @classmethod
+    def maybe_dense_stack(
+        cls,
+        items: Sequence[TensorDictBase],
+        dim: int = 0,
+        out: T | None = None,
+        strict: bool = False,
+    ) -> T:
+        """Stacks tensors or tensordicts densly if possible, or onto a LazyStackedTensorDict otherwise.
+
+        Examples:
+            >>> td0 = TensorDict({"a": 0}, [])
+            >>> td1 = TensorDict({"b": 0}, [])
+            >>> LazyStackedTensorDict.maybe_dense_stack([td0, td0])  # returns a TensorDict with shape [2]
+            >>> LazyStackedTensorDict.maybe_dense_stack([td0, td1])  # returns a LazyStackedTensorDict with shape [2]
+            >>> LazyStackedTensorDict.maybe_dense_stack(list(torch.randn(2)))  # returns a torch.Tensor with shape [2]
+        """
+        from ._torch_func import _stack
+
+        return _stack(items, dim=dim, out=out, strict=strict, maybe_dense_stack=True)
+
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        if self.is_memmap():
+            td = LazyStackedTensorDict.lazy_stack(
+                [td.cpu().as_tensor() for td in self.tensordicts], 0
+            )
+        else:
+            td = self
+        if in_dim < 0:
+            in_dim = self.ndim + in_dim
+        if in_dim == self.stack_dim:
+            result = self._cached_add_batch_dims(
+                td, in_dim=in_dim, vmap_level=vmap_level
+            )
+        else:
+            if in_dim < td.stack_dim:
+                # then we'll stack along a dim before
+                stack_dim = td.stack_dim - 1
+            else:
+                in_dim = in_dim - 1
+                stack_dim = td.stack_dim
+            tds = [
+                td._fast_apply(
+                    lambda _arg: _add_batch_dim(_arg, in_dim, vmap_level),
+                    batch_size=[b for i, b in enumerate(td.batch_size) if i != in_dim],
+                    names=[name for i, name in enumerate(td.names) if i != in_dim],
+                )
+                for td in td.tensordicts
+            ]
+            result = LazyStackedTensorDict(*tds, stack_dim=stack_dim)
+        if self.is_locked:
+            result.lock_()
+        return result
+
+    @classmethod
+    def _cached_add_batch_dims(cls, td, in_dim, vmap_level):
+        # we return a stack with hook_out, and hack the batch_size and names
+        # Per se it is still a LazyStack but the stacking dim is "hidden" from
+        # the outside
+        out = td.copy()
+
+        def hook_out(tensor, in_dim=in_dim, vmap_level=vmap_level):
+            if _is_tensor_collection(type(tensor)):
+                return tensor._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
+            return _add_batch_dim(tensor, in_dim, vmap_level)
+
+        n = len(td.tensordicts)
+
+        def hook_in(
+            tensor,
+            out_dim=in_dim,
+            batch_size=n,
+            vmap_level=vmap_level,
+        ):
+            if _is_tensor_collection(type(tensor)):
+                return tensor._remove_batch_dim(vmap_level, batch_size, out_dim)
+            return _remove_batch_dim(tensor, vmap_level, batch_size, out_dim)
+
+        out.hook_out = hook_out
+        out.hook_in = hook_in
+        out._is_vmapped = True
+        out._batch_size = torch.Size(
+            [dim for i, dim in enumerate(out._batch_size) if i != out.stack_dim]
+        )
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        if self.hook_out is not None:
+            # this is the hacked version. We just need to remove the hook_out and
+            # reset a proper batch size
+            result = LazyStackedTensorDict(
+                *self.tensordicts,
+                stack_dim=out_dim,
+            )
+            # return self._cache_remove_batch_dim(vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim)
+        else:
+            # we must call _remove_batch_dim on all tensordicts
+            # batch_size: size of the batch when we unhide it.
+            # out_dim: dimension where the output will be found
+            new_batch_size = list(self.batch_size)
+            new_batch_size.insert(out_dim, batch_size)
+            new_names = list(self.names)
+            new_names.insert(out_dim, None)
+            # rebuild the lazy stack
+            # the stack dim is the same if the out_dim is past it, but it
+            # must be incremented by one otherwise.
+            # In the first case, the out_dim must be decremented by one
+            if out_dim > self.stack_dim:
+                stack_dim = self.stack_dim
+                out_dim = out_dim - 1
+            else:
+                stack_dim = self.stack_dim + 1
+            result = LazyStackedTensorDict(
+                *[
+                    td._remove_batch_dim(
+                        vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                    )
+                    for td in self.tensordicts
+                ],
+                stack_dim=stack_dim,
+            )
+        if self.is_locked:
+            result.lock_()
+        return result
+
+    def get_nestedtensor(
+        self,
+        key: NestedKey,
+        default: Any = NO_DEFAULT,
+    ) -> CompatibleType:
+        """Returns a nested tensor when stacking cannot be achieved.
+
+        Args:
+            key (NestedKey): the entry to nest.
+            default (Any, optiona): the default value to return in case the key
+                isn't in all sub-tensordicts.
+
+                .. note:: In case the default is a tensor, this method will attempt
+                  the construction of a nestedtensor with it. Otherwise, the default
+                  value will be returned.
+
+        Examples:
+            >>> td0 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(4)}, [])
+            >>> td1 = TensorDict({"a": torch.ones(5)}, [])
+            >>> td = torch.stack([td0, td1], 0)
+            >>> a = td.get_nestedtensor("a")
+            >>> # using a tensor as default uses this default to build the nested tensor
+            >>> b = td.get_nestedtensor("b", default=torch.ones(4))
+            >>> assert (a == b).all()
+            >>> # using anything else as default returns the default
+            >>> b2 = td.get_nestedtensor("b", None)
+            >>> assert b2 is None
+
+        """
+        # disallow getting nested tensor if the stacking dimension is not 0
+        if self.stack_dim != 0:
+            raise RuntimeError(
+                "Because nested tensors can only be stacked along their first "
+                "dimension, LazyStackedTensorDict.get_nestedtensor can only be called "
+                "when the stack_dim is 0."
+            )
+
+        # we can handle the case where the key is a tuple of length 1
+        key = _unravel_key_to_tuple(key)
+        subkey = key[0]
+        if len(key) > 1:
+            tensordict = self.get(subkey, default)
+            if tensordict is default:
+                return default
+            return tensordict.get_nestedtensor(key[1:], default=default)
+        tensors = [td.get(subkey, default=default) for td in self.tensordicts]
+        if not isinstance(default, torch.Tensor) and any(
+            tensor is default for tensor in tensors
+        ):
+            # we don't stack but return the default
+            return default
+        return torch.nested.nested_tensor(tensors)
+
+    def is_contiguous(self) -> bool:
+        return False
+
+    def contiguous(self) -> T:
+        source = {key: value.contiguous() for key, value in self.items()}
+        batch_size = self.batch_size
+        device = self.device
+        out = TensorDict(
+            source=source,
+            batch_size=batch_size,
+            device=device,
+            names=self.names,
+            _run_checks=False,
+            lock=self.is_locked,
+        )
+        return out
+
+    def empty(
+        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
+    ) -> T:
+        name = None
+        if batch_size is not None:
+            return TensorDict.empty(
+                self,
+                recurse=recurse,
+                batch_size=batch_size,
+                device=device if device is not NO_DEFAULT else self.device,
+                names=names if names is not None else None,
+            )
+        if names is not None:
+            if len(names) > self.stack_dim:
+                name = names[self.stack_dim]
+            names = [name for i, name in enumerate(names) if i != self.stack_dim]
+        return type(self)(
+            *[
+                td.empty(
+                    recurse=recurse, batch_size=batch_size, device=device, names=names
+                )
+                for td in self.tensordicts
+            ],
+            stack_dim=self.stack_dim,
+            stack_dim_name=name,
+        )
+
+    def _clone(self, recurse: bool = True) -> T:
+        if recurse:
+            # This could be optimized using copy but we must be careful with
+            # metadata (_is_shared etc)
+            result = type(self)(
+                *[td._clone() for td in self.tensordicts],
+                stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
+            )
+        else:
+            result = type(self)(
+                *[td._clone(recurse=False) for td in self.tensordicts],
+                stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
+            )
+        return result
+
+    def pin_memory(self) -> T:
+        for td in self.tensordicts:
+            td.pin_memory()
+        return self
+
+    def to(self, *args, **kwargs) -> T:
+        non_blocking = kwargs.pop("non_blocking", None)
+        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
+        if batch_size is not None:
+            raise TypeError("Cannot pass batch-size to a LazyStackedTensorDict.")
+        result = self
+
+        if device is not None and dtype is None and device == self.device:
+            return result
+
+        if non_blocking in (None, True):
+            kwargs["non_blocking"] = True
+        else:
+            kwargs["non_blocking"] = False
+        non_blocking = bool(non_blocking)
+        result = type(self)(
+            *[td.to(*args, **kwargs) for td in self.tensordicts],
+            stack_dim=self.stack_dim,
+            hook_out=self.hook_out,
+            hook_in=self.hook_in,
+            stack_dim_name=self._td_dim_name,
+        )
+        if device is not None and not non_blocking:
+            self._sync_all()
+        if self.is_locked:
+            result.lock_()
+        return result
+
+    def _check_new_batch_size(self, new_size: torch.Size) -> None:
+        if len(new_size) <= self.stack_dim:
+            raise RuntimeError(
+                "Changing the batch_size of a LazyStackedTensorDicts can only "
+                "be done with sizes that are at least as long as the "
+                "stacking dimension."
+            )
+        super()._check_new_batch_size(new_size)
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _LazyStackedTensorDictKeysView:
+        keys = _LazyStackedTensorDictKeysView(
+            self,
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+            is_leaf=is_leaf,
+        )
+        return keys
+
+    def values(self, include_nested=False, leaves_only=False, is_leaf=None):
+        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
+            yield from super().values(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+        else:
+            for td in self.tensordicts:
+                yield from td.values(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=is_leaf,
+                )
+
+    def items(self, include_nested=False, leaves_only=False, is_leaf=None):
+        if is_leaf is not _NESTED_TENSORS_AS_LISTS:
+            yield from super().items(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+        else:
+            for i, td in enumerate(self.tensordicts):
+                for key, val in td.items(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=is_leaf,
+                ):
+                    if isinstance(key, str):
+                        key = (str(i), key)
+                    else:
+                        key = (str(i), *key)
+                    yield key, val
+
+    valid_keys = keys
+
+    def non_tensor_items(self, include_nested: bool = False):
+        """Returns all non-tensor leaves, maybe recursively."""
+        items = self.tensordicts[0].non_tensor_items(include_nested=include_nested)
+        return tuple(
+            (
+                key,
+                torch.stack(
+                    [val0, *[td.get(key) for td in self.tensordicts[1:]]],
+                    self.stack_dim,
+                ),
+            )
+            for (key, val0) in items
+        )
+
+    def _iterate_over_keys(self) -> None:
+        # this is about 20x faster than the version above
+        yield from self._key_list()
+
+    @cache  # noqa: B019
+    def _key_list(self):
+        keys = set(self.tensordicts[0].keys())
+        for td in self.tensordicts[1:]:
+            keys = keys.intersection(td.keys())
+        return sorted(keys, key=str)
+
+    @lock_blocked
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        key, val = self.tensordicts[0].popitem()
+        vals = [val]
+        for i, td in enumerate(self.tensordicts[1:]):
+            val = td.pop(key, None)
+            if val is not None:
+                vals.append(val)
+            else:
+                for j in range(i + 1):
+                    self.tensordicts[j].set(key, vals[j])
+                raise RuntimeError(f"Could not find key {key} in all tensordicts.")
+        return key, torch.stack(vals, dim=self.stack_dim)
+
+    def entry_class(self, key: NestedKey) -> type:
+        data_type = type(self.tensordicts[0].get(key))
+        if _is_tensor_collection(data_type):
+            return LazyStackedTensorDict
+        return data_type
+
+    def apply_(self, fn: Callable, *others, **kwargs):
+        others = (other.unbind(self.stack_dim) for other in others)
+        for td, *_others in zip(self.tensordicts, *others):
+            td._fast_apply(fn, *_others, inplace=True, propagate_lock=True, **kwargs)
+        return self
+
+    def _apply_nest(
+        self,
+        fn: Callable,
+        *others: T,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        checked: bool = False,
+        call_on_nested: bool = False,
+        default: Any = NO_DEFAULT,
+        named: bool = False,
+        nested_keys: bool = False,
+        prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable | None = None,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        if inplace and any(
+            arg for arg in (batch_size, device, names, constructor_kwargs)
+        ):
+            raise ValueError(
+                "Cannot pass other arguments to LazyStackedTensorDict.apply when inplace=True."
+            )
+        if out is not None:
+            if not isinstance(out, LazyStackedTensorDict):
+                raise ValueError(
+                    "out must be a LazyStackedTensorDict instance in lazy_stack.apply(..., out=out)."
+                )
+            out = out.tensordicts
+        elif batch_size is not None:
+            # any op that modifies the batch-size will result in a regular TensorDict
+            return TensorDict._apply_nest(
+                self,
+                fn,
+                *others,
+                batch_size=batch_size,
+                device=device,
+                names=names,
+                checked=checked,
+                call_on_nested=call_on_nested,
+                default=default,
+                named=named,
+                nested_keys=nested_keys,
+                prefix=prefix,
+                inplace=inplace,
+                filter_empty=filter_empty,
+                is_leaf=is_leaf,
+                **constructor_kwargs,
+            )
+
+        others = (other.unbind(self.stack_dim) for other in others)
+        results = [
+            td._apply_nest(
+                fn,
+                *oth,
+                checked=checked,
+                device=device,
+                call_on_nested=call_on_nested,
+                default=default,
+                named=named,
+                nested_keys=nested_keys,
+                prefix=prefix + (str(i),)
+                if is_leaf is _NESTED_TENSORS_AS_LISTS
+                else prefix,
+                inplace=inplace,
+                filter_empty=filter_empty,
+                is_leaf=is_leaf,
+                out=out[i] if out is not None else None,
+            )
+            for i, (td, *oth) in enumerate(zip(self.tensordicts, *others))
+        ]
+        if filter_empty and all(r is None for r in results):
+            return
+        if not inplace:
+            out = type(self)(
+                *results,
+                stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
+            )
+        else:
+            out = self
+        if names is not None:
+            out.names = names
+        return out
+
+    def _select(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        strict: bool = False,
+        set_shared: bool = True,
+    ) -> LazyStackedTensorDict:
+        # the following implementation keeps the hidden keys in the tensordicts
+        tensordicts = [
+            td._select(*keys, inplace=inplace, strict=strict, set_shared=set_shared)
+            for td in self.tensordicts
+        ]
+        if inplace:
+            return self
+        result = type(self)(
+            *tensordicts, stack_dim=self.stack_dim, stack_dim_name=self._td_dim_name
+        )
+        return result
+
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> LazyStackedTensorDict:
+        tensordicts = [
+            tensordict._exclude(*keys, inplace=inplace, set_shared=set_shared)
+            for tensordict in self.tensordicts
+        ]
+        if inplace:
+            self.tensordicts = tensordicts
+            return self
+        result = type(self)(
+            *tensordicts, stack_dim=self.stack_dim, stack_dim_name=self._td_dim_name
+        )
+        return result
+
+    def __setitem__(self, index: IndexType, value: T) -> T:
+        if isinstance(index, (tuple, str)):
+            # try:
+            index_unravel = _unravel_key_to_tuple(index)
+            if index_unravel:
+                self._set_tuple(
+                    index_unravel,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE
+                    if isinstance(self, _SubTensorDict)
+                    else False,
+                    validated=False,
+                    non_blocking=False,
+                )
+                return
+
+            if any(
+                isinstance(sub_index, (list, range, np.ndarray)) for sub_index in index
+            ):
+                index = tuple(
+                    torch.as_tensor(sub_index, device=self.device)
+                    if isinstance(sub_index, (list, range, np.ndarray))
+                    else sub_index
+                    for sub_index in index
+                )
+
+        if index is Ellipsis or (isinstance(index, tuple) and Ellipsis in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        elif isinstance(index, (list, range)):
+            index = torch.as_tensor(index, device=self.device)
+
+        if is_tensor_collection(value) or isinstance(value, dict):
+            indexed_bs = _getitem_batch_size(self.batch_size, index)
+            if isinstance(value, dict):
+                value = TensorDict(
+                    value, batch_size=indexed_bs, device=self.device, _run_checks=False
+                )
+            if value.batch_size != indexed_bs:
+                # try to expand
+                try:
+                    value = value.expand(indexed_bs)
+                except RuntimeError as err:
+                    raise RuntimeError(
+                        f"indexed destination TensorDict batch size is {indexed_bs} "
+                        f"(batch_size = {self.batch_size}, index={index}), "
+                        f"which differs from the source batch size {value.batch_size}"
+                    ) from err
+            split_index = self._split_index(index)
+            converted_idx = split_index["index_dict"]
+            num_single = split_index["num_single"]
+            isinteger = split_index["isinteger"]
+            has_bool = split_index["has_bool"]
+            num_squash = split_index.get("num_squash", 0)
+            num_none = split_index.get("num_none", 0)
+            is_nd_tensor = split_index.get("is_nd_tensor", False)
+            if isinteger:
+                # this will break if the index along the stack dim is [0] or :1 or smth
+                for i, _idx in converted_idx.items():
+                    if _idx == ():
+                        self.tensordicts[i].update(value, inplace=True)
+                    else:
+                        self.tensordicts[i][_idx] = value
+                return self
+            if is_nd_tensor:
+                unbind_dim = self.stack_dim - num_single + num_none - num_squash
+
+                # converted_idx is a nested list with (int, index) items
+                def assign(converted_idx, value=value):
+                    value = value.unbind(unbind_dim)
+                    for i, item in enumerate(converted_idx):
+                        if isinstance(item, list):
+                            assign(item)
+                        else:
+                            stack_item, idx = item
+                            self.tensordicts[stack_item][idx] = value[i]
+
+                assign(converted_idx)
+                return self
+            if not has_bool:
+                unbind_dim = self.stack_dim - num_single + num_none - num_squash
+                value_unbind = value.unbind(unbind_dim)
+                for (i, _idx), _value in zip(
+                    converted_idx.items(),
+                    value_unbind,
+                ):
+                    if _idx == ():
+                        self.tensordicts[i].update(_value, inplace=True)
+                    else:
+                        self.tensordicts[i][_idx] = _value
+            else:
+                # we must split, not unbind
+                mask_unbind = split_index["individual_masks"]
+                split_dim = split_index["split_dim"]
+                splits = [_mask_unbind.sum().item() for _mask_unbind in mask_unbind]
+                value_unbind = value.split(splits, split_dim)
+                if mask_unbind[0].ndim == 0:
+                    # we can return a stack
+                    for (i, _idx), mask, _value in zip(
+                        converted_idx.items(),
+                        mask_unbind,
+                        value_unbind,
+                    ):
+                        if mask.any():
+                            self.tensordicts[i][_idx] = _value
+                else:
+                    for (i, _idx), _value in zip(converted_idx.items(), value_unbind):
+                        self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                        self[self_idx][_idx] = _value
+        else:
+            for key in self.keys():
+                self.set_at_(key, value, index)
+
+    def __contains__(self, item: IndexType) -> bool:
+        if isinstance(item, TensorDictBase):
+            return any(item is td for td in self.tensordicts)
+        return super().__contains__(item)
+
+    def __getitem__(self, index: IndexType) -> T:
+        if isinstance(index, (tuple, str)):
+            index_key = _unravel_key_to_tuple(index)
+            if index_key:
+                leaf = self._get_tuple(index_key, NO_DEFAULT)
+                if is_non_tensor(leaf):
+                    result = getattr(leaf, "data", NO_DEFAULT)
+                    if result is NO_DEFAULT:
+                        return leaf.tolist()
+                    return result
+                return leaf
+        split_index = self._split_index(index)
+        converted_idx = split_index["index_dict"]
+        isinteger = split_index["isinteger"]
+        has_bool = split_index["has_bool"]
+        is_nd_tensor = split_index["is_nd_tensor"]
+        num_single = split_index.get("num_single", 0)
+        num_none = split_index.get("num_none", 0)
+        num_squash = split_index.get("num_squash", 0)
+        if has_bool:
+            mask_unbind = split_index["individual_masks"]
+            cat_dim = split_index["mask_loc"] - num_single
+            result = []
+            if mask_unbind[0].ndim == 0:
+                # we can return a stack
+                for (i, _idx), mask in zip(converted_idx.items(), mask_unbind):
+                    if mask.any():
+                        if mask.all() and self.tensordicts[i].ndim == 0:
+                            result.append(self.tensordicts[i])
+                        else:
+                            result.append(self.tensordicts[i][_idx])
+                            result[-1] = result[-1].squeeze(cat_dim)
+                return LazyStackedTensorDict.lazy_stack(result, cat_dim)
+            else:
+                for i, _idx in converted_idx.items():
+                    self_idx = (slice(None),) * split_index["mask_loc"] + (i,)
+                    result.append(self[self_idx][_idx])
+                return torch.cat(result, cat_dim)
+        elif is_nd_tensor:
+            new_stack_dim = self.stack_dim - num_single + num_none
+
+            def recompose(converted_idx, stack_dim=new_stack_dim):
+                stack = []
+                for item in converted_idx:
+                    if isinstance(item, list):
+                        stack.append(recompose(item, stack_dim=stack_dim))
+                    else:
+                        stack_elt, idx = item
+                        if idx != ():
+                            stack.append(self.tensordicts[stack_elt][idx])
+                        else:
+                            stack.append(self.tensordicts[stack_elt])
+
+                # TODO: this produces multiple dims with the same name
+                result = LazyStackedTensorDict.lazy_stack(
+                    stack, stack_dim, stack_dim_name=self._td_dim_name
+                )
+                if self.is_locked:
+                    result.lock_()
+                return result
+
+            return recompose(converted_idx)
+        else:
+            if isinteger:
+                for (
+                    i,
+                    _idx,
+                ) in (
+                    converted_idx.items()
+                ):  # for convenience but there's only one element
+                    result = self.tensordicts[i]
+                    if _idx is not None and _idx != ():
+                        result = result[_idx]
+                    return result
+            else:
+                result = []
+                new_stack_dim = self.stack_dim - num_single + num_none - num_squash
+                for i, _idx in converted_idx.items():
+                    if _idx == ():
+                        result.append(self.tensordicts[i])
+                    else:
+                        result.append(self.tensordicts[i][_idx])
+                result = LazyStackedTensorDict.lazy_stack(
+                    result, new_stack_dim, stack_dim_name=self._td_dim_name
+                )
+                if self.is_locked:
+                    result.lock_()
+                return result
+
+    def __eq__(self, other):
+        return self._dispatch_comparison(other, "__eq__", "__eq__", default=False)
+
+    def __ne__(self, other):
+        return self._dispatch_comparison(other, "__ne__", "__ne__", default=True)
+
+    def __or__(self, other):
+        return self._dispatch_comparison(other, "__or__", "__or__", default=NO_DEFAULT)
+
+    def __xor__(self, other):
+        return self._dispatch_comparison(
+            other, "__xor__", "__xor__", default=NO_DEFAULT
+        )
+
+    def __ge__(self, other):
+        return self._dispatch_comparison(other, "__ge__", "__le__", default=NO_DEFAULT)
+
+    def __gt__(self, other):
+        return self._dispatch_comparison(other, "__gt__", "__lt__", default=NO_DEFAULT)
+
+    def __le__(self, other):
+        return self._dispatch_comparison(other, "__le__", "__ge__", default=NO_DEFAULT)
+
+    def __lt__(self, other):
+        return self._dispatch_comparison(other, "__lt__", "__gt__", default=NO_DEFAULT)
+
+    def _dispatch_comparison(self, other, comparison_str, inverse_str, default):
+        if is_tensorclass(other):
+            return getattr(other, inverse_str)(self)
+        if isinstance(other, (dict,)):
+            # we may want to broadcast it instead
+            other = TensorDict.from_dict(other, batch_size=self.batch_size)
+        if _is_tensor_collection(other.__class__):
+            if other.batch_size != self.batch_size:
+                if self.ndim < other.ndim:
+                    self_expand = self.expand(other.batch_size)
+                elif self.ndim > other.ndim:
+                    other = other.expand(self.batch_size)
+                    self_expand = self
+                else:
+                    raise RuntimeError(
+                        f"Could not compare tensordicts with shapes {self.shape} and {other.shape}"
+                    )
+            else:
+                self_expand = self
+            out = []
+            for td0, td1 in zip(
+                self_expand.tensordicts, other.unbind(self_expand.stack_dim)
+            ):
+                out.append(getattr(td0, comparison_str)(td1))
+            return LazyStackedTensorDict.lazy_stack(out, self.stack_dim)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return LazyStackedTensorDict.lazy_stack(
+                [getattr(td, comparison_str)(other) for td in self.tensordicts],
+                self.stack_dim,
+            )
+        if default is NO_DEFAULT:
+            raise ValueError(
+                f"Incompatible value {type(other)} for op {comparison_str}."
+            )
+        return default
+
+    def _cast_reduction(
+        self,
+        *,
+        reduction_name,
+        dim=NO_DEFAULT,
+        keepdim=NO_DEFAULT,
+        tuple_ok=True,
+        **kwargs,
+    ):
+        try:
+            td = self.to_tensordict()
+        except Exception:
+            raise RuntimeError(
+                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
+                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
+                f" on github!"
+            )
+        return td._cast_reduction(
+            reduction_name=reduction_name,
+            dim=dim,
+            keepdim=keepdim,
+            tuple_ok=tuple_ok,
+            **kwargs,
+        )
+
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            # TODO: we need to adapt this to LazyStackedTensorDict too
+            if dim < 0:
+                dim = self.batch_dims + dim
+            return TensorDict(
+                source={key: value.all(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+            )
+        return all(value.all() for value in self.tensordicts)
+
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            # TODO: we need to adapt this to LazyStackedTensorDict too
+            if dim < 0:
+                dim = self.batch_dims + dim
+            return TensorDict(
+                source={key: value.any(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+            )
+        return any(value.any() for value in self.tensordicts)
+
+    def _send(
+        self,
+        dst: int,
+        _tag: int = -1,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> int:
+        for td in self.tensordicts:
+            _tag = td._send(dst, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
+        return _tag
+
+    def _isend(
+        self,
+        dst: int,
+        _tag: int = -1,
+        _futures: list[torch.Future] | None = None,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> int:
+        if _futures is None:
+            is_root = True
+            _futures = []
+        else:
+            is_root = False
+        for td in self.tensordicts:
+            _tag = td._isend(
+                dst, _tag=_tag, pseudo_rand=pseudo_rand, _futures=_futures, group=group
+            )
+        if is_root:
+            for future in _futures:
+                future.wait()
+        return _tag
+
+    def _recv(
+        self,
+        src: int,
+        _tag: int = -1,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> int:
+        for td in self.tensordicts:
+            _tag = td._recv(src, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
+        return _tag
+
+    def _irecv(
+        self,
+        src: int,
+        return_premature: bool = False,
+        _tag: int = -1,
+        _future_list: list[torch.Future] = None,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+        for td in self.tensordicts:
+            _tag, _future_list = td._irecv(
+                src=src,
+                return_premature=return_premature,
+                _tag=_tag,
+                _future_list=_future_list,
+                pseudo_rand=pseudo_rand,
+                group=group,
+            )
+
+        if not root:
+            return _tag, _future_list
+        elif return_premature:
+            return _future_list
+        else:
+            for future in _future_list:
+                future.wait()
+            return
+
+    @lock_blocked
+    def del_(self, key: NestedKey, **kwargs: Any) -> T:
+        ids = set()
+        cur_len = len(ids)
+        is_deleted = False
+        error = None
+        for td in self.tensordicts:
+            # checking that the td has not been processed yet.
+            # It could be that not all sub-tensordicts have the appropriate
+            # entry but one must have it (or an error is thrown).
+            tdid = id(td)
+            ids.add(tdid)
+            new_cur_len = len(ids)
+            if new_cur_len == cur_len:
+                continue
+            cur_len = new_cur_len
+            try:
+                td.del_(key, **kwargs)
+                is_deleted = True
+            except KeyError as err:
+                error = err
+                continue
+        if not is_deleted:
+            # we know err is defined because LazyStackedTensorDict cannot be empty
+            raise error
+        return self
+
+    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        # using try/except for get/del is suboptimal, but
+        # this is faster that checkink if key in self keys
+        key = _unravel_key_to_tuple(key)
+        if len(key) == 1:
+            key = key[0]
+        present = False
+        if isinstance(key, tuple):
+            if key in self.keys(True):
+                present = True
+                value = self._get_tuple(key, NO_DEFAULT)
+        elif key in self.keys():
+            present = True
+            value = self._get_str(key, NO_DEFAULT)
+        if present:
+            self.del_(key)
+        elif default is not NO_DEFAULT:
+            value = default
+        else:
+            raise KeyError(
+                f"You are trying to pop key `{key}` which is not in dict "
+                f"without providing default value."
+            )
+        return value
+
+    def share_memory_(self) -> T:
+        for td in self.tensordicts:
+            td.share_memory_()
+        self.lock_()
+        return self
+
+    def detach_(self) -> T:
+        for td in self.tensordicts:
+            td.detach_()
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        executor=None,
+        futures=None,
+        inplace=True,
+        like=False,
+        share_non_tensor,
+    ) -> T:
+        if prefix is not None:
+            prefix = Path(prefix)
+
+            def save_metadata(prefix=prefix, self=self):
+                prefix = Path(prefix)
+                if not prefix.exists():
+                    os.makedirs(prefix, exist_ok=True)
+                with open(prefix / "meta.json", "w") as f:
+                    json.dump(
+                        {"_type": str(self.__class__), "stack_dim": self.stack_dim}, f
+                    )
+
+            if executor is None:
+                save_metadata()
+            else:
+                futures.append(executor.submit(save_metadata))
+
+        results = []
+        for i, td in enumerate(self.tensordicts):
+            results.append(
+                td._memmap_(
+                    prefix=(prefix / str(i)) if prefix is not None else None,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=inplace,
+                    like=like,
+                    share_non_tensor=share_non_tensor,
+                )
+            )
+        if not inplace:
+            results = LazyStackedTensorDict.lazy_stack(results, dim=self.stack_dim)
+        else:
+            results = self
+        results._device = torch.device("cpu")
+        return results
+
+    @classmethod
+    def _load_memmap(
+        cls,
+        prefix: str,
+        metadata: dict,
+        device: torch.device | None = None,
+        *,
+        out=None,
+        **kwargs,
+    ) -> LazyStackedTensorDict:
+        tensordicts = []
+        i = 0
+        stack_dim = metadata["stack_dim"]
+        if out is not None:
+            out = out.unbind(stack_dim)
+        while (prefix / str(i)).exists():
+            tensordicts.append(
+                TensorDict.load_memmap(
+                    prefix / str(i),
+                    device=device,
+                    **kwargs,
+                    non_blocking=True,
+                    out=out[i] if out is not None else None,
+                )
+            )
+            i += 1
+        return cls(*tensordicts, stack_dim=stack_dim, **kwargs)
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
+            "it can't return a contiguous view of the lazy stacked tensors. "
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
+            "it can't return a contiguous view of the lazy stacked tensors. "
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for LazyStack as "
+            "it can't return a contiguous view of the lazy stacked tensors. "
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def expand(self, *args: int, inplace: bool = False) -> T:
+        if len(args) == 1 and isinstance(args[0], Sequence):
+            shape = tuple(args[0])
+        else:
+            shape = args
+        stack_dim = len(shape) + self.stack_dim - self.ndimension()
+        new_shape_tensordicts = [v for i, v in enumerate(shape) if i != stack_dim]
+        tensordicts = [td.expand(new_shape_tensordicts) for td in self.tensordicts]
+        if inplace:
+            self.tensordicts = tensordicts
+            self.stack_dim = stack_dim
+            return self
+        return LazyStackedTensorDict.maybe_dense_stack(tensordicts, dim=stack_dim)
+
+    def update(
+        self,
+        input_dict_or_td: T,
+        clone: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        non_blocking: bool = False,
+        **kwargs: Any,
+    ) -> T:
+        # This implementation of update is compatible with exclusive keys
+        # as well as vmapped lazy stacks.
+        # We iterate over the tensordicts rather than iterating over the keys,
+        # which requires stacking and unbinding but is also not robust to missing keys.
+        if input_dict_or_td is self:
+            # no op
+            return self
+        if isinstance(input_dict_or_td, dict):
+            input_dict_or_td = TensorDict.from_dict(
+                input_dict_or_td, batch_size=self.batch_size
+            )
+
+        if keys_to_update is not None:
+            keys_to_update = unravel_key_list(keys_to_update)
+            if len(keys_to_update) == 0:
+                return self
+
+        if (
+            isinstance(input_dict_or_td, LazyStackedTensorDict)
+            and input_dict_or_td.stack_dim == self.stack_dim
+        ):
+            if len(input_dict_or_td.tensordicts) != len(self.tensordicts):
+                raise ValueError(
+                    "cannot update stacked tensordicts with different shapes."
+                )
+            for td_dest, td_source in zip(
+                self.tensordicts, input_dict_or_td.tensordicts
+            ):
+                td_dest.update(
+                    td_source,
+                    clone=clone,
+                    keys_to_update=keys_to_update,
+                    non_blocking=non_blocking,
+                    **kwargs,
+                )
+            return self
+
+        if self.hook_in is not None:
+            self_upd = self.hook_in(self)
+            input_dict_or_td = self.hook_in(input_dict_or_td)
+        else:
+            self_upd = self
+        # Then we can decompose the tensordict along its stack dim
+        if input_dict_or_td.ndim <= self_upd.stack_dim or input_dict_or_td.batch_size[
+            self_upd.stack_dim
+        ] != len(self_upd.tensordicts):
+            try:
+                # if the batch-size does not permit unbinding, let's first try to reset the batch-size.
+                input_dict_or_td = input_dict_or_td.copy()
+                batch_size = self_upd.batch_size
+                if self_upd.hook_out is not None:
+                    batch_size = list(batch_size)
+                    batch_size.insert(self_upd.stack_dim, len(self_upd.tensordicts))
+                input_dict_or_td.batch_size = batch_size
+            except RuntimeError as err:
+                raise ValueError(
+                    "cannot update stacked tensordicts with different shapes."
+                ) from err
+        for td_dest, td_source in zip(
+            self_upd.tensordicts, input_dict_or_td.unbind(self_upd.stack_dim)
+        ):
+            td_dest.update(
+                td_source, clone=clone, keys_to_update=keys_to_update, **kwargs
+            )
+        if self.hook_out is not None:
+            self_upd = self.hook_out(self_upd)
+        else:
+            self_upd = self
+        return self_upd
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        **kwargs: Any,
+    ) -> T:
+        if input_dict_or_td is self:
+            # no op
+            return self
+        if not is_tensor_collection(input_dict_or_td):
+            input_dict_or_td = TensorDict.from_dict(
+                input_dict_or_td, batch_dims=self.batch_dims
+            )
+            if input_dict_or_td.batch_dims <= self.stack_dim:
+                raise RuntimeError(
+                    f"Built tensordict with ndim={input_dict_or_td.ndim} does not have enough dims."
+                )
+        if input_dict_or_td.batch_size[self.stack_dim] != len(self.tensordicts):
+            raise ValueError("cannot update stacked tensordicts with different shapes.")
+        for td_dest, td_source in zip(
+            self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
+        ):
+            td_dest.update_(td_source, clone=clone, non_blocking=non_blocking, **kwargs)
+        return self
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
+        if not _is_tensor_collection(type(input_dict_or_td)):
+            input_dict_or_td = TensorDict.from_dict(
+                input_dict_or_td, batch_size=self.batch_size
+            )
+        split_index = self._split_index(index)
+        converted_idx = split_index["index_dict"]
+        num_single = split_index["num_single"]
+        isinteger = split_index["isinteger"]
+        if isinteger:
+            # this will break if the index along the stack dim is [0] or :1 or smth
+            for i, _idx in converted_idx.items():
+                self.tensordicts[i].update_at_(
+                    input_dict_or_td,
+                    _idx,
+                    non_blocking=non_blocking,
+                )
+            return self
+        unbind_dim = self.stack_dim - num_single
+        for (i, _idx), _value in zip(
+            converted_idx.items(),
+            input_dict_or_td.unbind(unbind_dim),
+        ):
+            self.tensordicts[i].update_at_(
+                _value,
+                _idx,
+                non_blocking=non_blocking,
+            )
+        return self
+
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> T:
+        for td in self.tensordicts:
+            td.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    def where(self, condition, other, *, out=None, pad=None):
+        if condition.ndim < self.ndim:
+            condition = expand_right(condition, self.batch_size)
+        condition = condition.unbind(self.stack_dim)
+        if _is_tensor_collection(other.__class__) or (
+            isinstance(other, Tensor)
+            and other.shape[: self.stack_dim] == self.shape[: self.stack_dim]
+        ):
+            other = other.unbind(self.stack_dim)
+
+            def where(td, cond, other, pad):
+                if cond.numel() > 1:
+                    return td.where(cond, other, pad=pad)
+                return other if not cond else td
+
+            result = LazyStackedTensorDict.maybe_dense_stack(
+                [
+                    where(td, cond, _other, pad=pad)
+                    for td, cond, _other in zip(self.tensordicts, condition, other)
+                ],
+                self.stack_dim,
+            )
+        else:
+            result = LazyStackedTensorDict.maybe_dense_stack(
+                [
+                    td.where(cond, other, pad=pad)
+                    for td, cond in zip(self.tensordicts, condition)
+                ],
+                self.stack_dim,
+            )
+        # We should not pass out to stack because this will overwrite the tensors in-place, but
+        # we don't want that
+        if out is not None:
+            out.update(result)
+            return out
+        return result
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
+        mask_unbind = mask.unbind(dim=self.stack_dim)
+        for _mask, td in zip(mask_unbind, self.tensordicts):
+            td.masked_fill_(_mask, value)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    @lock_blocked
+    def insert(self, index: int, tensordict: T) -> None:
+        """Insert a TensorDict into the stack at the specified index.
+
+        Analogous to list.insert. The inserted TensorDict must have compatible
+        batch_size and device. Insertion is in-place, nothing is returned.
+
+        Args:
+            index (int): The index at which the new TensorDict should be inserted.
+            tensordict (TensorDictBase): The TensorDict to be inserted into the stack.
+
+        """
+        if not isinstance(tensordict, TensorDictBase):
+            raise TypeError(
+                "Expected new value to be TensorDictBase instance but got "
+                f"{type(tensordict)} instead."
+            )
+
+        batch_size = self.tensordicts[0].batch_size
+        device = self.tensordicts[0].device
+
+        _batch_size = tensordict.batch_size
+        _device = tensordict.device
+
+        if device != _device:
+            raise ValueError(
+                f"Devices differ: stack has device={device}, new value has "
+                f"device={_device}."
+            )
+        if _batch_size != batch_size:
+            raise ValueError(
+                f"Batch sizes in tensordicts differs: stack has "
+                f"batch_size={batch_size}, new_value has batch_size={_batch_size}."
+            )
+
+        self.tensordicts.insert(index, tensordict)
+
+        N = len(self.tensordicts)
+        self._batch_size = self._compute_batch_size(batch_size, self.stack_dim, N)
+
+    @lock_blocked
+    def append(self, tensordict: T) -> None:
+        """Append a TensorDict onto the stack.
+
+        Analogous to list.append. The appended TensorDict must have compatible
+        batch_size and device. The append operation is in-place, nothing is returned.
+
+        Args:
+            tensordict (TensorDictBase): The TensorDict to be appended onto the stack.
+
+        """
+        self.insert(len(self.tensordicts), tensordict)
+
+    @property
+    def is_locked(self) -> bool:
+        if self._is_locked is not None:
+            # if tensordicts have been locked through this Lazy stack, then we can
+            # trust this lazy stack to contain the info.
+            # In all other cases we must check
+            return self._is_locked
+        # If any of the tensordicts is not locked, we assume that the lazy stack
+        # is not locked either. Caching is then disabled and
+        for td in self.tensordicts:
+            if not td.is_locked:
+                return False
+        else:
+            # In this case, all tensordicts were locked before the lazy stack
+            # was created and they were not locked through the lazy stack.
+            # This means we cannot cache the value because this lazy stack
+            # if not part of the graph. We don't want it to be part of the graph
+            # because this object being locked is only a side-effect.
+            # Calling self.lock_() here could however speed things up.
+            return True
+
+    @is_locked.setter
+    def is_locked(self, value: bool) -> None:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @property
+    def _lock_parents_weakrefs(self):
+        """Weakrefs of all tensordicts that need to be unlocked for this to be unlocked."""
+        _lock_parents_weakrefs = []
+        for tensordict in self.tensordicts:
+            _lock_parents_weakrefs = (
+                _lock_parents_weakrefs + tensordict._lock_parents_weakrefs
+            )
+        _lock_parents_weakrefs = [
+            item for item in _lock_parents_weakrefs if item is not weakref.ref(self)
+        ]
+        return _lock_parents_weakrefs
+
+    def _propagate_lock(self, lock_parents_weakrefs=None):
+        """Registers the parent tensordict that handles the lock."""
+        self._is_locked = True
+        is_root = lock_parents_weakrefs is None
+        if is_root:
+            lock_parents_weakrefs = []
+
+        lock_parents_weakrefs = copy(lock_parents_weakrefs) + [weakref.ref(self)]
+        for dest in self.tensordicts:
+            dest._propagate_lock(lock_parents_weakrefs)
+
+    @erase_cache
+    def _propagate_unlock(self):
+        # we can't set _is_locked to False because after it's unlocked, anything
+        # can happen to a child tensordict.
+        self._is_locked = None
+        sub_tds = defaultdict()
+        for child in self.tensordicts:
+            # we want to make sure that if the same child is present twice in the
+            # stack we won't iterate multiple times over it
+            sub_tds[id(child)] = child._propagate_unlock() + [child]
+        sub_tds = [item for value in sub_tds.values() for item in value]
+        return sub_tds
+
+    def __repr__(self):
+        fields = _td_fields(self)
+        field_str = indent(f"fields={{{fields}}}", 4 * " ")
+        exclusive_fields_str = indent(
+            f"exclusive_fields={{{self._repr_exclusive_fields()}}}", 4 * " "
+        )
+        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+        device_str = indent(f"device={self.device}", 4 * " ")
+        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+        stack_dim = indent(f"stack_dim={self.stack_dim}", 4 * " ")
+        string = ",\n".join(
+            [
+                field_str,
+                exclusive_fields_str,
+                batch_size_str,
+                device_str,
+                is_shared_str,
+                stack_dim,
+            ]
+        )
+        return f"{type(self).__name__}(\n{string})"
+
+    def _repr_exclusive_fields(self):
+        keys = set(self.keys())
+        exclusive_keys = [
+            _td_fields(td, [k for k in td.keys() if k not in keys])
+            for td in self.tensordicts
+        ]
+        exclusive_key_str = ",\n".join(
+            [
+                indent(f"{i} ->{line}", 4 * " ")
+                for i, line in enumerate(exclusive_keys)
+                if line != "\n"
+            ]
+        )
+
+        return "\n" + exclusive_key_str
+
+    def _view(self, *args, **kwargs):
+        raise RuntimeError(
+            "Cannot call `view` on a lazy stacked tensordict. Call `reshape` instead."
+        )
+
+    def _transpose(self, dim0, dim1):
+        if self._is_vmapped:
+            raise RuntimeError("cannot call transpose within vmap.")
+        if dim0 == self.stack_dim:
+            # we know dim0 and dim1 are sorted so dim1 comes after dim0
+            # example: shape = [5, 4, 3, 2, 1], stack_dim=1, dim0=1, dim1=4
+            # resulting shape: [5, 1, 3, 2, 4]
+            if dim1 == dim0 + 1:
+                result = type(self)(
+                    *self.tensordicts, stack_dim=dim1, stack_dim_name=self._td_dim_name
+                )
+            else:
+                result = type(self)(
+                    *(td.transpose(dim0, dim1 - 1) for td in self.tensordicts),
+                    stack_dim=dim1,
+                    stack_dim_name=self._td_dim_name,
+                )
+        elif dim1 == self.stack_dim:
+            # example: shape = [5, 4, 3, 2, 1], stack_dim=3, dim0=1, dim1=3
+            # resulting shape: [5, 2, 3, 4, 1]
+            if dim0 + 1 == dim1:
+                result = type(self)(
+                    *self.tensordicts, stack_dim=dim0, stack_dim_name=self._td_dim_name
+                )
+            else:
+                result = type(self)(
+                    *(td.transpose(dim0 + 1, dim1) for td in self.tensordicts),
+                    stack_dim=dim0,
+                    stack_dim_name=self._td_dim_name,
+                )
+        else:
+            dim0 = dim0 if dim0 < self.stack_dim else dim0 - 1
+            dim1 = dim1 if dim1 < self.stack_dim else dim1 - 1
+            result = type(self)(
+                *(td.transpose(dim0, dim1) for td in self.tensordicts),
+                stack_dim=self.stack_dim,
+                stack_dim_name=self._td_dim_name,
+            )
+        return result
+
+    def _permute(
+        self,
+        *args,
+        **kwargs,
+    ):
+        dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
+        dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
+        dims_list_sort = np.argsort(dims_list)
+        # find the new stack dim
+        stack_dim = dims_list_sort[self.stack_dim]
+        # remove that dim from the dims_list
+        dims_list = [
+            d if d < self.stack_dim else d - 1 for d in dims_list if d != self.stack_dim
+        ]
+        result = LazyStackedTensorDict.lazy_stack(
+            [td.permute(dims_list) for td in self.tensordicts],
+            stack_dim,
+            stack_dim_name=self._td_dim_name,
+        )
+        return result
+
+    def _squeeze(self, dim=None):
+        if dim is not None:
+            new_dim = dim
+            if new_dim < 0:
+                new_dim = self.batch_dims + new_dim
+            if new_dim > self.batch_dims - 1 or new_dim < 0:
+                raise RuntimeError(
+                    f"The dim provided to squeeze is incompatible with the tensordict shape: dim={dim} and batch_size={self.batch_size}."
+                )
+            dim = new_dim
+            if self.batch_size[dim] != 1:
+                return self
+            if dim == self.stack_dim:
+                return self.tensordicts[0]
+            if dim > self.stack_dim:
+                dim = dim - 1
+                stack_dim = self.stack_dim
+            else:
+                stack_dim = self.stack_dim - 1
+            result = LazyStackedTensorDict.lazy_stack(
+                [td.squeeze(dim) for td in self.tensordicts],
+                stack_dim,
+                stack_dim_name=self._td_dim_name,
+            )
+        else:
+            result = self
+            for dim in range(self.batch_dims - 1, -1, -1):
+                if self.batch_size[dim] == 1:
+                    result = result.squeeze(dim)
+        return result
+
+    def _unsqueeze(self, dim):
+        new_dim = dim
+        if new_dim < 0:
+            new_dim = self.batch_dims + new_dim + 1
+        if new_dim > self.batch_dims or new_dim < 0:
+            raise RuntimeError(
+                f"The dim provided to unsqueeze is incompatible with the tensordict shape: dim={dim} and batch_size={self.batch_size}."
+            )
+        dim = new_dim
+        if dim > self.stack_dim:
+            dim = dim - 1
+            stack_dim = self.stack_dim
+        else:
+            stack_dim = self.stack_dim + 1
+        result = LazyStackedTensorDict.lazy_stack(
+            [td.unsqueeze(dim) for td in self.tensordicts],
+            stack_dim,
+            stack_dim_name=self._td_dim_name,
+        )
+        return result
+
+    lock_ = TensorDictBase.lock_
+    lock = _renamed_inplace_method(lock_)
+
+    unlock_ = TensorDictBase.unlock_
+    unlock = _renamed_inplace_method(unlock_)
+
+    _check_device = TensorDict._check_device
+    _check_is_shared = TensorDict._check_is_shared
+    _convert_to_tensordict = TensorDict._convert_to_tensordict
+    _index_tensordict = TensorDict._index_tensordict
+    masked_select = TensorDict.masked_select
+    reshape = TensorDict.reshape
+    split = TensorDict.split
+    _to_module = TensorDict._to_module
+    from_dict_instance = TensorDict.from_dict_instance
+
+
+class _CustomOpTensorDict(TensorDictBase):
+    """Encodes lazy operations on tensors contained in a TensorDict."""
+
+    _safe = False
+    _lazy = True
+
+    def __init__(
+        self,
+        source: T,
+        custom_op: str,
+        inv_op: str | None = None,
+        custom_op_kwargs: dict | None = None,
+        inv_op_kwargs: dict | None = None,
+        batch_size: Sequence[int] | None = None,
+    ) -> None:
+
+        if not isinstance(source, TensorDictBase):
+            raise TypeError(
+                f"Expected source to be a TensorDictBase isntance, "
+                f"but got {type(source)} instead."
+            )
+        self._source = source
+        self.custom_op = custom_op
+        self.inv_op = inv_op
+        self.custom_op_kwargs = custom_op_kwargs if custom_op_kwargs is not None else {}
+        self.inv_op_kwargs = inv_op_kwargs if inv_op_kwargs is not None else {}
+        self._batch_size = None
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+
+    # These attributes should never be set
+    @property
+    @cache  # noqa
+    def _is_shared(self):
+        return self._source._is_shared
+
+    @property
+    @cache  # noqa
+    def _is_memmap(self):
+        return self._source._is_memmap
+
+    def is_empty(self) -> bool:
+        return self._source.is_empty()
+
+    def is_memmap(self) -> bool:
+        return self._source.is_memmap()
+
+    def is_shared(self) -> bool:
+        return self._source.is_shared()
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        """Allows for a transformation to be customized for a certain shape, device or dtype.
+
+        By default, this is a no-op on self.custom_op_kwargs
+
+        Args:
+            source_tensor: corresponding Tensor
+
+        Returns:
+            a dictionary with the kwargs of the operation to execute
+            for the tensor
+
+        """
+        return self.custom_op_kwargs
+
+    def _update_inv_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        """Allows for an inverse transformation to be customized for a certain shape, device or dtype.
+
+        By default, this is a no-op on self.inv_op_kwargs
+
+        Args:
+            source_tensor: corresponding tensor
+
+        Returns:
+            a dictionary with the kwargs of the operation to execute for
+            the tensor
+
+        """
+        return self.inv_op_kwargs
+
+    def entry_class(self, key: NestedKey) -> type:
+        return type(self._source.get(key))
+
+    @property
+    def device(self) -> torch.device | None:
+        return self._source.device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        self._source.device = value
+
+    @property
+    def batch_size(self) -> torch.Size:
+        if self._batch_size is None:
+            self._batch_size = getattr(
+                torch.zeros(self._source.batch_size, device="meta"), self.custom_op
+            )(**self.custom_op_kwargs).shape
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            f"Cannot erase names of a {type(self)}. "
+            f"Erase source TensorDict's names instead."
+        )
+
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError(
+                    "Cannot rename dimensions of a lazy TensorDict with "
+                    "nested collections. Convert the instance to a regular "
+                    "tensordict by using the `to_tensordict()` method first."
+                )
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def _get_str(self, key, default):
+        tensor = self._source._get_str(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _get_tuple(self, key, default):
+        tensor = self._source._get_tuple(key, default)
+        if tensor is default:
+            return tensor
+        return self._transform_value(tensor)
+
+    def _transform_value(self, item):
+        return getattr(item, self.custom_op)(**self._update_custom_op_kwargs(item))
+
+    def _set_str(
+        self,
+        key,
+        value,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+            validated = True
+        value = getattr(value, self.inv_op)(**self._update_inv_op_kwargs(value))
+        self._source._set_str(
+            key,
+            value,
+            inplace=inplace,
+            validated=validated,
+            ignore_lock=ignore_lock,
+            non_blocking=non_blocking,
+        )
+        return self
+
+    def _set_tuple(
+        self, key, value, *, inplace: bool, validated: bool, non_blocking: bool
+    ):
+        if len(key) == 1:
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        source = self._source._get_str(key[0], None)
+        if source is None:
+            source = self._source._create_nested_str(key[0])
+        nested = type(self)(
+            source,
+            custom_op=self.custom_op,
+            inv_op=self.inv_op,
+            custom_op_kwargs=self._update_custom_op_kwargs(source),
+            inv_op_kwargs=self._update_inv_op_kwargs(source),
+        )
+        nested._set_tuple(
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
+        transformed_tensor, original_tensor = self._get_str(
+            key, NO_DEFAULT
+        ), self._source._get_str(key, NO_DEFAULT)
+        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
+            raise RuntimeError(
+                f"{self} original tensor and transformed_in do not point to the "
+                f"same storage. Setting values in place is not currently "
+                f"supported in this setting, consider calling "
+                f"`td.clone()` before `td.set_at_(...)`"
+            )
+        transformed_tensor[idx] = value
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
+        transformed_tensor, original_tensor = self._get_tuple(
+            key, NO_DEFAULT
+        ), self._source._get_tuple(key, NO_DEFAULT)
+        if transformed_tensor.data_ptr() != original_tensor.data_ptr():
+            raise RuntimeError(
+                f"{self} original tensor and transformed_in do not point to the "
+                f"same storage. Setting values in place is not currently "
+                f"supported in this setting, consider calling "
+                f"`td.clone()` before `td.set_at_(...)`"
+            )
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+
+        transformed_tensor[idx] = value
+        return self
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        raise RuntimeError(
+            f"stacking tensordicts is not allowed for type {type(self)}"
+            f"consider calling 'to_tensordict()` first"
+        )
+
+    def __repr__(self) -> str:
+        custom_op_kwargs_str = ", ".join(
+            [f"{key}={value}" for key, value in self.custom_op_kwargs.items()]
+        )
+        indented_source = textwrap.indent(f"source={self._source}", "\t")
+        return (
+            f"{self.__class__.__name__}(\n{indented_source}, "
+            f"\n\top={self.custom_op}({custom_op_kwargs_str}))"
+        )
+
+    # @cache  # noqa: B019
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _TensorDictKeysView:
+        return self._source.keys(
+            include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+        )
+
+    def _select(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        strict: bool = True,
+        set_shared: bool = True,
+    ) -> _CustomOpTensorDict:
+        if inplace:
+            raise RuntimeError("Cannot call select inplace on a lazy tensordict.")
+        return self.to_tensordict()._select(
+            *keys, inplace=False, strict=strict, set_shared=set_shared
+        )
+
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> _CustomOpTensorDict:
+        if inplace:
+            raise RuntimeError("Cannot call exclude inplace on a lazy tensordict.")
+        return self.to_tensordict()._exclude(
+            *keys, inplace=False, set_shared=set_shared
+        )
+
+    def _clone(self, recurse: bool = True) -> T:
+        """Clones the Lazy TensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`~.tensordict.TensorDict` instance will be returned.
+                Otherwise, another :class:`~.tensordict.SubTensorDict` with identical content
+                will be returned.
+        """
+        if not recurse:
+            return type(self)(
+                source=self._source.clone(False),
+                custom_op=self.custom_op,
+                inv_op=self.inv_op,
+                custom_op_kwargs=self.custom_op_kwargs,
+                inv_op_kwargs=self.inv_op_kwargs,
+                batch_size=self.batch_size,
+            )
+        return self.to_tensordict()
+
+    def is_contiguous(self) -> bool:
+        return all([value.is_contiguous() for _, value in self.items()])
+
+    def contiguous(self) -> T:
+        return self._fast_apply(lambda x: x.contiguous(), propagate_lock=True)
+
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> _CustomOpTensorDict:
+        self._source.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    rename_key = _renamed_inplace_method(rename_key_)
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> _CustomOpTensorDict:
+        self._source = self._source.del_(key)
+        return self
+
+    def to(self, *args, **kwargs) -> T:
+        non_blocking = kwargs.pop("non_blocking", None)
+        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
+        if batch_size is not None:
+            raise TypeError(f"Cannot pass batch-size to a {type(self)}.")
+        result = self
+
+        if device is not None and dtype is None and device == self.device:
+            return result
+
+        td = self._source.to(*args, non_blocking=non_blocking, **kwargs)
+        self_copy = copy(self)
+        self_copy._source = td
+        return self_copy
+
+    def pin_memory(self) -> _CustomOpTensorDict:
+        self._source.pin_memory()
+        return self
+
+    @lock_blocked
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        key, val = self._source.popitem()
+        return key, self._transform_value(val)
+
+    def detach_(self) -> _CustomOpTensorDict:
+        self._source.detach_()
+        return self
+
+    def where(self, condition, other, *, out=None, pad=None):
+        return self.to_tensordict().where(
+            condition=condition, other=other, out=out, pad=pad
+        )
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> _CustomOpTensorDict:
+        for key, item in self.items():
+            val = self._source.get(key)
+            mask_exp = expand_right(
+                mask, list(mask.shape) + list(val.shape[self._source.batch_dims :])
+            )
+            mask_proc_inv = getattr(mask_exp, self.inv_op)(
+                **self._update_inv_op_kwargs(item)
+            )
+            val[mask_proc_inv] = value
+            self._source.set(key, val)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None,
+        copy_existing: bool,
+        executor,
+        futures,
+        inplace,
+        like,
+        share_non_tensor,
+    ) -> T:
+        def save_metadata(data: TensorDictBase, filepath, metadata=None):
+            if metadata is None:
+                metadata = {}
+            metadata.update(
+                {
+                    "shape": list(data.shape),
+                    "device": str(data.device),
+                    "_type": str(data.__class__),
+                    "custom_op": data.custom_op,
+                    "inv_op": data.inv_op,
+                    "custom_op_kwargs": data.custom_op_kwargs,
+                    "inv_op_kwargs": data.inv_op_kwargs,
+                }
+            )
+            with open(filepath, "w") as json_metadata:
+                json.dump(metadata, json_metadata)
+
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            metadata = {}
+
+        dest_source = self._source._memmap_(
+            prefix=None if prefix is None else prefix / "_source",
+            copy_existing=copy_existing,
+            executor=executor,
+            futures=futures,
+            inplace=inplace,
+            like=like,
+            share_non_tensor=share_non_tensor,
+        )
+        if not inplace:
+            dest = type(self)(
+                dest_source,
+                custom_op=self.custom_op,
+                inv_op=self.inv_op,
+                custom_op_kwargs=self.custom_op_kwargs,
+                inv_op_kwargs=self.inv_op_kwargs,
+                batch_size=self.batch_size,
+            )
+        else:
+            dest = self
+
+        if prefix is not None:
+            if executor is None:
+                save_metadata(
+                    dest,
+                    prefix / "meta.json",
+                    metadata=metadata,
+                )
+            else:
+                futures.append(
+                    executor.submit(save_metadata, dest, prefix / "meta.json", metadata)
+                )
+        return dest
+
+    @classmethod
+    def _load_memmap(cls, prefix: str, metadata: dict, **kwargs) -> _CustomOpTensorDict:
+        custom_op = metadata.pop("custom_op")
+        inv_op = metadata.pop("inv_op")
+        custom_op_kwargs = metadata.pop("custom_op_kwargs")
+        inv_op_kwargs = metadata.pop("inv_op_kwargs")
+
+        source = TensorDict.load_memmap(prefix / "_source", **kwargs, non_blocking=True)
+
+        return cls(
+            source,
+            custom_op=custom_op,
+            inv_op=inv_op,
+            custom_op_kwargs=custom_op_kwargs,
+            inv_op_kwargs=inv_op_kwargs,
+        )
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for lazy tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def share_memory_(self) -> _CustomOpTensorDict:
+        self._source.share_memory_()
+        self.lock_()
+        return self
+
+    @property
+    def _td_dim_names(self):
+        # we also want for _td_dim_names to be accurate
+        if self._source._td_dim_names is None:
+            return None
+        return self.names
+
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> T:
+        self._source.lock_()
+        return self
+
+    @erase_cache
+    @as_decorator("is_locked")
+    def unlock_(self) -> T:
+        self._source.unlock_()
+        return self
+
+    def _remove_lock(self, lock_id):
+        return self._source._remove_lock(lock_id)
+
+    @erase_cache
+    def _propagate_lock(self, lock_ids):
+        return self._source._propagate_lock(lock_ids)
+
+    @erase_cache
+    def _propagate_unlock(self):
+        return self._source._propagate_unlock()
+
+    lock = _renamed_inplace_method(lock_)
+    unlock = _renamed_inplace_method(unlock_)
+
+    def __del__(self):
+        pass
+
+    @property
+    def sorted_keys(self):
+        return self._source.sorted_keys
+
+    def _view(self, *args, **kwargs):
+        raise RuntimeError(
+            "Cannot call `view` on a lazy tensordict. Call `reshape` instead."
+        )
+
+    def _transpose(self, dim0, dim1):
+        raise RuntimeError(
+            "Cannot call `transpose` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _permute(
+        self,
+        *args,
+        **kwargs,
+    ):
+        raise RuntimeError(
+            "Cannot call `permute` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _squeeze(self, dim=None):
+        raise RuntimeError(
+            "Cannot call `squeeze` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _unsqueeze(self, dim):
+        raise RuntimeError(
+            "Cannot call `unsqueeze` on a lazy tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _cast_reduction(
+        self,
+        *,
+        reduction_name,
+        dim=NO_DEFAULT,
+        keepdim=NO_DEFAULT,
+        tuple_ok=True,
+        **kwargs,
+    ):
+        try:
+            td = self.to_tensordict()
+        except Exception:
+            raise RuntimeError(
+                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
+                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
+                f" on github!"
+            )
+        return td._cast_reduction(
+            reduction_name=reduction_name,
+            dim=dim,
+            keepdim=keepdim,
+            tuple_ok=tuple_ok,
+            **kwargs,
+        )
+
+    __xor__ = TensorDict.__xor__
+    __or__ = TensorDict.__or__
+    __eq__ = TensorDict.__eq__
+    __ne__ = TensorDict.__ne__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
+    __setitem__ = TensorDict.__setitem__
+    _add_batch_dim = TensorDict._add_batch_dim
+    _check_device = TensorDict._check_device
+    _check_is_shared = TensorDict._check_is_shared
+    _convert_to_tensordict = TensorDict._convert_to_tensordict
+    _index_tensordict = TensorDict._index_tensordict
+    masked_select = TensorDict.masked_select
+    reshape = TensorDict.reshape
+    split = TensorDict.split
+    _to_module = TensorDict._to_module
+    _apply_nest = TensorDict._apply_nest
+    _remove_batch_dim = TensorDict._remove_batch_dim
+    all = TensorDict.all
+    any = TensorDict.any
+    expand = TensorDict.expand
+    _unbind = TensorDict._unbind
+    _get_names_idx = TensorDict._get_names_idx
+    from_dict_instance = TensorDict.from_dict_instance
+
+
+class _UnsqueezedTensorDict(_CustomOpTensorDict):
+    """A lazy view on an unsqueezed TensorDict.
+
+    When calling `tensordict.unsqueeze(dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.unsqueeze(dim).squeeze(dim) is tensordict
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> td = TensorDict({'a': torch.randn(3, 4)}, batch_size=[3])
+        >>> td_unsqueeze = td.unsqueeze(-1)
+        >>> print(td_unsqueeze.shape)
+        torch.Size([3, 1])
+        >>> print(td_unsqueeze.squeeze(-1) is td)
+        True
+    """
+
+    def _legacy_squeeze(self, dim: int | None) -> T:
+        if dim is not None and dim < 0:
+            dim = self.batch_dims + dim
+        if dim == self.custom_op_kwargs.get("dim"):
+            return self._source
+        return super()._legacy_squeeze(dim)
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        unsqueezed_dim = self.custom_op_kwargs["dim"]
+        diff_to_apply = 1 if dim < unsqueezed_dim else 0
+        list_item_unsqueeze = [
+            item.squeeze(unsqueezed_dim - diff_to_apply) for item in list_item
+        ]
+        return self._source._stack_onto_(list_item_unsqueeze, dim)
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs.get("dim")
+        names.insert(dim, None)
+        return names
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _SqueezedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a squeezed TensorDict.
+
+    See the `UnsqueezedTensorDict` class documentation for more information.
+
+    """
+
+    def _legacy_unsqueeze(self, dim: int) -> T:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+        inv_op_dim = self.inv_op_kwargs.get("dim")
+        if inv_op_dim < 0:
+            inv_op_dim = self.batch_dims + inv_op_dim + 1
+        if dim == inv_op_dim:
+            return self._source
+        return super()._legacy_unsqueeze(dim)
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        squeezed_dim = self.custom_op_kwargs["dim"]
+        # dim=0, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[4, 5], [4, 5], [4, 5]] => unsq 1
+        # dim=1, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 5], [3, 5], [3, 5], [3, 4]] => unsq 1
+        # dim=2, squeezed_dim=2, [3, 4, 5] [3, 4, 1, 5] [[3, 4], [3, 4], ...] => unsq 2
+        diff_to_apply = 1 if dim < squeezed_dim else 0
+        list_item_unsqueeze = [
+            item.unsqueeze(squeezed_dim - diff_to_apply) for item in list_item
+        ]
+        return self._source._stack_onto_(list_item_unsqueeze, dim)
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim = self.custom_op_kwargs["dim"]
+        if self._source.batch_size[dim] == 1:
+            del names[dim]
+        return names
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _ViewedTensorDict(_CustomOpTensorDict):
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        new_dim_list = list(self.custom_op_kwargs.get("size"))
+        new_dim_list += list(source_tensor.shape[self._source.batch_dims :])
+        new_dim = torch.Size(new_dim_list)
+        new_dict = deepcopy(self.custom_op_kwargs)
+        new_dict.update({"size": new_dim})
+        return new_dict
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict:
+        size = list(self.inv_op_kwargs.get("size"))
+        size += list(_shape(tensor)[self.batch_dims :])
+        new_dim = torch.Size(size)
+        new_dict = deepcopy(self.inv_op_kwargs)
+        new_dict.update({"size": new_dim})
+        return new_dict
+
+    def _legacy_view(
+        self, *shape: int, size: list | tuple | torch.Size | None = None
+    ) -> T:
+        if len(shape) == 0 and size is not None:
+            return self._legacy_view(*size)
+        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
+            return self._legacy_view(*shape[0])
+        elif not isinstance(shape, torch.Size):
+            shape = infer_size_impl(shape, self.numel())
+            shape = torch.Size(shape)
+        if shape == self._source.batch_size:
+            return self._source
+        return super()._legacy_view(*shape)
+
+    @property
+    def names(self):
+        return [None] * self.ndim
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _TransposedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a TensorDict with two batch dimensions transposed.
+
+    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.transpose(dims_list, dim).transpose(dims_list, dim) is tensordict
+
+    """
+
+    def _legacy_transpose(self, dim0, dim1) -> T:
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        dims = (self.inv_op_kwargs.get("dim0"), self.inv_op_kwargs.get("dim1"))
+        if dim0 in dims and dim1 in dims:
+            return self._source
+        return super()._legacy_transpose(dim0, dim1)
+
+    def add_missing_dims(
+        self, num_dims: int, batch_dims: tuple[int, ...]
+    ) -> tuple[int, ...]:
+        dim_diff = num_dims - len(batch_dims)
+        all_dims = list(range(num_dims))
+        for i, x in enumerate(batch_dims):
+            if x < 0:
+                x = x - dim_diff
+            all_dims[i] = x
+        return tuple(all_dims)
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
+        return self.custom_op_kwargs
+
+    def _stack_onto_(
+        self,
+        # key: str,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        trsp = self.custom_op_kwargs["dim0"], self.custom_op_kwargs["dim1"]
+        if dim == trsp[0]:
+            dim = trsp[1]
+        elif dim == trsp[1]:
+            dim = trsp[0]
+
+        list_permuted_items = []
+        for item in list_item:
+            list_permuted_items.append(item.transpose(*trsp))
+        self._source._stack_onto_(list_permuted_items, dim)
+        return self
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        dim0 = self.custom_op_kwargs["dim0"]
+        dim1 = self.custom_op_kwargs["dim1"]
+        names = [
+            names[dim0] if i == dim1 else names[dim1] if i == dim0 else name
+            for i, name in enumerate(names)
+        ]
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+class _PermutedTensorDict(_CustomOpTensorDict):
+    """A lazy view on a TensorDict with the batch dimensions permuted.
+
+    When calling `tensordict.permute(dims_list, dim)`, a lazy view of this operation is
+    returned such that the following code snippet works without raising an
+    exception:
+
+        >>> assert tensordict.permute(dims_list, dim).permute(dims_list, dim) is tensordict
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> import torch
+        >>> td = TensorDict({'a': torch.randn(4, 5, 6, 9)}, batch_size=[3])
+        >>> td_permute = td.permute(dims=(2, 1, 0))
+        >>> print(td_permute.shape)
+        torch.Size([6, 5, 4])
+        >>> print(td_permute.permute(dims=(2, 1, 0)) is td)
+        True
+
+    """
+
+    def _legacy_permute(
+        self,
+        *dims_list: int,
+        dims: Sequence[int] | None = None,
+    ) -> T:
+        if len(dims_list) == 0:
+            dims_list = dims
+        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
+            dims_list = dims_list[0]
+        if len(dims_list) != len(self.shape):
+            raise RuntimeError(
+                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
+            )
+        if not len(dims_list) and not self.batch_dims:
+            return self
+        if np.array_equal(dims_list, range(self.batch_dims)):
+            return self
+        if np.array_equal(np.argsort(dims_list), self.inv_op_kwargs.get("dims")):
+            return self._source
+        return super()._legacy_permute(*dims_list)
+
+    def add_missing_dims(
+        self, num_dims: int, batch_dims: tuple[int, ...]
+    ) -> tuple[int, ...]:
+        # Adds the feature dimensions to the permute dims
+        dim_diff = num_dims - len(batch_dims)
+        all_dims = list(range(num_dims))
+        for i, x in enumerate(batch_dims):
+            if x < 0:
+                x = x - dim_diff
+            all_dims[i] = x
+        return tuple(all_dims)
+
+    def _update_custom_op_kwargs(self, source_tensor: Tensor) -> dict[str, Any]:
+        new_dims = self.add_missing_dims(
+            len(source_tensor.shape), self.custom_op_kwargs["dims"]
+        )
+        kwargs = deepcopy(self.custom_op_kwargs)
+        kwargs.update({"dims": new_dims})
+        return kwargs
+
+    def _update_inv_op_kwargs(self, tensor: Tensor) -> dict[str, Any]:
+        new_dims = self.add_missing_dims(
+            self._source.batch_dims + len(_shape(tensor)[self.batch_dims :]),
+            self.custom_op_kwargs["dims"],
+        )
+        kwargs = deepcopy(self.custom_op_kwargs)
+        kwargs.update({"dims": tuple(np.argsort(new_dims))})
+        return kwargs
+
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        permute_dims = self.custom_op_kwargs["dims"]
+        inv_permute_dims = np.argsort(permute_dims)
+        new_dim = [i for i, v in enumerate(inv_permute_dims) if v == dim][0]
+        inv_permute_dims = [p for p in inv_permute_dims if p != dim]
+        inv_permute_dims = np.argsort(np.argsort(inv_permute_dims))
+
+        list_permuted_items = []
+        for item in list_item:
+            perm = list(inv_permute_dims) + list(
+                range(self.batch_dims - 1, item.ndimension())
+            )
+            list_permuted_items.append(item.permute(*perm))
+        self._source._stack_onto_(list_permuted_items, new_dim)
+        return self
+
+    @property
+    def names(self):
+        names = copy(self._source.names)
+        return [names[i] for i in self.custom_op_kwargs["dims"]]
+
+    @names.setter
+    def names(self, value):
+        if value[: self.batch_dims] == self.names:
+            return
+        raise RuntimeError(
+            "Names of a lazy tensordict cannot be modified. Call to_tensordict() first."
+        )
+
+
+def _iter_items_lazystack(
+    tensordict: LazyStackedTensorDict, return_none_for_het_values: bool = False
+) -> Iterator[tuple[str, CompatibleType]]:
+    for key in tensordict.tensordicts[0].keys():
+        values = tensordict._maybe_get_list(key)
+        if values is not None:
+            yield key, values
+
+
+_register_tensor_class(LazyStackedTensorDict)
+_register_tensor_class(_CustomOpTensorDict)
+_register_tensor_class(_PermutedTensorDict)
+_register_tensor_class(_SqueezedTensorDict)
+_register_tensor_class(_UnsqueezedTensorDict)
+_register_tensor_class(_TransposedTensorDict)
+_register_tensor_class(_ViewedTensorDict)
```

## tensordict/_pytree.py

 * *Ordering differences only*

```diff
@@ -1,276 +1,276 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from collections import defaultdict
-from typing import Any, Dict, List, Tuple
-
-import torch
-from tensordict import (
-    LazyStackedTensorDict,
-    PersistentTensorDict,
-    TensorDict,
-    TensorDictBase,
-)
-from tensordict._td import _SubTensorDict
-from tensordict.base import _NESTED_TENSORS_AS_LISTS
-from tensordict.utils import _shape, implement_for
-
-try:
-    from torch.utils._pytree import Context, MappingKey, register_pytree_node
-except ImportError:
-    from torch.utils._pytree import (
-        _register_pytree_node as register_pytree_node,
-        Context,
-    )
-
-PYTREE_REGISTERED_TDS = (
-    _SubTensorDict,
-    TensorDict,
-    PersistentTensorDict,
-)
-PYTREE_REGISTERED_LAZY_TDS = (LazyStackedTensorDict,)
-
-
-def _str_to_dict(str_spec: str) -> Tuple[List[str], str]:
-    assert str_spec[1] == "("
-    assert str_spec[-1] == ")"
-    context_and_child_strings = str_spec[2:-1]
-
-    child_strings = []
-    context_strings = []
-    nested_parentheses = 0
-    start_index = 0
-    for i, char in enumerate(context_and_child_strings):
-        if char == ":":
-            if nested_parentheses == 0:
-                context_strings.append(context_and_child_strings[start_index:i])
-                start_index = i + 1
-        elif char == "(":
-            nested_parentheses += 1
-        elif char == ")":
-            nested_parentheses -= 1
-
-        if nested_parentheses == 0 and char == ",":
-            child_strings.append(context_and_child_strings[start_index:i])
-            start_index = i + 1
-
-    child_strings.append(context_and_child_strings[start_index:])
-    return context_strings, ",".join(child_strings)
-
-
-def _str_to_tensordictdict(str_spec: str) -> Tuple[List[str], str]:
-    context_and_child_strings = str_spec[2:-1]
-
-    child_strings = []
-    context_strings = []
-    nested_parentheses = 0
-    start_index = 0
-    for i, char in enumerate(context_and_child_strings):
-        if char == ":":
-            if nested_parentheses == 0:
-                context_strings.append(context_and_child_strings[start_index:i])
-                start_index = i + 1
-        elif char == "(":
-            nested_parentheses += 1
-        elif char == ")":
-            nested_parentheses -= 1
-
-        if nested_parentheses == 0 and char == ",":
-            child_strings.append(context_and_child_strings[start_index:i])
-            start_index = i + 1
-
-    child_strings.append(context_and_child_strings[start_index:])
-    return context_strings, ",".join(child_strings)
-
-
-def _tensordict_flatten(d: TensorDict) -> Tuple[List[Any], Context]:
-    items = tuple(d.items())
-    if items:
-        keys, values = zip(*items)
-        keys = list(keys)
-        values = list(values)
-    else:
-        keys = []
-        values = []
-    return values, {
-        "keys": keys,
-        "batch_size": d.batch_size,
-        "names": d.names,
-        "device": d.device,
-        "constructor": _constructor(type(d)),
-        "non_tensor_data": d.non_tensor_items(),
-        "cls": type(d),
-    }
-
-
-def _lazy_tensordict_flatten(d: LazyStackedTensorDict) -> Tuple[List[Any], Context]:
-    return list(d.tensordicts), {
-        "stack_dim_name": d._td_dim_name,
-        "stack_dim": d.stack_dim,
-        "constructor": _lazy_tensordict_constructor,
-        "cls": type(d),
-    }
-
-
-def _tensordict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
-    device = context["device"]
-    if device is not None:
-        device = (
-            device
-            if all(val.device == device for val in values if hasattr(val, "device"))
-            else None
-        )
-    batch_size = context["batch_size"]
-    names = context["names"]
-    keys = context["keys"]
-    constructor = context["constructor"]
-    non_tensor_items = context["non_tensor_data"]
-    cls = context["cls"]
-    batch_dims = len(batch_size)
-    if any(tensor is None for tensor in values):
-        return
-    if any(_shape(tensor)[:batch_dims] != batch_size for tensor in values):
-        batch_size = torch.Size([])
-        names = None
-    return constructor(
-        cls=cls,
-        keys=keys,
-        values=values,
-        batch_size=batch_size,
-        names=names,
-        device=device,
-        non_tensor_items=non_tensor_items,
-    )
-
-
-def _lazy_tensordict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
-    stack_dim = context["stack_dim"]
-    return cls(*values, stack_dim=stack_dim, stack_dim_name=context["stack_dim_name"])
-
-
-def _td_flatten_with_keys(
-    d: TensorDictBase,
-):
-    items = tuple(d.items(is_leaf=_NESTED_TENSORS_AS_LISTS))
-    if items:
-        keys, values = zip(*items)
-        keys = list(keys)
-        values = list(values)
-    else:
-        keys = []
-        values = []
-    return [(MappingKey(k), v) for k, v in zip(keys, values)], {
-        "keys": keys,
-        "batch_size": d.batch_size,
-        "names": d.names,
-        "device": d.device,
-        "constructor": _constructor(type(d)),
-        "non_tensor_data": d.non_tensor_items(),
-        "cls": type(d),
-    }
-
-
-def _lazy_td_flatten_with_keys(
-    d: LazyStackedTensorDict,
-):
-    raise NotImplementedError
-
-
-@implement_for("torch", None, "2.3")
-def _register_td_node(cls):
-    register_pytree_node(
-        cls,
-        _tensordict_flatten,
-        _tensordict_unflatten,
-    )
-
-
-@implement_for("torch", "2.3")
-def _register_td_node(cls):  # noqa: F811
-    register_pytree_node(
-        cls,
-        _tensordict_flatten,
-        _tensordict_unflatten,
-        flatten_with_keys_fn=_td_flatten_with_keys,
-    )
-
-
-@implement_for("torch", None, "2.3")
-def _register_lazy_td_node(cls):
-    register_pytree_node(
-        cls,
-        _lazy_tensordict_flatten,
-        _lazy_tensordict_unflatten,
-    )
-
-
-@implement_for("torch", "2.3")
-def _register_lazy_td_node(cls):  # noqa: F811
-    register_pytree_node(
-        cls,
-        _lazy_tensordict_flatten,
-        _lazy_tensordict_unflatten,
-        flatten_with_keys_fn=_lazy_td_flatten_with_keys,
-    )
-
-
-def _constructor(cls):
-    return _CONSTRUCTORS[cls]
-
-
-def _tensorclass_constructor(
-    *, cls, keys, values, batch_size, names, device, non_tensor_items
-):
-    result = _tensordict_constructor(
-        cls=TensorDict,
-        keys=keys,
-        values=values,
-        batch_size=batch_size,
-        names=names,
-        device=device,
-        non_tensor_items=(),
-    )
-    result = cls._from_tensordict(result, dict(non_tensor_items))
-    return result
-
-
-def _tensordict_constructor(
-    *, cls, keys, values, batch_size, names, device, non_tensor_items
-):
-    result = cls(
-        dict(zip(keys, values)),
-        batch_size=batch_size,
-        names=names,
-        device=device,
-        _run_checks=False,
-    )
-    for key, item in non_tensor_items:
-        result.set_non_tensor(key, item)
-    return result
-
-
-def _lazy_tensordict_constructor(
-    *, cls, keys, values, batch_size, names, device, non_tensor_items
-):
-
-    result = cls(
-        dict(zip(keys, values)),
-        batch_size=batch_size,
-        names=names,
-        device=device,
-        _run_checks=False,
-    )
-    for key, item in non_tensor_items:
-        result.set_non_tensor(key, item)
-    return result
-
-
-_CONSTRUCTORS = defaultdict(lambda: _tensordict_constructor)
-_CONSTRUCTORS[LazyStackedTensorDict] = _lazy_tensordict_constructor
-
-
-for cls in PYTREE_REGISTERED_TDS:
-    _register_td_node(cls)
-for cls in PYTREE_REGISTERED_LAZY_TDS:
-    _register_lazy_td_node(cls)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from collections import defaultdict
+from typing import Any, Dict, List, Tuple
+
+import torch
+from tensordict import (
+    LazyStackedTensorDict,
+    PersistentTensorDict,
+    TensorDict,
+    TensorDictBase,
+)
+from tensordict._td import _SubTensorDict
+from tensordict.base import _NESTED_TENSORS_AS_LISTS
+from tensordict.utils import _shape, implement_for
+
+try:
+    from torch.utils._pytree import Context, MappingKey, register_pytree_node
+except ImportError:
+    from torch.utils._pytree import (
+        _register_pytree_node as register_pytree_node,
+        Context,
+    )
+
+PYTREE_REGISTERED_TDS = (
+    _SubTensorDict,
+    TensorDict,
+    PersistentTensorDict,
+)
+PYTREE_REGISTERED_LAZY_TDS = (LazyStackedTensorDict,)
+
+
+def _str_to_dict(str_spec: str) -> Tuple[List[str], str]:
+    assert str_spec[1] == "("
+    assert str_spec[-1] == ")"
+    context_and_child_strings = str_spec[2:-1]
+
+    child_strings = []
+    context_strings = []
+    nested_parentheses = 0
+    start_index = 0
+    for i, char in enumerate(context_and_child_strings):
+        if char == ":":
+            if nested_parentheses == 0:
+                context_strings.append(context_and_child_strings[start_index:i])
+                start_index = i + 1
+        elif char == "(":
+            nested_parentheses += 1
+        elif char == ")":
+            nested_parentheses -= 1
+
+        if nested_parentheses == 0 and char == ",":
+            child_strings.append(context_and_child_strings[start_index:i])
+            start_index = i + 1
+
+    child_strings.append(context_and_child_strings[start_index:])
+    return context_strings, ",".join(child_strings)
+
+
+def _str_to_tensordictdict(str_spec: str) -> Tuple[List[str], str]:
+    context_and_child_strings = str_spec[2:-1]
+
+    child_strings = []
+    context_strings = []
+    nested_parentheses = 0
+    start_index = 0
+    for i, char in enumerate(context_and_child_strings):
+        if char == ":":
+            if nested_parentheses == 0:
+                context_strings.append(context_and_child_strings[start_index:i])
+                start_index = i + 1
+        elif char == "(":
+            nested_parentheses += 1
+        elif char == ")":
+            nested_parentheses -= 1
+
+        if nested_parentheses == 0 and char == ",":
+            child_strings.append(context_and_child_strings[start_index:i])
+            start_index = i + 1
+
+    child_strings.append(context_and_child_strings[start_index:])
+    return context_strings, ",".join(child_strings)
+
+
+def _tensordict_flatten(d: TensorDict) -> Tuple[List[Any], Context]:
+    items = tuple(d.items())
+    if items:
+        keys, values = zip(*items)
+        keys = list(keys)
+        values = list(values)
+    else:
+        keys = []
+        values = []
+    return values, {
+        "keys": keys,
+        "batch_size": d.batch_size,
+        "names": d.names,
+        "device": d.device,
+        "constructor": _constructor(type(d)),
+        "non_tensor_data": d.non_tensor_items(),
+        "cls": type(d),
+    }
+
+
+def _lazy_tensordict_flatten(d: LazyStackedTensorDict) -> Tuple[List[Any], Context]:
+    return list(d.tensordicts), {
+        "stack_dim_name": d._td_dim_name,
+        "stack_dim": d.stack_dim,
+        "constructor": _lazy_tensordict_constructor,
+        "cls": type(d),
+    }
+
+
+def _tensordict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
+    device = context["device"]
+    if device is not None:
+        device = (
+            device
+            if all(val.device == device for val in values if hasattr(val, "device"))
+            else None
+        )
+    batch_size = context["batch_size"]
+    names = context["names"]
+    keys = context["keys"]
+    constructor = context["constructor"]
+    non_tensor_items = context["non_tensor_data"]
+    cls = context["cls"]
+    batch_dims = len(batch_size)
+    if any(tensor is None for tensor in values):
+        return
+    if any(_shape(tensor)[:batch_dims] != batch_size for tensor in values):
+        batch_size = torch.Size([])
+        names = None
+    return constructor(
+        cls=cls,
+        keys=keys,
+        values=values,
+        batch_size=batch_size,
+        names=names,
+        device=device,
+        non_tensor_items=non_tensor_items,
+    )
+
+
+def _lazy_tensordict_unflatten(values: List[Any], context: Context) -> Dict[Any, Any]:
+    stack_dim = context["stack_dim"]
+    return cls(*values, stack_dim=stack_dim, stack_dim_name=context["stack_dim_name"])
+
+
+def _td_flatten_with_keys(
+    d: TensorDictBase,
+):
+    items = tuple(d.items(is_leaf=_NESTED_TENSORS_AS_LISTS))
+    if items:
+        keys, values = zip(*items)
+        keys = list(keys)
+        values = list(values)
+    else:
+        keys = []
+        values = []
+    return [(MappingKey(k), v) for k, v in zip(keys, values)], {
+        "keys": keys,
+        "batch_size": d.batch_size,
+        "names": d.names,
+        "device": d.device,
+        "constructor": _constructor(type(d)),
+        "non_tensor_data": d.non_tensor_items(),
+        "cls": type(d),
+    }
+
+
+def _lazy_td_flatten_with_keys(
+    d: LazyStackedTensorDict,
+):
+    raise NotImplementedError
+
+
+@implement_for("torch", None, "2.3")
+def _register_td_node(cls):
+    register_pytree_node(
+        cls,
+        _tensordict_flatten,
+        _tensordict_unflatten,
+    )
+
+
+@implement_for("torch", "2.3")
+def _register_td_node(cls):  # noqa: F811
+    register_pytree_node(
+        cls,
+        _tensordict_flatten,
+        _tensordict_unflatten,
+        flatten_with_keys_fn=_td_flatten_with_keys,
+    )
+
+
+@implement_for("torch", None, "2.3")
+def _register_lazy_td_node(cls):
+    register_pytree_node(
+        cls,
+        _lazy_tensordict_flatten,
+        _lazy_tensordict_unflatten,
+    )
+
+
+@implement_for("torch", "2.3")
+def _register_lazy_td_node(cls):  # noqa: F811
+    register_pytree_node(
+        cls,
+        _lazy_tensordict_flatten,
+        _lazy_tensordict_unflatten,
+        flatten_with_keys_fn=_lazy_td_flatten_with_keys,
+    )
+
+
+def _constructor(cls):
+    return _CONSTRUCTORS[cls]
+
+
+def _tensorclass_constructor(
+    *, cls, keys, values, batch_size, names, device, non_tensor_items
+):
+    result = _tensordict_constructor(
+        cls=TensorDict,
+        keys=keys,
+        values=values,
+        batch_size=batch_size,
+        names=names,
+        device=device,
+        non_tensor_items=(),
+    )
+    result = cls._from_tensordict(result, dict(non_tensor_items))
+    return result
+
+
+def _tensordict_constructor(
+    *, cls, keys, values, batch_size, names, device, non_tensor_items
+):
+    result = cls(
+        dict(zip(keys, values)),
+        batch_size=batch_size,
+        names=names,
+        device=device,
+        _run_checks=False,
+    )
+    for key, item in non_tensor_items:
+        result.set_non_tensor(key, item)
+    return result
+
+
+def _lazy_tensordict_constructor(
+    *, cls, keys, values, batch_size, names, device, non_tensor_items
+):
+
+    result = cls(
+        dict(zip(keys, values)),
+        batch_size=batch_size,
+        names=names,
+        device=device,
+        _run_checks=False,
+    )
+    for key, item in non_tensor_items:
+        result.set_non_tensor(key, item)
+    return result
+
+
+_CONSTRUCTORS = defaultdict(lambda: _tensordict_constructor)
+_CONSTRUCTORS[LazyStackedTensorDict] = _lazy_tensordict_constructor
+
+
+for cls in PYTREE_REGISTERED_TDS:
+    _register_td_node(cls)
+for cls in PYTREE_REGISTERED_LAZY_TDS:
+    _register_lazy_td_node(cls)
```

## tensordict/_td.py

 * *Ordering differences only*

```diff
@@ -1,4114 +1,4114 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import json
-import numbers
-import os
-from collections import defaultdict
-from copy import copy
-from numbers import Number
-from pathlib import Path
-from textwrap import indent
-from typing import Any, Callable, Iterable, Iterator, List, Sequence, Tuple, Type
-from warnings import warn
-
-import numpy as np
-import torch
-
-try:
-    from functorch import dim as ftdim
-
-    _has_funcdim = True
-except ImportError:
-    from tensordict.utils import _ftdim_mock as ftdim
-
-    _has_funcdim = False
-
-from tensordict.base import (
-    _ACCEPTED_CLASSES,
-    _default_is_leaf,
-    _is_tensor_collection,
-    _load_metadata,
-    _register_tensor_class,
-    BEST_ATTEMPT_INPLACE,
-    CompatibleType,
-    is_tensor_collection,
-    NO_DEFAULT,
-    T,
-    TensorDictBase,
-)
-
-from tensordict.memmap import MemoryMappedTensor
-from tensordict.utils import (
-    _add_batch_dim_pre_hook,
-    _BatchedUninitializedBuffer,
-    _BatchedUninitializedParameter,
-    _clone_value,
-    _expand_to_match_shape,
-    _get_item,
-    _get_leaf_tensordict,
-    _get_shape_from_args,
-    _getitem_batch_size,
-    _index_preserve_data_ptr,
-    _is_number,
-    _is_shared,
-    _is_tensorclass,
-    _KEY_ERROR,
-    _LOCK_ERROR,
-    _NON_STR_KEY_ERR,
-    _NON_STR_KEY_TUPLE_ERR,
-    _parse_to,
-    _prune_selected_keys,
-    _set_item,
-    _set_max_batch_size,
-    _shape,
-    _STRDTYPE2DTYPE,
-    _StringOnlyDict,
-    _sub_index,
-    _unravel_key_to_tuple,
-    as_decorator,
-    Buffer,
-    cache,
-    convert_ellipsis_to_idx,
-    DeviceType,
-    expand_as_right,
-    IndexType,
-    is_non_tensor,
-    is_tensorclass,
-    KeyedJaggedTensor,
-    lock_blocked,
-    NestedKey,
-    unravel_key,
-    unravel_key_list,
-)
-from torch import Tensor
-from torch.jit._shape_functions import infer_size_impl
-from torch.utils._pytree import tree_map
-
-_register_tensor_class(ftdim.Tensor)
-
-__base__setattr__ = torch.nn.Module.__setattr__
-
-_has_mps = torch.backends.mps.is_available()
-_has_cuda = torch.cuda.is_available()
-_has_functorch = False
-try:
-    try:
-        from torch._C._functorch import (
-            _add_batch_dim,
-            _remove_batch_dim,
-            is_batchedtensor,
-        )
-    except ImportError:
-        from functorch._C import is_batchedtensor
-
-    _has_functorch = True
-except ImportError:
-    _has_functorch = False
-
-    def is_batchedtensor(tensor: Tensor) -> bool:
-        """Placeholder for the functorch function."""
-        return False
-
-
-class TensorDict(TensorDictBase):
-    """A batched dictionary of tensors.
-
-    TensorDict is a tensor container where all tensors are stored in a
-    key-value pair fashion and where each element shares the same first ``N``
-    leading dimensions shape, where is an arbitrary number with ``N >= 0``.
-
-    Additionally, if the tensordict has a specified device, then each element
-    must share that device.
-
-    TensorDict instances support many regular tensor operations with the notable
-    exception of algebraic operations:
-
-    - operations on shape: when a shape operation is called (indexing,
-      reshape, view, expand, transpose, permute,
-      unsqueeze, squeeze, masking etc), the operations is done as if it
-      was executed on a tensor of the same shape as the batch size then
-      expended to the right, e.g.:
-
-        >>> td = TensorDict({'a': torch.zeros(3, 4, 5)}, batch_size=[3, 4])
-        >>> # returns a TensorDict of batch size [3, 4, 1]:
-        >>> td_unsqueeze = td.unsqueeze(-1)
-        >>> # returns a TensorDict of batch size [12]
-        >>> td_view = td.view(-1)
-        >>> # returns a tensor of batch size [12, 4]
-        >>> a_view = td.view(-1).get("a")
-
-    - casting operations: a TensorDict can be cast on a different device using
-
-        >>> td_cpu = td.to("cpu")
-        >>> dictionary = td.to_dict()
-
-      A call of the `.to()` method with a dtype will return an error.
-
-    - Cloning (:meth:`~TensorDictBase.clone`), contiguous (:meth:`~TensorDictBase.contiguous`);
-
-    - Reading: `td.get(key)`, `td.get_at(key, index)`
-
-    - Content modification: :obj:`td.set(key, value)`, :obj:`td.set_(key, value)`,
-      :obj:`td.update(td_or_dict)`, :obj:`td.update_(td_or_dict)`, :obj:`td.fill_(key,
-      value)`, :obj:`td.rename_key_(old_name, new_name)`, etc.
-
-    - Operations on multiple tensordicts: `torch.cat(tensordict_list, dim)`,
-      `torch.stack(tensordict_list, dim)`, `td1 == td2`, `td.apply(lambda x+y, other_td)` etc.
-
-    Args:
-        source (TensorDict or Dict[NestedKey, Union[Tensor, TensorDictBase]]): a
-            data source. If empty, the tensordict can be populated subsequently.
-        batch_size (iterable of int, optional): a batch size for the
-            tensordict. The batch size can be modified subsequently as long
-            as it is compatible with its content.
-            If not batch-size is provided, an empty batch-size is assumed (it
-            is not inferred automatically from the data). To automatically set
-            the batch-size, refer to :meth:`~.auto_batch_size_`.
-        device (torch.device or compatible type, optional): a device for the
-            TensorDict. If provided, all tensors will be stored on that device.
-            If not, tensors on different devices are allowed.
-        names (lsit of str, optional): the names of the dimensions of the
-            tensordict. If provided, its length must match the one of the
-            ``batch_size``. Defaults to ``None`` (no dimension name, or ``None``
-            for every dimension).
-        non_blocking (bool, optional): if ``True`` and a device is passed, the tensordict
-            is delivered without synchronization. This is the fastest option but is only
-            safe when casting from cpu to cuda (otherwise a synchronization call must be
-            implemented by the user).
-            If ``False`` is passed, every tensor movement will be done synchronously.
-            If ``None`` (default), the device casting will be done asynchronously but
-            a synchronization will be executed after creation if required. This option
-            should generally be faster than ``False`` and potentially slower than ``True``.
-        lock (bool, optional): if ``True``, the resulting tensordict will be
-            locked.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> source = {'random': torch.randn(3, 4),
-        ...     'zeros': torch.zeros(3, 4, 5)}
-        >>> batch_size = [3]
-        >>> td = TensorDict(source, batch_size=batch_size)
-        >>> print(td.shape)  # equivalent to td.batch_size
-        torch.Size([3])
-        >>> td_unqueeze = td.unsqueeze(-1)
-        >>> print(td_unqueeze.get("zeros").shape)
-        torch.Size([3, 1, 4, 5])
-        >>> print(td_unqueeze[0].shape)
-        torch.Size([1])
-        >>> print(td_unqueeze.view(-1).shape)
-        torch.Size([3])
-        >>> print((td.clone()==td).all())
-        True
-
-    """
-
-    _td_dim_names = None
-    _is_shared = False
-    _is_memmap = False
-    _has_exclusive_keys = False
-
-    def __init__(
-        self,
-        source: T | dict[str, CompatibleType] = None,
-        batch_size: Sequence[int] | torch.Size | int | None = None,
-        device: DeviceType | None = None,
-        names: Sequence[str] | None = None,
-        non_blocking: bool = None,
-        lock: bool = False,
-        _run_checks: bool = True,
-    ) -> None:
-        has_device = False
-        sub_non_blocking = False
-        if device is not None:
-            has_device = True
-            if non_blocking is None:
-                sub_non_blocking = True
-                non_blocking = False
-            else:
-                sub_non_blocking = non_blocking
-            device = torch.device(device)
-            if _has_mps:
-                # With MPS, an explicit sync is required
-                sub_non_blocking = True
-        self._device = device
-
-        self._tensordict = _tensordict = _StringOnlyDict()
-        if not _run_checks:
-            self._batch_size = batch_size
-            if source:  # faster than calling items
-                for key, value in source.items():
-                    if isinstance(value, dict):
-                        value = TensorDict(
-                            value,
-                            batch_size=self._batch_size,
-                            device=self._device,
-                            _run_checks=_run_checks,
-                            non_blocking=sub_non_blocking,
-                        )
-                    _tensordict[key] = value
-            self._td_dim_names = names
-        else:
-            if source is None:
-                source = {}
-            if not isinstance(source, (TensorDictBase, dict)):
-                raise ValueError(
-                    "A TensorDict source is expected to be a TensorDictBase "
-                    f"sub-type or a dictionary, found type(source)={type(source)}."
-                )
-            self._batch_size = self._parse_batch_size(source, batch_size)
-            self.names = names
-
-            for key, value in source.items():
-                self.set(key, value, non_blocking=sub_non_blocking)
-        if not non_blocking and sub_non_blocking and has_device:
-            self._sync_all()
-        if lock:
-            self.lock_()
-
-    @classmethod
-    def from_module(
-        cls,
-        module: torch.nn.Module,
-        as_module: bool = False,
-        lock: bool = False,
-        use_state_dict: bool = False,
-    ):
-        result = cls._from_module(
-            module=module, as_module=as_module, use_state_dict=use_state_dict
-        )
-        if lock:
-            result.lock_()
-        return result
-
-    @classmethod
-    def _from_module(
-        cls,
-        module: torch.nn.Module,
-        as_module: bool = False,
-        use_state_dict: bool = False,
-        prefix="",
-    ):
-        from tensordict.nn import TensorDictParams
-
-        if isinstance(module, TensorDictParams):
-            return module
-        destination = {}
-        if use_state_dict:
-            keep_vars = False
-            # do we need this feature atm?
-            local_metadata = {}
-            # if hasattr(destination, "_metadata"):
-            #     destination._metadata[prefix[:-1]] = local_metadata
-            for hook in module._state_dict_pre_hooks.values():
-                hook(module, prefix, keep_vars)
-            module._save_to_state_dict(destination, "", keep_vars)
-        else:
-            for name, param in module._parameters.items():
-                if param is None:
-                    continue
-                destination[name] = param
-            for name, buffer in module._buffers.items():
-                if buffer is None:
-                    continue
-                destination[name] = buffer
-
-        if use_state_dict:
-            for hook in module._state_dict_hooks.values():
-                hook_result = hook(module, destination, prefix, local_metadata)
-                if hook_result is not None:
-                    destination = hook_result
-        destination = TensorDict(destination, batch_size=[])
-        for name, submodule in module._modules.items():
-            if submodule is not None:
-                subtd = cls._from_module(
-                    module=submodule,
-                    as_module=False,
-                    use_state_dict=use_state_dict,
-                    prefix=prefix + name + ".",
-                )
-                destination._set_str(
-                    name, subtd, validated=True, inplace=False, non_blocking=False
-                )
-
-        if as_module:
-            from tensordict.nn.params import TensorDictParams
-
-            return TensorDictParams(destination, no_convert=True)
-        return destination
-
-    def is_empty(self):
-
-        for item in self._tensordict.values():
-            # we need to check if item is empty
-            if _is_tensor_collection(type(item)):
-                if not item.is_empty():
-                    return False
-
-                if is_non_tensor(item):
-                    return False
-            else:
-                return False
-        return True
-
-    def _to_module(
-        self,
-        module,
-        *,
-        inplace: bool | None = None,
-        return_swap: bool = True,
-        swap_dest=None,
-        memo=None,
-        use_state_dict: bool = False,
-        non_blocking: bool = False,
-    ):
-
-        if not use_state_dict and isinstance(module, TensorDictBase):
-            if return_swap:
-                swap = module.copy()
-                module._param_td = getattr(self, "_param_td", self)
-                return swap
-            else:
-                module.update(self)
-                return
-
-        # we use __dict__ directly to avoid the getattr/setattr overhead whenever we can
-        __dict__ = module.__dict__
-
-        hooks = memo["hooks"]
-        if return_swap:
-            _swap = {}
-            memo[id(module)] = _swap
-
-        if use_state_dict:
-            if inplace is not None:
-                raise RuntimeError(
-                    "inplace argument cannot be passed when use_state_dict=True."
-                )
-            # execute module's pre-hooks
-            state_dict = self.flatten_keys(".")
-            prefix = ""
-            strict = True
-            local_metadata = {}
-            missing_keys = []
-            unexpected_keys = []
-            error_msgs = []
-            for hook in module._load_state_dict_pre_hooks.values():
-                hook(
-                    state_dict,
-                    prefix,
-                    local_metadata,
-                    strict,
-                    missing_keys,
-                    unexpected_keys,
-                    error_msgs,
-                )
-
-            def convert_type(x, y):
-                if isinstance(y, torch.nn.Parameter):
-                    return torch.nn.Parameter(x)
-                if isinstance(y, Buffer):
-                    return Buffer(x)
-                return x
-
-            input = state_dict.unflatten_keys(".")._fast_apply(
-                convert_type, self, propagate_lock=True
-            )
-        else:
-            input = self
-            inplace = bool(inplace)
-
-        for key, value in input.items():
-            if isinstance(value, (Tensor, ftdim.Tensor)):
-                if module.__class__.__setattr__ is __base__setattr__:
-                    # if setattr is the native nn.Module.setattr, we can rely on _set_tensor_dict
-                    local_out = _set_tensor_dict(
-                        __dict__, hooks, module, key, value, inplace
-                    )
-                else:
-                    if return_swap:
-                        local_out = getattr(module, key)
-                    if not inplace:
-                        # use specialized __setattr__ if needed
-                        delattr(module, key)
-                        setattr(module, key, value)
-                    else:
-                        new_val = local_out
-                        if return_swap:
-                            local_out = local_out.clone()
-                        new_val.data.copy_(value.data, non_blocking=non_blocking)
-            else:
-                if value.is_empty():
-                    # if there is at least one key, we must populate the module.
-                    # Otherwise, we just go to the next key
-                    continue
-                child = __dict__["_modules"][key]
-                local_out = memo.get(id(child), NO_DEFAULT)
-
-                if local_out is NO_DEFAULT:
-                    # if isinstance(child, TensorDictBase):
-                    #     # then child is a TensorDictParams
-                    #     from tensordict.nn import TensorDictParams
-                    #
-                    #     local_out = child
-                    #     if not isinstance(value, TensorDictParams):
-                    #         value = TensorDictParams(value, no_convert=True)
-                    #     __dict__["_modules"][key] = value
-                    # else:
-                    local_out = value._to_module(
-                        child,
-                        inplace=inplace,
-                        return_swap=return_swap,
-                        swap_dest={},  # we'll be calling update later
-                        memo=memo,
-                        use_state_dict=use_state_dict,
-                        non_blocking=non_blocking,
-                    )
-
-            if return_swap:
-                _swap[key] = local_out
-        if return_swap:
-            if isinstance(swap_dest, dict):
-                return _swap
-            elif swap_dest is not None:
-
-                def _quick_set(swap_dict, swap_td):
-                    for key, val in swap_dict.items():
-                        if isinstance(val, dict):
-                            _quick_set(val, swap_td._get_str(key, default=NO_DEFAULT))
-                        elif swap_td._get_str(key, None) is not val:
-                            swap_td._set_str(
-                                key,
-                                val,
-                                inplace=False,
-                                validated=True,
-                                non_blocking=non_blocking,
-                            )
-
-                _quick_set(_swap, swap_dest)
-                return swap_dest
-            else:
-                return TensorDict(_swap, batch_size=[], _run_checks=False)
-
-    def __ne__(self, other: object) -> T | bool:
-        if _is_tensorclass(other):
-            return other != self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                raise KeyError(
-                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
-                )
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 != other.get(key)
-            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value != other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return True
-
-    def __xor__(self, other: object) -> T | bool:
-        if _is_tensorclass(other):
-            return other ^ self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                raise KeyError(
-                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
-                )
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 ^ other.get(key)
-            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value ^ other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return True
-
-    def __or__(self, other: object) -> T | bool:
-        if _is_tensorclass(other):
-            return other | self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                raise KeyError(
-                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
-                )
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 | other.get(key)
-            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value | other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __eq__(self, other: object) -> T | bool:
-        if is_tensorclass(other):
-            return other == self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                keys1 = sorted(
-                    keys1,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                keys2 = sorted(
-                    keys2,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 == other.get(key)
-            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value == other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __ge__(self, other: object) -> T | bool:
-        if is_tensorclass(other):
-            return other <= self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                keys1 = sorted(
-                    keys1,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                keys2 = sorted(
-                    keys2,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 >= other.get(key)
-            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value >= other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __gt__(self, other: object) -> T | bool:
-        if is_tensorclass(other):
-            return other < self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                keys1 = sorted(
-                    keys1,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                keys2 = sorted(
-                    keys2,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 > other.get(key)
-            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value > other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __le__(self, other: object) -> T | bool:
-        if is_tensorclass(other):
-            return other >= self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                keys1 = sorted(
-                    keys1,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                keys2 = sorted(
-                    keys2,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 <= other.get(key)
-            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value <= other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __lt__(self, other: object) -> T | bool:
-        if is_tensorclass(other):
-            return other > self
-        if isinstance(other, (dict,)):
-            other = self.from_dict_instance(other)
-        if _is_tensor_collection(other.__class__):
-            keys1 = set(self.keys())
-            keys2 = set(other.keys())
-            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
-                keys1 = sorted(
-                    keys1,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                keys2 = sorted(
-                    keys2,
-                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
-                )
-                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
-            d = {}
-            for key, item1 in self.items():
-                d[key] = item1 < other.get(key)
-            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
-        if isinstance(other, (numbers.Number, Tensor)):
-            return TensorDict(
-                {key: value < other for key, value in self.items()},
-                self.batch_size,
-                device=self.device,
-            )
-        return False
-
-    def __setitem__(
-        self,
-        index: IndexType,
-        value: T | dict | numbers.Number | CompatibleType,
-    ) -> None:
-        istuple = isinstance(index, tuple)
-        if istuple or isinstance(index, str):
-            # try:
-            index_unravel = _unravel_key_to_tuple(index)
-            if index_unravel:
-                self._set_tuple(
-                    index_unravel,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE
-                    if isinstance(self, _SubTensorDict)
-                    else False,
-                    validated=False,
-                    non_blocking=False,
-                )
-                return
-
-        # we must use any and because using Ellipsis in index can break with some indices
-        if index is Ellipsis or (
-            isinstance(index, tuple) and any(idx is Ellipsis for idx in index)
-        ):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-
-        if isinstance(value, (TensorDictBase, dict)):
-            indexed_bs = _getitem_batch_size(self.batch_size, index)
-            if isinstance(value, dict):
-                value = self.from_dict_instance(value, batch_size=indexed_bs)
-                # value = self.empty(recurse=True)[index].update(value)
-            if value.batch_size != indexed_bs:
-                if value.shape == indexed_bs[-len(value.shape) :]:
-                    # try to expand on the left (broadcasting)
-                    value = value.expand(indexed_bs)
-                else:
-                    try:
-                        # copy and change batch_size if can't be expanded
-                        value = value.copy()
-                        value.batch_size = indexed_bs
-                    except RuntimeError as err:
-                        raise RuntimeError(
-                            f"indexed destination TensorDict batch size is {indexed_bs} "
-                            f"(batch_size = {self.batch_size}, index={index}), "
-                            f"which differs from the source batch size {value.batch_size}"
-                        ) from err
-
-            keys = set(self.keys())
-            if any(key not in keys for key in value.keys()):
-                subtd = self._get_sub_tensordict(index)
-            for key, item in value.items():
-                if key in keys:
-                    self._set_at_str(
-                        key, item, index, validated=False, non_blocking=False
-                    )
-                else:
-                    subtd.set(key, item, inplace=True, non_blocking=False)
-        else:
-            for key in self.keys():
-                self.set_at_(key, value, index)
-
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            if dim < 0:
-                dim = self.batch_dims + dim
-
-            names = None
-            if self._has_names():
-                names = copy(self.names)
-                names = [name for i, name in enumerate(names) if i != dim]
-
-            return TensorDict(
-                source={key: value.all(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-                names=names,
-            )
-        return all(value.all() for value in self.values())
-
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
-            raise RuntimeError(
-                "dim must be greater than or equal to -tensordict.batch_dims and "
-                "smaller than tensordict.batch_dims"
-            )
-        if dim is not None:
-            if dim < 0:
-                dim = self.batch_dims + dim
-
-            names = None
-            if self._has_names():
-                names = copy(self.names)
-                names = [name for i, name in enumerate(names) if i != dim]
-
-            return TensorDict(
-                source={key: value.any(dim=dim) for key, value in self.items()},
-                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
-                device=self.device,
-                names=names,
-            )
-        return any([value.any() for value in self.values()])
-
-    def _cast_reduction(
-        self,
-        *,
-        reduction_name,
-        dim=NO_DEFAULT,
-        keepdim=NO_DEFAULT,
-        tuple_ok=True,
-        **kwargs,
-    ):
-        def proc_dim(dim, tuple_ok=True):
-            if dim is None:
-                return dim
-            if isinstance(dim, tuple):
-                if tuple_ok:
-                    return tuple(_d for d in dim for _d in proc_dim(d, tuple_ok=False))
-                return dim
-            if dim >= self.batch_dims or dim < -self.batch_dims:
-                raise RuntimeError(
-                    "dim must be greater than or equal to -tensordict.batch_dims and "
-                    "smaller than tensordict.batch_dims"
-                )
-            if dim < 0:
-                return (self.batch_dims + dim,)
-            return (dim,)
-
-        if dim is not NO_DEFAULT:
-            dim = proc_dim(dim, tuple_ok=tuple_ok)
-            if not tuple_ok:
-                dim = dim[0]
-        if dim is not NO_DEFAULT or keepdim:
-            names = None
-            if self._has_names():
-                names = copy(self.names)
-                if not keepdim and isinstance(dim, tuple):
-                    names = [name for i, name in enumerate(names) if i not in dim]
-                else:
-                    names = [name for i, name in enumerate(names) if i != dim]
-            if dim is not NO_DEFAULT:
-                kwargs["dim"] = dim
-            if keepdim is not NO_DEFAULT:
-                kwargs["keepdim"] = keepdim
-
-            def reduction(val):
-                result = getattr(val, reduction_name)(
-                    **kwargs,
-                )
-                return result
-
-            if dim not in (None, NO_DEFAULT):
-                if not keepdim:
-                    if isinstance(dim, tuple):
-                        batch_size = [
-                            b for i, b in enumerate(self.batch_size) if i not in dim
-                        ]
-                    else:
-                        batch_size = [
-                            b for i, b in enumerate(self.batch_size) if i != dim
-                        ]
-                else:
-                    if isinstance(dim, tuple):
-                        batch_size = [
-                            b if i not in dim else 1
-                            for i, b in enumerate(self.batch_size)
-                        ]
-                    else:
-                        batch_size = [
-                            b if i != dim else 1 for i, b in enumerate(self.batch_size)
-                        ]
-
-            else:
-                batch_size = [1 for b in self.batch_size]
-
-            return self._fast_apply(
-                reduction,
-                call_on_nested=True,
-                batch_size=torch.Size(batch_size),
-                device=self.device,
-                names=names,
-            )
-
-        def reduction(val):
-            return getattr(val, reduction_name)(**kwargs)
-
-        return self._fast_apply(
-            reduction,
-            call_on_nested=True,
-            batch_size=torch.Size([]),
-            device=self.device,
-            names=None,
-        )
-
-    def _apply_nest(
-        self,
-        fn: Callable,
-        *others: T,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        checked: bool = False,
-        call_on_nested: bool = False,
-        default: Any = NO_DEFAULT,
-        named: bool = False,
-        nested_keys: bool = False,
-        prefix: tuple = (),
-        filter_empty: bool | None = None,
-        is_leaf: Callable = None,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        if inplace:
-            result = self
-            is_locked = result.is_locked
-        elif out is not None:
-            result = out
-            if out.is_locked:
-                raise RuntimeError(_LOCK_ERROR)
-            is_locked = False
-            if batch_size is not None and batch_size != out.batch_size:
-                raise RuntimeError(
-                    "batch_size and out.batch_size must be equal when both are provided."
-                )
-            if device is not NO_DEFAULT and device != out.device:
-                raise RuntimeError(
-                    "device and out.device must be equal when both are provided."
-                )
-        else:
-
-            def make_result(names=names, batch_size=batch_size):
-                if batch_size is not None and names is None:
-                    # erase names
-                    names = [None] * len(batch_size)
-                return self.empty(batch_size=batch_size, device=device, names=names)
-
-            result = None
-            is_locked = False
-
-        any_set = False
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-
-        for key, item in self.items():
-            if (
-                not call_on_nested
-                and not is_leaf(item.__class__)
-                # and not is_non_tensor(item)
-            ):
-                if default is not NO_DEFAULT:
-                    _others = [_other._get_str(key, default=None) for _other in others]
-                    _others = [
-                        self.empty(recurse=True) if _other is None else _other
-                        for _other in _others
-                    ]
-                else:
-                    _others = [
-                        _other._get_str(key, default=NO_DEFAULT) for _other in others
-                    ]
-
-                item_trsf = item._apply_nest(
-                    fn,
-                    *_others,
-                    inplace=inplace,
-                    batch_size=batch_size,
-                    device=device,
-                    checked=checked,
-                    named=named,
-                    nested_keys=nested_keys,
-                    default=default,
-                    prefix=prefix + (key,),
-                    filter_empty=filter_empty,
-                    is_leaf=is_leaf,
-                    out=out._get_str(key, default=None) if out is not None else None,
-                    **constructor_kwargs,
-                )
-            else:
-                _others = [_other._get_str(key, default=default) for _other in others]
-                if named:
-                    if nested_keys:
-                        item_trsf = fn(
-                            prefix + (key,) if prefix != () else key, item, *_others
-                        )
-                    else:
-                        item_trsf = fn(key, item, *_others)
-                else:
-                    item_trsf = fn(item, *_others)
-            if item_trsf is not None:
-                if not any_set:
-                    if result is None:
-                        result = make_result()
-                    any_set = True
-                if isinstance(self, _SubTensorDict):
-                    result.set(key, item_trsf, inplace=inplace)
-                else:
-                    result._set_str(
-                        key,
-                        item_trsf,
-                        inplace=BEST_ATTEMPT_INPLACE if inplace else False,
-                        validated=checked,
-                        non_blocking=False,
-                    )
-
-        if filter_empty and not any_set:
-            return
-        elif filter_empty is None and not any_set and not self.is_empty():
-            # we raise the deprecation warning only if the tensordict wasn't already empty.
-            # After we introduce the new behaviour, we will have to consider what happens
-            # to empty tensordicts by default: will they disappear or stay?
-            warn(
-                "Your resulting tensordict has no leaves but you did not specify filter_empty=False. "
-                "Currently, this returns an empty tree (filter_empty=True), but from v0.5 it will return "
-                "a None unless filter_empty=False. "
-                "To silence this warning, set filter_empty to the desired value in your call to `apply`.",
-                category=DeprecationWarning,
-            )
-        if result is None:
-            result = make_result()
-
-        if not inplace and is_locked:
-            result.lock_()
-        return result
-
-    # Functorch compatibility
-    @cache  # noqa: B019
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        td = self
-
-        def _add_batch_dim_wrapper(key, value):
-            if is_tensor_collection(value):
-                return value._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
-
-            if isinstance(
-                value, (_BatchedUninitializedParameter, _BatchedUninitializedBuffer)
-            ):
-                value.in_dim = in_dim
-                value.vmap_level = vmap_level
-                return value
-            return _add_batch_dim(value, in_dim, vmap_level)
-
-        out = TensorDict(
-            {key: _add_batch_dim_wrapper(key, value) for key, value in td.items()},
-            batch_size=torch.Size(
-                [b for i, b in enumerate(td.batch_size) if i != in_dim]
-            ),
-            names=[name for i, name in enumerate(td.names) if i != in_dim],
-            _run_checks=False,
-            lock=self.is_locked,
-        )
-        return out
-
-    @cache  # noqa: B019
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        new_batch_size = list(self.batch_size)
-        new_batch_size.insert(out_dim, batch_size)
-        new_names = list(self.names)
-        new_names.insert(out_dim, None)
-        out = TensorDict(
-            {
-                key: value._remove_batch_dim(
-                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                )
-                if is_tensor_collection(value)
-                else _remove_batch_dim(value, vmap_level, batch_size, out_dim)
-                for key, value in self.items()
-            },
-            batch_size=new_batch_size,
-            names=new_names,
-            lock=self.is_locked,
-        )
-        return out
-
-    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> T:
-        return TensorDict(
-            dict_value,
-            batch_size=self.batch_size,
-            device=self.device,
-        )
-
-    def _index_tensordict(
-        self,
-        index: IndexType,
-        new_batch_size: torch.Size | None = None,
-        names: List[str] | None = None,
-    ) -> T:
-        batch_size = self.batch_size
-        batch_dims = len(batch_size)
-        if (
-            not batch_size
-            and index is not None
-            and (not isinstance(index, tuple) or any(idx is not None for idx in index))
-        ):
-            raise RuntimeError(
-                f"indexing a tensordict with td.batch_dims==0 is not permitted. Got index {index}."
-            )
-        if new_batch_size is not None:
-            batch_size = new_batch_size
-        else:
-            batch_size = _getitem_batch_size(batch_size, index)
-
-        if names is None:
-            names = self._get_names_idx(index)
-
-        source = {}
-        for key, item in self.items():
-            if isinstance(item, TensorDict):
-                # this is the simplest case, we can pre-compute the batch size easily
-                new_batch_size = batch_size + item.batch_size[batch_dims:]
-                source[key] = item._index_tensordict(
-                    index, new_batch_size=new_batch_size
-                )
-            else:
-                source[key] = _get_item(item, index)
-        result = TensorDict(
-            source=source,
-            batch_size=batch_size,
-            device=self.device,
-            names=names,
-            _run_checks=False,
-            # lock=self.is_locked,
-        )
-        if self._is_memmap and _index_preserve_data_ptr(index):
-            result._is_memmap = True
-            result.lock_()
-        elif self._is_shared and _index_preserve_data_ptr(index):
-            result._is_shared = True
-            result.lock_()
-        return result
-
-    def expand(self, *args, **kwargs) -> T:
-        tensordict_dims = self.batch_dims
-        shape = _get_shape_from_args(*args, **kwargs)
-
-        # new shape dim check
-        if len(shape) < len(self.shape):
-            raise RuntimeError(
-                f"the number of sizes provided ({len(shape)}) must be greater or equal to the number of "
-                f"dimensions in the TensorDict ({tensordict_dims})"
-            )
-
-        # new shape compatability check
-        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
-            if old_dim != 1 and new_dim != old_dim:
-                raise RuntimeError(
-                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
-                    f"as the original length. target_shape = {shape}, existing_shape = {self.batch_size}"
-                )
-
-        def _expand(tensor):
-            tensor_shape = tensor.shape
-            tensor_dims = len(tensor_shape)
-            last_n_dims = tensor_dims - tensordict_dims
-            if last_n_dims > 0:
-                new_shape = (*shape, *tensor_shape[-last_n_dims:])
-            else:
-                new_shape = shape
-            return tensor.expand(new_shape)
-
-        names = [None] * (len(shape) - tensordict_dims) + self.names
-        return self._fast_apply(
-            _expand,
-            batch_size=shape,
-            call_on_nested=True,
-            names=names,
-            propagate_lock=True,
-        )
-
-    def _unbind(self, dim: int):
-        batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
-        names = None
-        if self._has_names():
-            names = copy(self.names)
-            names = [name for i, name in enumerate(names) if i != dim]
-        device = self.device
-
-        is_shared = self._is_shared
-        is_memmap = self._is_memmap
-
-        def empty():
-            result = TensorDict(
-                {}, batch_size=batch_size, names=names, _run_checks=False, device=device
-            )
-            result._is_shared = is_shared
-            result._is_memmap = is_memmap
-            return result
-
-        tds = tuple(empty() for _ in range(self.batch_size[dim]))
-
-        def unbind(key, val, tds=tds):
-            unbound = (
-                val.unbind(dim)
-                if not isinstance(val, TensorDictBase)
-                # tensorclass is also unbound using plain unbind
-                else val._unbind(dim)
-            )
-            for td, _val in zip(tds, unbound):
-                td._set_str(
-                    key, _val, validated=True, inplace=False, non_blocking=False
-                )
-
-        for key, val in self.items():
-            unbind(key, val)
-        return tds
-
-    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
-        # we must use slices to keep the storage of the tensors
-        WRONG_TYPE = "split(): argument 'split_size' must be int or list of ints"
-        batch_size = self.batch_size
-        batch_sizes = []
-        batch_dims = len(batch_size)
-        if dim < 0:
-            dim = len(batch_size) + dim
-        if dim >= batch_dims or dim < 0:
-            raise IndexError(
-                f"Dimension out of range (expected to be in range of [-{self.batch_dims}, {self.batch_dims - 1}], but got {dim})"
-            )
-        max_size = batch_size[dim]
-        if isinstance(split_size, int):
-            idx0 = 0
-            idx1 = min(max_size, split_size)
-            split_sizes = [slice(idx0, idx1)]
-            batch_sizes.append(
-                torch.Size(
-                    tuple(
-                        d if i != dim else idx1 - idx0 for i, d in enumerate(batch_size)
-                    )
-                )
-            )
-            while idx1 < max_size:
-                idx0 = idx1
-                idx1 = min(max_size, idx1 + split_size)
-                split_sizes.append(slice(idx0, idx1))
-                batch_sizes.append(
-                    torch.Size(
-                        tuple(
-                            d if i != dim else idx1 - idx0
-                            for i, d in enumerate(batch_size)
-                        )
-                    )
-                )
-        elif isinstance(split_size, (list, tuple)):
-            if len(split_size) == 0:
-                raise RuntimeError("Insufficient number of elements in split_size.")
-            try:
-                idx0 = 0
-                idx1 = split_size[0]
-                split_sizes = [slice(idx0, idx1)]
-                batch_sizes.append(
-                    torch.Size(
-                        tuple(
-                            d if i != dim else idx1 - idx0
-                            for i, d in enumerate(batch_size)
-                        )
-                    )
-                )
-                for idx in split_size[1:]:
-                    idx0 = idx1
-                    idx1 = min(max_size, idx1 + idx)
-                    split_sizes.append(slice(idx0, idx1))
-                    batch_sizes.append(
-                        torch.Size(
-                            tuple(
-                                d if i != dim else idx1 - idx0
-                                for i, d in enumerate(batch_size)
-                            )
-                        )
-                    )
-            except TypeError:
-                raise TypeError(WRONG_TYPE)
-
-            if idx1 < batch_size[dim]:
-                raise RuntimeError(
-                    f"Split method expects split_size to sum exactly to {self.batch_size[dim]} (tensor's size at dimension {dim}), but got split_size={split_size}"
-                )
-        else:
-            raise TypeError(WRONG_TYPE)
-        index = (slice(None),) * dim
-        names = self.names
-        return tuple(
-            self._index_tensordict(index + (ss,), new_batch_size=bs, names=names)
-            for ss, bs in zip(split_sizes, batch_sizes)
-        )
-
-    def masked_select(self, mask: Tensor) -> T:
-        d = {}
-        mask_expand = mask
-        while mask_expand.ndimension() > self.batch_dims:
-            mndim = mask_expand.ndimension()
-            mask_expand = mask_expand.squeeze(-1)
-            if mndim == mask_expand.ndimension():  # no more squeeze
-                break
-        for key, value in self.items():
-            d[key] = value[mask_expand]
-        dim = int(mask.sum().item())
-        other_dim = self.shape[mask.ndim :]
-        return TensorDict(
-            device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
-        )
-
-    def _view(
-        self,
-        *args,
-        **kwargs,
-    ) -> T:
-        shape = _get_shape_from_args(*args, **kwargs)
-        if any(dim < 0 for dim in shape):
-            shape = infer_size_impl(shape, self.numel())
-        if torch.Size(shape) == self.shape:
-            return self
-        batch_dims = self.batch_dims
-
-        def _view(tensor):
-            return tensor.view((*shape, *tensor.shape[batch_dims:]))
-
-        result = self._fast_apply(
-            _view, batch_size=shape, call_on_nested=True, propagate_lock=True
-        )
-        self._maybe_set_shared_attributes(result)
-        return result
-
-    def reshape(
-        self,
-        *args,
-        **kwargs,
-    ) -> T:
-        shape = _get_shape_from_args(*args, **kwargs)
-        if any(dim < 0 for dim in shape):
-            shape = infer_size_impl(shape, self.numel())
-            shape = torch.Size(shape)
-        if torch.Size(shape) == self.shape:
-            return self
-        batch_dims = self.batch_dims
-
-        def _reshape(tensor):
-            return tensor.reshape((*shape, *tensor.shape[batch_dims:]))
-
-        return self._fast_apply(
-            _reshape, batch_size=shape, call_on_nested=True, propagate_lock=True
-        )
-
-    def _transpose(self, dim0, dim1):
-        def _transpose(tensor):
-            return tensor.transpose(dim0, dim1)
-
-        batch_size = list(self.batch_size)
-        v0 = batch_size[dim0]
-        v1 = batch_size[dim1]
-        batch_size[dim1] = v0
-        batch_size[dim0] = v1
-        if self._has_names():
-            names = self.names
-            names = [
-                names[dim0] if i == dim1 else names[dim1] if i == dim0 else names[i]
-                for i in range(self.ndim)
-            ]
-        else:
-            names = None
-        result = self._fast_apply(
-            _transpose,
-            batch_size=torch.Size(batch_size),
-            call_on_nested=True,
-            names=names,
-            propagate_lock=True,
-        )
-        self._maybe_set_shared_attributes(result)
-        return result
-
-    def _permute(self, *args, **kwargs):
-        dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
-        dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
-        if any(dim < 0 or dim >= self.ndim for dim in dims_list):
-            raise ValueError(
-                "Received an permutation order incompatible with the tensordict shape."
-            )
-        # note: to allow this to work recursively, we must allow permutation order with fewer elements than dims,
-        # as long as this list is complete.
-        if not np.array_equal(sorted(dims_list), range(len(dims_list))):
-            raise ValueError(
-                f"Cannot compute the permutation, got dims={dims_list} but expected a permutation of {list(range(len(dims_list)))}."
-            )
-        if not len(dims_list) and not self.batch_dims:
-            return self
-        if np.array_equal(dims_list, range(len(dims_list))):
-            return self
-
-        def _permute(tensor):
-            return tensor.permute(*dims_list, *range(len(dims_list), tensor.ndim))
-
-        batch_size = self.batch_size
-        batch_size = [batch_size[p] for p in dims_list] + list(
-            batch_size[len(dims_list) :]
-        )
-        if self._has_names():
-            names = self.names
-            names = [names[i] for i in dims_list]
-        else:
-            names = None
-        result = self._fast_apply(
-            _permute,
-            batch_size=batch_size,
-            call_on_nested=True,
-            names=names,
-            propagate_lock=True,
-        )
-        self._maybe_set_shared_attributes(result)
-        return result
-
-    def _squeeze(self, dim=None):
-        batch_size = self.batch_size
-        if dim is None:
-            names = list(self.names)
-            batch_size, names = zip(
-                *[(size, name) for size, name in zip(batch_size, names) if size != 1]
-            )
-            batch_size = torch.Size(batch_size)
-            if batch_size == self.batch_size:
-                return self
-
-            # we only want to squeeze dimensions lower than the batch dim, and view
-            # is the perfect op for this
-            def _squeeze(tensor):
-                return tensor.view(*batch_size, *tensor.shape[self.batch_dims :])
-
-            return self._fast_apply(
-                _squeeze,
-                batch_size=batch_size,
-                names=names,
-                inplace=False,
-                call_on_nested=True,
-                propagate_lock=True,
-            )
-        # make the dim positive
-        if dim < 0:
-            newdim = self.batch_dims + dim
-        else:
-            newdim = dim
-
-        if (newdim >= self.batch_dims) or (newdim < 0):
-            raise RuntimeError(
-                f"squeezing is allowed for dims comprised between "
-                f"`-td.batch_dims` and `td.batch_dims - 1` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        if batch_size[dim] != 1:
-            return self
-        batch_size = list(batch_size)
-        batch_size.pop(dim)
-        batch_size = list(batch_size)
-        names = list(self.names)
-        names.pop(dim)
-
-        result = self._fast_apply(
-            lambda x: x.squeeze(newdim),
-            batch_size=batch_size,
-            names=names,
-            inplace=False,
-            call_on_nested=True,
-            propagate_lock=True,
-        )
-        self._maybe_set_shared_attributes(result)
-        return result
-
-    def _unsqueeze(self, dim):
-        # make the dim positive
-        if dim < 0:
-            newdim = self.batch_dims + dim + 1
-        else:
-            newdim = dim
-
-        if (newdim > self.batch_dims) or (newdim < 0):
-            raise RuntimeError(
-                f"unsqueezing is allowed for dims comprised between "
-                f"`-td.batch_dims - 1` and `td.batch_dims` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        batch_size = list(self.batch_size)
-        batch_size.insert(newdim, 1)
-        batch_size = torch.Size(batch_size)
-
-        names = copy(self.names)
-        names.insert(newdim, None)
-
-        def _unsqueeze(tensor):
-            return tensor.unsqueeze(newdim)
-
-        result = self._fast_apply(
-            _unsqueeze,
-            batch_size=batch_size,
-            names=names,
-            inplace=False,
-            call_on_nested=True,
-            propagate_lock=True,
-        )
-        self._maybe_set_shared_attributes(result)
-        return result
-
-    @classmethod
-    def from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
-        """Returns a TensorDict created from a dictionary or another :class:`~.tensordict.TensorDict`.
-
-        If ``batch_size`` is not specified, returns the maximum batch size possible.
-
-        This function works on nested dictionaries too, or can be used to determine the
-        batch-size of a nested tensordict.
-
-        Args:
-            input_dict (dictionary, optional): a dictionary to use as a data source
-                (nested keys compatible).
-            batch_size (iterable of int, optional): a batch size for the tensordict.
-            device (torch.device or compatible type, optional): a device for the TensorDict.
-            batch_dims (int, optional): the ``batch_dims`` (ie number of leading dimensions
-                to be considered for ``batch_size``). Exclusinve with ``batch_size``.
-                Note that this is the __maximum__ number of batch dims of the tensordict,
-                a smaller number is tolerated.
-
-        Examples:
-            >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
-            >>> print(TensorDict.from_dict(input_dict))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> # nested dict: the nested TensorDict can have a different batch-size
-            >>> # as long as its leading dims match.
-            >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
-            >>> print(TensorDict.from_dict(input_dict))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 4]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> # we can also use this to work out the batch sie of a tensordict
-            >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
-            >>> print(TensorDict.from_dict(input_td))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 4]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-
-        """
-        if batch_dims is not None and batch_size is not None:
-            raise ValueError(
-                "Cannot pass both batch_size and batch_dims to `from_dict`."
-            )
-
-        batch_size_set = torch.Size(()) if batch_size is None else batch_size
-        input_dict = copy(input_dict)
-        for key, value in list(input_dict.items()):
-            if isinstance(value, (dict,)):
-                # we don't know if another tensor of smaller size is coming
-                # so we can't be sure that the batch-size will still be valid later
-                input_dict[key] = TensorDict.from_dict(
-                    value, batch_size=[], device=device, batch_dims=None
-                )
-        # _run_checks=False breaks because a tensor may have the same batch-size as the tensordict
-        out = cls(
-            input_dict,
-            batch_size=batch_size_set,
-            device=device,
-        )
-        if batch_size is None:
-            _set_max_batch_size(out, batch_dims)
-        else:
-            out.batch_size = batch_size
-        return out
-
-    def from_dict_instance(
-        self, input_dict, batch_size=None, device=None, batch_dims=None
-    ):
-        if batch_dims is not None and batch_size is not None:
-            raise ValueError(
-                "Cannot pass both batch_size and batch_dims to `from_dict`."
-            )
-        from tensordict import TensorDict
-
-        batch_size_set = torch.Size(()) if batch_size is None else batch_size
-        input_dict = copy(input_dict)
-        for key, value in list(input_dict.items()):
-            if isinstance(value, (dict,)):
-                cur_value = self.get(key, None)
-                if cur_value is not None:
-                    input_dict[key] = cur_value.from_dict_instance(
-                        value, batch_size=[], device=device, batch_dims=None
-                    )
-                    continue
-                # we don't know if another tensor of smaller size is coming
-                # so we can't be sure that the batch-size will still be valid later
-                input_dict[key] = TensorDict.from_dict(
-                    value, batch_size=[], device=device, batch_dims=None
-                )
-        out = TensorDict.from_dict(
-            input_dict,
-            batch_size=batch_size_set,
-            device=device,
-        )
-        if batch_size is None:
-            _set_max_batch_size(out, batch_dims)
-        else:
-            out.batch_size = batch_size
-        return out
-
-    @staticmethod
-    def _parse_batch_size(
-        source: T | dict,
-        batch_size: Sequence[int] | torch.Size | int | None = None,
-    ) -> torch.Size:
-        try:
-            return torch.Size(batch_size)
-        except Exception:
-            if batch_size is None:
-                return torch.Size([])
-            elif isinstance(batch_size, Number):
-                return torch.Size([batch_size])
-            elif isinstance(source, TensorDictBase):
-                return source.batch_size
-            raise ValueError(
-                "batch size was not specified when creating the TensorDict "
-                "instance and it could not be retrieved from source."
-            )
-
-    @property
-    def batch_dims(self) -> int:
-        return len(self.batch_size)
-
-    @batch_dims.setter
-    def batch_dims(self, value: int) -> None:
-        raise RuntimeError(
-            f"Setting batch dims on {self.__class__.__name__} instances is "
-            f"not allowed."
-        )
-
-    def _has_names(self):
-        return self._td_dim_names is not None
-
-    def _erase_names(self):
-        self._td_dim_names = None
-
-    @property
-    def names(self):
-        names = self._td_dim_names
-        if names is None:
-            return [None for _ in range(self.batch_dims)]
-        return names
-
-    def _get_names_idx(self, idx):
-        if not self._has_names():
-            names = None
-        else:
-
-            def is_boolean(idx):
-                try:
-                    from functorch import dim as ftdim
-
-                except ImportError:
-                    from tensordict.utils import _ftdim_mock as ftdim
-
-                if isinstance(idx, ftdim.Dim):
-                    return None
-                if isinstance(idx, tuple) and len(idx) == 1:
-                    return is_boolean(idx[0])
-                if hasattr(idx, "dtype") and idx.dtype is torch.bool:
-                    return idx.ndim
-                return None
-
-            num_boolean_dim = is_boolean(idx)
-            names = self.names
-            if num_boolean_dim:
-                names = [None] + names[num_boolean_dim:]
-            else:
-                if not isinstance(idx, tuple):
-                    idx = (idx,)
-                if len([_idx for _idx in idx if _idx is not None]) < self.ndim:
-                    idx = (*idx, Ellipsis)
-                idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
-                # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
-                count = 0
-                idx_to_take = []
-                no_more_tensors = False
-                for _idx in idx_names:
-                    if _idx is None:
-                        idx_to_take.append(None)
-                    elif _is_number(_idx):
-                        count += 1
-                    elif isinstance(_idx, (torch.Tensor, np.ndarray)):
-                        if not no_more_tensors:
-                            idx_to_take.extend([count] * _idx.ndim)
-                            count += 1
-                            no_more_tensors = True
-                        else:
-                            # skip this one
-                            count += 1
-                    else:
-                        idx_to_take.append(count)
-                        count += 1
-                names = [names[i] if i is not None else None for i in idx_to_take]
-        return names
-
-    @names.setter
-    def names(self, value):
-        # we don't run checks on types for efficiency purposes
-        if value is None:
-            self._rename_subtds(value)
-            self._erase_names()
-            return
-        value = list(value)
-        num_none = sum(v is None for v in value)
-        if num_none:
-            num_none -= 1
-        if len(set(value)) != len(value) - num_none:
-            raise ValueError(f"Some dimension names are non-unique: {value}.")
-        if len(value) != self.batch_dims:
-            raise ValueError(
-                "the length of the dimension names must equate the tensordict batch_dims attribute. "
-                f"Got {value} for batch_dims {self.batch_dims}."
-            )
-        self._rename_subtds(value)
-        self._td_dim_names = list(value)
-
-    def _rename_subtds(self, names):
-        if names is None:
-            for item in self._tensordict.values():
-                if _is_tensor_collection(type(item)):
-                    item._erase_names()
-            return
-        for item in self._tensordict.values():
-            if _is_tensor_collection(item.__class__):
-                item_names = item.names
-                td_names = list(names) + item_names[len(names) :]
-                item.rename_(*td_names)
-
-    @property
-    def device(self) -> torch.device | None:
-        """Device of the tensordict.
-
-        Returns `None` if device hasn't been provided in the constructor or set via `tensordict.to(device)`.
-
-        """
-        return self._device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        raise RuntimeError(
-            "device cannot be set using tensordict.device = device, "
-            "because device cannot be updated in-place. To update device, use "
-            "tensordict.to(new_device), which will return a new tensordict "
-            "on the new device."
-        )
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    # Checks
-    def _check_is_shared(self) -> bool:
-        share_list = [_is_shared(value) for value in self.values()]
-        if any(share_list) and not all(share_list):
-            shared_str = ", ".join(
-                [f"{key}: {_is_shared(value)}" for key, value in self.items()]
-            )
-            raise RuntimeError(
-                f"tensors must be either all shared or not, but mixed "
-                f"features is not allowed. "
-                f"Found: {shared_str}"
-            )
-        return all(share_list) and len(share_list) > 0
-
-    def _check_device(self) -> None:
-        devices = {value.device for value in self.values()}
-        if self.device is not None and len(devices) >= 1 and devices != {self.device}:
-            raise RuntimeError(
-                f"TensorDict.device is {self._device}, but elements have "
-                f"device values {devices}. If TensorDict.device is set then "
-                "all elements must share that device."
-            )
-
-    @lock_blocked
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        return self._tensordict.popitem()
-
-    def pin_memory(self) -> T:
-        def pin_mem(tensor):
-            return tensor.pin_memory()
-
-        return self._fast_apply(pin_mem, propagate_lock=True)
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ) -> T:
-        if inplace is not False:
-            best_attempt = inplace is BEST_ATTEMPT_INPLACE
-            inplace = self._convert_inplace(inplace, key)
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-        if not inplace:
-            if self._is_locked and not ignore_lock:
-                raise RuntimeError(_LOCK_ERROR)
-            self._tensordict[key] = value
-        else:
-            try:
-                dest = self._get_str(key, default=NO_DEFAULT)
-                if best_attempt and _is_tensor_collection(dest.__class__):
-                    dest.update(value, inplace=True, non_blocking=non_blocking)
-                else:
-                    if dest is not value:
-                        try:
-                            dest.copy_(value, non_blocking=non_blocking)
-                        except RuntimeError:
-                            # if we're updating a param and the storages match, nothing needs to be done
-                            if not (
-                                isinstance(dest, torch.Tensor)
-                                and dest.data.untyped_storage().data_ptr()
-                                == value.data.untyped_storage().data_ptr()
-                            ):
-                                raise
-            except KeyError as err:
-                raise err
-            except Exception as err:
-                raise ValueError(
-                    f"Failed to update '{key}' in tensordict {self}"
-                ) from err
-        return self
-
-    def _set_dict(
-        self,
-        d: dict[str, CompatibleType],
-        *,
-        validated: bool,
-    ):
-        if not validated:
-            raise RuntimeError("Not Implemented for non-validated inputs")
-        self._tensordict = d
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        non_blocking: bool = False,
-    ) -> T:
-        if len(key) == 1:
-            return self._set_str(
-                key[0],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        td = self._get_str(key[0], None)
-        if td is None:
-            td = self._create_nested_str(key[0])
-            inplace = False
-        elif not _is_tensor_collection(td.__class__):
-            raise KeyError(
-                f"The entry {key[0]} is already present in tensordict {self}."
-            )
-        td._set_tuple(
-            key[1:],
-            value,
-            inplace=inplace,
-            validated=validated,
-            non_blocking=non_blocking,
-        )
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        tensor_in = self._get_str(key, NO_DEFAULT)
-
-        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-            warn(
-                "Multiple indexing can lead to unexpected behaviours when "
-                "setting items, for instance `td[idx1][idx2] = other` may "
-                "not write to the desired location if idx1 is a list/tensor."
-            )
-            tensor_in = _sub_index(tensor_in, idx)
-            tensor_in.copy_(value, non_blocking=non_blocking)
-        else:
-            tensor_out = _set_item(
-                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
-            )
-            if tensor_in is not tensor_out:
-                if self._is_shared or self._is_memmap:
-                    raise RuntimeError(
-                        "You're attempting to update a leaf in-place with a shared "
-                        "tensordict, but the new value does not match the previous. "
-                        "If you're using NonTensorData, see the class documentation "
-                        "to see how to properly pre-allocate memory in shared contexts."
-                    )
-                # this happens only when a NonTensorData becomes a NonTensorStack
-                # so it is legitimate (there is no in-place modification of a tensor
-                # that was expected to happen but didn't).
-                # For this reason we can ignore the locked attribute of the td.
-                self._set_str(
-                    key,
-                    tensor_out,
-                    validated=True,
-                    inplace=False,
-                    ignore_lock=True,
-                    non_blocking=non_blocking,
-                )
-
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
-        if len(key) == 1:
-            return self._set_at_str(
-                key[0], value, idx, validated=validated, non_blocking=non_blocking
-            )
-        if key[0] not in self.keys():
-            # this won't work
-            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
-        else:
-            td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(
-            key[1:], value, idx, validated=validated, non_blocking=non_blocking
-        )
-        return self
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> T:
-        key = _unravel_key_to_tuple(key)
-        if len(key) > 1:
-            td, subkey = _get_leaf_tensordict(self, key)
-            td.del_(subkey)
-            return self
-
-        del self._tensordict[key[0]]
-        return self
-
-    @lock_blocked
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> T:
-        # these checks are not perfect, tuples that are not tuples of strings or empty
-        # tuples could go through but (1) it will raise an error anyway and (2)
-        # those checks are expensive when repeated often.
-        if old_key == new_key:
-            return self
-        if not isinstance(old_key, (str, tuple)):
-            raise TypeError(
-                f"Expected old_name to be a string or a tuple of strings but found {type(old_key)}"
-            )
-        if not isinstance(new_key, (str, tuple)):
-            raise TypeError(
-                f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
-            )
-        if safe and (new_key in self.keys(include_nested=True)):
-            raise KeyError(f"key {new_key} already present in TensorDict.")
-
-        if isinstance(new_key, str):
-            self._set_str(
-                new_key,
-                self.get(old_key),
-                inplace=False,
-                validated=True,
-                non_blocking=False,
-            )
-        else:
-            self._set_tuple(
-                new_key,
-                self.get(old_key),
-                inplace=False,
-                validated=True,
-                non_blocking=False,
-            )
-        self.del_(old_key)
-        return self
-
-    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> TensorDict:
-        # if not isinstance(key, str):
-        #     raise ValueError("_stack_onto_ expects string keys.")
-        for key in self.keys():
-            vals = [item._get_str(key, None) for item in list_item]
-            if all(v is None for v in vals):
-                continue
-            dest = self._get_str(key, NO_DEFAULT)
-            torch.stack(
-                vals,
-                dim=dim,
-                out=dest,
-            )
-        return self
-
-    def entry_class(self, key: NestedKey) -> type:
-        return type(self.get(key))
-
-    def _stack_onto_at_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-        idx: IndexType,
-    ) -> TensorDict:
-        if not isinstance(idx, tuple):
-            idx = (idx,)
-        idx = convert_ellipsis_to_idx(idx, self.batch_size)
-        for key in self.keys():
-            vals = [td._get_str(key, NO_DEFAULT) for td in list_item]
-            if all(v is None for v in vals):
-                continue
-            v = self._get_str(key, NO_DEFAULT)
-            v_idx = v[idx]
-            if v.data_ptr() != v_idx.data_ptr():
-                raise IndexError(
-                    f"Index {idx} is incompatible with stack(..., out=data) as the storages of the indexed tensors differ."
-                )
-            torch.stack(vals, dim=dim, out=v_idx)
-            # raise ValueError(
-            #     f"Cannot stack onto an indexed tensor with index {idx} "
-            #     f"as its storage differs."
-            # )
-        return self
-
-    def _get_str(self, key, default):
-        first_key = key
-        out = self._tensordict.get(first_key, None)
-        if out is None:
-            return self._default_get(first_key, default)
-        return out
-
-    def _get_tuple(self, key, default):
-        first = self._get_str(key[0], default)
-        if len(key) == 1 or first is default:
-            return first
-        try:
-            return first._get_tuple(key[1:], default=default)
-        except AttributeError as err:
-            if "has no attribute" in str(err):
-                raise ValueError(
-                    f"Expected a TensorDictBase instance but got {type(first)} instead"
-                    f" for key '{key[1:]}' in tensordict:\n{self}."
-                )
-
-    def share_memory_(self) -> T:
-        if self.is_memmap():
-            raise RuntimeError(
-                "memmap and shared memory are mutually exclusive features."
-            )
-        if self.device is not None and self.device.type == "cuda":
-            # cuda tensors are shared by default
-            return self
-        for value in self.values():
-            if (
-                isinstance(value, Tensor)
-                and value.device.type == "cpu"
-                or _is_tensor_collection(value.__class__)
-            ):
-                value.share_memory_()
-        self._is_shared = True
-        self.lock_()
-        return self
-
-    def detach_(self) -> T:
-        for value in self.values():
-            value.detach_()
-        return self
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None,
-        copy_existing: bool,
-        executor,
-        futures,
-        inplace,
-        like,
-        share_non_tensor,
-    ) -> T:
-
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            metadata = {}
-        if inplace and self._is_shared:
-            raise RuntimeError(
-                "memmap and shared memory are mutually exclusive features."
-            )
-        dest = (
-            self
-            if inplace
-            else TensorDict(
-                {},
-                batch_size=self.batch_size,
-                names=self.names if self._has_names() else None,
-                device=torch.device("cpu"),
-            )
-        )
-
-        # We must set these attributes before memmapping because we need the metadata
-        # to match the tensordict content.
-        if inplace:
-            self._is_memmap = True
-            self._is_shared = False  # since they are mutually exclusive
-            self._device = torch.device("cpu")
-        else:
-            dest._is_memmap = True
-            dest._is_shared = False  # since they are mutually exclusive
-
-        for key, value in self.items():
-            type_value = type(value)
-            if _is_tensor_collection(type_value):
-                dest._tensordict[key] = value._memmap_(
-                    prefix=prefix / key if prefix is not None else None,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=inplace,
-                    like=like,
-                    share_non_tensor=share_non_tensor,
-                )
-                if prefix is not None:
-                    _update_metadata(
-                        metadata=metadata, key=key, value=value, is_collection=True
-                    )
-                continue
-            else:
-
-                if executor is None:
-                    _populate_memmap(
-                        dest=dest,
-                        value=value,
-                        key=key,
-                        copy_existing=copy_existing,
-                        prefix=prefix,
-                        like=like,
-                    )
-                else:
-                    futures.append(
-                        executor.submit(
-                            _populate_memmap,
-                            dest=dest,
-                            value=value,
-                            key=key,
-                            copy_existing=copy_existing,
-                            prefix=prefix,
-                            like=like,
-                        )
-                    )
-                if prefix is not None:
-                    _update_metadata(
-                        metadata=metadata, key=key, value=value, is_collection=False
-                    )
-
-        if prefix is not None:
-            if executor is None:
-                _save_metadata(
-                    dest,
-                    prefix,
-                    metadata=metadata,
-                )
-            else:
-                futures.append(executor.submit(_save_metadata, dest, prefix, metadata))
-        dest._is_locked = True
-        dest._memmap_prefix = prefix
-        return dest
-
-    @classmethod
-    def _load_memmap(
-        cls,
-        prefix: str,
-        metadata: dict,
-        device: torch.device | None = None,
-        out=None,
-    ) -> T:
-        if metadata["device"] == "None":
-            metadata["device"] = None
-        else:
-            metadata["device"] = torch.device(metadata["device"])
-        metadata["shape"] = torch.Size(metadata["shape"])
-
-        if out is None:
-            result = cls(
-                {},
-                batch_size=metadata.pop("shape"),
-                device=metadata.pop("device") if device is None else device,
-            )
-        else:
-            result = out
-
-        paths = set()
-        for key, entry_metadata in metadata.items():
-            if not isinstance(entry_metadata, dict):
-                # there can be other metadata
-                continue
-            type_value = entry_metadata.get("type", None)
-            if type_value is not None:
-                paths.add(key)
-                continue
-            dtype = entry_metadata.get("dtype", None)
-            shape = entry_metadata.get("shape", None)
-            if (
-                not (prefix / f"{key}.memmap").exists()
-                or dtype is None
-                or shape is None
-            ):
-                # invalid dict means
-                continue
-            if (
-                device is None or device != torch.device("meta")
-            ) and not torch._guards.active_fake_mode():
-                if entry_metadata.get("is_nested", False):
-                    # The shape is the shape of the shape, get the shape from it
-                    shape = MemoryMappedTensor.from_filename(
-                        (prefix / f"{key}.memmap").with_suffix(".shape.memmap"),
-                        shape=shape,
-                        dtype=torch.long,
-                    )
-                else:
-                    shape = torch.Size(shape)
-                tensor = MemoryMappedTensor.from_filename(
-                    dtype=_STRDTYPE2DTYPE[dtype],
-                    shape=shape,
-                    filename=str(prefix / f"{key}.memmap"),
-                )
-                if device is not None:
-                    tensor = tensor.to(device, non_blocking=True)
-            else:
-                tensor = torch.zeros(
-                    torch.Size(shape),
-                    device=device,
-                    dtype=_STRDTYPE2DTYPE[dtype],
-                )
-            result._set_str(
-                key,
-                tensor,
-                validated=True,
-                inplace=False,
-                non_blocking=False,
-            )
-        # iterate over folders and load them
-        for path in prefix.iterdir():
-            if path.is_dir() and path.parts[-1] in paths:
-                key = path.parts[-1]  # path.parts[len(prefix.parts) :]
-                existing_elt = result._get_str(key, default=None)
-                if existing_elt is not None:
-                    existing_elt.load_memmap_(path)
-                else:
-                    result._set_str(
-                        key,
-                        TensorDict.load_memmap(path, device=device, non_blocking=True),
-                        inplace=False,
-                        validated=False,
-                    )
-        result._memmap_prefix = prefix
-        return result
-
-    def _make_memmap_subtd(self, key):
-        """Creates a sub-tensordict given a tuple key."""
-        result = self
-        for key_str in key:
-            result_tmp = result._get_str(key_str, default=None)
-            if result_tmp is None:
-                result_tmp = result.empty()
-                if result._memmap_prefix is not None:
-                    result_tmp.memmap_(prefix=result._memmap_prefix / key_str)
-                    metadata = _load_metadata(result._memmap_prefix)
-                    _update_metadata(
-                        metadata=metadata,
-                        key=key_str,
-                        value=result_tmp,
-                        is_collection=True,
-                    )
-                    _save_metadata(
-                        result, prefix=result._memmap_prefix, metadata=metadata
-                    )
-                result._tensordict[key_str] = result_tmp
-            result = result_tmp
-        return result
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        if not self.is_memmap():
-            raise RuntimeError(
-                "Can only make a memmap tensor within a memory-mapped tensordict."
-            )
-
-        key = unravel_key(key)
-        if isinstance(key, tuple):
-            last_node = self._make_memmap_subtd(key[:-1])
-            last_key = key[-1]
-        else:
-            last_node = self
-            last_key = key
-        if last_key in last_node.keys():
-            raise RuntimeError(
-                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
-                f"overwriting it."
-            )
-        if dtype is None:
-            dtype = torch.get_default_dtype()
-        if last_node._memmap_prefix is not None:
-            metadata = _load_metadata(last_node._memmap_prefix)
-            memmap_tensor = _populate_empty(
-                key=last_key,
-                dest=last_node,
-                prefix=last_node._memmap_prefix,
-                shape=shape,
-                dtype=dtype,
-            )
-            _update_metadata(
-                metadata=metadata,
-                key=last_key,
-                value=memmap_tensor,
-                is_collection=False,
-            )
-            _save_metadata(
-                last_node, prefix=last_node._memmap_prefix, metadata=metadata
-            )
-        else:
-            memmap_tensor = MemoryMappedTensor.empty(shape=shape, dtype=dtype)
-
-        last_node._set_str(
-            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
-        )
-
-        return memmap_tensor
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        if not self.is_memmap():
-            raise RuntimeError(
-                "Can only make a memmap tensor within a memory-mapped tensordict."
-            )
-
-        key = unravel_key(key)
-        if isinstance(key, tuple):
-            last_node = self._make_memmap_subtd(key[:-1])
-            last_key = key[-1]
-        else:
-            last_node = self
-            last_key = key
-        if last_key in last_node.keys():
-            raise RuntimeError(
-                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
-                f"overwriting it."
-            )
-        if dtype is None:
-            dtype = torch.get_default_dtype()
-
-        if last_node._memmap_prefix is not None:
-            metadata = _load_metadata(last_node._memmap_prefix)
-            memmap_tensor = _populate_storage(
-                key=last_key,
-                dest=last_node,
-                prefix=last_node._memmap_prefix,
-                storage=storage,
-                shape=shape,
-                dtype=dtype,
-            )
-            _update_metadata(
-                metadata=metadata,
-                key=last_key,
-                value=memmap_tensor,
-                is_collection=False,
-            )
-            _save_metadata(
-                last_node, prefix=last_node._memmap_prefix, metadata=metadata
-            )
-        else:
-            memmap_tensor = MemoryMappedTensor.from_storage(
-                storage=storage, shape=shape, dtype=dtype
-            )
-
-        last_node._set_str(
-            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
-        )
-
-        return memmap_tensor
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        if not self.is_memmap():
-            raise RuntimeError(
-                "Can only make a memmap tensor within a memory-mapped tensordict."
-            )
-
-        key = unravel_key(key)
-        if isinstance(key, tuple):
-            last_node = self._make_memmap_subtd(key[:-1])
-            last_key = key[-1]
-        else:
-            last_node = self
-            last_key = key
-        if last_key in last_node.keys():
-            raise RuntimeError(
-                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
-                f"overwriting it."
-            )
-
-        if last_node._memmap_prefix is not None:
-            metadata = _load_metadata(last_node._memmap_prefix)
-            memmap_tensor = _populate_memmap(
-                dest=last_node,
-                value=tensor,
-                key=last_key,
-                copy_existing=True,
-                prefix=last_node._memmap_prefix,
-                like=not copy_data,
-            )
-            _update_metadata(
-                metadata=metadata,
-                key=last_key,
-                value=memmap_tensor,
-                is_collection=False,
-            )
-            _save_metadata(
-                last_node, prefix=last_node._memmap_prefix, metadata=metadata
-            )
-        else:
-            memmap_tensor = MemoryMappedTensor.from_tensor(tensor)
-
-        last_node._set_str(
-            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
-        )
-
-        return memmap_tensor
-
-    def to(self, *args, **kwargs: Any) -> T:
-        non_blocking = kwargs.pop("non_blocking", None)
-        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
-        result = self
-
-        if device is not None and dtype is None and device == self.device:
-            return result
-
-        if non_blocking is None:
-            sub_non_blocking = True
-            non_blocking = False
-        else:
-            sub_non_blocking = non_blocking
-
-        if convert_to_format is not None:
-
-            def to(tensor):
-                return tensor.to(
-                    device,
-                    dtype,
-                    non_blocking=sub_non_blocking,
-                    convert_to_format=convert_to_format,
-                )
-
-        else:
-
-            def to(tensor):
-                return tensor.to(
-                    device=device, dtype=dtype, non_blocking=sub_non_blocking
-                )
-
-        apply_kwargs = {}
-        if device is not None or dtype is not None:
-            apply_kwargs["device"] = device if device is not None else self.device
-            apply_kwargs["batch_size"] = batch_size
-            result = result._fast_apply(to, propagate_lock=True, **apply_kwargs)
-        elif batch_size is not None:
-            result.batch_size = batch_size
-        if device is not None and sub_non_blocking and not non_blocking:
-            self._sync_all()
-        return result
-
-    def where(self, condition, other, *, out=None, pad=None):
-        if _is_tensor_collection(other.__class__):
-
-            def func(tensor, _other, key):
-                if tensor is None:
-                    if pad is not None:
-                        tensor = _other
-                        _other = torch.tensor(pad, dtype=_other.dtype)
-                    else:
-                        raise KeyError(
-                            f"Key {key} not found and no pad value provided."
-                        )
-                    cond = expand_as_right(~condition, tensor)
-                elif _other is None:
-                    if pad is not None:
-                        _other = torch.tensor(pad, dtype=tensor.dtype)
-                    else:
-                        raise KeyError(
-                            f"Key {key} not found and no pad value provided."
-                        )
-                    cond = expand_as_right(condition, tensor)
-                else:
-                    cond = expand_as_right(condition, tensor)
-                return torch.where(
-                    condition=cond,
-                    input=tensor,
-                    other=_other,
-                )
-
-            result = self.empty() if out is None else out
-            other_keys = set(other.keys())
-            # we turn into a list because out could be = to self!
-            for key in list(self.keys()):
-                tensor = self._get_str(key, default=NO_DEFAULT)
-                _other = other._get_str(key, default=None)
-                if _is_tensor_collection(type(tensor)):
-                    _out = None if out is None else out._get_str(key, None)
-                    if _other is None:
-                        _other = tensor.empty()
-                    val = tensor.where(
-                        condition=condition, other=_other, out=_out, pad=pad
-                    )
-                else:
-                    val = func(tensor, _other, key)
-                result._set_str(
-                    key, val, inplace=False, validated=True, non_blocking=False
-                )
-                other_keys.discard(key)
-            for key in other_keys:
-                tensor = None
-                _other = other._get_str(key, default=NO_DEFAULT)
-                if _is_tensor_collection(type(_other)):
-                    try:
-                        tensor = _other.empty()
-                    except NotImplementedError:
-                        # H5 tensordicts do not support select()
-                        tensor = _other.to_tensordict().empty()
-                    val = _other.where(
-                        condition=~condition, other=tensor, out=None, pad=pad
-                    )
-                else:
-                    val = func(tensor, _other, key)
-                result._set_str(
-                    key, val, inplace=False, validated=True, non_blocking=False
-                )
-            return result
-        else:
-            if out is None:
-
-                def func(tensor):
-                    return torch.where(
-                        condition=expand_as_right(condition, tensor),
-                        input=tensor,
-                        other=other,
-                    )
-
-                return self._fast_apply(func, propagate_lock=True)
-            else:
-
-                def func(tensor, _out):
-                    return torch.where(
-                        condition=expand_as_right(condition, tensor),
-                        input=tensor,
-                        other=other,
-                        out=_out,
-                    )
-
-                return self._fast_apply(func, out, propagate_lock=True)
-
-    def masked_fill_(self, mask: Tensor, value: float | int | bool) -> T:
-        for item in self.values():
-            mask_expand = expand_as_right(mask, item)
-            item.masked_fill_(mask_expand, value)
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def is_contiguous(self) -> bool:
-        return all([value.is_contiguous() for _, value in self.items()])
-
-    def _clone(self, recurse: bool = True) -> T:
-        result = TensorDict(
-            source={key: _clone_value(value, recurse) for key, value in self.items()},
-            batch_size=self.batch_size,
-            device=self.device,
-            names=copy(self._td_dim_names),
-            _run_checks=False,
-        )
-        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
-        # This may be undesirable, not sure if this should be the default behaviour
-        # (one usually does a copy to modify it).
-        # if not recurse:
-        #     self._maybe_set_shared_attributes(result)
-        return result
-
-    def contiguous(self) -> T:
-        source = {key: value.contiguous() for key, value in self.items()}
-        batch_size = self.batch_size
-        device = self.device
-        out = TensorDict(
-            source=source,
-            batch_size=batch_size,
-            device=device,
-            names=self.names,
-            _run_checks=False,
-        )
-        return out
-
-    def empty(
-        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
-    ) -> T:
-        if not recurse:
-            return TensorDict(
-                device=self._device if device is NO_DEFAULT else device,
-                batch_size=self._batch_size
-                if batch_size is None
-                else torch.Size(batch_size),
-                source={},
-                names=self._td_dim_names if names is None else names,
-                _run_checks=False,
-            )
-        return super().empty(recurse=recurse)
-
-    def _select(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        strict: bool = True,
-        set_shared: bool = True,
-    ) -> T:
-        if inplace and self.is_locked:
-            raise RuntimeError(_LOCK_ERROR)
-
-        source = {}
-        if len(keys):
-            keys_to_select = None
-            for key in keys:
-                if isinstance(key, str):
-                    subkey = []
-                else:
-                    key, subkey = key[0], key[1:]
-
-                val = self._get_str(key, default=None if not strict else NO_DEFAULT)
-                if val is None:
-                    continue
-                source[key] = val
-                if len(subkey):
-                    if keys_to_select is None:
-                        # delay creation of defaultdict
-                        keys_to_select = defaultdict(list)
-                    keys_to_select[key].append(subkey)
-
-            if keys_to_select is not None:
-                for key, val in keys_to_select.items():
-                    source[key] = source[key]._select(
-                        *val, strict=strict, inplace=inplace, set_shared=set_shared
-                    )
-
-        result = TensorDict(
-            device=self.device,
-            batch_size=self.batch_size,
-            source=source,
-            # names=self.names if self._has_names() else None,
-            names=self._td_dim_names,
-            _run_checks=False,
-        )
-        if inplace:
-            self._tensordict = result._tensordict
-            return self
-        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
-        # This may be undesirable, not sure if this should be the default behaviour
-        # (one usually does a copy to modify it).
-        # if set_shared:
-        #     self._maybe_set_shared_attributes(result)
-        return result
-
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> T:
-        # faster than Base.exclude
-        if not len(keys):
-            return self.copy() if not inplace else self
-        if not inplace:
-            _tensordict = copy(self._tensordict)
-        else:
-            _tensordict = self._tensordict
-        keys_to_exclude = None
-        for key in keys:
-            key = unravel_key(key)
-            if isinstance(key, str):
-                _tensordict.pop(key, None)
-            else:
-                if keys_to_exclude is None:
-                    # delay creation of defaultdict
-                    keys_to_exclude = defaultdict(list)
-                if key[0] in self._tensordict:
-                    keys_to_exclude[key[0]].append(key[1:])
-        if keys_to_exclude is not None:
-            for key, cur_keys in keys_to_exclude.items():
-                val = _tensordict.get(key, None)
-                if val is not None:
-                    val = val._exclude(
-                        *cur_keys, inplace=inplace, set_shared=set_shared
-                    )
-                    if not inplace:
-                        _tensordict[key] = val
-        if inplace:
-            return self
-        result = TensorDict(
-            _tensordict,
-            batch_size=self.batch_size,
-            device=self.device,
-            names=self.names if self._has_names() else None,
-            _run_checks=False,
-        )
-        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
-        # This may be undesirable, not sure if this should be the default behaviour
-        # (one usually does a copy to modify it).
-        # if set_shared:
-        #     self._maybe_set_shared_attributes(result)
-        return result
-
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _TensorDictKeysView:
-        if not include_nested and not leaves_only:
-            return self._tensordict.keys()
-        else:
-            return self._nested_keys(
-                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-            )
-
-    @cache  # noqa: B019
-    def _nested_keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _TensorDictKeysView:
-        return _TensorDictKeysView(
-            self,
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-            is_leaf=is_leaf,
-        )
-
-    def __getstate__(self):
-        result = {
-            key: val
-            for key, val in self.__dict__.items()
-            if key
-            not in ("_last_op", "_cache", "__last_op_queue", "__lock_parents_weakrefs")
-        }
-        return result
-
-    def __setstate__(self, state):
-        for key, value in state.items():
-            setattr(self, key, value)
-        self._cache = None
-        self.__last_op_queue = None
-        self._last_op = None
-        if self._is_locked:
-            # this can cause avoidable overhead, as we will be locking the leaves
-            # then locking their parent, and the parent of the parent, every
-            # time re-locking tensordicts that have already been locked.
-            # To avoid this, we should lock only at the root, but it isn't easy
-            # to spot what the root is...
-            self._is_locked = False
-            self.lock_()
-
-    # some custom methods for efficiency
-    def items(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        if not include_nested and not leaves_only:
-            return self._tensordict.items()
-        elif include_nested and leaves_only:
-            is_leaf = _default_is_leaf if is_leaf is None else is_leaf
-
-            def fast_iter():
-                for k, val in self._tensordict.items():
-                    if not is_leaf(val.__class__):
-                        yield from (
-                            ((k, *((_key,) if isinstance(_key, str) else _key)), _val)
-                            for _key, _val in val.items(
-                                include_nested=include_nested,
-                                leaves_only=leaves_only,
-                                is_leaf=is_leaf,
-                            )
-                        )
-                    else:
-                        yield k, val
-
-            return fast_iter()
-        else:
-            return super().items(
-                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-            )
-
-    def values(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        if not include_nested and not leaves_only:
-            return self._tensordict.values()
-        else:
-            return super().values(
-                include_nested=include_nested,
-                leaves_only=leaves_only,
-                is_leaf=is_leaf,
-            )
-
-
-class _SubTensorDict(TensorDictBase):
-    """A TensorDict that only sees an index of the stored tensors."""
-
-    _lazy = True
-    _inplace_set = True
-    _safe = False
-
-    def __init__(
-        self,
-        source: T,
-        idx: IndexType,
-        batch_size: Sequence[int] | None = None,
-    ) -> None:
-        if not _is_tensor_collection(source.__class__):
-            raise TypeError(
-                f"Expected source to be a subclass of TensorDictBase, "
-                f"got {type(source)}"
-            )
-        self._source = source
-        idx = (
-            (idx,)
-            if not isinstance(
-                idx,
-                (
-                    tuple,
-                    list,
-                ),
-            )
-            else tuple(idx)
-        )
-        if any(item is Ellipsis for item in idx):
-            idx = convert_ellipsis_to_idx(idx, self._source.batch_size)
-        self._batch_size = _getitem_batch_size(self._source.batch_size, idx)
-        self.idx = idx
-
-        if batch_size is not None and batch_size != self.batch_size:
-            raise RuntimeError("batch_size does not match self.batch_size.")
-
-    # These attributes should never be set
-    @property
-    def _is_shared(self):
-        return self._source._is_shared
-
-    @property
-    def _is_memmap(self):
-        return self._source._is_memmap
-
-    @staticmethod
-    def _convert_ellipsis(idx, shape):
-        if any(_idx is Ellipsis for _idx in idx):
-            new_idx = []
-            cursor = -1
-            for _idx in idx:
-                if _idx is Ellipsis:
-                    if cursor == len(idx) - 1:
-                        # then we can just skip
-                        continue
-                    n_upcoming = len(idx) - cursor - 1
-                    while cursor < len(shape) - n_upcoming:
-                        cursor += 1
-                        new_idx.append(slice(None))
-                else:
-                    new_idx.append(_idx)
-            return tuple(new_idx)
-        return idx
-
-    @property
-    def batch_size(self) -> torch.Size:
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, new_size: torch.Size) -> None:
-        self._batch_size_setter(new_size)
-
-    @property
-    def names(self):
-        names = self._source._get_names_idx(self.idx)
-        if names is None:
-            return [None] * self.batch_dims
-        return names
-
-    @names.setter
-    def names(self, value):
-        raise RuntimeError(
-            "Names of a subtensordict cannot be modified. Instantiate it as a TensorDict first."
-        )
-
-    def _has_names(self):
-        return self._source._has_names()
-
-    def _erase_names(self):
-        raise RuntimeError(
-            "Cannot erase names of a _SubTensorDict. Erase source TensorDict's names instead."
-        )
-
-    def _rename_subtds(self, names):
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
-
-    @property
-    def device(self) -> None | torch.device:
-        return self._source.device
-
-    @device.setter
-    def device(self, value: DeviceType) -> None:
-        self._source.device = value
-
-    def _preallocate(self, key: NestedKey, value: CompatibleType) -> T:
-        return self._source.set(key, value)
-
-    def _convert_inplace(self, inplace, key):
-        has_key = key in self.keys()
-        if inplace is not False:
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
-                )
-            inplace = has_key
-        if not inplace and has_key:
-            raise RuntimeError(
-                "Calling `_SubTensorDict.set(key, value, inplace=False)` is "
-                "prohibited for existing tensors. Consider calling "
-                "_SubTensorDict.set_(...) or cloning your tensordict first."
-            )
-        elif not inplace and self.is_locked:
-            raise RuntimeError(_LOCK_ERROR)
-        return inplace
-
-    from_dict_instance = TensorDict.from_dict_instance
-
-    def _set_str(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ) -> T:
-        inplace = self._convert_inplace(inplace, key)
-        # it is assumed that if inplace=False then the key doesn't exist. This is
-        # checked in set method, but not here. responsibility lies with the caller
-        # so that this method can have minimal overhead from runtime checks
-        parent = self._source
-        if not validated:
-            value = self._validate_value(value, check_shape=True)
-            validated = True
-        if not inplace:
-            if _is_tensor_collection(value.__class__):
-                value_expand = _expand_to_match_shape(
-                    parent.batch_size, value, self.batch_dims, self.device
-                )
-                for _key, _tensor in value.items():
-                    value_expand._set_str(
-                        _key,
-                        _expand_to_match_shape(
-                            parent.batch_size, _tensor, self.batch_dims, self.device
-                        ),
-                        inplace=inplace,
-                        validated=validated,
-                        ignore_lock=ignore_lock,
-                        non_blocking=non_blocking,
-                    )
-            else:
-                value_expand = torch.zeros(
-                    (
-                        *parent.batch_size,
-                        *_shape(value)[self.batch_dims :],
-                    ),
-                    dtype=value.dtype,
-                    device=self.device,
-                )
-                if self._is_shared:
-                    value_expand.share_memory_()
-                elif self._is_memmap:
-                    value_expand = MemoryMappedTensor.from_tensor(value_expand)
-            parent._set_str(
-                key,
-                value_expand,
-                inplace=False,
-                validated=validated,
-                ignore_lock=ignore_lock,
-                non_blocking=non_blocking,
-            )
-
-        parent._set_at_str(
-            key, value, self.idx, validated=validated, non_blocking=non_blocking
-        )
-        return self
-
-    def _set_tuple(
-        self,
-        key: NestedKey,
-        value: dict[str, CompatibleType] | CompatibleType,
-        *,
-        inplace: bool,
-        validated: bool,
-        non_blocking: bool = False,
-    ) -> T:
-        if len(key) == 1:
-            return self._set_str(
-                key[0],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        parent = self._source
-        td = parent._get_str(key[0], None)
-        if td is None:
-            td = parent.select()
-            parent._set_str(
-                key[0], td, inplace=False, validated=True, non_blocking=non_blocking
-            )
-        _SubTensorDict(td, self.idx)._set_tuple(
-            key[1:],
-            value,
-            inplace=inplace,
-            validated=validated,
-            non_blocking=non_blocking,
-        )
-        return self
-
-    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
-        tensor_in = self._get_str(key, NO_DEFAULT)
-        if not validated:
-            value = self._validate_value(value, check_shape=False)
-            validated = True
-        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-            warn(
-                "Multiple indexing can lead to unexpected behaviours when "
-                "setting items, for instance `td[idx1][idx2] = other` may "
-                "not write to the desired location if idx1 is a list/tensor."
-            )
-            tensor_in = _sub_index(tensor_in, idx)
-            tensor_in.copy_(value)
-            tensor_out = tensor_in
-        else:
-            tensor_out = _set_item(
-                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
-            )
-        # make sure that the value is updated
-        self._source._set_at_str(
-            key, tensor_out, self.idx, validated=validated, non_blocking=non_blocking
-        )
-        return self
-
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
-        if len(key) == 1:
-            return self._set_at_str(
-                key[0], value, idx, validated=validated, non_blocking=non_blocking
-            )
-        if key[0] not in self.keys():
-            # this won't work
-            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
-        else:
-            td = self._get_str(key[0], NO_DEFAULT)
-        td._set_at_tuple(
-            key[1:], value, idx, validated=validated, non_blocking=non_blocking
-        )
-        return self
-
-    # @cache  # noqa: B019
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _TensorDictKeysView:
-        return self._source.keys(
-            include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
-        )
-
-    def entry_class(self, key: NestedKey) -> type:
-        source_type = type(self._source.get(key))
-        if _is_tensor_collection(source_type):
-            return self.__class__
-        return source_type
-
-    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> _SubTensorDict:
-        self._source._stack_onto_at_(list_item, dim=dim, idx=self.idx)
-        return self
-
-    def to(self, *args, **kwargs: Any) -> T:
-        device, dtype, non_blocking, convert_to_format, batch_size = _parse_to(
-            *args, **kwargs
-        )
-        result = self
-
-        if device is not None and dtype is None and device == self.device:
-            return result
-        return self.to_tensordict().to(*args, **kwargs)
-
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        if not hasattr(self, "_orig_batch_size"):
-            self._orig_batch_size = self.batch_size
-        elif self._orig_batch_size == new_size:
-            del self._orig_batch_size
-        self._batch_size = new_size
-
-    def get(
-        self,
-        key: NestedKey,
-        default: Tensor | str | None = NO_DEFAULT,
-    ) -> CompatibleType:
-        return self._source.get_at(key, self.idx, default=default)
-
-    def _get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
-        out = super()._get_non_tensor(key, default=default)
-
-        if isinstance(out, _SubTensorDict) and is_non_tensor(out._source):
-            return out._source
-        return out
-
-    def _get_str(self, key, default):
-        if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
-            data = self._source._get_str(key, NO_DEFAULT)
-            if is_non_tensor(data):
-                return data[self.idx]
-            return _SubTensorDict(data, self.idx)
-        return self._source._get_at_str(key, self.idx, default=default)
-
-    def _get_tuple(self, key, default):
-        return self._source._get_at_tuple(key, self.idx, default=default)
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-        **kwargs,
-    ) -> _SubTensorDict:
-        if input_dict_or_td is self:
-            # no op
-            return self
-
-        if getattr(self._source, "_has_exclusive_keys", False):
-            raise RuntimeError(
-                "Cannot use _SubTensorDict.update with a LazyStackedTensorDict that has exclusive keys."
-            )
-        if keys_to_update is not None:
-            if len(keys_to_update) == 0:
-                return self
-            keys_to_update = unravel_key_list(keys_to_update)
-        keys = set(self.keys(False))
-        for key, value in input_dict_or_td.items():
-            key = _unravel_key_to_tuple(key)
-            firstkey, subkey = key[0], key[1:]
-            if keys_to_update and not any(
-                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
-                for ktu in keys_to_update
-            ):
-                continue
-            if clone and hasattr(value, "clone"):
-                value = value.clone()
-            elif clone:
-                value = tree_map(torch.clone, value)
-            # the key must be a string by now. Let's check if it is present
-            if firstkey in keys:
-                target_class = self.entry_class(firstkey)
-                if _is_tensor_collection(target_class):
-                    target = self._source.get(firstkey)._get_sub_tensordict(self.idx)
-                    if len(subkey):
-                        sub_keys_to_update = _prune_selected_keys(
-                            keys_to_update, firstkey
-                        )
-                        target.update(
-                            {subkey: value},
-                            inplace=False,
-                            keys_to_update=sub_keys_to_update,
-                            non_blocking=non_blocking,
-                        )
-                        continue
-                    elif isinstance(value, dict) or _is_tensor_collection(
-                        value.__class__
-                    ):
-                        sub_keys_to_update = _prune_selected_keys(
-                            keys_to_update, firstkey
-                        )
-                        target.update(
-                            value,
-                            keys_to_update=sub_keys_to_update,
-                            non_blocking=non_blocking,
-                        )
-                        continue
-                    raise ValueError(
-                        f"Tried to replace a tensordict with an incompatible object of type {type(value)}"
-                    )
-                else:
-                    self._set_tuple(
-                        key,
-                        value,
-                        inplace=True,
-                        validated=False,
-                        non_blocking=non_blocking,
-                    )
-            else:
-                self._set_tuple(
-                    key,
-                    value,
-                    inplace=BEST_ATTEMPT_INPLACE if inplace else False,
-                    validated=False,
-                    non_blocking=non_blocking,
-                )
-        return self
-
-    def update_(
-        self,
-        input_dict: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> _SubTensorDict:
-        return self.update_at_(
-            input_dict,
-            idx=self.idx,
-            discard_idx_attr=True,
-            clone=clone,
-            keys_to_update=keys_to_update,
-            non_blocking=non_blocking,
-        )
-
-    def update_at_(
-        self,
-        input_dict: dict[str, CompatibleType] | TensorDictBase,
-        idx: IndexType,
-        *,
-        discard_idx_attr: bool = False,
-        clone: bool = False,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> _SubTensorDict:
-        if keys_to_update is not None:
-            if len(keys_to_update) == 0:
-                return self
-            keys_to_update = unravel_key_list(keys_to_update)
-        for key, value in input_dict.items():
-            key = _unravel_key_to_tuple(key)
-            firstkey, _ = key[0], key[1:]
-            if keys_to_update and not any(
-                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
-                for ktu in keys_to_update
-            ):
-                continue
-            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            if discard_idx_attr:
-                self._source._set_at_tuple(
-                    key,
-                    value,
-                    idx,
-                    non_blocking=non_blocking,
-                    validated=False,
-                )
-            else:
-                self._set_at_tuple(
-                    key, value, idx, validated=False, non_blocking=non_blocking
-                )
-        return self
-
-    def get_parent_tensordict(self) -> T:
-        if not isinstance(self._source, TensorDictBase):
-            raise TypeError(
-                f"_SubTensorDict was initialized with a source of type"
-                f" {self._source.__class__.__name__}, "
-                "parent tensordict not accessible"
-            )
-        return self._source
-
-    @lock_blocked
-    def del_(self, key: NestedKey) -> T:
-        self._source = self._source.del_(key)
-        return self
-
-    @lock_blocked
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        raise NotImplementedError(
-            f"popitem not implemented for class {type(self).__name__}."
-        )
-
-    def _clone(self, recurse: bool = True) -> _SubTensorDict:
-        """Clones the _SubTensorDict.
-
-        Args:
-            recurse (bool, optional): if ``True`` (default), a regular
-                :class:`~.tensordict.TensorDict` instance will be created from the :class:`~.tensordict._SubTensorDict`.
-                Otherwise, another :class:`~.tensordict._SubTensorDict` with identical content
-                will be returned.
-
-        Examples:
-            >>> data = TensorDict({"a": torch.arange(4).reshape(2, 2,)}, batch_size=[2, 2])
-            >>> sub_data = data._get_sub_tensordict([0,])
-            >>> print(sub_data)
-            _SubTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>> # the data of both subtensordict is the same
-            >>> print(data.get("a").data_ptr(), sub_data.get("a").data_ptr())
-            140183705558208 140183705558208
-            >>> sub_data_clone = sub_data.clone(recurse=True)
-            >>> print(sub_data_clone)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>. print(sub_data.get("a").data_ptr())
-            140183705558208
-            >>> sub_data_clone = sub_data.clone(recurse=False)
-            >>> print(sub_data_clone)
-            _SubTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>> print(sub_data.get("a").data_ptr())
-            140183705558208
-        """
-        if not recurse:
-            return _SubTensorDict(
-                source=self._source._clone(recurse=False), idx=self.idx
-            )
-        return self.to_tensordict()
-
-    def is_contiguous(self) -> bool:
-        return all(value.is_contiguous() for value in self.values())
-
-    def contiguous(self) -> T:
-        return TensorDict(
-            batch_size=self.batch_size,
-            source={key: value.contiguous() for key, value in self.items()},
-            device=self.device,
-            names=self.names,
-            _run_checks=False,
-        )
-
-    def _select(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        strict: bool = True,
-        set_shared: bool = True,
-    ) -> T:
-        if inplace:
-            raise RuntimeError("Cannot call select inplace on a lazy tensordict.")
-        return self.to_tensordict()._select(
-            *keys, inplace=False, strict=strict, set_shared=set_shared
-        )
-
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> T:
-        if inplace:
-            raise RuntimeError("Cannot call exclude inplace on a lazy tensordict.")
-        return self.to_tensordict()._exclude(
-            *keys, inplace=False, set_shared=set_shared
-        )
-
-    def expand(self, *args: int, inplace: bool = False) -> T:
-        if len(args) == 1 and isinstance(args[0], Sequence):
-            shape = tuple(args[0])
-        else:
-            shape = args
-        return self._fast_apply(
-            lambda x: x.expand((*shape, *x.shape[self.ndim :])),
-            batch_size=shape,
-            propagate_lock=True,
-        )
-
-    def is_shared(self) -> bool:
-        return self._source.is_shared()
-
-    def is_memmap(self) -> bool:
-        return self._source.is_memmap()
-
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> _SubTensorDict:
-        self._source.rename_key_(old_key, new_key, safe=safe)
-        return self
-
-    def pin_memory(self) -> T:
-        self._source.pin_memory()
-        return self
-
-    def detach_(self) -> T:
-        raise RuntimeError("Detaching a sub-tensordict in-place cannot be done.")
-
-    def where(self, condition, other, *, out=None, pad=None):
-        return self.to_tensordict().where(
-            condition=condition, other=other, out=out, pad=pad
-        )
-
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
-        for key, item in self.items():
-            self.set_(key, torch.full_like(item, value))
-        return self
-
-    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
-        td_copy = self.clone()
-        return td_copy.masked_fill_(mask, value)
-
-    def memmap_(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        num_threads: int = 0,
-    ) -> T:
-        raise RuntimeError(
-            "Converting a sub-tensordict values to memmap cannot be done."
-        )
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None,
-        copy_existing: bool,
-        executor,
-        futures,
-        inplace,
-        like,
-        share_non_tensor,
-    ) -> T:
-        if prefix is not None:
-
-            def save_metadata(prefix=prefix, self=self):
-                prefix = Path(prefix)
-                if not prefix.exists():
-                    os.makedirs(prefix, exist_ok=True)
-                with open(prefix / "meta.json", "w") as f:
-                    json.dump(
-                        {
-                            "_type": str(self.__class__),
-                            "index": _index_to_str(self.idx),
-                        },
-                        f,
-                    )
-
-            if executor is None:
-                save_metadata()
-            else:
-                futures.append(executor.submit(save_metadata))
-
-        _source = self._source._memmap_(
-            prefix=prefix / "_source" if prefix is not None else None,
-            copy_existing=copy_existing,
-            executor=executor,
-            futures=futures,
-            inplace=inplace,
-            like=like,
-            share_non_tensor=share_non_tensor,
-        )
-        if not inplace:
-            result = _SubTensorDict(_source, idx=self.idx)
-        else:
-            result = self
-        return result
-
-    @classmethod
-    def _load_memmap(
-        cls, prefix: Path, metadata: dict, device: torch.device | None = None
-    ):
-        index = metadata["index"]
-        return _SubTensorDict(
-            TensorDict.load_memmap(prefix / "_source", device=device),
-            _str_to_index(index),
-        )
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def share_memory_(self) -> T:
-        raise RuntimeError(
-            "Casting a sub-tensordict values to shared memory cannot be done."
-        )
-
-    @property
-    def is_locked(self) -> bool:
-        return self._source.is_locked
-
-    @is_locked.setter
-    def is_locked(self, value) -> bool:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    @as_decorator("is_locked")
-    def lock_(self) -> T:
-        # we can't lock sub-tensordicts because that would mean that the
-        # parent tensordict cannot be modified either.
-        if not self.is_locked:
-            raise RuntimeError(
-                "Cannot lock a _SubTensorDict. Lock the parent tensordict instead."
-            )
-        return self
-
-    @as_decorator("is_locked")
-    def unlock_(self) -> T:
-        if self.is_locked:
-            raise RuntimeError(
-                "Cannot unlock a _SubTensorDict. Unlock the parent tensordict instead."
-            )
-        return self
-
-    def _remove_lock(self, lock_id):
-        raise RuntimeError(
-            "Cannot unlock a _SubTensorDict. Unlock the parent tensordict instead."
-        )
-
-    def _propagate_lock(self, lock_ids=None):
-        raise RuntimeError(
-            "Cannot lock a _SubTensorDict. Lock the parent tensordict instead."
-        )
-
-    def __del__(self):
-        pass
-
-    def _create_nested_str(self, key):
-        # this may fail with a sub-sub tensordict
-        out = self._source.empty()
-        self._source._set_str(
-            key, out, inplace=False, validated=True, non_blocking=False
-        )
-        # the id of out changes
-        return self._get_str(key, default=NO_DEFAULT)
-
-    def _cast_reduction(
-        self,
-        *,
-        reduction_name,
-        dim=NO_DEFAULT,
-        keepdim=NO_DEFAULT,
-        tuple_ok=True,
-        **kwargs,
-    ):
-        try:
-            td = self.to_tensordict()
-        except Exception:
-            raise RuntimeError(
-                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
-                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
-                f" on github!"
-            )
-        return td._cast_reduction(
-            reduction_name=reduction_name,
-            dim=dim,
-            keepdim=keepdim,
-            tuple_ok=tuple_ok,
-            **kwargs,
-        )
-
-    # TODO: check these implementations
-    __eq__ = TensorDict.__eq__
-    __ne__ = TensorDict.__ne__
-    __ge__ = TensorDict.__ge__
-    __gt__ = TensorDict.__gt__
-    __le__ = TensorDict.__le__
-    __lt__ = TensorDict.__lt__
-    __setitem__ = TensorDict.__setitem__
-    __xor__ = TensorDict.__xor__
-    __or__ = TensorDict.__or__
-    _check_device = TensorDict._check_device
-    _check_is_shared = TensorDict._check_is_shared
-    all = TensorDict.all
-    any = TensorDict.any
-    masked_select = TensorDict.masked_select
-    memmap_like = TensorDict.memmap_like
-    reshape = TensorDict.reshape
-    split = TensorDict.split
-    _to_module = TensorDict._to_module
-    _unbind = TensorDict._unbind
-
-    def _view(self, *args, **kwargs):
-        raise RuntimeError(
-            "Cannot call `view` on a sub-tensordict. Call `reshape` instead."
-        )
-
-    def _transpose(self, dim0, dim1):
-        raise RuntimeError(
-            "Cannot call `transpose` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _permute(
-        self,
-        *args,
-        **kwargs,
-    ):
-        raise RuntimeError(
-            "Cannot call `permute` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _squeeze(self, dim=None):
-        raise RuntimeError(
-            "Cannot call `squeeze` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _unsqueeze(self, dim):
-        raise RuntimeError(
-            "Cannot call `unsqueeze` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    _add_batch_dim = TensorDict._add_batch_dim
-
-    _apply_nest = TensorDict._apply_nest
-    # def _apply_nest(self, *args, **kwargs):
-    #     return self.to_tensordict()._apply_nest(*args, **kwargs)
-    _convert_to_tensordict = TensorDict._convert_to_tensordict
-
-    _get_names_idx = TensorDict._get_names_idx
-
-    def _index_tensordict(self, index, new_batch_size=None, names=None):
-        # we ignore the names and new_batch_size which are only provided for
-        # efficiency purposes
-        return self._get_sub_tensordict(index)
-
-    def _remove_batch_dim(self, *args, **kwargs):
-        raise NotImplementedError
-
-
-###########################
-# Keys utils
-
-
-class _TensorDictKeysView:
-    """A Key view for TensorDictBase instance.
-
-    _TensorDictKeysView is returned when accessing tensordict.keys() and holds a
-    reference to the original TensorDict. This class enables us to support nested keys
-    when performing membership checks and when iterating over keys.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-
-        >>> td = TensorDict(
-        >>>     {"a": TensorDict({"b": torch.rand(1, 2)}, [1, 2]), "c": torch.rand(1)},
-        >>>     [1],
-        >>> )
-
-        >>> assert "a" in td.keys()
-        >>> assert ("a",) in td.keys()
-        >>> assert ("a", "b") in td.keys()
-        >>> assert ("a", "c") not in td.keys()
-
-        >>> assert set(td.keys()) == {("a", "b"), "c"}
-    """
-
-    def __init__(
-        self,
-        tensordict: T,
-        include_nested: bool,
-        leaves_only: bool,
-        is_leaf: Callable[[Type], bool] = None,
-    ) -> None:
-        self.tensordict = tensordict
-        self.include_nested = include_nested
-        self.leaves_only = leaves_only
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-        self.is_leaf = is_leaf
-
-    def __iter__(self) -> Iterable[str] | Iterable[tuple[str, ...]]:
-        if not self.include_nested:
-            if self.leaves_only:
-                for key in self._keys():
-                    target_class = self.tensordict.entry_class(key)
-                    if _is_tensor_collection(target_class):
-                        continue
-                    yield key
-            else:
-                yield from self._keys()
-        else:
-            yield from (
-                key if len(key) > 1 else key[0]
-                for key in self._iter_helper(self.tensordict)
-            )
-
-    def _iter_helper(
-        self, tensordict: T, prefix: str | None = None
-    ) -> Iterable[str] | Iterable[tuple[str, ...]]:
-        for key, value in self._items(tensordict):
-            full_key = self._combine_keys(prefix, key)
-            cls = value.__class__
-            while cls is list:
-                # For lazy stacks
-                value = value[0]
-                cls = value.__class__
-            is_leaf = self.is_leaf(cls)
-            if self.include_nested and not is_leaf:
-                yield from self._iter_helper(value, prefix=full_key)
-            if not self.leaves_only or is_leaf:
-                yield full_key
-
-    def _combine_keys(self, prefix: tuple | None, key: NestedKey) -> tuple:
-        if prefix is not None:
-            return prefix + (key,)
-        return (key,)
-
-    def __len__(self) -> int:
-        return sum(1 for _ in self)
-
-    def _items(
-        self, tensordict: TensorDictBase | None = None
-    ) -> Iterable[tuple[NestedKey, CompatibleType]]:
-        if tensordict is None:
-            tensordict = self.tensordict
-        if isinstance(tensordict, TensorDict) or is_tensorclass(tensordict):
-            return tensordict._tensordict.items()
-        from tensordict.nn import TensorDictParams
-
-        if isinstance(tensordict, TensorDictParams):
-            return tensordict._param_td.items()
-        if isinstance(tensordict, KeyedJaggedTensor):
-            return tuple((key, tensordict[key]) for key in tensordict.keys())
-        from tensordict._lazy import (
-            _CustomOpTensorDict,
-            _iter_items_lazystack,
-            LazyStackedTensorDict,
-        )
-
-        if isinstance(tensordict, LazyStackedTensorDict):
-            return _iter_items_lazystack(tensordict, return_none_for_het_values=True)
-        if isinstance(tensordict, _CustomOpTensorDict):
-            # it's possible that a TensorDict contains a nested LazyStackedTensorDict,
-            # or _CustomOpTensorDict, so as we iterate through the contents we need to
-            # be careful to not rely on tensordict._tensordict existing.
-            return (
-                (key, tensordict._get_str(key, NO_DEFAULT))
-                for key in tensordict._source.keys()
-            )
-        raise NotImplementedError(type(tensordict))
-
-    def _keys(self) -> _TensorDictKeysView:
-        return self.tensordict._tensordict.keys()
-
-    def __contains__(self, key: NestedKey) -> bool:
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise TypeError(_NON_STR_KEY_ERR)
-
-        if isinstance(key, str):
-            if key in self._keys():
-                if self.leaves_only:
-                    # TODO: make this faster for LazyStacked without compromising regular
-                    return not _is_tensor_collection(
-                        type(self.tensordict._get_str(key))
-                    )
-                return True
-            return False
-        else:
-            # thanks to _unravel_key_to_tuple we know the key is a tuple
-            if len(key) == 1:
-                return key[0] in self._keys()
-            elif self.include_nested:
-                item_root = self.tensordict._get_str(key[0], default=None)
-                if item_root is not None:
-                    entry_type = type(item_root)
-                    if issubclass(entry_type, Tensor):
-                        return False
-                    elif entry_type is KeyedJaggedTensor:
-                        if len(key) > 2:
-                            return False
-                        return key[1] in item_root.keys()
-                    # TODO: make this faster for LazyStacked without compromising regular
-                    _is_tensordict = _is_tensor_collection(entry_type)
-                    if _is_tensordict:
-                        # # this will call _unravel_key_to_tuple many times
-                        # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
-                        # this won't call _unravel_key_to_tuple but requires to get the default which can be suboptimal
-                        if len(key) >= 3:
-                            leaf_td = item_root._get_tuple(key[1:-1], None)
-                            if leaf_td is None or (
-                                not _is_tensor_collection(leaf_td.__class__)
-                                and not isinstance(leaf_td, KeyedJaggedTensor)
-                            ):
-                                return False
-                        else:
-                            leaf_td = item_root
-                        return key[-1] in leaf_td.keys()
-                return False
-            # this is reached whenever there is more than one key but include_nested is False
-            if all(isinstance(subkey, str) for subkey in key):
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-
-    def __repr__(self):
-        include_nested = f"include_nested={self.include_nested}"
-        leaves_only = f"leaves_only={self.leaves_only}"
-        return f"{self.__class__.__name__}({list(self)},\n{indent(include_nested, 4*' ')},\n{indent(leaves_only, 4*' ')})"
-
-
-def _set_tensor_dict(  # noqa: F811
-    module_dict,
-    hooks,
-    module: torch.nn.Module,
-    name: str,
-    tensor: torch.Tensor,
-    inplace: bool,
-) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    was_buffer = False
-    out = module_dict["_parameters"].pop(name, None)  # type: ignore[assignment]
-    if out is None:
-        out = module_dict["_buffers"].pop(name, None)
-        was_buffer = out is not None
-    if out is None:
-        out = module_dict.pop(name)
-    if inplace:
-        # swap tensor and out after updating out
-        out_tmp = out.clone()
-        out.data.copy_(tensor.data)
-        tensor = out
-        out = out_tmp
-
-    if isinstance(tensor, torch.nn.Parameter):
-        for hook in hooks:
-            output = hook(module, name, tensor)
-            if output is not None:
-                tensor = output
-        module_dict["_parameters"][name] = tensor
-
-        if isinstance(
-            tensor, (_BatchedUninitializedParameter, _BatchedUninitializedBuffer)
-        ):
-            module.register_forward_pre_hook(
-                _add_batch_dim_pre_hook(), with_kwargs=True
-            )
-
-    elif was_buffer and isinstance(tensor, torch.Tensor):
-        module_dict["_buffers"][name] = tensor
-    else:
-        module_dict[name] = tensor
-    return out
-
-
-def _index_to_str(index):
-    if isinstance(index, tuple):
-        return tuple(_index_to_str(elt) for elt in index)
-    if isinstance(index, slice):
-        return ("slice", {"start": index.start, "stop": index.stop, "step": index.step})
-    if isinstance(index, range):
-        return ("range", {"start": index.start, "stop": index.stop, "step": index.step})
-    if isinstance(index, Tensor):
-        return ("tensor", index.tolist(), str(index.device))
-    return index
-
-
-def _str_to_index(index):
-    if isinstance(index, tuple):
-        if not len(index):
-            return index
-        if index[0] == "slice":
-            index = index[1]
-            return slice(index["start"], index["stop"], index["step"])
-        if index[0] == "range":
-            index = index[1]
-            return range(index["start"], index["stop"], index["step"])
-        if index[0] == "tensor":
-            index, device = index[1:]
-            return torch.tensor(index, device=device)
-        return tuple(_index_to_str(elt) for elt in index)
-    return index
-
-
-_register_tensor_class(TensorDict)
-_register_tensor_class(_SubTensorDict)
-
-
-def _save_metadata(data: TensorDictBase, prefix: Path, metadata=None):
-    """Saves the metadata of a memmap tensordict on disk."""
-    filepath = prefix / "meta.json"
-    if metadata is None:
-        metadata = {}
-    metadata.update(
-        {
-            "shape": list(data.shape),
-            "device": str(data.device),
-            "_type": str(data.__class__),
-        }
-    )
-    with open(filepath, "w") as json_metadata:
-        json.dump(metadata, json_metadata)
-
-
-# user did specify location and memmap is in wrong place, so we copy
-def _populate_memmap(*, dest, value, key, copy_existing, prefix, like):
-    filename = None if prefix is None else str(prefix / f"{key}.memmap")
-    if value.is_nested:
-        shape = value._nested_tensor_size()
-        # Make the shape a memmap tensor too
-        if prefix is not None:
-            shape_filename = Path(filename)
-            shape_filename = shape_filename.with_suffix(".shape.memmap")
-            MemoryMappedTensor.from_tensor(
-                shape,
-                filename=shape_filename,
-                copy_existing=copy_existing,
-                existsok=True,
-                copy_data=True,
-            )
-    else:
-        shape = None
-    memmap_tensor = MemoryMappedTensor.from_tensor(
-        value.data if value.requires_grad else value,
-        filename=filename,
-        copy_existing=copy_existing,
-        existsok=True,
-        copy_data=not like,
-        shape=shape,
-    )
-    dest._tensordict[key] = memmap_tensor
-    return memmap_tensor
-
-
-def _populate_empty(
-    *,
-    dest,
-    key,
-    shape,
-    dtype,
-    prefix,
-):
-    filename = None if prefix is None else str(prefix / f"{key}.memmap")
-    if isinstance(shape, torch.Tensor):
-        # Make the shape a memmap tensor too
-        if prefix is not None:
-            shape_filename = Path(filename)
-            shape_filename = shape_filename.with_suffix(".shape.memmap")
-            MemoryMappedTensor.from_tensor(
-                shape,
-                filename=shape_filename,
-                existsok=True,
-                copy_data=True,
-            )
-    memmap_tensor = MemoryMappedTensor.empty(
-        shape=shape,
-        dtype=dtype,
-        filename=filename,
-        existsok=True,
-    )
-    dest._tensordict[key] = memmap_tensor
-    return memmap_tensor
-
-
-def _populate_storage(
-    *,
-    dest,
-    key,
-    shape,
-    dtype,
-    prefix,
-    storage,
-):
-    filename = None if prefix is None else str(prefix / f"{key}.memmap")
-    if isinstance(shape, torch.Tensor):
-        # Make the shape a memmap tensor too
-        if prefix is not None:
-            shape_filename = Path(filename)
-            shape_filename = shape_filename.with_suffix(".shape.memmap")
-            MemoryMappedTensor.from_tensor(
-                shape,
-                filename=shape_filename,
-                existsok=True,
-                copy_data=True,
-            )
-    memmap_tensor = MemoryMappedTensor.from_storage(
-        storage=storage,
-        shape=shape,
-        dtype=dtype,
-        filename=filename,
-    )
-    dest._tensordict[key] = memmap_tensor
-    return memmap_tensor
-
-
-def _update_metadata(*, metadata, key, value, is_collection):
-    if not is_collection:
-        metadata[key] = {
-            "device": str(value.device),
-            "shape": list(value.shape)
-            if not value.is_nested
-            else value._nested_tensor_size().shape,
-            "dtype": str(value.dtype),
-            "is_nested": value.is_nested,
-        }
-    else:
-        metadata[key] = {
-            "type": type(value).__name__,
-        }
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import json
+import numbers
+import os
+from collections import defaultdict
+from copy import copy
+from numbers import Number
+from pathlib import Path
+from textwrap import indent
+from typing import Any, Callable, Iterable, Iterator, List, Sequence, Tuple, Type
+from warnings import warn
+
+import numpy as np
+import torch
+
+try:
+    from functorch import dim as ftdim
+
+    _has_funcdim = True
+except ImportError:
+    from tensordict.utils import _ftdim_mock as ftdim
+
+    _has_funcdim = False
+
+from tensordict.base import (
+    _ACCEPTED_CLASSES,
+    _default_is_leaf,
+    _is_tensor_collection,
+    _load_metadata,
+    _register_tensor_class,
+    BEST_ATTEMPT_INPLACE,
+    CompatibleType,
+    is_tensor_collection,
+    NO_DEFAULT,
+    T,
+    TensorDictBase,
+)
+
+from tensordict.memmap import MemoryMappedTensor
+from tensordict.utils import (
+    _add_batch_dim_pre_hook,
+    _BatchedUninitializedBuffer,
+    _BatchedUninitializedParameter,
+    _clone_value,
+    _expand_to_match_shape,
+    _get_item,
+    _get_leaf_tensordict,
+    _get_shape_from_args,
+    _getitem_batch_size,
+    _index_preserve_data_ptr,
+    _is_number,
+    _is_shared,
+    _is_tensorclass,
+    _KEY_ERROR,
+    _LOCK_ERROR,
+    _NON_STR_KEY_ERR,
+    _NON_STR_KEY_TUPLE_ERR,
+    _parse_to,
+    _prune_selected_keys,
+    _set_item,
+    _set_max_batch_size,
+    _shape,
+    _STRDTYPE2DTYPE,
+    _StringOnlyDict,
+    _sub_index,
+    _unravel_key_to_tuple,
+    as_decorator,
+    Buffer,
+    cache,
+    convert_ellipsis_to_idx,
+    DeviceType,
+    expand_as_right,
+    IndexType,
+    is_non_tensor,
+    is_tensorclass,
+    KeyedJaggedTensor,
+    lock_blocked,
+    NestedKey,
+    unravel_key,
+    unravel_key_list,
+)
+from torch import Tensor
+from torch.jit._shape_functions import infer_size_impl
+from torch.utils._pytree import tree_map
+
+_register_tensor_class(ftdim.Tensor)
+
+__base__setattr__ = torch.nn.Module.__setattr__
+
+_has_mps = torch.backends.mps.is_available()
+_has_cuda = torch.cuda.is_available()
+_has_functorch = False
+try:
+    try:
+        from torch._C._functorch import (
+            _add_batch_dim,
+            _remove_batch_dim,
+            is_batchedtensor,
+        )
+    except ImportError:
+        from functorch._C import is_batchedtensor
+
+    _has_functorch = True
+except ImportError:
+    _has_functorch = False
+
+    def is_batchedtensor(tensor: Tensor) -> bool:
+        """Placeholder for the functorch function."""
+        return False
+
+
+class TensorDict(TensorDictBase):
+    """A batched dictionary of tensors.
+
+    TensorDict is a tensor container where all tensors are stored in a
+    key-value pair fashion and where each element shares the same first ``N``
+    leading dimensions shape, where is an arbitrary number with ``N >= 0``.
+
+    Additionally, if the tensordict has a specified device, then each element
+    must share that device.
+
+    TensorDict instances support many regular tensor operations with the notable
+    exception of algebraic operations:
+
+    - operations on shape: when a shape operation is called (indexing,
+      reshape, view, expand, transpose, permute,
+      unsqueeze, squeeze, masking etc), the operations is done as if it
+      was executed on a tensor of the same shape as the batch size then
+      expended to the right, e.g.:
+
+        >>> td = TensorDict({'a': torch.zeros(3, 4, 5)}, batch_size=[3, 4])
+        >>> # returns a TensorDict of batch size [3, 4, 1]:
+        >>> td_unsqueeze = td.unsqueeze(-1)
+        >>> # returns a TensorDict of batch size [12]
+        >>> td_view = td.view(-1)
+        >>> # returns a tensor of batch size [12, 4]
+        >>> a_view = td.view(-1).get("a")
+
+    - casting operations: a TensorDict can be cast on a different device using
+
+        >>> td_cpu = td.to("cpu")
+        >>> dictionary = td.to_dict()
+
+      A call of the `.to()` method with a dtype will return an error.
+
+    - Cloning (:meth:`~TensorDictBase.clone`), contiguous (:meth:`~TensorDictBase.contiguous`);
+
+    - Reading: `td.get(key)`, `td.get_at(key, index)`
+
+    - Content modification: :obj:`td.set(key, value)`, :obj:`td.set_(key, value)`,
+      :obj:`td.update(td_or_dict)`, :obj:`td.update_(td_or_dict)`, :obj:`td.fill_(key,
+      value)`, :obj:`td.rename_key_(old_name, new_name)`, etc.
+
+    - Operations on multiple tensordicts: `torch.cat(tensordict_list, dim)`,
+      `torch.stack(tensordict_list, dim)`, `td1 == td2`, `td.apply(lambda x+y, other_td)` etc.
+
+    Args:
+        source (TensorDict or Dict[NestedKey, Union[Tensor, TensorDictBase]]): a
+            data source. If empty, the tensordict can be populated subsequently.
+        batch_size (iterable of int, optional): a batch size for the
+            tensordict. The batch size can be modified subsequently as long
+            as it is compatible with its content.
+            If not batch-size is provided, an empty batch-size is assumed (it
+            is not inferred automatically from the data). To automatically set
+            the batch-size, refer to :meth:`~.auto_batch_size_`.
+        device (torch.device or compatible type, optional): a device for the
+            TensorDict. If provided, all tensors will be stored on that device.
+            If not, tensors on different devices are allowed.
+        names (lsit of str, optional): the names of the dimensions of the
+            tensordict. If provided, its length must match the one of the
+            ``batch_size``. Defaults to ``None`` (no dimension name, or ``None``
+            for every dimension).
+        non_blocking (bool, optional): if ``True`` and a device is passed, the tensordict
+            is delivered without synchronization. This is the fastest option but is only
+            safe when casting from cpu to cuda (otherwise a synchronization call must be
+            implemented by the user).
+            If ``False`` is passed, every tensor movement will be done synchronously.
+            If ``None`` (default), the device casting will be done asynchronously but
+            a synchronization will be executed after creation if required. This option
+            should generally be faster than ``False`` and potentially slower than ``True``.
+        lock (bool, optional): if ``True``, the resulting tensordict will be
+            locked.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> source = {'random': torch.randn(3, 4),
+        ...     'zeros': torch.zeros(3, 4, 5)}
+        >>> batch_size = [3]
+        >>> td = TensorDict(source, batch_size=batch_size)
+        >>> print(td.shape)  # equivalent to td.batch_size
+        torch.Size([3])
+        >>> td_unqueeze = td.unsqueeze(-1)
+        >>> print(td_unqueeze.get("zeros").shape)
+        torch.Size([3, 1, 4, 5])
+        >>> print(td_unqueeze[0].shape)
+        torch.Size([1])
+        >>> print(td_unqueeze.view(-1).shape)
+        torch.Size([3])
+        >>> print((td.clone()==td).all())
+        True
+
+    """
+
+    _td_dim_names = None
+    _is_shared = False
+    _is_memmap = False
+    _has_exclusive_keys = False
+
+    def __init__(
+        self,
+        source: T | dict[str, CompatibleType] = None,
+        batch_size: Sequence[int] | torch.Size | int | None = None,
+        device: DeviceType | None = None,
+        names: Sequence[str] | None = None,
+        non_blocking: bool = None,
+        lock: bool = False,
+        _run_checks: bool = True,
+    ) -> None:
+        has_device = False
+        sub_non_blocking = False
+        if device is not None:
+            has_device = True
+            if non_blocking is None:
+                sub_non_blocking = True
+                non_blocking = False
+            else:
+                sub_non_blocking = non_blocking
+            device = torch.device(device)
+            if _has_mps:
+                # With MPS, an explicit sync is required
+                sub_non_blocking = True
+        self._device = device
+
+        self._tensordict = _tensordict = _StringOnlyDict()
+        if not _run_checks:
+            self._batch_size = batch_size
+            if source:  # faster than calling items
+                for key, value in source.items():
+                    if isinstance(value, dict):
+                        value = TensorDict(
+                            value,
+                            batch_size=self._batch_size,
+                            device=self._device,
+                            _run_checks=_run_checks,
+                            non_blocking=sub_non_blocking,
+                        )
+                    _tensordict[key] = value
+            self._td_dim_names = names
+        else:
+            if source is None:
+                source = {}
+            if not isinstance(source, (TensorDictBase, dict)):
+                raise ValueError(
+                    "A TensorDict source is expected to be a TensorDictBase "
+                    f"sub-type or a dictionary, found type(source)={type(source)}."
+                )
+            self._batch_size = self._parse_batch_size(source, batch_size)
+            self.names = names
+
+            for key, value in source.items():
+                self.set(key, value, non_blocking=sub_non_blocking)
+        if not non_blocking and sub_non_blocking and has_device:
+            self._sync_all()
+        if lock:
+            self.lock_()
+
+    @classmethod
+    def from_module(
+        cls,
+        module: torch.nn.Module,
+        as_module: bool = False,
+        lock: bool = False,
+        use_state_dict: bool = False,
+    ):
+        result = cls._from_module(
+            module=module, as_module=as_module, use_state_dict=use_state_dict
+        )
+        if lock:
+            result.lock_()
+        return result
+
+    @classmethod
+    def _from_module(
+        cls,
+        module: torch.nn.Module,
+        as_module: bool = False,
+        use_state_dict: bool = False,
+        prefix="",
+    ):
+        from tensordict.nn import TensorDictParams
+
+        if isinstance(module, TensorDictParams):
+            return module
+        destination = {}
+        if use_state_dict:
+            keep_vars = False
+            # do we need this feature atm?
+            local_metadata = {}
+            # if hasattr(destination, "_metadata"):
+            #     destination._metadata[prefix[:-1]] = local_metadata
+            for hook in module._state_dict_pre_hooks.values():
+                hook(module, prefix, keep_vars)
+            module._save_to_state_dict(destination, "", keep_vars)
+        else:
+            for name, param in module._parameters.items():
+                if param is None:
+                    continue
+                destination[name] = param
+            for name, buffer in module._buffers.items():
+                if buffer is None:
+                    continue
+                destination[name] = buffer
+
+        if use_state_dict:
+            for hook in module._state_dict_hooks.values():
+                hook_result = hook(module, destination, prefix, local_metadata)
+                if hook_result is not None:
+                    destination = hook_result
+        destination = TensorDict(destination, batch_size=[])
+        for name, submodule in module._modules.items():
+            if submodule is not None:
+                subtd = cls._from_module(
+                    module=submodule,
+                    as_module=False,
+                    use_state_dict=use_state_dict,
+                    prefix=prefix + name + ".",
+                )
+                destination._set_str(
+                    name, subtd, validated=True, inplace=False, non_blocking=False
+                )
+
+        if as_module:
+            from tensordict.nn.params import TensorDictParams
+
+            return TensorDictParams(destination, no_convert=True)
+        return destination
+
+    def is_empty(self):
+
+        for item in self._tensordict.values():
+            # we need to check if item is empty
+            if _is_tensor_collection(type(item)):
+                if not item.is_empty():
+                    return False
+
+                if is_non_tensor(item):
+                    return False
+            else:
+                return False
+        return True
+
+    def _to_module(
+        self,
+        module,
+        *,
+        inplace: bool | None = None,
+        return_swap: bool = True,
+        swap_dest=None,
+        memo=None,
+        use_state_dict: bool = False,
+        non_blocking: bool = False,
+    ):
+
+        if not use_state_dict and isinstance(module, TensorDictBase):
+            if return_swap:
+                swap = module.copy()
+                module._param_td = getattr(self, "_param_td", self)
+                return swap
+            else:
+                module.update(self)
+                return
+
+        # we use __dict__ directly to avoid the getattr/setattr overhead whenever we can
+        __dict__ = module.__dict__
+
+        hooks = memo["hooks"]
+        if return_swap:
+            _swap = {}
+            memo[id(module)] = _swap
+
+        if use_state_dict:
+            if inplace is not None:
+                raise RuntimeError(
+                    "inplace argument cannot be passed when use_state_dict=True."
+                )
+            # execute module's pre-hooks
+            state_dict = self.flatten_keys(".")
+            prefix = ""
+            strict = True
+            local_metadata = {}
+            missing_keys = []
+            unexpected_keys = []
+            error_msgs = []
+            for hook in module._load_state_dict_pre_hooks.values():
+                hook(
+                    state_dict,
+                    prefix,
+                    local_metadata,
+                    strict,
+                    missing_keys,
+                    unexpected_keys,
+                    error_msgs,
+                )
+
+            def convert_type(x, y):
+                if isinstance(y, torch.nn.Parameter):
+                    return torch.nn.Parameter(x)
+                if isinstance(y, Buffer):
+                    return Buffer(x)
+                return x
+
+            input = state_dict.unflatten_keys(".")._fast_apply(
+                convert_type, self, propagate_lock=True
+            )
+        else:
+            input = self
+            inplace = bool(inplace)
+
+        for key, value in input.items():
+            if isinstance(value, (Tensor, ftdim.Tensor)):
+                if module.__class__.__setattr__ is __base__setattr__:
+                    # if setattr is the native nn.Module.setattr, we can rely on _set_tensor_dict
+                    local_out = _set_tensor_dict(
+                        __dict__, hooks, module, key, value, inplace
+                    )
+                else:
+                    if return_swap:
+                        local_out = getattr(module, key)
+                    if not inplace:
+                        # use specialized __setattr__ if needed
+                        delattr(module, key)
+                        setattr(module, key, value)
+                    else:
+                        new_val = local_out
+                        if return_swap:
+                            local_out = local_out.clone()
+                        new_val.data.copy_(value.data, non_blocking=non_blocking)
+            else:
+                if value.is_empty():
+                    # if there is at least one key, we must populate the module.
+                    # Otherwise, we just go to the next key
+                    continue
+                child = __dict__["_modules"][key]
+                local_out = memo.get(id(child), NO_DEFAULT)
+
+                if local_out is NO_DEFAULT:
+                    # if isinstance(child, TensorDictBase):
+                    #     # then child is a TensorDictParams
+                    #     from tensordict.nn import TensorDictParams
+                    #
+                    #     local_out = child
+                    #     if not isinstance(value, TensorDictParams):
+                    #         value = TensorDictParams(value, no_convert=True)
+                    #     __dict__["_modules"][key] = value
+                    # else:
+                    local_out = value._to_module(
+                        child,
+                        inplace=inplace,
+                        return_swap=return_swap,
+                        swap_dest={},  # we'll be calling update later
+                        memo=memo,
+                        use_state_dict=use_state_dict,
+                        non_blocking=non_blocking,
+                    )
+
+            if return_swap:
+                _swap[key] = local_out
+        if return_swap:
+            if isinstance(swap_dest, dict):
+                return _swap
+            elif swap_dest is not None:
+
+                def _quick_set(swap_dict, swap_td):
+                    for key, val in swap_dict.items():
+                        if isinstance(val, dict):
+                            _quick_set(val, swap_td._get_str(key, default=NO_DEFAULT))
+                        elif swap_td._get_str(key, None) is not val:
+                            swap_td._set_str(
+                                key,
+                                val,
+                                inplace=False,
+                                validated=True,
+                                non_blocking=non_blocking,
+                            )
+
+                _quick_set(_swap, swap_dest)
+                return swap_dest
+            else:
+                return TensorDict(_swap, batch_size=[], _run_checks=False)
+
+    def __ne__(self, other: object) -> T | bool:
+        if _is_tensorclass(other):
+            return other != self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                raise KeyError(
+                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
+                )
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 != other.get(key)
+            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value != other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return True
+
+    def __xor__(self, other: object) -> T | bool:
+        if _is_tensorclass(other):
+            return other ^ self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                raise KeyError(
+                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
+                )
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 ^ other.get(key)
+            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value ^ other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return True
+
+    def __or__(self, other: object) -> T | bool:
+        if _is_tensorclass(other):
+            return other | self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                raise KeyError(
+                    f"keys in {self} and {other} mismatch, got {keys1} and {keys2}"
+                )
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 | other.get(key)
+            return TensorDict(batch_size=self.batch_size, source=d, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value | other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __eq__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other == self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 == other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value == other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __ge__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other <= self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 >= other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value >= other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __gt__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other < self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 > other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value > other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __le__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other >= self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 <= other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value <= other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __lt__(self, other: object) -> T | bool:
+        if is_tensorclass(other):
+            return other > self
+        if isinstance(other, (dict,)):
+            other = self.from_dict_instance(other)
+        if _is_tensor_collection(other.__class__):
+            keys1 = set(self.keys())
+            keys2 = set(other.keys())
+            if len(keys1.difference(keys2)) or len(keys1) != len(keys2):
+                keys1 = sorted(
+                    keys1,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                keys2 = sorted(
+                    keys2,
+                    key=lambda key: "".join(key) if isinstance(key, tuple) else key,
+                )
+                raise KeyError(f"keys in tensordicts mismatch, got {keys1} and {keys2}")
+            d = {}
+            for key, item1 in self.items():
+                d[key] = item1 < other.get(key)
+            return TensorDict(source=d, batch_size=self.batch_size, device=self.device)
+        if isinstance(other, (numbers.Number, Tensor)):
+            return TensorDict(
+                {key: value < other for key, value in self.items()},
+                self.batch_size,
+                device=self.device,
+            )
+        return False
+
+    def __setitem__(
+        self,
+        index: IndexType,
+        value: T | dict | numbers.Number | CompatibleType,
+    ) -> None:
+        istuple = isinstance(index, tuple)
+        if istuple or isinstance(index, str):
+            # try:
+            index_unravel = _unravel_key_to_tuple(index)
+            if index_unravel:
+                self._set_tuple(
+                    index_unravel,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE
+                    if isinstance(self, _SubTensorDict)
+                    else False,
+                    validated=False,
+                    non_blocking=False,
+                )
+                return
+
+        # we must use any and because using Ellipsis in index can break with some indices
+        if index is Ellipsis or (
+            isinstance(index, tuple) and any(idx is Ellipsis for idx in index)
+        ):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+
+        if isinstance(value, (TensorDictBase, dict)):
+            indexed_bs = _getitem_batch_size(self.batch_size, index)
+            if isinstance(value, dict):
+                value = self.from_dict_instance(value, batch_size=indexed_bs)
+                # value = self.empty(recurse=True)[index].update(value)
+            if value.batch_size != indexed_bs:
+                if value.shape == indexed_bs[-len(value.shape) :]:
+                    # try to expand on the left (broadcasting)
+                    value = value.expand(indexed_bs)
+                else:
+                    try:
+                        # copy and change batch_size if can't be expanded
+                        value = value.copy()
+                        value.batch_size = indexed_bs
+                    except RuntimeError as err:
+                        raise RuntimeError(
+                            f"indexed destination TensorDict batch size is {indexed_bs} "
+                            f"(batch_size = {self.batch_size}, index={index}), "
+                            f"which differs from the source batch size {value.batch_size}"
+                        ) from err
+
+            keys = set(self.keys())
+            if any(key not in keys for key in value.keys()):
+                subtd = self._get_sub_tensordict(index)
+            for key, item in value.items():
+                if key in keys:
+                    self._set_at_str(
+                        key, item, index, validated=False, non_blocking=False
+                    )
+                else:
+                    subtd.set(key, item, inplace=True, non_blocking=False)
+        else:
+            for key in self.keys():
+                self.set_at_(key, value, index)
+
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            if dim < 0:
+                dim = self.batch_dims + dim
+
+            names = None
+            if self._has_names():
+                names = copy(self.names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
+            return TensorDict(
+                source={key: value.all(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+                names=names,
+            )
+        return all(value.all() for value in self.values())
+
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        if dim is not None and (dim >= self.batch_dims or dim < -self.batch_dims):
+            raise RuntimeError(
+                "dim must be greater than or equal to -tensordict.batch_dims and "
+                "smaller than tensordict.batch_dims"
+            )
+        if dim is not None:
+            if dim < 0:
+                dim = self.batch_dims + dim
+
+            names = None
+            if self._has_names():
+                names = copy(self.names)
+                names = [name for i, name in enumerate(names) if i != dim]
+
+            return TensorDict(
+                source={key: value.any(dim=dim) for key, value in self.items()},
+                batch_size=[b for i, b in enumerate(self.batch_size) if i != dim],
+                device=self.device,
+                names=names,
+            )
+        return any([value.any() for value in self.values()])
+
+    def _cast_reduction(
+        self,
+        *,
+        reduction_name,
+        dim=NO_DEFAULT,
+        keepdim=NO_DEFAULT,
+        tuple_ok=True,
+        **kwargs,
+    ):
+        def proc_dim(dim, tuple_ok=True):
+            if dim is None:
+                return dim
+            if isinstance(dim, tuple):
+                if tuple_ok:
+                    return tuple(_d for d in dim for _d in proc_dim(d, tuple_ok=False))
+                return dim
+            if dim >= self.batch_dims or dim < -self.batch_dims:
+                raise RuntimeError(
+                    "dim must be greater than or equal to -tensordict.batch_dims and "
+                    "smaller than tensordict.batch_dims"
+                )
+            if dim < 0:
+                return (self.batch_dims + dim,)
+            return (dim,)
+
+        if dim is not NO_DEFAULT:
+            dim = proc_dim(dim, tuple_ok=tuple_ok)
+            if not tuple_ok:
+                dim = dim[0]
+        if dim is not NO_DEFAULT or keepdim:
+            names = None
+            if self._has_names():
+                names = copy(self.names)
+                if not keepdim and isinstance(dim, tuple):
+                    names = [name for i, name in enumerate(names) if i not in dim]
+                else:
+                    names = [name for i, name in enumerate(names) if i != dim]
+            if dim is not NO_DEFAULT:
+                kwargs["dim"] = dim
+            if keepdim is not NO_DEFAULT:
+                kwargs["keepdim"] = keepdim
+
+            def reduction(val):
+                result = getattr(val, reduction_name)(
+                    **kwargs,
+                )
+                return result
+
+            if dim not in (None, NO_DEFAULT):
+                if not keepdim:
+                    if isinstance(dim, tuple):
+                        batch_size = [
+                            b for i, b in enumerate(self.batch_size) if i not in dim
+                        ]
+                    else:
+                        batch_size = [
+                            b for i, b in enumerate(self.batch_size) if i != dim
+                        ]
+                else:
+                    if isinstance(dim, tuple):
+                        batch_size = [
+                            b if i not in dim else 1
+                            for i, b in enumerate(self.batch_size)
+                        ]
+                    else:
+                        batch_size = [
+                            b if i != dim else 1 for i, b in enumerate(self.batch_size)
+                        ]
+
+            else:
+                batch_size = [1 for b in self.batch_size]
+
+            return self._fast_apply(
+                reduction,
+                call_on_nested=True,
+                batch_size=torch.Size(batch_size),
+                device=self.device,
+                names=names,
+            )
+
+        def reduction(val):
+            return getattr(val, reduction_name)(**kwargs)
+
+        return self._fast_apply(
+            reduction,
+            call_on_nested=True,
+            batch_size=torch.Size([]),
+            device=self.device,
+            names=None,
+        )
+
+    def _apply_nest(
+        self,
+        fn: Callable,
+        *others: T,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        checked: bool = False,
+        call_on_nested: bool = False,
+        default: Any = NO_DEFAULT,
+        named: bool = False,
+        nested_keys: bool = False,
+        prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable = None,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        if inplace:
+            result = self
+            is_locked = result.is_locked
+        elif out is not None:
+            result = out
+            if out.is_locked:
+                raise RuntimeError(_LOCK_ERROR)
+            is_locked = False
+            if batch_size is not None and batch_size != out.batch_size:
+                raise RuntimeError(
+                    "batch_size and out.batch_size must be equal when both are provided."
+                )
+            if device is not NO_DEFAULT and device != out.device:
+                raise RuntimeError(
+                    "device and out.device must be equal when both are provided."
+                )
+        else:
+
+            def make_result(names=names, batch_size=batch_size):
+                if batch_size is not None and names is None:
+                    # erase names
+                    names = [None] * len(batch_size)
+                return self.empty(batch_size=batch_size, device=device, names=names)
+
+            result = None
+            is_locked = False
+
+        any_set = False
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+
+        for key, item in self.items():
+            if (
+                not call_on_nested
+                and not is_leaf(item.__class__)
+                # and not is_non_tensor(item)
+            ):
+                if default is not NO_DEFAULT:
+                    _others = [_other._get_str(key, default=None) for _other in others]
+                    _others = [
+                        self.empty(recurse=True) if _other is None else _other
+                        for _other in _others
+                    ]
+                else:
+                    _others = [
+                        _other._get_str(key, default=NO_DEFAULT) for _other in others
+                    ]
+
+                item_trsf = item._apply_nest(
+                    fn,
+                    *_others,
+                    inplace=inplace,
+                    batch_size=batch_size,
+                    device=device,
+                    checked=checked,
+                    named=named,
+                    nested_keys=nested_keys,
+                    default=default,
+                    prefix=prefix + (key,),
+                    filter_empty=filter_empty,
+                    is_leaf=is_leaf,
+                    out=out._get_str(key, default=None) if out is not None else None,
+                    **constructor_kwargs,
+                )
+            else:
+                _others = [_other._get_str(key, default=default) for _other in others]
+                if named:
+                    if nested_keys:
+                        item_trsf = fn(
+                            prefix + (key,) if prefix != () else key, item, *_others
+                        )
+                    else:
+                        item_trsf = fn(key, item, *_others)
+                else:
+                    item_trsf = fn(item, *_others)
+            if item_trsf is not None:
+                if not any_set:
+                    if result is None:
+                        result = make_result()
+                    any_set = True
+                if isinstance(self, _SubTensorDict):
+                    result.set(key, item_trsf, inplace=inplace)
+                else:
+                    result._set_str(
+                        key,
+                        item_trsf,
+                        inplace=BEST_ATTEMPT_INPLACE if inplace else False,
+                        validated=checked,
+                        non_blocking=False,
+                    )
+
+        if filter_empty and not any_set:
+            return
+        elif filter_empty is None and not any_set and not self.is_empty():
+            # we raise the deprecation warning only if the tensordict wasn't already empty.
+            # After we introduce the new behaviour, we will have to consider what happens
+            # to empty tensordicts by default: will they disappear or stay?
+            warn(
+                "Your resulting tensordict has no leaves but you did not specify filter_empty=False. "
+                "Currently, this returns an empty tree (filter_empty=True), but from v0.5 it will return "
+                "a None unless filter_empty=False. "
+                "To silence this warning, set filter_empty to the desired value in your call to `apply`.",
+                category=DeprecationWarning,
+            )
+        if result is None:
+            result = make_result()
+
+        if not inplace and is_locked:
+            result.lock_()
+        return result
+
+    # Functorch compatibility
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        td = self
+
+        def _add_batch_dim_wrapper(key, value):
+            if is_tensor_collection(value):
+                return value._add_batch_dim(in_dim=in_dim, vmap_level=vmap_level)
+
+            if isinstance(
+                value, (_BatchedUninitializedParameter, _BatchedUninitializedBuffer)
+            ):
+                value.in_dim = in_dim
+                value.vmap_level = vmap_level
+                return value
+            return _add_batch_dim(value, in_dim, vmap_level)
+
+        out = TensorDict(
+            {key: _add_batch_dim_wrapper(key, value) for key, value in td.items()},
+            batch_size=torch.Size(
+                [b for i, b in enumerate(td.batch_size) if i != in_dim]
+            ),
+            names=[name for i, name in enumerate(td.names) if i != in_dim],
+            _run_checks=False,
+            lock=self.is_locked,
+        )
+        return out
+
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        new_batch_size = list(self.batch_size)
+        new_batch_size.insert(out_dim, batch_size)
+        new_names = list(self.names)
+        new_names.insert(out_dim, None)
+        out = TensorDict(
+            {
+                key: value._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                )
+                if is_tensor_collection(value)
+                else _remove_batch_dim(value, vmap_level, batch_size, out_dim)
+                for key, value in self.items()
+            },
+            batch_size=new_batch_size,
+            names=new_names,
+            lock=self.is_locked,
+        )
+        return out
+
+    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> T:
+        return TensorDict(
+            dict_value,
+            batch_size=self.batch_size,
+            device=self.device,
+        )
+
+    def _index_tensordict(
+        self,
+        index: IndexType,
+        new_batch_size: torch.Size | None = None,
+        names: List[str] | None = None,
+    ) -> T:
+        batch_size = self.batch_size
+        batch_dims = len(batch_size)
+        if (
+            not batch_size
+            and index is not None
+            and (not isinstance(index, tuple) or any(idx is not None for idx in index))
+        ):
+            raise RuntimeError(
+                f"indexing a tensordict with td.batch_dims==0 is not permitted. Got index {index}."
+            )
+        if new_batch_size is not None:
+            batch_size = new_batch_size
+        else:
+            batch_size = _getitem_batch_size(batch_size, index)
+
+        if names is None:
+            names = self._get_names_idx(index)
+
+        source = {}
+        for key, item in self.items():
+            if isinstance(item, TensorDict):
+                # this is the simplest case, we can pre-compute the batch size easily
+                new_batch_size = batch_size + item.batch_size[batch_dims:]
+                source[key] = item._index_tensordict(
+                    index, new_batch_size=new_batch_size
+                )
+            else:
+                source[key] = _get_item(item, index)
+        result = TensorDict(
+            source=source,
+            batch_size=batch_size,
+            device=self.device,
+            names=names,
+            _run_checks=False,
+            # lock=self.is_locked,
+        )
+        if self._is_memmap and _index_preserve_data_ptr(index):
+            result._is_memmap = True
+            result.lock_()
+        elif self._is_shared and _index_preserve_data_ptr(index):
+            result._is_shared = True
+            result.lock_()
+        return result
+
+    def expand(self, *args, **kwargs) -> T:
+        tensordict_dims = self.batch_dims
+        shape = _get_shape_from_args(*args, **kwargs)
+
+        # new shape dim check
+        if len(shape) < len(self.shape):
+            raise RuntimeError(
+                f"the number of sizes provided ({len(shape)}) must be greater or equal to the number of "
+                f"dimensions in the TensorDict ({tensordict_dims})"
+            )
+
+        # new shape compatability check
+        for old_dim, new_dim in zip(self.batch_size, shape[-tensordict_dims:]):
+            if old_dim != 1 and new_dim != old_dim:
+                raise RuntimeError(
+                    "Incompatible expanded shape: The expanded shape length at non-singleton dimension should be same "
+                    f"as the original length. target_shape = {shape}, existing_shape = {self.batch_size}"
+                )
+
+        def _expand(tensor):
+            tensor_shape = tensor.shape
+            tensor_dims = len(tensor_shape)
+            last_n_dims = tensor_dims - tensordict_dims
+            if last_n_dims > 0:
+                new_shape = (*shape, *tensor_shape[-last_n_dims:])
+            else:
+                new_shape = shape
+            return tensor.expand(new_shape)
+
+        names = [None] * (len(shape) - tensordict_dims) + self.names
+        return self._fast_apply(
+            _expand,
+            batch_size=shape,
+            call_on_nested=True,
+            names=names,
+            propagate_lock=True,
+        )
+
+    def _unbind(self, dim: int):
+        batch_size = torch.Size([s for i, s in enumerate(self.batch_size) if i != dim])
+        names = None
+        if self._has_names():
+            names = copy(self.names)
+            names = [name for i, name in enumerate(names) if i != dim]
+        device = self.device
+
+        is_shared = self._is_shared
+        is_memmap = self._is_memmap
+
+        def empty():
+            result = TensorDict(
+                {}, batch_size=batch_size, names=names, _run_checks=False, device=device
+            )
+            result._is_shared = is_shared
+            result._is_memmap = is_memmap
+            return result
+
+        tds = tuple(empty() for _ in range(self.batch_size[dim]))
+
+        def unbind(key, val, tds=tds):
+            unbound = (
+                val.unbind(dim)
+                if not isinstance(val, TensorDictBase)
+                # tensorclass is also unbound using plain unbind
+                else val._unbind(dim)
+            )
+            for td, _val in zip(tds, unbound):
+                td._set_str(
+                    key, _val, validated=True, inplace=False, non_blocking=False
+                )
+
+        for key, val in self.items():
+            unbind(key, val)
+        return tds
+
+    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
+        # we must use slices to keep the storage of the tensors
+        WRONG_TYPE = "split(): argument 'split_size' must be int or list of ints"
+        batch_size = self.batch_size
+        batch_sizes = []
+        batch_dims = len(batch_size)
+        if dim < 0:
+            dim = len(batch_size) + dim
+        if dim >= batch_dims or dim < 0:
+            raise IndexError(
+                f"Dimension out of range (expected to be in range of [-{self.batch_dims}, {self.batch_dims - 1}], but got {dim})"
+            )
+        max_size = batch_size[dim]
+        if isinstance(split_size, int):
+            idx0 = 0
+            idx1 = min(max_size, split_size)
+            split_sizes = [slice(idx0, idx1)]
+            batch_sizes.append(
+                torch.Size(
+                    tuple(
+                        d if i != dim else idx1 - idx0 for i, d in enumerate(batch_size)
+                    )
+                )
+            )
+            while idx1 < max_size:
+                idx0 = idx1
+                idx1 = min(max_size, idx1 + split_size)
+                split_sizes.append(slice(idx0, idx1))
+                batch_sizes.append(
+                    torch.Size(
+                        tuple(
+                            d if i != dim else idx1 - idx0
+                            for i, d in enumerate(batch_size)
+                        )
+                    )
+                )
+        elif isinstance(split_size, (list, tuple)):
+            if len(split_size) == 0:
+                raise RuntimeError("Insufficient number of elements in split_size.")
+            try:
+                idx0 = 0
+                idx1 = split_size[0]
+                split_sizes = [slice(idx0, idx1)]
+                batch_sizes.append(
+                    torch.Size(
+                        tuple(
+                            d if i != dim else idx1 - idx0
+                            for i, d in enumerate(batch_size)
+                        )
+                    )
+                )
+                for idx in split_size[1:]:
+                    idx0 = idx1
+                    idx1 = min(max_size, idx1 + idx)
+                    split_sizes.append(slice(idx0, idx1))
+                    batch_sizes.append(
+                        torch.Size(
+                            tuple(
+                                d if i != dim else idx1 - idx0
+                                for i, d in enumerate(batch_size)
+                            )
+                        )
+                    )
+            except TypeError:
+                raise TypeError(WRONG_TYPE)
+
+            if idx1 < batch_size[dim]:
+                raise RuntimeError(
+                    f"Split method expects split_size to sum exactly to {self.batch_size[dim]} (tensor's size at dimension {dim}), but got split_size={split_size}"
+                )
+        else:
+            raise TypeError(WRONG_TYPE)
+        index = (slice(None),) * dim
+        names = self.names
+        return tuple(
+            self._index_tensordict(index + (ss,), new_batch_size=bs, names=names)
+            for ss, bs in zip(split_sizes, batch_sizes)
+        )
+
+    def masked_select(self, mask: Tensor) -> T:
+        d = {}
+        mask_expand = mask
+        while mask_expand.ndimension() > self.batch_dims:
+            mndim = mask_expand.ndimension()
+            mask_expand = mask_expand.squeeze(-1)
+            if mndim == mask_expand.ndimension():  # no more squeeze
+                break
+        for key, value in self.items():
+            d[key] = value[mask_expand]
+        dim = int(mask.sum().item())
+        other_dim = self.shape[mask.ndim :]
+        return TensorDict(
+            device=self.device, source=d, batch_size=torch.Size([dim, *other_dim])
+        )
+
+    def _view(
+        self,
+        *args,
+        **kwargs,
+    ) -> T:
+        shape = _get_shape_from_args(*args, **kwargs)
+        if any(dim < 0 for dim in shape):
+            shape = infer_size_impl(shape, self.numel())
+        if torch.Size(shape) == self.shape:
+            return self
+        batch_dims = self.batch_dims
+
+        def _view(tensor):
+            return tensor.view((*shape, *tensor.shape[batch_dims:]))
+
+        result = self._fast_apply(
+            _view, batch_size=shape, call_on_nested=True, propagate_lock=True
+        )
+        self._maybe_set_shared_attributes(result)
+        return result
+
+    def reshape(
+        self,
+        *args,
+        **kwargs,
+    ) -> T:
+        shape = _get_shape_from_args(*args, **kwargs)
+        if any(dim < 0 for dim in shape):
+            shape = infer_size_impl(shape, self.numel())
+            shape = torch.Size(shape)
+        if torch.Size(shape) == self.shape:
+            return self
+        batch_dims = self.batch_dims
+
+        def _reshape(tensor):
+            return tensor.reshape((*shape, *tensor.shape[batch_dims:]))
+
+        return self._fast_apply(
+            _reshape, batch_size=shape, call_on_nested=True, propagate_lock=True
+        )
+
+    def _transpose(self, dim0, dim1):
+        def _transpose(tensor):
+            return tensor.transpose(dim0, dim1)
+
+        batch_size = list(self.batch_size)
+        v0 = batch_size[dim0]
+        v1 = batch_size[dim1]
+        batch_size[dim1] = v0
+        batch_size[dim0] = v1
+        if self._has_names():
+            names = self.names
+            names = [
+                names[dim0] if i == dim1 else names[dim1] if i == dim0 else names[i]
+                for i in range(self.ndim)
+            ]
+        else:
+            names = None
+        result = self._fast_apply(
+            _transpose,
+            batch_size=torch.Size(batch_size),
+            call_on_nested=True,
+            names=names,
+            propagate_lock=True,
+        )
+        self._maybe_set_shared_attributes(result)
+        return result
+
+    def _permute(self, *args, **kwargs):
+        dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
+        dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
+        if any(dim < 0 or dim >= self.ndim for dim in dims_list):
+            raise ValueError(
+                "Received an permutation order incompatible with the tensordict shape."
+            )
+        # note: to allow this to work recursively, we must allow permutation order with fewer elements than dims,
+        # as long as this list is complete.
+        if not np.array_equal(sorted(dims_list), range(len(dims_list))):
+            raise ValueError(
+                f"Cannot compute the permutation, got dims={dims_list} but expected a permutation of {list(range(len(dims_list)))}."
+            )
+        if not len(dims_list) and not self.batch_dims:
+            return self
+        if np.array_equal(dims_list, range(len(dims_list))):
+            return self
+
+        def _permute(tensor):
+            return tensor.permute(*dims_list, *range(len(dims_list), tensor.ndim))
+
+        batch_size = self.batch_size
+        batch_size = [batch_size[p] for p in dims_list] + list(
+            batch_size[len(dims_list) :]
+        )
+        if self._has_names():
+            names = self.names
+            names = [names[i] for i in dims_list]
+        else:
+            names = None
+        result = self._fast_apply(
+            _permute,
+            batch_size=batch_size,
+            call_on_nested=True,
+            names=names,
+            propagate_lock=True,
+        )
+        self._maybe_set_shared_attributes(result)
+        return result
+
+    def _squeeze(self, dim=None):
+        batch_size = self.batch_size
+        if dim is None:
+            names = list(self.names)
+            batch_size, names = zip(
+                *[(size, name) for size, name in zip(batch_size, names) if size != 1]
+            )
+            batch_size = torch.Size(batch_size)
+            if batch_size == self.batch_size:
+                return self
+
+            # we only want to squeeze dimensions lower than the batch dim, and view
+            # is the perfect op for this
+            def _squeeze(tensor):
+                return tensor.view(*batch_size, *tensor.shape[self.batch_dims :])
+
+            return self._fast_apply(
+                _squeeze,
+                batch_size=batch_size,
+                names=names,
+                inplace=False,
+                call_on_nested=True,
+                propagate_lock=True,
+            )
+        # make the dim positive
+        if dim < 0:
+            newdim = self.batch_dims + dim
+        else:
+            newdim = dim
+
+        if (newdim >= self.batch_dims) or (newdim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims - 1` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        if batch_size[dim] != 1:
+            return self
+        batch_size = list(batch_size)
+        batch_size.pop(dim)
+        batch_size = list(batch_size)
+        names = list(self.names)
+        names.pop(dim)
+
+        result = self._fast_apply(
+            lambda x: x.squeeze(newdim),
+            batch_size=batch_size,
+            names=names,
+            inplace=False,
+            call_on_nested=True,
+            propagate_lock=True,
+        )
+        self._maybe_set_shared_attributes(result)
+        return result
+
+    def _unsqueeze(self, dim):
+        # make the dim positive
+        if dim < 0:
+            newdim = self.batch_dims + dim + 1
+        else:
+            newdim = dim
+
+        if (newdim > self.batch_dims) or (newdim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims - 1` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        batch_size = list(self.batch_size)
+        batch_size.insert(newdim, 1)
+        batch_size = torch.Size(batch_size)
+
+        names = copy(self.names)
+        names.insert(newdim, None)
+
+        def _unsqueeze(tensor):
+            return tensor.unsqueeze(newdim)
+
+        result = self._fast_apply(
+            _unsqueeze,
+            batch_size=batch_size,
+            names=names,
+            inplace=False,
+            call_on_nested=True,
+            propagate_lock=True,
+        )
+        self._maybe_set_shared_attributes(result)
+        return result
+
+    @classmethod
+    def from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
+        """Returns a TensorDict created from a dictionary or another :class:`~.tensordict.TensorDict`.
+
+        If ``batch_size`` is not specified, returns the maximum batch size possible.
+
+        This function works on nested dictionaries too, or can be used to determine the
+        batch-size of a nested tensordict.
+
+        Args:
+            input_dict (dictionary, optional): a dictionary to use as a data source
+                (nested keys compatible).
+            batch_size (iterable of int, optional): a batch size for the tensordict.
+            device (torch.device or compatible type, optional): a device for the TensorDict.
+            batch_dims (int, optional): the ``batch_dims`` (ie number of leading dimensions
+                to be considered for ``batch_size``). Exclusinve with ``batch_size``.
+                Note that this is the __maximum__ number of batch dims of the tensordict,
+                a smaller number is tolerated.
+
+        Examples:
+            >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
+            >>> print(TensorDict.from_dict(input_dict))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> # nested dict: the nested TensorDict can have a different batch-size
+            >>> # as long as its leading dims match.
+            >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
+            >>> print(TensorDict.from_dict(input_dict))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 4]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> # we can also use this to work out the batch sie of a tensordict
+            >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
+            >>> print(TensorDict.from_dict(input_td))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 4]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+
+        """
+        if batch_dims is not None and batch_size is not None:
+            raise ValueError(
+                "Cannot pass both batch_size and batch_dims to `from_dict`."
+            )
+
+        batch_size_set = torch.Size(()) if batch_size is None else batch_size
+        input_dict = copy(input_dict)
+        for key, value in list(input_dict.items()):
+            if isinstance(value, (dict,)):
+                # we don't know if another tensor of smaller size is coming
+                # so we can't be sure that the batch-size will still be valid later
+                input_dict[key] = TensorDict.from_dict(
+                    value, batch_size=[], device=device, batch_dims=None
+                )
+        # _run_checks=False breaks because a tensor may have the same batch-size as the tensordict
+        out = cls(
+            input_dict,
+            batch_size=batch_size_set,
+            device=device,
+        )
+        if batch_size is None:
+            _set_max_batch_size(out, batch_dims)
+        else:
+            out.batch_size = batch_size
+        return out
+
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        if batch_dims is not None and batch_size is not None:
+            raise ValueError(
+                "Cannot pass both batch_size and batch_dims to `from_dict`."
+            )
+        from tensordict import TensorDict
+
+        batch_size_set = torch.Size(()) if batch_size is None else batch_size
+        input_dict = copy(input_dict)
+        for key, value in list(input_dict.items()):
+            if isinstance(value, (dict,)):
+                cur_value = self.get(key, None)
+                if cur_value is not None:
+                    input_dict[key] = cur_value.from_dict_instance(
+                        value, batch_size=[], device=device, batch_dims=None
+                    )
+                    continue
+                # we don't know if another tensor of smaller size is coming
+                # so we can't be sure that the batch-size will still be valid later
+                input_dict[key] = TensorDict.from_dict(
+                    value, batch_size=[], device=device, batch_dims=None
+                )
+        out = TensorDict.from_dict(
+            input_dict,
+            batch_size=batch_size_set,
+            device=device,
+        )
+        if batch_size is None:
+            _set_max_batch_size(out, batch_dims)
+        else:
+            out.batch_size = batch_size
+        return out
+
+    @staticmethod
+    def _parse_batch_size(
+        source: T | dict,
+        batch_size: Sequence[int] | torch.Size | int | None = None,
+    ) -> torch.Size:
+        try:
+            return torch.Size(batch_size)
+        except Exception:
+            if batch_size is None:
+                return torch.Size([])
+            elif isinstance(batch_size, Number):
+                return torch.Size([batch_size])
+            elif isinstance(source, TensorDictBase):
+                return source.batch_size
+            raise ValueError(
+                "batch size was not specified when creating the TensorDict "
+                "instance and it could not be retrieved from source."
+            )
+
+    @property
+    def batch_dims(self) -> int:
+        return len(self.batch_size)
+
+    @batch_dims.setter
+    def batch_dims(self, value: int) -> None:
+        raise RuntimeError(
+            f"Setting batch dims on {self.__class__.__name__} instances is "
+            f"not allowed."
+        )
+
+    def _has_names(self):
+        return self._td_dim_names is not None
+
+    def _erase_names(self):
+        self._td_dim_names = None
+
+    @property
+    def names(self):
+        names = self._td_dim_names
+        if names is None:
+            return [None for _ in range(self.batch_dims)]
+        return names
+
+    def _get_names_idx(self, idx):
+        if not self._has_names():
+            names = None
+        else:
+
+            def is_boolean(idx):
+                try:
+                    from functorch import dim as ftdim
+
+                except ImportError:
+                    from tensordict.utils import _ftdim_mock as ftdim
+
+                if isinstance(idx, ftdim.Dim):
+                    return None
+                if isinstance(idx, tuple) and len(idx) == 1:
+                    return is_boolean(idx[0])
+                if hasattr(idx, "dtype") and idx.dtype is torch.bool:
+                    return idx.ndim
+                return None
+
+            num_boolean_dim = is_boolean(idx)
+            names = self.names
+            if num_boolean_dim:
+                names = [None] + names[num_boolean_dim:]
+            else:
+                if not isinstance(idx, tuple):
+                    idx = (idx,)
+                if len([_idx for _idx in idx if _idx is not None]) < self.ndim:
+                    idx = (*idx, Ellipsis)
+                idx_names = convert_ellipsis_to_idx(idx, self.batch_size)
+                # this will convert a [None, :, :, 0, None, 0] in [None, 0, 1, None, 3]
+                count = 0
+                idx_to_take = []
+                no_more_tensors = False
+                for _idx in idx_names:
+                    if _idx is None:
+                        idx_to_take.append(None)
+                    elif _is_number(_idx):
+                        count += 1
+                    elif isinstance(_idx, (torch.Tensor, np.ndarray)):
+                        if not no_more_tensors:
+                            idx_to_take.extend([count] * _idx.ndim)
+                            count += 1
+                            no_more_tensors = True
+                        else:
+                            # skip this one
+                            count += 1
+                    else:
+                        idx_to_take.append(count)
+                        count += 1
+                names = [names[i] if i is not None else None for i in idx_to_take]
+        return names
+
+    @names.setter
+    def names(self, value):
+        # we don't run checks on types for efficiency purposes
+        if value is None:
+            self._rename_subtds(value)
+            self._erase_names()
+            return
+        value = list(value)
+        num_none = sum(v is None for v in value)
+        if num_none:
+            num_none -= 1
+        if len(set(value)) != len(value) - num_none:
+            raise ValueError(f"Some dimension names are non-unique: {value}.")
+        if len(value) != self.batch_dims:
+            raise ValueError(
+                "the length of the dimension names must equate the tensordict batch_dims attribute. "
+                f"Got {value} for batch_dims {self.batch_dims}."
+            )
+        self._rename_subtds(value)
+        self._td_dim_names = list(value)
+
+    def _rename_subtds(self, names):
+        if names is None:
+            for item in self._tensordict.values():
+                if _is_tensor_collection(type(item)):
+                    item._erase_names()
+            return
+        for item in self._tensordict.values():
+            if _is_tensor_collection(item.__class__):
+                item_names = item.names
+                td_names = list(names) + item_names[len(names) :]
+                item.rename_(*td_names)
+
+    @property
+    def device(self) -> torch.device | None:
+        """Device of the tensordict.
+
+        Returns `None` if device hasn't been provided in the constructor or set via `tensordict.to(device)`.
+
+        """
+        return self._device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        raise RuntimeError(
+            "device cannot be set using tensordict.device = device, "
+            "because device cannot be updated in-place. To update device, use "
+            "tensordict.to(new_device), which will return a new tensordict "
+            "on the new device."
+        )
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    # Checks
+    def _check_is_shared(self) -> bool:
+        share_list = [_is_shared(value) for value in self.values()]
+        if any(share_list) and not all(share_list):
+            shared_str = ", ".join(
+                [f"{key}: {_is_shared(value)}" for key, value in self.items()]
+            )
+            raise RuntimeError(
+                f"tensors must be either all shared or not, but mixed "
+                f"features is not allowed. "
+                f"Found: {shared_str}"
+            )
+        return all(share_list) and len(share_list) > 0
+
+    def _check_device(self) -> None:
+        devices = {value.device for value in self.values()}
+        if self.device is not None and len(devices) >= 1 and devices != {self.device}:
+            raise RuntimeError(
+                f"TensorDict.device is {self._device}, but elements have "
+                f"device values {devices}. If TensorDict.device is set then "
+                "all elements must share that device."
+            )
+
+    @lock_blocked
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        return self._tensordict.popitem()
+
+    def pin_memory(self) -> T:
+        def pin_mem(tensor):
+            return tensor.pin_memory()
+
+        return self._fast_apply(pin_mem, propagate_lock=True)
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ) -> T:
+        if inplace is not False:
+            best_attempt = inplace is BEST_ATTEMPT_INPLACE
+            inplace = self._convert_inplace(inplace, key)
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+        if not inplace:
+            if self._is_locked and not ignore_lock:
+                raise RuntimeError(_LOCK_ERROR)
+            self._tensordict[key] = value
+        else:
+            try:
+                dest = self._get_str(key, default=NO_DEFAULT)
+                if best_attempt and _is_tensor_collection(dest.__class__):
+                    dest.update(value, inplace=True, non_blocking=non_blocking)
+                else:
+                    if dest is not value:
+                        try:
+                            dest.copy_(value, non_blocking=non_blocking)
+                        except RuntimeError:
+                            # if we're updating a param and the storages match, nothing needs to be done
+                            if not (
+                                isinstance(dest, torch.Tensor)
+                                and dest.data.untyped_storage().data_ptr()
+                                == value.data.untyped_storage().data_ptr()
+                            ):
+                                raise
+            except KeyError as err:
+                raise err
+            except Exception as err:
+                raise ValueError(
+                    f"Failed to update '{key}' in tensordict {self}"
+                ) from err
+        return self
+
+    def _set_dict(
+        self,
+        d: dict[str, CompatibleType],
+        *,
+        validated: bool,
+    ):
+        if not validated:
+            raise RuntimeError("Not Implemented for non-validated inputs")
+        self._tensordict = d
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        non_blocking: bool = False,
+    ) -> T:
+        if len(key) == 1:
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        td = self._get_str(key[0], None)
+        if td is None:
+            td = self._create_nested_str(key[0])
+            inplace = False
+        elif not _is_tensor_collection(td.__class__):
+            raise KeyError(
+                f"The entry {key[0]} is already present in tensordict {self}."
+            )
+        td._set_tuple(
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        tensor_in = self._get_str(key, NO_DEFAULT)
+
+        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+            warn(
+                "Multiple indexing can lead to unexpected behaviours when "
+                "setting items, for instance `td[idx1][idx2] = other` may "
+                "not write to the desired location if idx1 is a list/tensor."
+            )
+            tensor_in = _sub_index(tensor_in, idx)
+            tensor_in.copy_(value, non_blocking=non_blocking)
+        else:
+            tensor_out = _set_item(
+                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
+            )
+            if tensor_in is not tensor_out:
+                if self._is_shared or self._is_memmap:
+                    raise RuntimeError(
+                        "You're attempting to update a leaf in-place with a shared "
+                        "tensordict, but the new value does not match the previous. "
+                        "If you're using NonTensorData, see the class documentation "
+                        "to see how to properly pre-allocate memory in shared contexts."
+                    )
+                # this happens only when a NonTensorData becomes a NonTensorStack
+                # so it is legitimate (there is no in-place modification of a tensor
+                # that was expected to happen but didn't).
+                # For this reason we can ignore the locked attribute of the td.
+                self._set_str(
+                    key,
+                    tensor_out,
+                    validated=True,
+                    inplace=False,
+                    ignore_lock=True,
+                    non_blocking=non_blocking,
+                )
+
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
+        if len(key) == 1:
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
+        if key[0] not in self.keys():
+            # this won't work
+            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
+        else:
+            td = self._get_str(key[0], NO_DEFAULT)
+        td._set_at_tuple(
+            key[1:], value, idx, validated=validated, non_blocking=non_blocking
+        )
+        return self
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> T:
+        key = _unravel_key_to_tuple(key)
+        if len(key) > 1:
+            td, subkey = _get_leaf_tensordict(self, key)
+            td.del_(subkey)
+            return self
+
+        del self._tensordict[key[0]]
+        return self
+
+    @lock_blocked
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> T:
+        # these checks are not perfect, tuples that are not tuples of strings or empty
+        # tuples could go through but (1) it will raise an error anyway and (2)
+        # those checks are expensive when repeated often.
+        if old_key == new_key:
+            return self
+        if not isinstance(old_key, (str, tuple)):
+            raise TypeError(
+                f"Expected old_name to be a string or a tuple of strings but found {type(old_key)}"
+            )
+        if not isinstance(new_key, (str, tuple)):
+            raise TypeError(
+                f"Expected new_name to be a string or a tuple of strings but found {type(new_key)}"
+            )
+        if safe and (new_key in self.keys(include_nested=True)):
+            raise KeyError(f"key {new_key} already present in TensorDict.")
+
+        if isinstance(new_key, str):
+            self._set_str(
+                new_key,
+                self.get(old_key),
+                inplace=False,
+                validated=True,
+                non_blocking=False,
+            )
+        else:
+            self._set_tuple(
+                new_key,
+                self.get(old_key),
+                inplace=False,
+                validated=True,
+                non_blocking=False,
+            )
+        self.del_(old_key)
+        return self
+
+    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> TensorDict:
+        # if not isinstance(key, str):
+        #     raise ValueError("_stack_onto_ expects string keys.")
+        for key in self.keys():
+            vals = [item._get_str(key, None) for item in list_item]
+            if all(v is None for v in vals):
+                continue
+            dest = self._get_str(key, NO_DEFAULT)
+            torch.stack(
+                vals,
+                dim=dim,
+                out=dest,
+            )
+        return self
+
+    def entry_class(self, key: NestedKey) -> type:
+        return type(self.get(key))
+
+    def _stack_onto_at_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+        idx: IndexType,
+    ) -> TensorDict:
+        if not isinstance(idx, tuple):
+            idx = (idx,)
+        idx = convert_ellipsis_to_idx(idx, self.batch_size)
+        for key in self.keys():
+            vals = [td._get_str(key, NO_DEFAULT) for td in list_item]
+            if all(v is None for v in vals):
+                continue
+            v = self._get_str(key, NO_DEFAULT)
+            v_idx = v[idx]
+            if v.data_ptr() != v_idx.data_ptr():
+                raise IndexError(
+                    f"Index {idx} is incompatible with stack(..., out=data) as the storages of the indexed tensors differ."
+                )
+            torch.stack(vals, dim=dim, out=v_idx)
+            # raise ValueError(
+            #     f"Cannot stack onto an indexed tensor with index {idx} "
+            #     f"as its storage differs."
+            # )
+        return self
+
+    def _get_str(self, key, default):
+        first_key = key
+        out = self._tensordict.get(first_key, None)
+        if out is None:
+            return self._default_get(first_key, default)
+        return out
+
+    def _get_tuple(self, key, default):
+        first = self._get_str(key[0], default)
+        if len(key) == 1 or first is default:
+            return first
+        try:
+            return first._get_tuple(key[1:], default=default)
+        except AttributeError as err:
+            if "has no attribute" in str(err):
+                raise ValueError(
+                    f"Expected a TensorDictBase instance but got {type(first)} instead"
+                    f" for key '{key[1:]}' in tensordict:\n{self}."
+                )
+
+    def share_memory_(self) -> T:
+        if self.is_memmap():
+            raise RuntimeError(
+                "memmap and shared memory are mutually exclusive features."
+            )
+        if self.device is not None and self.device.type == "cuda":
+            # cuda tensors are shared by default
+            return self
+        for value in self.values():
+            if (
+                isinstance(value, Tensor)
+                and value.device.type == "cpu"
+                or _is_tensor_collection(value.__class__)
+            ):
+                value.share_memory_()
+        self._is_shared = True
+        self.lock_()
+        return self
+
+    def detach_(self) -> T:
+        for value in self.values():
+            value.detach_()
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None,
+        copy_existing: bool,
+        executor,
+        futures,
+        inplace,
+        like,
+        share_non_tensor,
+    ) -> T:
+
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            metadata = {}
+        if inplace and self._is_shared:
+            raise RuntimeError(
+                "memmap and shared memory are mutually exclusive features."
+            )
+        dest = (
+            self
+            if inplace
+            else TensorDict(
+                {},
+                batch_size=self.batch_size,
+                names=self.names if self._has_names() else None,
+                device=torch.device("cpu"),
+            )
+        )
+
+        # We must set these attributes before memmapping because we need the metadata
+        # to match the tensordict content.
+        if inplace:
+            self._is_memmap = True
+            self._is_shared = False  # since they are mutually exclusive
+            self._device = torch.device("cpu")
+        else:
+            dest._is_memmap = True
+            dest._is_shared = False  # since they are mutually exclusive
+
+        for key, value in self.items():
+            type_value = type(value)
+            if _is_tensor_collection(type_value):
+                dest._tensordict[key] = value._memmap_(
+                    prefix=prefix / key if prefix is not None else None,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=inplace,
+                    like=like,
+                    share_non_tensor=share_non_tensor,
+                )
+                if prefix is not None:
+                    _update_metadata(
+                        metadata=metadata, key=key, value=value, is_collection=True
+                    )
+                continue
+            else:
+
+                if executor is None:
+                    _populate_memmap(
+                        dest=dest,
+                        value=value,
+                        key=key,
+                        copy_existing=copy_existing,
+                        prefix=prefix,
+                        like=like,
+                    )
+                else:
+                    futures.append(
+                        executor.submit(
+                            _populate_memmap,
+                            dest=dest,
+                            value=value,
+                            key=key,
+                            copy_existing=copy_existing,
+                            prefix=prefix,
+                            like=like,
+                        )
+                    )
+                if prefix is not None:
+                    _update_metadata(
+                        metadata=metadata, key=key, value=value, is_collection=False
+                    )
+
+        if prefix is not None:
+            if executor is None:
+                _save_metadata(
+                    dest,
+                    prefix,
+                    metadata=metadata,
+                )
+            else:
+                futures.append(executor.submit(_save_metadata, dest, prefix, metadata))
+        dest._is_locked = True
+        dest._memmap_prefix = prefix
+        return dest
+
+    @classmethod
+    def _load_memmap(
+        cls,
+        prefix: str,
+        metadata: dict,
+        device: torch.device | None = None,
+        out=None,
+    ) -> T:
+        if metadata["device"] == "None":
+            metadata["device"] = None
+        else:
+            metadata["device"] = torch.device(metadata["device"])
+        metadata["shape"] = torch.Size(metadata["shape"])
+
+        if out is None:
+            result = cls(
+                {},
+                batch_size=metadata.pop("shape"),
+                device=metadata.pop("device") if device is None else device,
+            )
+        else:
+            result = out
+
+        paths = set()
+        for key, entry_metadata in metadata.items():
+            if not isinstance(entry_metadata, dict):
+                # there can be other metadata
+                continue
+            type_value = entry_metadata.get("type", None)
+            if type_value is not None:
+                paths.add(key)
+                continue
+            dtype = entry_metadata.get("dtype", None)
+            shape = entry_metadata.get("shape", None)
+            if (
+                not (prefix / f"{key}.memmap").exists()
+                or dtype is None
+                or shape is None
+            ):
+                # invalid dict means
+                continue
+            if (
+                device is None or device != torch.device("meta")
+            ) and not torch._guards.active_fake_mode():
+                if entry_metadata.get("is_nested", False):
+                    # The shape is the shape of the shape, get the shape from it
+                    shape = MemoryMappedTensor.from_filename(
+                        (prefix / f"{key}.memmap").with_suffix(".shape.memmap"),
+                        shape=shape,
+                        dtype=torch.long,
+                    )
+                else:
+                    shape = torch.Size(shape)
+                tensor = MemoryMappedTensor.from_filename(
+                    dtype=_STRDTYPE2DTYPE[dtype],
+                    shape=shape,
+                    filename=str(prefix / f"{key}.memmap"),
+                )
+                if device is not None:
+                    tensor = tensor.to(device, non_blocking=True)
+            else:
+                tensor = torch.zeros(
+                    torch.Size(shape),
+                    device=device,
+                    dtype=_STRDTYPE2DTYPE[dtype],
+                )
+            result._set_str(
+                key,
+                tensor,
+                validated=True,
+                inplace=False,
+                non_blocking=False,
+            )
+        # iterate over folders and load them
+        for path in prefix.iterdir():
+            if path.is_dir() and path.parts[-1] in paths:
+                key = path.parts[-1]  # path.parts[len(prefix.parts) :]
+                existing_elt = result._get_str(key, default=None)
+                if existing_elt is not None:
+                    existing_elt.load_memmap_(path)
+                else:
+                    result._set_str(
+                        key,
+                        TensorDict.load_memmap(path, device=device, non_blocking=True),
+                        inplace=False,
+                        validated=False,
+                    )
+        result._memmap_prefix = prefix
+        return result
+
+    def _make_memmap_subtd(self, key):
+        """Creates a sub-tensordict given a tuple key."""
+        result = self
+        for key_str in key:
+            result_tmp = result._get_str(key_str, default=None)
+            if result_tmp is None:
+                result_tmp = result.empty()
+                if result._memmap_prefix is not None:
+                    result_tmp.memmap_(prefix=result._memmap_prefix / key_str)
+                    metadata = _load_metadata(result._memmap_prefix)
+                    _update_metadata(
+                        metadata=metadata,
+                        key=key_str,
+                        value=result_tmp,
+                        is_collection=True,
+                    )
+                    _save_metadata(
+                        result, prefix=result._memmap_prefix, metadata=metadata
+                    )
+                result._tensordict[key_str] = result_tmp
+            result = result_tmp
+        return result
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        if not self.is_memmap():
+            raise RuntimeError(
+                "Can only make a memmap tensor within a memory-mapped tensordict."
+            )
+
+        key = unravel_key(key)
+        if isinstance(key, tuple):
+            last_node = self._make_memmap_subtd(key[:-1])
+            last_key = key[-1]
+        else:
+            last_node = self
+            last_key = key
+        if last_key in last_node.keys():
+            raise RuntimeError(
+                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
+                f"overwriting it."
+            )
+        if dtype is None:
+            dtype = torch.get_default_dtype()
+        if last_node._memmap_prefix is not None:
+            metadata = _load_metadata(last_node._memmap_prefix)
+            memmap_tensor = _populate_empty(
+                key=last_key,
+                dest=last_node,
+                prefix=last_node._memmap_prefix,
+                shape=shape,
+                dtype=dtype,
+            )
+            _update_metadata(
+                metadata=metadata,
+                key=last_key,
+                value=memmap_tensor,
+                is_collection=False,
+            )
+            _save_metadata(
+                last_node, prefix=last_node._memmap_prefix, metadata=metadata
+            )
+        else:
+            memmap_tensor = MemoryMappedTensor.empty(shape=shape, dtype=dtype)
+
+        last_node._set_str(
+            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
+        )
+
+        return memmap_tensor
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        if not self.is_memmap():
+            raise RuntimeError(
+                "Can only make a memmap tensor within a memory-mapped tensordict."
+            )
+
+        key = unravel_key(key)
+        if isinstance(key, tuple):
+            last_node = self._make_memmap_subtd(key[:-1])
+            last_key = key[-1]
+        else:
+            last_node = self
+            last_key = key
+        if last_key in last_node.keys():
+            raise RuntimeError(
+                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
+                f"overwriting it."
+            )
+        if dtype is None:
+            dtype = torch.get_default_dtype()
+
+        if last_node._memmap_prefix is not None:
+            metadata = _load_metadata(last_node._memmap_prefix)
+            memmap_tensor = _populate_storage(
+                key=last_key,
+                dest=last_node,
+                prefix=last_node._memmap_prefix,
+                storage=storage,
+                shape=shape,
+                dtype=dtype,
+            )
+            _update_metadata(
+                metadata=metadata,
+                key=last_key,
+                value=memmap_tensor,
+                is_collection=False,
+            )
+            _save_metadata(
+                last_node, prefix=last_node._memmap_prefix, metadata=metadata
+            )
+        else:
+            memmap_tensor = MemoryMappedTensor.from_storage(
+                storage=storage, shape=shape, dtype=dtype
+            )
+
+        last_node._set_str(
+            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
+        )
+
+        return memmap_tensor
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        if not self.is_memmap():
+            raise RuntimeError(
+                "Can only make a memmap tensor within a memory-mapped tensordict."
+            )
+
+        key = unravel_key(key)
+        if isinstance(key, tuple):
+            last_node = self._make_memmap_subtd(key[:-1])
+            last_key = key[-1]
+        else:
+            last_node = self
+            last_key = key
+        if last_key in last_node.keys():
+            raise RuntimeError(
+                f"The key {last_key} already exists within the target tensordict. Delete that entry before "
+                f"overwriting it."
+            )
+
+        if last_node._memmap_prefix is not None:
+            metadata = _load_metadata(last_node._memmap_prefix)
+            memmap_tensor = _populate_memmap(
+                dest=last_node,
+                value=tensor,
+                key=last_key,
+                copy_existing=True,
+                prefix=last_node._memmap_prefix,
+                like=not copy_data,
+            )
+            _update_metadata(
+                metadata=metadata,
+                key=last_key,
+                value=memmap_tensor,
+                is_collection=False,
+            )
+            _save_metadata(
+                last_node, prefix=last_node._memmap_prefix, metadata=metadata
+            )
+        else:
+            memmap_tensor = MemoryMappedTensor.from_tensor(tensor)
+
+        last_node._set_str(
+            last_key, memmap_tensor, validated=False, inplace=False, ignore_lock=True
+        )
+
+        return memmap_tensor
+
+    def to(self, *args, **kwargs: Any) -> T:
+        non_blocking = kwargs.pop("non_blocking", None)
+        device, dtype, _, convert_to_format, batch_size = _parse_to(*args, **kwargs)
+        result = self
+
+        if device is not None and dtype is None and device == self.device:
+            return result
+
+        if non_blocking is None:
+            sub_non_blocking = True
+            non_blocking = False
+        else:
+            sub_non_blocking = non_blocking
+
+        if convert_to_format is not None:
+
+            def to(tensor):
+                return tensor.to(
+                    device,
+                    dtype,
+                    non_blocking=sub_non_blocking,
+                    convert_to_format=convert_to_format,
+                )
+
+        else:
+
+            def to(tensor):
+                return tensor.to(
+                    device=device, dtype=dtype, non_blocking=sub_non_blocking
+                )
+
+        apply_kwargs = {}
+        if device is not None or dtype is not None:
+            apply_kwargs["device"] = device if device is not None else self.device
+            apply_kwargs["batch_size"] = batch_size
+            result = result._fast_apply(to, propagate_lock=True, **apply_kwargs)
+        elif batch_size is not None:
+            result.batch_size = batch_size
+        if device is not None and sub_non_blocking and not non_blocking:
+            self._sync_all()
+        return result
+
+    def where(self, condition, other, *, out=None, pad=None):
+        if _is_tensor_collection(other.__class__):
+
+            def func(tensor, _other, key):
+                if tensor is None:
+                    if pad is not None:
+                        tensor = _other
+                        _other = torch.tensor(pad, dtype=_other.dtype)
+                    else:
+                        raise KeyError(
+                            f"Key {key} not found and no pad value provided."
+                        )
+                    cond = expand_as_right(~condition, tensor)
+                elif _other is None:
+                    if pad is not None:
+                        _other = torch.tensor(pad, dtype=tensor.dtype)
+                    else:
+                        raise KeyError(
+                            f"Key {key} not found and no pad value provided."
+                        )
+                    cond = expand_as_right(condition, tensor)
+                else:
+                    cond = expand_as_right(condition, tensor)
+                return torch.where(
+                    condition=cond,
+                    input=tensor,
+                    other=_other,
+                )
+
+            result = self.empty() if out is None else out
+            other_keys = set(other.keys())
+            # we turn into a list because out could be = to self!
+            for key in list(self.keys()):
+                tensor = self._get_str(key, default=NO_DEFAULT)
+                _other = other._get_str(key, default=None)
+                if _is_tensor_collection(type(tensor)):
+                    _out = None if out is None else out._get_str(key, None)
+                    if _other is None:
+                        _other = tensor.empty()
+                    val = tensor.where(
+                        condition=condition, other=_other, out=_out, pad=pad
+                    )
+                else:
+                    val = func(tensor, _other, key)
+                result._set_str(
+                    key, val, inplace=False, validated=True, non_blocking=False
+                )
+                other_keys.discard(key)
+            for key in other_keys:
+                tensor = None
+                _other = other._get_str(key, default=NO_DEFAULT)
+                if _is_tensor_collection(type(_other)):
+                    try:
+                        tensor = _other.empty()
+                    except NotImplementedError:
+                        # H5 tensordicts do not support select()
+                        tensor = _other.to_tensordict().empty()
+                    val = _other.where(
+                        condition=~condition, other=tensor, out=None, pad=pad
+                    )
+                else:
+                    val = func(tensor, _other, key)
+                result._set_str(
+                    key, val, inplace=False, validated=True, non_blocking=False
+                )
+            return result
+        else:
+            if out is None:
+
+                def func(tensor):
+                    return torch.where(
+                        condition=expand_as_right(condition, tensor),
+                        input=tensor,
+                        other=other,
+                    )
+
+                return self._fast_apply(func, propagate_lock=True)
+            else:
+
+                def func(tensor, _out):
+                    return torch.where(
+                        condition=expand_as_right(condition, tensor),
+                        input=tensor,
+                        other=other,
+                        out=_out,
+                    )
+
+                return self._fast_apply(func, out, propagate_lock=True)
+
+    def masked_fill_(self, mask: Tensor, value: float | int | bool) -> T:
+        for item in self.values():
+            mask_expand = expand_as_right(mask, item)
+            item.masked_fill_(mask_expand, value)
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def is_contiguous(self) -> bool:
+        return all([value.is_contiguous() for _, value in self.items()])
+
+    def _clone(self, recurse: bool = True) -> T:
+        result = TensorDict(
+            source={key: _clone_value(value, recurse) for key, value in self.items()},
+            batch_size=self.batch_size,
+            device=self.device,
+            names=copy(self._td_dim_names),
+            _run_checks=False,
+        )
+        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
+        # This may be undesirable, not sure if this should be the default behaviour
+        # (one usually does a copy to modify it).
+        # if not recurse:
+        #     self._maybe_set_shared_attributes(result)
+        return result
+
+    def contiguous(self) -> T:
+        source = {key: value.contiguous() for key, value in self.items()}
+        batch_size = self.batch_size
+        device = self.device
+        out = TensorDict(
+            source=source,
+            batch_size=batch_size,
+            device=device,
+            names=self.names,
+            _run_checks=False,
+        )
+        return out
+
+    def empty(
+        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
+    ) -> T:
+        if not recurse:
+            return TensorDict(
+                device=self._device if device is NO_DEFAULT else device,
+                batch_size=self._batch_size
+                if batch_size is None
+                else torch.Size(batch_size),
+                source={},
+                names=self._td_dim_names if names is None else names,
+                _run_checks=False,
+            )
+        return super().empty(recurse=recurse)
+
+    def _select(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        strict: bool = True,
+        set_shared: bool = True,
+    ) -> T:
+        if inplace and self.is_locked:
+            raise RuntimeError(_LOCK_ERROR)
+
+        source = {}
+        if len(keys):
+            keys_to_select = None
+            for key in keys:
+                if isinstance(key, str):
+                    subkey = []
+                else:
+                    key, subkey = key[0], key[1:]
+
+                val = self._get_str(key, default=None if not strict else NO_DEFAULT)
+                if val is None:
+                    continue
+                source[key] = val
+                if len(subkey):
+                    if keys_to_select is None:
+                        # delay creation of defaultdict
+                        keys_to_select = defaultdict(list)
+                    keys_to_select[key].append(subkey)
+
+            if keys_to_select is not None:
+                for key, val in keys_to_select.items():
+                    source[key] = source[key]._select(
+                        *val, strict=strict, inplace=inplace, set_shared=set_shared
+                    )
+
+        result = TensorDict(
+            device=self.device,
+            batch_size=self.batch_size,
+            source=source,
+            # names=self.names if self._has_names() else None,
+            names=self._td_dim_names,
+            _run_checks=False,
+        )
+        if inplace:
+            self._tensordict = result._tensordict
+            return self
+        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
+        # This may be undesirable, not sure if this should be the default behaviour
+        # (one usually does a copy to modify it).
+        # if set_shared:
+        #     self._maybe_set_shared_attributes(result)
+        return result
+
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> T:
+        # faster than Base.exclude
+        if not len(keys):
+            return self.copy() if not inplace else self
+        if not inplace:
+            _tensordict = copy(self._tensordict)
+        else:
+            _tensordict = self._tensordict
+        keys_to_exclude = None
+        for key in keys:
+            key = unravel_key(key)
+            if isinstance(key, str):
+                _tensordict.pop(key, None)
+            else:
+                if keys_to_exclude is None:
+                    # delay creation of defaultdict
+                    keys_to_exclude = defaultdict(list)
+                if key[0] in self._tensordict:
+                    keys_to_exclude[key[0]].append(key[1:])
+        if keys_to_exclude is not None:
+            for key, cur_keys in keys_to_exclude.items():
+                val = _tensordict.get(key, None)
+                if val is not None:
+                    val = val._exclude(
+                        *cur_keys, inplace=inplace, set_shared=set_shared
+                    )
+                    if not inplace:
+                        _tensordict[key] = val
+        if inplace:
+            return self
+        result = TensorDict(
+            _tensordict,
+            batch_size=self.batch_size,
+            device=self.device,
+            names=self.names if self._has_names() else None,
+            _run_checks=False,
+        )
+        # If this is uncommented, a shallow copy of a shared/memmap will be shared and locked too
+        # This may be undesirable, not sure if this should be the default behaviour
+        # (one usually does a copy to modify it).
+        # if set_shared:
+        #     self._maybe_set_shared_attributes(result)
+        return result
+
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _TensorDictKeysView:
+        if not include_nested and not leaves_only:
+            return self._tensordict.keys()
+        else:
+            return self._nested_keys(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+
+    @cache  # noqa: B019
+    def _nested_keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _TensorDictKeysView:
+        return _TensorDictKeysView(
+            self,
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+            is_leaf=is_leaf,
+        )
+
+    def __getstate__(self):
+        result = {
+            key: val
+            for key, val in self.__dict__.items()
+            if key
+            not in ("_last_op", "_cache", "__last_op_queue", "__lock_parents_weakrefs")
+        }
+        return result
+
+    def __setstate__(self, state):
+        for key, value in state.items():
+            setattr(self, key, value)
+        self._cache = None
+        self.__last_op_queue = None
+        self._last_op = None
+        if self._is_locked:
+            # this can cause avoidable overhead, as we will be locking the leaves
+            # then locking their parent, and the parent of the parent, every
+            # time re-locking tensordicts that have already been locked.
+            # To avoid this, we should lock only at the root, but it isn't easy
+            # to spot what the root is...
+            self._is_locked = False
+            self.lock_()
+
+    # some custom methods for efficiency
+    def items(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.items()
+        elif include_nested and leaves_only:
+            is_leaf = _default_is_leaf if is_leaf is None else is_leaf
+
+            def fast_iter():
+                for k, val in self._tensordict.items():
+                    if not is_leaf(val.__class__):
+                        yield from (
+                            ((k, *((_key,) if isinstance(_key, str) else _key)), _val)
+                            for _key, _val in val.items(
+                                include_nested=include_nested,
+                                leaves_only=leaves_only,
+                                is_leaf=is_leaf,
+                            )
+                        )
+                    else:
+                        yield k, val
+
+            return fast_iter()
+        else:
+            return super().items(
+                include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+            )
+
+    def values(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        if not include_nested and not leaves_only:
+            return self._tensordict.values()
+        else:
+            return super().values(
+                include_nested=include_nested,
+                leaves_only=leaves_only,
+                is_leaf=is_leaf,
+            )
+
+
+class _SubTensorDict(TensorDictBase):
+    """A TensorDict that only sees an index of the stored tensors."""
+
+    _lazy = True
+    _inplace_set = True
+    _safe = False
+
+    def __init__(
+        self,
+        source: T,
+        idx: IndexType,
+        batch_size: Sequence[int] | None = None,
+    ) -> None:
+        if not _is_tensor_collection(source.__class__):
+            raise TypeError(
+                f"Expected source to be a subclass of TensorDictBase, "
+                f"got {type(source)}"
+            )
+        self._source = source
+        idx = (
+            (idx,)
+            if not isinstance(
+                idx,
+                (
+                    tuple,
+                    list,
+                ),
+            )
+            else tuple(idx)
+        )
+        if any(item is Ellipsis for item in idx):
+            idx = convert_ellipsis_to_idx(idx, self._source.batch_size)
+        self._batch_size = _getitem_batch_size(self._source.batch_size, idx)
+        self.idx = idx
+
+        if batch_size is not None and batch_size != self.batch_size:
+            raise RuntimeError("batch_size does not match self.batch_size.")
+
+    # These attributes should never be set
+    @property
+    def _is_shared(self):
+        return self._source._is_shared
+
+    @property
+    def _is_memmap(self):
+        return self._source._is_memmap
+
+    @staticmethod
+    def _convert_ellipsis(idx, shape):
+        if any(_idx is Ellipsis for _idx in idx):
+            new_idx = []
+            cursor = -1
+            for _idx in idx:
+                if _idx is Ellipsis:
+                    if cursor == len(idx) - 1:
+                        # then we can just skip
+                        continue
+                    n_upcoming = len(idx) - cursor - 1
+                    while cursor < len(shape) - n_upcoming:
+                        cursor += 1
+                        new_idx.append(slice(None))
+                else:
+                    new_idx.append(_idx)
+            return tuple(new_idx)
+        return idx
+
+    @property
+    def batch_size(self) -> torch.Size:
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, new_size: torch.Size) -> None:
+        self._batch_size_setter(new_size)
+
+    @property
+    def names(self):
+        names = self._source._get_names_idx(self.idx)
+        if names is None:
+            return [None] * self.batch_dims
+        return names
+
+    @names.setter
+    def names(self, value):
+        raise RuntimeError(
+            "Names of a subtensordict cannot be modified. Instantiate it as a TensorDict first."
+        )
+
+    def _has_names(self):
+        return self._source._has_names()
+
+    def _erase_names(self):
+        raise RuntimeError(
+            "Cannot erase names of a _SubTensorDict. Erase source TensorDict's names instead."
+        )
+
+    def _rename_subtds(self, names):
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                raise RuntimeError("Cannot rename nested sub-tensordict dimensions.")
+
+    @property
+    def device(self) -> None | torch.device:
+        return self._source.device
+
+    @device.setter
+    def device(self, value: DeviceType) -> None:
+        self._source.device = value
+
+    def _preallocate(self, key: NestedKey, value: CompatibleType) -> T:
+        return self._source.set(key, value)
+
+    def _convert_inplace(self, inplace, key):
+        has_key = key in self.keys()
+        if inplace is not False:
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
+                )
+            inplace = has_key
+        if not inplace and has_key:
+            raise RuntimeError(
+                "Calling `_SubTensorDict.set(key, value, inplace=False)` is "
+                "prohibited for existing tensors. Consider calling "
+                "_SubTensorDict.set_(...) or cloning your tensordict first."
+            )
+        elif not inplace and self.is_locked:
+            raise RuntimeError(_LOCK_ERROR)
+        return inplace
+
+    from_dict_instance = TensorDict.from_dict_instance
+
+    def _set_str(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ) -> T:
+        inplace = self._convert_inplace(inplace, key)
+        # it is assumed that if inplace=False then the key doesn't exist. This is
+        # checked in set method, but not here. responsibility lies with the caller
+        # so that this method can have minimal overhead from runtime checks
+        parent = self._source
+        if not validated:
+            value = self._validate_value(value, check_shape=True)
+            validated = True
+        if not inplace:
+            if _is_tensor_collection(value.__class__):
+                value_expand = _expand_to_match_shape(
+                    parent.batch_size, value, self.batch_dims, self.device
+                )
+                for _key, _tensor in value.items():
+                    value_expand._set_str(
+                        _key,
+                        _expand_to_match_shape(
+                            parent.batch_size, _tensor, self.batch_dims, self.device
+                        ),
+                        inplace=inplace,
+                        validated=validated,
+                        ignore_lock=ignore_lock,
+                        non_blocking=non_blocking,
+                    )
+            else:
+                value_expand = torch.zeros(
+                    (
+                        *parent.batch_size,
+                        *_shape(value)[self.batch_dims :],
+                    ),
+                    dtype=value.dtype,
+                    device=self.device,
+                )
+                if self._is_shared:
+                    value_expand.share_memory_()
+                elif self._is_memmap:
+                    value_expand = MemoryMappedTensor.from_tensor(value_expand)
+            parent._set_str(
+                key,
+                value_expand,
+                inplace=False,
+                validated=validated,
+                ignore_lock=ignore_lock,
+                non_blocking=non_blocking,
+            )
+
+        parent._set_at_str(
+            key, value, self.idx, validated=validated, non_blocking=non_blocking
+        )
+        return self
+
+    def _set_tuple(
+        self,
+        key: NestedKey,
+        value: dict[str, CompatibleType] | CompatibleType,
+        *,
+        inplace: bool,
+        validated: bool,
+        non_blocking: bool = False,
+    ) -> T:
+        if len(key) == 1:
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        parent = self._source
+        td = parent._get_str(key[0], None)
+        if td is None:
+            td = parent.select()
+            parent._set_str(
+                key[0], td, inplace=False, validated=True, non_blocking=non_blocking
+            )
+        _SubTensorDict(td, self.idx)._set_tuple(
+            key[1:],
+            value,
+            inplace=inplace,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+        return self
+
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
+        tensor_in = self._get_str(key, NO_DEFAULT)
+        if not validated:
+            value = self._validate_value(value, check_shape=False)
+            validated = True
+        if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+            warn(
+                "Multiple indexing can lead to unexpected behaviours when "
+                "setting items, for instance `td[idx1][idx2] = other` may "
+                "not write to the desired location if idx1 is a list/tensor."
+            )
+            tensor_in = _sub_index(tensor_in, idx)
+            tensor_in.copy_(value)
+            tensor_out = tensor_in
+        else:
+            tensor_out = _set_item(
+                tensor_in, idx, value, validated=validated, non_blocking=non_blocking
+            )
+        # make sure that the value is updated
+        self._source._set_at_str(
+            key, tensor_out, self.idx, validated=validated, non_blocking=non_blocking
+        )
+        return self
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
+        if len(key) == 1:
+            return self._set_at_str(
+                key[0], value, idx, validated=validated, non_blocking=non_blocking
+            )
+        if key[0] not in self.keys():
+            # this won't work
+            raise KeyError(f"key {key} not found in set_at_ with tensordict {self}.")
+        else:
+            td = self._get_str(key[0], NO_DEFAULT)
+        td._set_at_tuple(
+            key[1:], value, idx, validated=validated, non_blocking=non_blocking
+        )
+        return self
+
+    # @cache  # noqa: B019
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _TensorDictKeysView:
+        return self._source.keys(
+            include_nested=include_nested, leaves_only=leaves_only, is_leaf=is_leaf
+        )
+
+    def entry_class(self, key: NestedKey) -> type:
+        source_type = type(self._source.get(key))
+        if _is_tensor_collection(source_type):
+            return self.__class__
+        return source_type
+
+    def _stack_onto_(self, list_item: list[CompatibleType], dim: int) -> _SubTensorDict:
+        self._source._stack_onto_at_(list_item, dim=dim, idx=self.idx)
+        return self
+
+    def to(self, *args, **kwargs: Any) -> T:
+        device, dtype, non_blocking, convert_to_format, batch_size = _parse_to(
+            *args, **kwargs
+        )
+        result = self
+
+        if device is not None and dtype is None and device == self.device:
+            return result
+        return self.to_tensordict().to(*args, **kwargs)
+
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        if not hasattr(self, "_orig_batch_size"):
+            self._orig_batch_size = self.batch_size
+        elif self._orig_batch_size == new_size:
+            del self._orig_batch_size
+        self._batch_size = new_size
+
+    def get(
+        self,
+        key: NestedKey,
+        default: Tensor | str | None = NO_DEFAULT,
+    ) -> CompatibleType:
+        return self._source.get_at(key, self.idx, default=default)
+
+    def _get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
+        out = super()._get_non_tensor(key, default=default)
+
+        if isinstance(out, _SubTensorDict) and is_non_tensor(out._source):
+            return out._source
+        return out
+
+    def _get_str(self, key, default):
+        if key in self.keys() and _is_tensor_collection(self.entry_class(key)):
+            data = self._source._get_str(key, NO_DEFAULT)
+            if is_non_tensor(data):
+                return data[self.idx]
+            return _SubTensorDict(data, self.idx)
+        return self._source._get_at_str(key, self.idx, default=default)
+
+    def _get_tuple(self, key, default):
+        return self._source._get_at_tuple(key, self.idx, default=default)
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        **kwargs,
+    ) -> _SubTensorDict:
+        if input_dict_or_td is self:
+            # no op
+            return self
+
+        if getattr(self._source, "_has_exclusive_keys", False):
+            raise RuntimeError(
+                "Cannot use _SubTensorDict.update with a LazyStackedTensorDict that has exclusive keys."
+            )
+        if keys_to_update is not None:
+            if len(keys_to_update) == 0:
+                return self
+            keys_to_update = unravel_key_list(keys_to_update)
+        keys = set(self.keys(False))
+        for key, value in input_dict_or_td.items():
+            key = _unravel_key_to_tuple(key)
+            firstkey, subkey = key[0], key[1:]
+            if keys_to_update and not any(
+                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
+                for ktu in keys_to_update
+            ):
+                continue
+            if clone and hasattr(value, "clone"):
+                value = value.clone()
+            elif clone:
+                value = tree_map(torch.clone, value)
+            # the key must be a string by now. Let's check if it is present
+            if firstkey in keys:
+                target_class = self.entry_class(firstkey)
+                if _is_tensor_collection(target_class):
+                    target = self._source.get(firstkey)._get_sub_tensordict(self.idx)
+                    if len(subkey):
+                        sub_keys_to_update = _prune_selected_keys(
+                            keys_to_update, firstkey
+                        )
+                        target.update(
+                            {subkey: value},
+                            inplace=False,
+                            keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
+                        )
+                        continue
+                    elif isinstance(value, dict) or _is_tensor_collection(
+                        value.__class__
+                    ):
+                        sub_keys_to_update = _prune_selected_keys(
+                            keys_to_update, firstkey
+                        )
+                        target.update(
+                            value,
+                            keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
+                        )
+                        continue
+                    raise ValueError(
+                        f"Tried to replace a tensordict with an incompatible object of type {type(value)}"
+                    )
+                else:
+                    self._set_tuple(
+                        key,
+                        value,
+                        inplace=True,
+                        validated=False,
+                        non_blocking=non_blocking,
+                    )
+            else:
+                self._set_tuple(
+                    key,
+                    value,
+                    inplace=BEST_ATTEMPT_INPLACE if inplace else False,
+                    validated=False,
+                    non_blocking=non_blocking,
+                )
+        return self
+
+    def update_(
+        self,
+        input_dict: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> _SubTensorDict:
+        return self.update_at_(
+            input_dict,
+            idx=self.idx,
+            discard_idx_attr=True,
+            clone=clone,
+            keys_to_update=keys_to_update,
+            non_blocking=non_blocking,
+        )
+
+    def update_at_(
+        self,
+        input_dict: dict[str, CompatibleType] | TensorDictBase,
+        idx: IndexType,
+        *,
+        discard_idx_attr: bool = False,
+        clone: bool = False,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> _SubTensorDict:
+        if keys_to_update is not None:
+            if len(keys_to_update) == 0:
+                return self
+            keys_to_update = unravel_key_list(keys_to_update)
+        for key, value in input_dict.items():
+            key = _unravel_key_to_tuple(key)
+            firstkey, _ = key[0], key[1:]
+            if keys_to_update and not any(
+                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
+                for ktu in keys_to_update
+            ):
+                continue
+            if not isinstance(value, tuple(_ACCEPTED_CLASSES)):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            if discard_idx_attr:
+                self._source._set_at_tuple(
+                    key,
+                    value,
+                    idx,
+                    non_blocking=non_blocking,
+                    validated=False,
+                )
+            else:
+                self._set_at_tuple(
+                    key, value, idx, validated=False, non_blocking=non_blocking
+                )
+        return self
+
+    def get_parent_tensordict(self) -> T:
+        if not isinstance(self._source, TensorDictBase):
+            raise TypeError(
+                f"_SubTensorDict was initialized with a source of type"
+                f" {self._source.__class__.__name__}, "
+                "parent tensordict not accessible"
+            )
+        return self._source
+
+    @lock_blocked
+    def del_(self, key: NestedKey) -> T:
+        self._source = self._source.del_(key)
+        return self
+
+    @lock_blocked
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        raise NotImplementedError(
+            f"popitem not implemented for class {type(self).__name__}."
+        )
+
+    def _clone(self, recurse: bool = True) -> _SubTensorDict:
+        """Clones the _SubTensorDict.
+
+        Args:
+            recurse (bool, optional): if ``True`` (default), a regular
+                :class:`~.tensordict.TensorDict` instance will be created from the :class:`~.tensordict._SubTensorDict`.
+                Otherwise, another :class:`~.tensordict._SubTensorDict` with identical content
+                will be returned.
+
+        Examples:
+            >>> data = TensorDict({"a": torch.arange(4).reshape(2, 2,)}, batch_size=[2, 2])
+            >>> sub_data = data._get_sub_tensordict([0,])
+            >>> print(sub_data)
+            _SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> # the data of both subtensordict is the same
+            >>> print(data.get("a").data_ptr(), sub_data.get("a").data_ptr())
+            140183705558208 140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=True)
+            >>> print(sub_data_clone)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>. print(sub_data.get("a").data_ptr())
+            140183705558208
+            >>> sub_data_clone = sub_data.clone(recurse=False)
+            >>> print(sub_data_clone)
+            _SubTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> print(sub_data.get("a").data_ptr())
+            140183705558208
+        """
+        if not recurse:
+            return _SubTensorDict(
+                source=self._source._clone(recurse=False), idx=self.idx
+            )
+        return self.to_tensordict()
+
+    def is_contiguous(self) -> bool:
+        return all(value.is_contiguous() for value in self.values())
+
+    def contiguous(self) -> T:
+        return TensorDict(
+            batch_size=self.batch_size,
+            source={key: value.contiguous() for key, value in self.items()},
+            device=self.device,
+            names=self.names,
+            _run_checks=False,
+        )
+
+    def _select(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        strict: bool = True,
+        set_shared: bool = True,
+    ) -> T:
+        if inplace:
+            raise RuntimeError("Cannot call select inplace on a lazy tensordict.")
+        return self.to_tensordict()._select(
+            *keys, inplace=False, strict=strict, set_shared=set_shared
+        )
+
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> T:
+        if inplace:
+            raise RuntimeError("Cannot call exclude inplace on a lazy tensordict.")
+        return self.to_tensordict()._exclude(
+            *keys, inplace=False, set_shared=set_shared
+        )
+
+    def expand(self, *args: int, inplace: bool = False) -> T:
+        if len(args) == 1 and isinstance(args[0], Sequence):
+            shape = tuple(args[0])
+        else:
+            shape = args
+        return self._fast_apply(
+            lambda x: x.expand((*shape, *x.shape[self.ndim :])),
+            batch_size=shape,
+            propagate_lock=True,
+        )
+
+    def is_shared(self) -> bool:
+        return self._source.is_shared()
+
+    def is_memmap(self) -> bool:
+        return self._source.is_memmap()
+
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> _SubTensorDict:
+        self._source.rename_key_(old_key, new_key, safe=safe)
+        return self
+
+    def pin_memory(self) -> T:
+        self._source.pin_memory()
+        return self
+
+    def detach_(self) -> T:
+        raise RuntimeError("Detaching a sub-tensordict in-place cannot be done.")
+
+    def where(self, condition, other, *, out=None, pad=None):
+        return self.to_tensordict().where(
+            condition=condition, other=other, out=out, pad=pad
+        )
+
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
+        for key, item in self.items():
+            self.set_(key, torch.full_like(item, value))
+        return self
+
+    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
+        td_copy = self.clone()
+        return td_copy.masked_fill_(mask, value)
+
+    def memmap_(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        num_threads: int = 0,
+    ) -> T:
+        raise RuntimeError(
+            "Converting a sub-tensordict values to memmap cannot be done."
+        )
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None,
+        copy_existing: bool,
+        executor,
+        futures,
+        inplace,
+        like,
+        share_non_tensor,
+    ) -> T:
+        if prefix is not None:
+
+            def save_metadata(prefix=prefix, self=self):
+                prefix = Path(prefix)
+                if not prefix.exists():
+                    os.makedirs(prefix, exist_ok=True)
+                with open(prefix / "meta.json", "w") as f:
+                    json.dump(
+                        {
+                            "_type": str(self.__class__),
+                            "index": _index_to_str(self.idx),
+                        },
+                        f,
+                    )
+
+            if executor is None:
+                save_metadata()
+            else:
+                futures.append(executor.submit(save_metadata))
+
+        _source = self._source._memmap_(
+            prefix=prefix / "_source" if prefix is not None else None,
+            copy_existing=copy_existing,
+            executor=executor,
+            futures=futures,
+            inplace=inplace,
+            like=like,
+            share_non_tensor=share_non_tensor,
+        )
+        if not inplace:
+            result = _SubTensorDict(_source, idx=self.idx)
+        else:
+            result = self
+        return result
+
+    @classmethod
+    def _load_memmap(
+        cls, prefix: Path, metadata: dict, device: torch.device | None = None
+    ):
+        index = metadata["index"]
+        return _SubTensorDict(
+            TensorDict.load_memmap(prefix / "_source", device=device),
+            _str_to_index(index),
+        )
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for _SubTensorDict."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def share_memory_(self) -> T:
+        raise RuntimeError(
+            "Casting a sub-tensordict values to shared memory cannot be done."
+        )
+
+    @property
+    def is_locked(self) -> bool:
+        return self._source.is_locked
+
+    @is_locked.setter
+    def is_locked(self, value) -> bool:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    @as_decorator("is_locked")
+    def lock_(self) -> T:
+        # we can't lock sub-tensordicts because that would mean that the
+        # parent tensordict cannot be modified either.
+        if not self.is_locked:
+            raise RuntimeError(
+                "Cannot lock a _SubTensorDict. Lock the parent tensordict instead."
+            )
+        return self
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> T:
+        if self.is_locked:
+            raise RuntimeError(
+                "Cannot unlock a _SubTensorDict. Unlock the parent tensordict instead."
+            )
+        return self
+
+    def _remove_lock(self, lock_id):
+        raise RuntimeError(
+            "Cannot unlock a _SubTensorDict. Unlock the parent tensordict instead."
+        )
+
+    def _propagate_lock(self, lock_ids=None):
+        raise RuntimeError(
+            "Cannot lock a _SubTensorDict. Lock the parent tensordict instead."
+        )
+
+    def __del__(self):
+        pass
+
+    def _create_nested_str(self, key):
+        # this may fail with a sub-sub tensordict
+        out = self._source.empty()
+        self._source._set_str(
+            key, out, inplace=False, validated=True, non_blocking=False
+        )
+        # the id of out changes
+        return self._get_str(key, default=NO_DEFAULT)
+
+    def _cast_reduction(
+        self,
+        *,
+        reduction_name,
+        dim=NO_DEFAULT,
+        keepdim=NO_DEFAULT,
+        tuple_ok=True,
+        **kwargs,
+    ):
+        try:
+            td = self.to_tensordict()
+        except Exception:
+            raise RuntimeError(
+                f"{reduction_name} requires this object to be cast to a regular TensorDict. "
+                f"If you need {type(self)} to support {reduction_name}, help us by filing an issue"
+                f" on github!"
+            )
+        return td._cast_reduction(
+            reduction_name=reduction_name,
+            dim=dim,
+            keepdim=keepdim,
+            tuple_ok=tuple_ok,
+            **kwargs,
+        )
+
+    # TODO: check these implementations
+    __eq__ = TensorDict.__eq__
+    __ne__ = TensorDict.__ne__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
+    __setitem__ = TensorDict.__setitem__
+    __xor__ = TensorDict.__xor__
+    __or__ = TensorDict.__or__
+    _check_device = TensorDict._check_device
+    _check_is_shared = TensorDict._check_is_shared
+    all = TensorDict.all
+    any = TensorDict.any
+    masked_select = TensorDict.masked_select
+    memmap_like = TensorDict.memmap_like
+    reshape = TensorDict.reshape
+    split = TensorDict.split
+    _to_module = TensorDict._to_module
+    _unbind = TensorDict._unbind
+
+    def _view(self, *args, **kwargs):
+        raise RuntimeError(
+            "Cannot call `view` on a sub-tensordict. Call `reshape` instead."
+        )
+
+    def _transpose(self, dim0, dim1):
+        raise RuntimeError(
+            "Cannot call `transpose` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _permute(
+        self,
+        *args,
+        **kwargs,
+    ):
+        raise RuntimeError(
+            "Cannot call `permute` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _squeeze(self, dim=None):
+        raise RuntimeError(
+            "Cannot call `squeeze` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _unsqueeze(self, dim):
+        raise RuntimeError(
+            "Cannot call `unsqueeze` on a sub-tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    _add_batch_dim = TensorDict._add_batch_dim
+
+    _apply_nest = TensorDict._apply_nest
+    # def _apply_nest(self, *args, **kwargs):
+    #     return self.to_tensordict()._apply_nest(*args, **kwargs)
+    _convert_to_tensordict = TensorDict._convert_to_tensordict
+
+    _get_names_idx = TensorDict._get_names_idx
+
+    def _index_tensordict(self, index, new_batch_size=None, names=None):
+        # we ignore the names and new_batch_size which are only provided for
+        # efficiency purposes
+        return self._get_sub_tensordict(index)
+
+    def _remove_batch_dim(self, *args, **kwargs):
+        raise NotImplementedError
+
+
+###########################
+# Keys utils
+
+
+class _TensorDictKeysView:
+    """A Key view for TensorDictBase instance.
+
+    _TensorDictKeysView is returned when accessing tensordict.keys() and holds a
+    reference to the original TensorDict. This class enables us to support nested keys
+    when performing membership checks and when iterating over keys.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+
+        >>> td = TensorDict(
+        >>>     {"a": TensorDict({"b": torch.rand(1, 2)}, [1, 2]), "c": torch.rand(1)},
+        >>>     [1],
+        >>> )
+
+        >>> assert "a" in td.keys()
+        >>> assert ("a",) in td.keys()
+        >>> assert ("a", "b") in td.keys()
+        >>> assert ("a", "c") not in td.keys()
+
+        >>> assert set(td.keys()) == {("a", "b"), "c"}
+    """
+
+    def __init__(
+        self,
+        tensordict: T,
+        include_nested: bool,
+        leaves_only: bool,
+        is_leaf: Callable[[Type], bool] = None,
+    ) -> None:
+        self.tensordict = tensordict
+        self.include_nested = include_nested
+        self.leaves_only = leaves_only
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+        self.is_leaf = is_leaf
+
+    def __iter__(self) -> Iterable[str] | Iterable[tuple[str, ...]]:
+        if not self.include_nested:
+            if self.leaves_only:
+                for key in self._keys():
+                    target_class = self.tensordict.entry_class(key)
+                    if _is_tensor_collection(target_class):
+                        continue
+                    yield key
+            else:
+                yield from self._keys()
+        else:
+            yield from (
+                key if len(key) > 1 else key[0]
+                for key in self._iter_helper(self.tensordict)
+            )
+
+    def _iter_helper(
+        self, tensordict: T, prefix: str | None = None
+    ) -> Iterable[str] | Iterable[tuple[str, ...]]:
+        for key, value in self._items(tensordict):
+            full_key = self._combine_keys(prefix, key)
+            cls = value.__class__
+            while cls is list:
+                # For lazy stacks
+                value = value[0]
+                cls = value.__class__
+            is_leaf = self.is_leaf(cls)
+            if self.include_nested and not is_leaf:
+                yield from self._iter_helper(value, prefix=full_key)
+            if not self.leaves_only or is_leaf:
+                yield full_key
+
+    def _combine_keys(self, prefix: tuple | None, key: NestedKey) -> tuple:
+        if prefix is not None:
+            return prefix + (key,)
+        return (key,)
+
+    def __len__(self) -> int:
+        return sum(1 for _ in self)
+
+    def _items(
+        self, tensordict: TensorDictBase | None = None
+    ) -> Iterable[tuple[NestedKey, CompatibleType]]:
+        if tensordict is None:
+            tensordict = self.tensordict
+        if isinstance(tensordict, TensorDict) or is_tensorclass(tensordict):
+            return tensordict._tensordict.items()
+        from tensordict.nn import TensorDictParams
+
+        if isinstance(tensordict, TensorDictParams):
+            return tensordict._param_td.items()
+        if isinstance(tensordict, KeyedJaggedTensor):
+            return tuple((key, tensordict[key]) for key in tensordict.keys())
+        from tensordict._lazy import (
+            _CustomOpTensorDict,
+            _iter_items_lazystack,
+            LazyStackedTensorDict,
+        )
+
+        if isinstance(tensordict, LazyStackedTensorDict):
+            return _iter_items_lazystack(tensordict, return_none_for_het_values=True)
+        if isinstance(tensordict, _CustomOpTensorDict):
+            # it's possible that a TensorDict contains a nested LazyStackedTensorDict,
+            # or _CustomOpTensorDict, so as we iterate through the contents we need to
+            # be careful to not rely on tensordict._tensordict existing.
+            return (
+                (key, tensordict._get_str(key, NO_DEFAULT))
+                for key in tensordict._source.keys()
+            )
+        raise NotImplementedError(type(tensordict))
+
+    def _keys(self) -> _TensorDictKeysView:
+        return self.tensordict._tensordict.keys()
+
+    def __contains__(self, key: NestedKey) -> bool:
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise TypeError(_NON_STR_KEY_ERR)
+
+        if isinstance(key, str):
+            if key in self._keys():
+                if self.leaves_only:
+                    # TODO: make this faster for LazyStacked without compromising regular
+                    return not _is_tensor_collection(
+                        type(self.tensordict._get_str(key))
+                    )
+                return True
+            return False
+        else:
+            # thanks to _unravel_key_to_tuple we know the key is a tuple
+            if len(key) == 1:
+                return key[0] in self._keys()
+            elif self.include_nested:
+                item_root = self.tensordict._get_str(key[0], default=None)
+                if item_root is not None:
+                    entry_type = type(item_root)
+                    if issubclass(entry_type, Tensor):
+                        return False
+                    elif entry_type is KeyedJaggedTensor:
+                        if len(key) > 2:
+                            return False
+                        return key[1] in item_root.keys()
+                    # TODO: make this faster for LazyStacked without compromising regular
+                    _is_tensordict = _is_tensor_collection(entry_type)
+                    if _is_tensordict:
+                        # # this will call _unravel_key_to_tuple many times
+                        # return key[1:] in self.tensordict._get_str(key[0], NO_DEFAULT).keys(include_nested=self.include_nested)
+                        # this won't call _unravel_key_to_tuple but requires to get the default which can be suboptimal
+                        if len(key) >= 3:
+                            leaf_td = item_root._get_tuple(key[1:-1], None)
+                            if leaf_td is None or (
+                                not _is_tensor_collection(leaf_td.__class__)
+                                and not isinstance(leaf_td, KeyedJaggedTensor)
+                            ):
+                                return False
+                        else:
+                            leaf_td = item_root
+                        return key[-1] in leaf_td.keys()
+                return False
+            # this is reached whenever there is more than one key but include_nested is False
+            if all(isinstance(subkey, str) for subkey in key):
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+
+    def __repr__(self):
+        include_nested = f"include_nested={self.include_nested}"
+        leaves_only = f"leaves_only={self.leaves_only}"
+        return f"{self.__class__.__name__}({list(self)},\n{indent(include_nested, 4*' ')},\n{indent(leaves_only, 4*' ')})"
+
+
+def _set_tensor_dict(  # noqa: F811
+    module_dict,
+    hooks,
+    module: torch.nn.Module,
+    name: str,
+    tensor: torch.Tensor,
+    inplace: bool,
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    was_buffer = False
+    out = module_dict["_parameters"].pop(name, None)  # type: ignore[assignment]
+    if out is None:
+        out = module_dict["_buffers"].pop(name, None)
+        was_buffer = out is not None
+    if out is None:
+        out = module_dict.pop(name)
+    if inplace:
+        # swap tensor and out after updating out
+        out_tmp = out.clone()
+        out.data.copy_(tensor.data)
+        tensor = out
+        out = out_tmp
+
+    if isinstance(tensor, torch.nn.Parameter):
+        for hook in hooks:
+            output = hook(module, name, tensor)
+            if output is not None:
+                tensor = output
+        module_dict["_parameters"][name] = tensor
+
+        if isinstance(
+            tensor, (_BatchedUninitializedParameter, _BatchedUninitializedBuffer)
+        ):
+            module.register_forward_pre_hook(
+                _add_batch_dim_pre_hook(), with_kwargs=True
+            )
+
+    elif was_buffer and isinstance(tensor, torch.Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+    return out
+
+
+def _index_to_str(index):
+    if isinstance(index, tuple):
+        return tuple(_index_to_str(elt) for elt in index)
+    if isinstance(index, slice):
+        return ("slice", {"start": index.start, "stop": index.stop, "step": index.step})
+    if isinstance(index, range):
+        return ("range", {"start": index.start, "stop": index.stop, "step": index.step})
+    if isinstance(index, Tensor):
+        return ("tensor", index.tolist(), str(index.device))
+    return index
+
+
+def _str_to_index(index):
+    if isinstance(index, tuple):
+        if not len(index):
+            return index
+        if index[0] == "slice":
+            index = index[1]
+            return slice(index["start"], index["stop"], index["step"])
+        if index[0] == "range":
+            index = index[1]
+            return range(index["start"], index["stop"], index["step"])
+        if index[0] == "tensor":
+            index, device = index[1:]
+            return torch.tensor(index, device=device)
+        return tuple(_index_to_str(elt) for elt in index)
+    return index
+
+
+_register_tensor_class(TensorDict)
+_register_tensor_class(_SubTensorDict)
+
+
+def _save_metadata(data: TensorDictBase, prefix: Path, metadata=None):
+    """Saves the metadata of a memmap tensordict on disk."""
+    filepath = prefix / "meta.json"
+    if metadata is None:
+        metadata = {}
+    metadata.update(
+        {
+            "shape": list(data.shape),
+            "device": str(data.device),
+            "_type": str(data.__class__),
+        }
+    )
+    with open(filepath, "w") as json_metadata:
+        json.dump(metadata, json_metadata)
+
+
+# user did specify location and memmap is in wrong place, so we copy
+def _populate_memmap(*, dest, value, key, copy_existing, prefix, like):
+    filename = None if prefix is None else str(prefix / f"{key}.memmap")
+    if value.is_nested:
+        shape = value._nested_tensor_size()
+        # Make the shape a memmap tensor too
+        if prefix is not None:
+            shape_filename = Path(filename)
+            shape_filename = shape_filename.with_suffix(".shape.memmap")
+            MemoryMappedTensor.from_tensor(
+                shape,
+                filename=shape_filename,
+                copy_existing=copy_existing,
+                existsok=True,
+                copy_data=True,
+            )
+    else:
+        shape = None
+    memmap_tensor = MemoryMappedTensor.from_tensor(
+        value.data if value.requires_grad else value,
+        filename=filename,
+        copy_existing=copy_existing,
+        existsok=True,
+        copy_data=not like,
+        shape=shape,
+    )
+    dest._tensordict[key] = memmap_tensor
+    return memmap_tensor
+
+
+def _populate_empty(
+    *,
+    dest,
+    key,
+    shape,
+    dtype,
+    prefix,
+):
+    filename = None if prefix is None else str(prefix / f"{key}.memmap")
+    if isinstance(shape, torch.Tensor):
+        # Make the shape a memmap tensor too
+        if prefix is not None:
+            shape_filename = Path(filename)
+            shape_filename = shape_filename.with_suffix(".shape.memmap")
+            MemoryMappedTensor.from_tensor(
+                shape,
+                filename=shape_filename,
+                existsok=True,
+                copy_data=True,
+            )
+    memmap_tensor = MemoryMappedTensor.empty(
+        shape=shape,
+        dtype=dtype,
+        filename=filename,
+        existsok=True,
+    )
+    dest._tensordict[key] = memmap_tensor
+    return memmap_tensor
+
+
+def _populate_storage(
+    *,
+    dest,
+    key,
+    shape,
+    dtype,
+    prefix,
+    storage,
+):
+    filename = None if prefix is None else str(prefix / f"{key}.memmap")
+    if isinstance(shape, torch.Tensor):
+        # Make the shape a memmap tensor too
+        if prefix is not None:
+            shape_filename = Path(filename)
+            shape_filename = shape_filename.with_suffix(".shape.memmap")
+            MemoryMappedTensor.from_tensor(
+                shape,
+                filename=shape_filename,
+                existsok=True,
+                copy_data=True,
+            )
+    memmap_tensor = MemoryMappedTensor.from_storage(
+        storage=storage,
+        shape=shape,
+        dtype=dtype,
+        filename=filename,
+    )
+    dest._tensordict[key] = memmap_tensor
+    return memmap_tensor
+
+
+def _update_metadata(*, metadata, key, value, is_collection):
+    if not is_collection:
+        metadata[key] = {
+            "device": str(value.device),
+            "shape": list(value.shape)
+            if not value.is_nested
+            else value._nested_tensor_size().shape,
+            "dtype": str(value.dtype),
+            "is_nested": value.is_nested,
+        }
+    else:
+        metadata[key] = {
+            "type": type(value).__name__,
+        }
```

## tensordict/_torch_func.py

 * *Ordering differences only*

```diff
@@ -1,642 +1,642 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-
-from typing import Any, Callable, Sequence, TypeVar
-
-import torch
-from tensordict._lazy import LazyStackedTensorDict
-from tensordict._td import TensorDict
-from tensordict.base import (
-    _is_leaf_nontensor,
-    _is_tensor_collection,
-    NO_DEFAULT,
-    TensorDictBase,
-)
-from tensordict.persistent import PersistentTensorDict
-from tensordict.utils import (
-    _check_keys,
-    _ErrorInteceptor,
-    _shape,
-    DeviceType,
-    is_non_tensor,
-    is_tensorclass,
-    lazy_legacy,
-    set_lazy_legacy,
-)
-from torch import Tensor
-from torch.nn.parameter import (
-    UninitializedBuffer,
-    UninitializedParameter,
-    UninitializedTensorMixin,
-)
-
-TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
-LAZY_TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
-T = TypeVar("T", bound="TensorDictBase")
-
-
-def implements_for_td(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDict."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        TD_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-def implements_for_lazy_td(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDict."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        LAZY_TD_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-@implements_for_td(torch.unbind)
-def _unbind(td: T, *args: Any, **kwargs: Any) -> tuple[T, ...]:
-    return td.unbind(*args, **kwargs)
-
-
-@implements_for_td(torch.gather)
-def _gather(
-    input: T,
-    dim: int,
-    index: Tensor,
-    *,
-    sparse_grad: bool = False,
-    out: T | None = None,
-) -> T:
-    if sparse_grad:
-        raise NotImplementedError(
-            "sparse_grad=True not implemented for torch.gather(tensordict, ...)"
-        )
-    # the index must have as many dims as the tensordict
-    if not len(index):
-        raise RuntimeError("Cannot use torch.gather with an empty index")
-    dim_orig = dim
-    if dim < 0:
-        dim = input.batch_dims + dim
-    if dim > input.batch_dims - 1 or dim < 0:
-        raise RuntimeError(
-            f"Cannot gather tensordict with shape {input.shape} along dim {dim_orig}."
-        )
-
-    def _gather_tensor(tensor, dest_container=None, dest_key=None):
-        if dest_container is not None:
-            dest = dest_container.get(dest_key)
-        else:
-            dest = None
-        index_expand = index
-        while index_expand.ndim < tensor.ndim:
-            index_expand = index_expand.unsqueeze(-1)
-        target_shape = list(tensor.shape)
-        target_shape[dim] = index_expand.shape[dim]
-        index_expand = index_expand.expand(target_shape)
-        out = torch.gather(tensor, dim, index_expand, out=dest)
-        return out
-
-    if out is None:
-        if len(index.shape) == input.ndim and input._has_names():
-            names = input.names
-        else:
-            names = None
-        return TensorDict(
-            {
-                key: _gather_tensor(value)
-                for key, value in input.items(is_leaf=_is_leaf_nontensor)
-            },
-            batch_size=index.shape,
-            names=names,
-        )
-    TensorDict(
-        {
-            key: _gather_tensor(value, out, key)
-            for key, value in input.items(is_leaf=_is_leaf_nontensor)
-        },
-        batch_size=index.shape,
-    )
-    return out
-
-
-@implements_for_td(torch.full_like)
-def _full_like(td: T, fill_value: float, *args, **kwargs: Any) -> T:
-    return td._fast_apply(
-        lambda x: torch.full_like(x, fill_value, *args, **kwargs),
-        inplace=True,
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-
-
-@implements_for_td(torch.zeros_like)
-def _zeros_like(td: T, *args, **kwargs: Any) -> T:
-    td_clone = td._fast_apply(
-        lambda x: torch.zeros_like(x, *args, **kwargs),
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-    if "dtype" in kwargs:
-        raise ValueError("Cannot pass dtype to full_like with TensorDict")
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.ones_like)
-def _ones_like(td: T, *args, **kwargs: Any) -> T:
-    td_clone = td._fast_apply(
-        lambda x: torch.ones_like(x, *args, **kwargs),
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.rand_like)
-def _rand_like(td: T, *args, **kwargs: Any) -> T:
-    td_clone = td._fast_apply(
-        lambda x: torch.rand_like(x, *args, **kwargs),
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.randn_like)
-def _randn_like(td: T, *args, **kwargs: Any) -> T:
-    td_clone = td._fast_apply(
-        lambda x: torch.randn_like(x, *args, **kwargs),
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-    if "device" in kwargs:
-        td_clone = td_clone.to(kwargs.pop("device"))
-    if len(kwargs):
-        raise RuntimeError(
-            f"keyword arguments {list(kwargs.keys())} are not "
-            f"supported with full_like with TensorDict"
-        )
-    return td_clone
-
-
-@implements_for_td(torch.empty_like)
-def _empty_like(td: T, *args, **kwargs) -> T:
-    return td._fast_apply(
-        lambda x: torch.empty_like(x, *args, **kwargs),
-        propagate_lock=True,
-        device=kwargs.get("device", NO_DEFAULT),
-    )
-
-
-@implements_for_td(torch.clone)
-def _clone(td: T, *args: Any, **kwargs: Any) -> T:
-    return td.clone(*args, **kwargs)
-
-
-@implements_for_td(torch.squeeze)
-def _squeeze(td: T, *args: Any, **kwargs: Any) -> T:
-    return td.squeeze(*args, **kwargs)
-
-
-@implements_for_td(torch.unsqueeze)
-def _unsqueeze(td: T, *args: Any, **kwargs: Any) -> T:
-    return td.unsqueeze(*args, **kwargs)
-
-
-@implements_for_td(torch.masked_select)
-def _masked_select(td: T, *args: Any, **kwargs: Any) -> T:
-    return td.masked_select(*args, **kwargs)
-
-
-@implements_for_td(torch.permute)
-def _permute(td: T, dims: Sequence[int]) -> T:
-    return td.permute(*dims)
-
-
-@implements_for_td(torch.cat)
-def _cat(
-    list_of_tensordicts: Sequence[T],
-    dim: int = 0,
-    device: DeviceType | None = None,
-    out: T | None = None,
-) -> T:
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    batch_size = list(list_of_tensordicts[0].batch_size)
-    if dim < 0:
-        dim = len(batch_size) + dim
-    if dim >= len(batch_size):
-        raise RuntimeError(
-            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
-            f"={dim} and batch_size={batch_size}"
-        )
-    batch_size[dim] = sum([td.batch_size[dim] for td in list_of_tensordicts])
-    batch_size = torch.Size(batch_size)
-
-    # check that all tensordict match
-    keys = _check_keys(list_of_tensordicts, strict=True)
-    if out is None:
-        out = {}
-        for key in keys:
-            with _ErrorInteceptor(
-                key, "Attempted to concatenate tensors on different devices at key"
-            ):
-                out[key] = torch.cat(
-                    [td._get_str(key, NO_DEFAULT) for td in list_of_tensordicts], dim
-                )
-        if device is None:
-            device = list_of_tensordicts[0].device
-            for td in list_of_tensordicts[1:]:
-                if device == td.device:
-                    continue
-                else:
-                    device = None
-                    break
-        names = None
-        if list_of_tensordicts[0]._has_names():
-            names = list_of_tensordicts[0].names
-        return TensorDict(
-            out, device=device, batch_size=batch_size, _run_checks=False, names=names
-        )
-    else:
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and cat batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-
-        for key in keys:
-            with _ErrorInteceptor(
-                key, "Attempted to concatenate tensors on different devices at key"
-            ):
-                if isinstance(out, TensorDict):
-                    torch.cat(
-                        [td.get(key) for td in list_of_tensordicts],
-                        dim,
-                        out=out.get(key),
-                    )
-                else:
-                    out.set_(
-                        key, torch.cat([td.get(key) for td in list_of_tensordicts], dim)
-                    )
-        return out
-
-
-@implements_for_lazy_td(torch.cat)
-def _lazy_cat(
-    list_of_tensordicts: Sequence[LazyStackedTensorDict],
-    dim: int = 0,
-    out: LazyStackedTensorDict | None = None,
-) -> LazyStackedTensorDict:
-    # why aren't they feeding you?
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    batch_size = list(list_of_tensordicts[0].batch_size)
-    if dim < 0:
-        dim = len(batch_size) + dim
-    if dim >= len(batch_size):
-        raise RuntimeError(
-            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
-            f"={dim} and batch_size={batch_size}"
-        )
-    stack_dim = list_of_tensordicts[0].stack_dim
-    if any((td.stack_dim != stack_dim) for td in list_of_tensordicts):
-        raise RuntimeError("cat lazy stacked tds must have same stack dim")
-
-    batch_size[dim] = sum(td.batch_size[dim] for td in list_of_tensordicts)
-    batch_size = torch.Size(batch_size)
-
-    new_dim = dim
-    if dim > stack_dim:
-        new_dim = dim - 1
-
-    if out is None:
-        out = []
-        if dim == stack_dim:  # if dim is stack, just add all to the same list
-            for lazy_td in list_of_tensordicts:
-                out += lazy_td.tensordicts
-        else:
-            for i in range(len(list_of_tensordicts[0].tensordicts)):
-                out.append(
-                    torch.cat(
-                        [lazy_td.tensordicts[i] for lazy_td in list_of_tensordicts],
-                        new_dim,
-                    )
-                )
-        return LazyStackedTensorDict(*out, stack_dim=stack_dim)
-    else:
-        if not isinstance(out, LazyStackedTensorDict):
-            return _cat(list_of_tensordicts, dim=dim, out=out)
-
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and cat batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-        if out.stack_dim != dim:
-            index_base = (slice(None),) * out.stack_dim
-            for i, sub_dest in enumerate(out.tensordicts):
-                index = index_base + (i,)
-                tds_to_cat = [_td[index] for _td in list_of_tensordicts]
-                torch.cat(tds_to_cat, dim, out=sub_dest)
-        else:
-            init_idx = 0
-            for td_in in list_of_tensordicts:
-                sub_dest = out.tensordicts[init_idx : init_idx + td_in.shape[dim]]
-                init_idx += init_idx + td_in.shape[dim]
-                LazyStackedTensorDict.maybe_dense_stack(sub_dest, out.stack_dim).update(
-                    td_in, inplace=True
-                )
-
-        return out
-
-
-@implements_for_td(torch.stack)
-def _stack(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    dim: int = 0,
-    device: DeviceType | None = None,
-    out: T | None = None,
-    strict: bool = False,
-    contiguous: bool = False,
-    maybe_dense_stack: bool = False,
-) -> T:
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    if all(is_non_tensor(td) for td in list_of_tensordicts):
-        from tensordict.tensorclass import NonTensorData
-
-        return NonTensorData._stack_non_tensor(list_of_tensordicts, dim=dim)
-
-    batch_size = list_of_tensordicts[0].batch_size
-    if dim < 0:
-        dim = len(batch_size) + dim + 1
-
-    for td in list_of_tensordicts[1:]:
-        if td.batch_size != list_of_tensordicts[0].batch_size:
-            raise RuntimeError(
-                "stacking tensordicts requires them to have congruent batch sizes, "
-                f"got td1.batch_size={td.batch_size} and td2.batch_size="
-                f"{list_of_tensordicts[0].batch_size}"
-            )
-
-    # check that all tensordict match
-    # Read lazy_legacy
-    _lazy_legacy = lazy_legacy()
-
-    if out is None:
-        # We need to handle tensordicts with exclusive keys and tensordicts with
-        # mismatching shapes.
-        # The first case is handled within _check_keys which fails if keys
-        # don't match exactly.
-        # The second requires a check over the tensor shapes.
-        device = list_of_tensordicts[0].device
-        if contiguous or not _lazy_legacy:
-            try:
-                keys = _check_keys(list_of_tensordicts, strict=True)
-            except KeyError:
-                if not _lazy_legacy and not contiguous:
-                    if maybe_dense_stack:
-                        with set_lazy_legacy(True):
-                            return _stack(list_of_tensordicts, dim=dim)
-                    else:
-                        raise RuntimeError(
-                            "The sets of keys in the tensordicts to stack are exclusive. "
-                            "Consider using `LazyStackedTensorDict.maybe_dense_stack` instead."
-                        )
-                raise
-
-            if all(
-                isinstance(_tensordict, LazyStackedTensorDict)
-                for _tensordict in list_of_tensordicts
-            ):
-                # Let's try to see if all tensors have the same shape
-                # If so, we can assume that we can densly stack the sub-tds
-                leaves = [
-                    torch.utils._pytree.tree_leaves(td) for td in list_of_tensordicts
-                ]
-                for x in zip(*leaves):
-                    # TODO: check what happens with non-tensor data here
-                    if len(x) == 1 or all(_x.shape == x[0].shape for _x in x[1:]):
-                        continue
-                    else:
-                        break
-                else:
-                    # make sure we completed the zip, since strict=True is only available for python >= 3.10
-                    if len(leaves) == 1 or all(
-                        len(_leaves) == len(leaves[0]) for _leaves in leaves[1:]
-                    ):
-                        lazy_stack_dim = list_of_tensordicts[0].stack_dim
-                        if dim <= lazy_stack_dim:
-                            lazy_stack_dim += 1
-                        else:
-                            dim = dim - 1
-                        return LazyStackedTensorDict(
-                            *[
-                                _stack(list(subtds), dim=dim)
-                                for subtds in zip(
-                                    *[td.tensordicts for td in list_of_tensordicts]
-                                )
-                            ],
-                            stack_dim=lazy_stack_dim,
-                        )
-                lazy_stack_dim = list_of_tensordicts[0].stack_dim
-                if dim <= lazy_stack_dim:
-                    lazy_stack_dim += 1
-                else:
-                    dim = dim - 1
-                return LazyStackedTensorDict(
-                    *[
-                        _stack(list_of_td, dim, maybe_dense_stack=maybe_dense_stack)
-                        for list_of_td in zip(
-                            *[td.tensordicts for td in list_of_tensordicts]
-                        )
-                    ],
-                    stack_dim=lazy_stack_dim,
-                )
-
-            out = {}
-            for key in keys:
-                out[key] = []
-                is_not_init = None
-                tensor_shape = None
-                is_tensor = None
-                for _tensordict in list_of_tensordicts:
-                    tensor = _tensordict._get_str(key, default=NO_DEFAULT)
-                    if is_tensor is None:
-                        tensor_cls = type(tensor)
-                        is_tensor = (
-                            not _is_tensor_collection(tensor_cls)
-                        ) or is_tensorclass(tensor_cls)
-                    if is_not_init is None:
-                        is_not_init = isinstance(tensor, UninitializedTensorMixin)
-                    if not is_not_init:
-                        new_tensor_shape = _shape(tensor)
-                        if tensor_shape is not None:
-                            if len(new_tensor_shape) != len(tensor_shape) or not all(
-                                s1 == s2 and s1 != -1
-                                for s1, s2 in zip(_shape(tensor), tensor_shape)
-                            ):
-                                # Nested tensors will require a lazy stack
-                                if maybe_dense_stack:
-                                    with set_lazy_legacy(True):
-                                        return _stack(list_of_tensordicts, dim=dim)
-                                else:
-                                    raise RuntimeError(
-                                        "The shapes of the tensors to stack is incompatible."
-                                    )
-                        else:
-                            tensor_shape = new_tensor_shape
-
-                    out[key].append(tensor)
-                out[key] = (out[key], is_not_init, is_tensor)
-
-            def stack_fn(key, values, is_not_init, is_tensor):
-                if is_not_init:
-                    return _stack_uninit_params(values, dim)
-                if is_tensor:
-                    return torch.stack(values, dim)
-                with _ErrorInteceptor(
-                    key, "Attempted to stack tensors on different devices at key"
-                ):
-                    return _stack(values, dim, maybe_dense_stack=maybe_dense_stack)
-
-            out = {
-                key: stack_fn(key, values, is_not_init, is_tensor)
-                for key, (values, is_not_init, is_tensor) in out.items()
-            }
-
-            return TensorDict(
-                out,
-                batch_size=LazyStackedTensorDict._compute_batch_size(
-                    batch_size, dim, len(list_of_tensordicts)
-                ),
-                device=device,
-                _run_checks=False,
-            )
-        else:
-            out = LazyStackedTensorDict(
-                *list_of_tensordicts,
-                stack_dim=dim,
-            )
-    else:
-        keys = _check_keys(list_of_tensordicts)
-        batch_size = list(batch_size)
-        batch_size.insert(dim, len(list_of_tensordicts))
-        batch_size = torch.Size(batch_size)
-
-        if out.batch_size != batch_size:
-            raise RuntimeError(
-                "out.batch_size and stacked batch size must match, "
-                f"got out.batch_size={out.batch_size} and batch_size"
-                f"={batch_size}"
-            )
-
-        out_keys = set(out.keys())
-        if strict:
-            in_keys = set(keys)
-            if len(out_keys - in_keys) > 0:
-                raise RuntimeError(
-                    "The output tensordict has keys that are missing in the "
-                    "tensordict that has to be written: {out_keys - in_keys}. "
-                    "As per the call to `stack(..., strict=True)`, this "
-                    "is not permitted."
-                )
-            elif len(in_keys - out_keys) > 0:
-                raise RuntimeError(
-                    "The resulting tensordict has keys that are missing in "
-                    f"its destination: {in_keys - out_keys}. As per the call "
-                    "to `stack(..., strict=True)`, this is not permitted."
-                )
-
-        try:
-            out._stack_onto_(list_of_tensordicts, dim)
-        except KeyError as err:
-            raise err
-    return out
-
-
-@implements_for_td(torch.split)
-def _split(
-    td: TensorDict, split_size_or_sections: int | list[int], dim: int = 0
-) -> list[TensorDictBase]:
-    return td.split(split_size_or_sections, dim)
-
-
-@implements_for_td(torch.where)
-def where(condition, input, other, *, out=None):
-    """Return a ``TensorDict`` of elements selected from either input or other, depending on condition.
-
-    Args:
-        condition (BoolTensor): When ``True`` (nonzero), yield ``input``, otherwise yield ``other``.
-        input (TensorDictBase or Scalar): value (if ``input`` is a scalar) or values selected at indices where condition is ``True``.
-        other (TensorDictBase or Scalar): value (if ``other`` is a scalar) or values selected at indices where condition is ``False``.
-        out (Tensor, optional): the output ``TensorDictBase`` instance.
-
-    """
-    if isinstance(out, PersistentTensorDict):
-        raise RuntimeError(
-            "Cannot use a persistent tensordict as output of torch.where."
-        )
-    return input.where(condition, other, out=out)
-
-
-def _stack_uninit_params(list_of_params, dim=0, out=None):
-    if out is not None:
-        raise NotImplementedError
-    if dim > 0:
-        raise NotImplementedError
-    from tensordict.utils import (
-        _BatchedUninitializedBuffer,
-        _BatchedUninitializedParameter,
-    )
-
-    if isinstance(list_of_params[0], UninitializedParameter):
-        out = _BatchedUninitializedParameter(
-            requires_grad=list_of_params[0].requires_grad,
-            device=list_of_params[0].device,
-            dtype=list_of_params[0].dtype,
-        )
-    elif isinstance(list_of_params[0], UninitializedBuffer):
-        out = _BatchedUninitializedBuffer(
-            requires_grad=list_of_params[0].requires_grad,
-            device=list_of_params[0].device,
-            dtype=list_of_params[0].dtype,
-        )
-    out.batch_size = torch.Size([len(list_of_params)])
-    return out
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+
+from typing import Any, Callable, Sequence, TypeVar
+
+import torch
+from tensordict._lazy import LazyStackedTensorDict
+from tensordict._td import TensorDict
+from tensordict.base import (
+    _is_leaf_nontensor,
+    _is_tensor_collection,
+    NO_DEFAULT,
+    TensorDictBase,
+)
+from tensordict.persistent import PersistentTensorDict
+from tensordict.utils import (
+    _check_keys,
+    _ErrorInteceptor,
+    _shape,
+    DeviceType,
+    is_non_tensor,
+    is_tensorclass,
+    lazy_legacy,
+    set_lazy_legacy,
+)
+from torch import Tensor
+from torch.nn.parameter import (
+    UninitializedBuffer,
+    UninitializedParameter,
+    UninitializedTensorMixin,
+)
+
+TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
+LAZY_TD_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
+T = TypeVar("T", bound="TensorDictBase")
+
+
+def implements_for_td(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDict."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        TD_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+def implements_for_lazy_td(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDict."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        LAZY_TD_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+@implements_for_td(torch.unbind)
+def _unbind(td: T, *args: Any, **kwargs: Any) -> tuple[T, ...]:
+    return td.unbind(*args, **kwargs)
+
+
+@implements_for_td(torch.gather)
+def _gather(
+    input: T,
+    dim: int,
+    index: Tensor,
+    *,
+    sparse_grad: bool = False,
+    out: T | None = None,
+) -> T:
+    if sparse_grad:
+        raise NotImplementedError(
+            "sparse_grad=True not implemented for torch.gather(tensordict, ...)"
+        )
+    # the index must have as many dims as the tensordict
+    if not len(index):
+        raise RuntimeError("Cannot use torch.gather with an empty index")
+    dim_orig = dim
+    if dim < 0:
+        dim = input.batch_dims + dim
+    if dim > input.batch_dims - 1 or dim < 0:
+        raise RuntimeError(
+            f"Cannot gather tensordict with shape {input.shape} along dim {dim_orig}."
+        )
+
+    def _gather_tensor(tensor, dest_container=None, dest_key=None):
+        if dest_container is not None:
+            dest = dest_container.get(dest_key)
+        else:
+            dest = None
+        index_expand = index
+        while index_expand.ndim < tensor.ndim:
+            index_expand = index_expand.unsqueeze(-1)
+        target_shape = list(tensor.shape)
+        target_shape[dim] = index_expand.shape[dim]
+        index_expand = index_expand.expand(target_shape)
+        out = torch.gather(tensor, dim, index_expand, out=dest)
+        return out
+
+    if out is None:
+        if len(index.shape) == input.ndim and input._has_names():
+            names = input.names
+        else:
+            names = None
+        return TensorDict(
+            {
+                key: _gather_tensor(value)
+                for key, value in input.items(is_leaf=_is_leaf_nontensor)
+            },
+            batch_size=index.shape,
+            names=names,
+        )
+    TensorDict(
+        {
+            key: _gather_tensor(value, out, key)
+            for key, value in input.items(is_leaf=_is_leaf_nontensor)
+        },
+        batch_size=index.shape,
+    )
+    return out
+
+
+@implements_for_td(torch.full_like)
+def _full_like(td: T, fill_value: float, *args, **kwargs: Any) -> T:
+    return td._fast_apply(
+        lambda x: torch.full_like(x, fill_value, *args, **kwargs),
+        inplace=True,
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+
+
+@implements_for_td(torch.zeros_like)
+def _zeros_like(td: T, *args, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(
+        lambda x: torch.zeros_like(x, *args, **kwargs),
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+    if "dtype" in kwargs:
+        raise ValueError("Cannot pass dtype to full_like with TensorDict")
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.ones_like)
+def _ones_like(td: T, *args, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(
+        lambda x: torch.ones_like(x, *args, **kwargs),
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.rand_like)
+def _rand_like(td: T, *args, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(
+        lambda x: torch.rand_like(x, *args, **kwargs),
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.randn_like)
+def _randn_like(td: T, *args, **kwargs: Any) -> T:
+    td_clone = td._fast_apply(
+        lambda x: torch.randn_like(x, *args, **kwargs),
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+    if "device" in kwargs:
+        td_clone = td_clone.to(kwargs.pop("device"))
+    if len(kwargs):
+        raise RuntimeError(
+            f"keyword arguments {list(kwargs.keys())} are not "
+            f"supported with full_like with TensorDict"
+        )
+    return td_clone
+
+
+@implements_for_td(torch.empty_like)
+def _empty_like(td: T, *args, **kwargs) -> T:
+    return td._fast_apply(
+        lambda x: torch.empty_like(x, *args, **kwargs),
+        propagate_lock=True,
+        device=kwargs.get("device", NO_DEFAULT),
+    )
+
+
+@implements_for_td(torch.clone)
+def _clone(td: T, *args: Any, **kwargs: Any) -> T:
+    return td.clone(*args, **kwargs)
+
+
+@implements_for_td(torch.squeeze)
+def _squeeze(td: T, *args: Any, **kwargs: Any) -> T:
+    return td.squeeze(*args, **kwargs)
+
+
+@implements_for_td(torch.unsqueeze)
+def _unsqueeze(td: T, *args: Any, **kwargs: Any) -> T:
+    return td.unsqueeze(*args, **kwargs)
+
+
+@implements_for_td(torch.masked_select)
+def _masked_select(td: T, *args: Any, **kwargs: Any) -> T:
+    return td.masked_select(*args, **kwargs)
+
+
+@implements_for_td(torch.permute)
+def _permute(td: T, dims: Sequence[int]) -> T:
+    return td.permute(*dims)
+
+
+@implements_for_td(torch.cat)
+def _cat(
+    list_of_tensordicts: Sequence[T],
+    dim: int = 0,
+    device: DeviceType | None = None,
+    out: T | None = None,
+) -> T:
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    batch_size = list(list_of_tensordicts[0].batch_size)
+    if dim < 0:
+        dim = len(batch_size) + dim
+    if dim >= len(batch_size):
+        raise RuntimeError(
+            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
+            f"={dim} and batch_size={batch_size}"
+        )
+    batch_size[dim] = sum([td.batch_size[dim] for td in list_of_tensordicts])
+    batch_size = torch.Size(batch_size)
+
+    # check that all tensordict match
+    keys = _check_keys(list_of_tensordicts, strict=True)
+    if out is None:
+        out = {}
+        for key in keys:
+            with _ErrorInteceptor(
+                key, "Attempted to concatenate tensors on different devices at key"
+            ):
+                out[key] = torch.cat(
+                    [td._get_str(key, NO_DEFAULT) for td in list_of_tensordicts], dim
+                )
+        if device is None:
+            device = list_of_tensordicts[0].device
+            for td in list_of_tensordicts[1:]:
+                if device == td.device:
+                    continue
+                else:
+                    device = None
+                    break
+        names = None
+        if list_of_tensordicts[0]._has_names():
+            names = list_of_tensordicts[0].names
+        return TensorDict(
+            out, device=device, batch_size=batch_size, _run_checks=False, names=names
+        )
+    else:
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and cat batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+
+        for key in keys:
+            with _ErrorInteceptor(
+                key, "Attempted to concatenate tensors on different devices at key"
+            ):
+                if isinstance(out, TensorDict):
+                    torch.cat(
+                        [td.get(key) for td in list_of_tensordicts],
+                        dim,
+                        out=out.get(key),
+                    )
+                else:
+                    out.set_(
+                        key, torch.cat([td.get(key) for td in list_of_tensordicts], dim)
+                    )
+        return out
+
+
+@implements_for_lazy_td(torch.cat)
+def _lazy_cat(
+    list_of_tensordicts: Sequence[LazyStackedTensorDict],
+    dim: int = 0,
+    out: LazyStackedTensorDict | None = None,
+) -> LazyStackedTensorDict:
+    # why aren't they feeding you?
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    batch_size = list(list_of_tensordicts[0].batch_size)
+    if dim < 0:
+        dim = len(batch_size) + dim
+    if dim >= len(batch_size):
+        raise RuntimeError(
+            f"dim must be in the range 0 <= dim < len(batch_size), got dim"
+            f"={dim} and batch_size={batch_size}"
+        )
+    stack_dim = list_of_tensordicts[0].stack_dim
+    if any((td.stack_dim != stack_dim) for td in list_of_tensordicts):
+        raise RuntimeError("cat lazy stacked tds must have same stack dim")
+
+    batch_size[dim] = sum(td.batch_size[dim] for td in list_of_tensordicts)
+    batch_size = torch.Size(batch_size)
+
+    new_dim = dim
+    if dim > stack_dim:
+        new_dim = dim - 1
+
+    if out is None:
+        out = []
+        if dim == stack_dim:  # if dim is stack, just add all to the same list
+            for lazy_td in list_of_tensordicts:
+                out += lazy_td.tensordicts
+        else:
+            for i in range(len(list_of_tensordicts[0].tensordicts)):
+                out.append(
+                    torch.cat(
+                        [lazy_td.tensordicts[i] for lazy_td in list_of_tensordicts],
+                        new_dim,
+                    )
+                )
+        return LazyStackedTensorDict(*out, stack_dim=stack_dim)
+    else:
+        if not isinstance(out, LazyStackedTensorDict):
+            return _cat(list_of_tensordicts, dim=dim, out=out)
+
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and cat batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+        if out.stack_dim != dim:
+            index_base = (slice(None),) * out.stack_dim
+            for i, sub_dest in enumerate(out.tensordicts):
+                index = index_base + (i,)
+                tds_to_cat = [_td[index] for _td in list_of_tensordicts]
+                torch.cat(tds_to_cat, dim, out=sub_dest)
+        else:
+            init_idx = 0
+            for td_in in list_of_tensordicts:
+                sub_dest = out.tensordicts[init_idx : init_idx + td_in.shape[dim]]
+                init_idx += init_idx + td_in.shape[dim]
+                LazyStackedTensorDict.maybe_dense_stack(sub_dest, out.stack_dim).update(
+                    td_in, inplace=True
+                )
+
+        return out
+
+
+@implements_for_td(torch.stack)
+def _stack(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    dim: int = 0,
+    device: DeviceType | None = None,
+    out: T | None = None,
+    strict: bool = False,
+    contiguous: bool = False,
+    maybe_dense_stack: bool = False,
+) -> T:
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    if all(is_non_tensor(td) for td in list_of_tensordicts):
+        from tensordict.tensorclass import NonTensorData
+
+        return NonTensorData._stack_non_tensor(list_of_tensordicts, dim=dim)
+
+    batch_size = list_of_tensordicts[0].batch_size
+    if dim < 0:
+        dim = len(batch_size) + dim + 1
+
+    for td in list_of_tensordicts[1:]:
+        if td.batch_size != list_of_tensordicts[0].batch_size:
+            raise RuntimeError(
+                "stacking tensordicts requires them to have congruent batch sizes, "
+                f"got td1.batch_size={td.batch_size} and td2.batch_size="
+                f"{list_of_tensordicts[0].batch_size}"
+            )
+
+    # check that all tensordict match
+    # Read lazy_legacy
+    _lazy_legacy = lazy_legacy()
+
+    if out is None:
+        # We need to handle tensordicts with exclusive keys and tensordicts with
+        # mismatching shapes.
+        # The first case is handled within _check_keys which fails if keys
+        # don't match exactly.
+        # The second requires a check over the tensor shapes.
+        device = list_of_tensordicts[0].device
+        if contiguous or not _lazy_legacy:
+            try:
+                keys = _check_keys(list_of_tensordicts, strict=True)
+            except KeyError:
+                if not _lazy_legacy and not contiguous:
+                    if maybe_dense_stack:
+                        with set_lazy_legacy(True):
+                            return _stack(list_of_tensordicts, dim=dim)
+                    else:
+                        raise RuntimeError(
+                            "The sets of keys in the tensordicts to stack are exclusive. "
+                            "Consider using `LazyStackedTensorDict.maybe_dense_stack` instead."
+                        )
+                raise
+
+            if all(
+                isinstance(_tensordict, LazyStackedTensorDict)
+                for _tensordict in list_of_tensordicts
+            ):
+                # Let's try to see if all tensors have the same shape
+                # If so, we can assume that we can densly stack the sub-tds
+                leaves = [
+                    torch.utils._pytree.tree_leaves(td) for td in list_of_tensordicts
+                ]
+                for x in zip(*leaves):
+                    # TODO: check what happens with non-tensor data here
+                    if len(x) == 1 or all(_x.shape == x[0].shape for _x in x[1:]):
+                        continue
+                    else:
+                        break
+                else:
+                    # make sure we completed the zip, since strict=True is only available for python >= 3.10
+                    if len(leaves) == 1 or all(
+                        len(_leaves) == len(leaves[0]) for _leaves in leaves[1:]
+                    ):
+                        lazy_stack_dim = list_of_tensordicts[0].stack_dim
+                        if dim <= lazy_stack_dim:
+                            lazy_stack_dim += 1
+                        else:
+                            dim = dim - 1
+                        return LazyStackedTensorDict(
+                            *[
+                                _stack(list(subtds), dim=dim)
+                                for subtds in zip(
+                                    *[td.tensordicts for td in list_of_tensordicts]
+                                )
+                            ],
+                            stack_dim=lazy_stack_dim,
+                        )
+                lazy_stack_dim = list_of_tensordicts[0].stack_dim
+                if dim <= lazy_stack_dim:
+                    lazy_stack_dim += 1
+                else:
+                    dim = dim - 1
+                return LazyStackedTensorDict(
+                    *[
+                        _stack(list_of_td, dim, maybe_dense_stack=maybe_dense_stack)
+                        for list_of_td in zip(
+                            *[td.tensordicts for td in list_of_tensordicts]
+                        )
+                    ],
+                    stack_dim=lazy_stack_dim,
+                )
+
+            out = {}
+            for key in keys:
+                out[key] = []
+                is_not_init = None
+                tensor_shape = None
+                is_tensor = None
+                for _tensordict in list_of_tensordicts:
+                    tensor = _tensordict._get_str(key, default=NO_DEFAULT)
+                    if is_tensor is None:
+                        tensor_cls = type(tensor)
+                        is_tensor = (
+                            not _is_tensor_collection(tensor_cls)
+                        ) or is_tensorclass(tensor_cls)
+                    if is_not_init is None:
+                        is_not_init = isinstance(tensor, UninitializedTensorMixin)
+                    if not is_not_init:
+                        new_tensor_shape = _shape(tensor)
+                        if tensor_shape is not None:
+                            if len(new_tensor_shape) != len(tensor_shape) or not all(
+                                s1 == s2 and s1 != -1
+                                for s1, s2 in zip(_shape(tensor), tensor_shape)
+                            ):
+                                # Nested tensors will require a lazy stack
+                                if maybe_dense_stack:
+                                    with set_lazy_legacy(True):
+                                        return _stack(list_of_tensordicts, dim=dim)
+                                else:
+                                    raise RuntimeError(
+                                        "The shapes of the tensors to stack is incompatible."
+                                    )
+                        else:
+                            tensor_shape = new_tensor_shape
+
+                    out[key].append(tensor)
+                out[key] = (out[key], is_not_init, is_tensor)
+
+            def stack_fn(key, values, is_not_init, is_tensor):
+                if is_not_init:
+                    return _stack_uninit_params(values, dim)
+                if is_tensor:
+                    return torch.stack(values, dim)
+                with _ErrorInteceptor(
+                    key, "Attempted to stack tensors on different devices at key"
+                ):
+                    return _stack(values, dim, maybe_dense_stack=maybe_dense_stack)
+
+            out = {
+                key: stack_fn(key, values, is_not_init, is_tensor)
+                for key, (values, is_not_init, is_tensor) in out.items()
+            }
+
+            return TensorDict(
+                out,
+                batch_size=LazyStackedTensorDict._compute_batch_size(
+                    batch_size, dim, len(list_of_tensordicts)
+                ),
+                device=device,
+                _run_checks=False,
+            )
+        else:
+            out = LazyStackedTensorDict(
+                *list_of_tensordicts,
+                stack_dim=dim,
+            )
+    else:
+        keys = _check_keys(list_of_tensordicts)
+        batch_size = list(batch_size)
+        batch_size.insert(dim, len(list_of_tensordicts))
+        batch_size = torch.Size(batch_size)
+
+        if out.batch_size != batch_size:
+            raise RuntimeError(
+                "out.batch_size and stacked batch size must match, "
+                f"got out.batch_size={out.batch_size} and batch_size"
+                f"={batch_size}"
+            )
+
+        out_keys = set(out.keys())
+        if strict:
+            in_keys = set(keys)
+            if len(out_keys - in_keys) > 0:
+                raise RuntimeError(
+                    "The output tensordict has keys that are missing in the "
+                    "tensordict that has to be written: {out_keys - in_keys}. "
+                    "As per the call to `stack(..., strict=True)`, this "
+                    "is not permitted."
+                )
+            elif len(in_keys - out_keys) > 0:
+                raise RuntimeError(
+                    "The resulting tensordict has keys that are missing in "
+                    f"its destination: {in_keys - out_keys}. As per the call "
+                    "to `stack(..., strict=True)`, this is not permitted."
+                )
+
+        try:
+            out._stack_onto_(list_of_tensordicts, dim)
+        except KeyError as err:
+            raise err
+    return out
+
+
+@implements_for_td(torch.split)
+def _split(
+    td: TensorDict, split_size_or_sections: int | list[int], dim: int = 0
+) -> list[TensorDictBase]:
+    return td.split(split_size_or_sections, dim)
+
+
+@implements_for_td(torch.where)
+def where(condition, input, other, *, out=None):
+    """Return a ``TensorDict`` of elements selected from either input or other, depending on condition.
+
+    Args:
+        condition (BoolTensor): When ``True`` (nonzero), yield ``input``, otherwise yield ``other``.
+        input (TensorDictBase or Scalar): value (if ``input`` is a scalar) or values selected at indices where condition is ``True``.
+        other (TensorDictBase or Scalar): value (if ``other`` is a scalar) or values selected at indices where condition is ``False``.
+        out (Tensor, optional): the output ``TensorDictBase`` instance.
+
+    """
+    if isinstance(out, PersistentTensorDict):
+        raise RuntimeError(
+            "Cannot use a persistent tensordict as output of torch.where."
+        )
+    return input.where(condition, other, out=out)
+
+
+def _stack_uninit_params(list_of_params, dim=0, out=None):
+    if out is not None:
+        raise NotImplementedError
+    if dim > 0:
+        raise NotImplementedError
+    from tensordict.utils import (
+        _BatchedUninitializedBuffer,
+        _BatchedUninitializedParameter,
+    )
+
+    if isinstance(list_of_params[0], UninitializedParameter):
+        out = _BatchedUninitializedParameter(
+            requires_grad=list_of_params[0].requires_grad,
+            device=list_of_params[0].device,
+            dtype=list_of_params[0].dtype,
+        )
+    elif isinstance(list_of_params[0], UninitializedBuffer):
+        out = _BatchedUninitializedBuffer(
+            requires_grad=list_of_params[0].requires_grad,
+            device=list_of_params[0].device,
+            dtype=list_of_params[0].dtype,
+        )
+    out.batch_size = torch.Size([len(list_of_params)])
+    return out
```

## tensordict/base.py

 * *Ordering differences only*

```diff
@@ -1,7616 +1,7616 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import abc
-import collections
-import concurrent.futures
-import contextlib
-import importlib
-import json
-import numbers
-import weakref
-from collections.abc import MutableMapping
-
-from concurrent.futures import ThreadPoolExecutor
-from copy import copy
-from functools import wraps
-from pathlib import Path
-from textwrap import indent
-from typing import (
-    Any,
-    Callable,
-    Generator,
-    Iterator,
-    List,
-    Optional,
-    OrderedDict,
-    overload,
-    Sequence,
-    Tuple,
-    Type,
-    TypeVar,
-    Union,
-)
-
-import numpy as np
-import torch
-
-from tensordict.memmap import MemoryMappedTensor
-from tensordict.utils import (
-    _CloudpickleWrapper,
-    _GENERIC_NESTED_ERR,
-    _get_shape_from_args,
-    _is_non_tensor,
-    _is_tensorclass,
-    _KEY_ERROR,
-    _proc_init,
-    _prune_selected_keys,
-    _set_max_batch_size,
-    _shape,
-    _split_tensordict,
-    _td_fields,
-    _unravel_key_to_tuple,
-    as_decorator,
-    Buffer,
-    cache,
-    convert_ellipsis_to_idx,
-    DeviceType,
-    erase_cache,
-    implement_for,
-    IndexType,
-    infer_size_impl,
-    int_generator,
-    is_non_tensor,
-    lazy_legacy,
-    lock_blocked,
-    NestedKey,
-    prod,
-    set_lazy_legacy,
-    TensorDictFuture,
-    unravel_key,
-    unravel_key_list,
-)
-from torch import distributed as dist, multiprocessing as mp, nn, Tensor
-from torch.nn.parameter import UninitializedTensorMixin
-from torch.utils._pytree import tree_map
-
-
-# NO_DEFAULT is used as a placeholder whenever the default is not provided.
-# Using None is not an option since `td.get(key, default=None)` is a valid usage.
-class _NoDefault:
-    def __new__(cls):
-        if not hasattr(cls, "instance"):
-            cls.instance = super(_NoDefault, cls).__new__(cls)
-        return cls.instance
-
-    def __bool__(self):
-        return False
-
-
-NO_DEFAULT = _NoDefault()
-
-
-class _NestedTensorsAsLists:
-    """Class used to iterate over leaves of lazily stacked tensordicts."""
-
-    def __new__(cls):
-        if not hasattr(cls, "instance"):
-            cls.instance = super(_NestedTensorsAsLists, cls).__new__(cls)
-        return cls.instance
-
-    def __bool__(self):
-        return False
-
-    def __call__(self, val):
-        return _default_is_leaf(val)
-
-
-_NESTED_TENSORS_AS_LISTS = _NestedTensorsAsLists()
-
-T = TypeVar("T", bound="TensorDictBase")
-
-
-class _BEST_ATTEMPT_INPLACE:
-    def __bool__(self):
-        # we use an exception to exit when running `inplace = BEST_ATTEMPT_INPLACE if inplace else False`
-        # more than once
-        raise NotImplementedError
-
-
-_has_mps = torch.backends.mps.is_available()
-_has_cuda = torch.cuda.is_available()
-
-BEST_ATTEMPT_INPLACE = _BEST_ATTEMPT_INPLACE()
-
-# some complex string used as separator to concatenate and split keys in
-# distributed frameworks
-CompatibleType = Union[
-    Tensor,
-]
-
-_STR_MIXED_INDEX_ERROR = "Received a mixed string-non string index. Only string-only or string-free indices are supported."
-
-_HEURISTIC_EXCLUDED = (Tensor, tuple, list, set, dict, np.ndarray)
-
-_TENSOR_COLLECTION_MEMO = {}
-
-
-class TensorDictBase(MutableMapping):
-    """TensorDictBase is an abstract parent class for TensorDicts, a torch.Tensor data container."""
-
-    _safe: bool = False
-    _lazy: bool = False
-    _inplace_set: bool = False
-    is_meta: bool = False
-    _is_locked: bool = False
-    _cache: bool = None
-    _is_non_tensor: bool = False
-    _memmap_prefix = None
-
-    def __bool__(self) -> bool:
-        raise RuntimeError("Converting a tensordict to boolean value is not permitted")
-
-    @abc.abstractmethod
-    def __ne__(self, other: object) -> T:
-        """NOT operation over two tensordicts, for evey key.
-
-        The two tensordicts must have the same key set.
-
-        Args:
-            other (TensorDictBase, dict, or float): the value to compare against.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __xor__(self, other: TensorDictBase | float):
-        """XOR operation over two tensordicts, for evey key.
-
-        The two tensordicts must have the same key set.
-
-        Args:
-            other (TensorDictBase, dict, or float): the value to compare against.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __or__(self, other: TensorDictBase | float) -> T:
-        """OR operation over two tensordicts, for evey key.
-
-        The two tensordicts must have the same key set.
-
-        Args:
-            other (TensorDictBase, dict, or float): the value to compare against.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __eq__(self, other: object) -> T:
-        """Compares two tensordicts against each other, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __ge__(self, other: object) -> T:
-        """Compares two tensordicts against each other using the "greater or equal" operator, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __gt__(self, other: object) -> T:
-        """Compares two tensordicts against each other using the "greater than" operator, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __le__(self, other: object) -> T:
-        """Compares two tensordicts against each other using the "lower or equal" operator, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def __lt__(self, other: object) -> T:
-        """Compares two tensordicts against each other using the "lower than" operator, for every key. The two tensordicts must have the same key set.
-
-        Returns:
-            a new TensorDict instance with all tensors are boolean
-            tensors of the same shape as the original tensors.
-
-        """
-        ...
-
-    def __repr__(self) -> str:
-        fields = _td_fields(self)
-        field_str = indent(f"fields={{{fields}}}", 4 * " ")
-        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-        device_str = indent(f"device={self.device}", 4 * " ")
-        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-        string = ",\n".join([field_str, batch_size_str, device_str, is_shared_str])
-        return f"{type(self).__name__}(\n{string})"
-
-    def __iter__(self) -> Generator:
-        """Iterates over the first shape-dimension of the tensordict."""
-        if not self.batch_dims:
-            raise StopIteration
-        yield from self.unbind(0)
-
-    def __len__(self) -> int:
-        """Returns the length of first dimension, if there is, otherwise 0."""
-        return self.shape[0] if self.batch_dims else 0
-
-    def __contains__(self, key: NestedKey) -> bool:
-        if isinstance(key, str):
-            return key in self.keys()
-        if isinstance(key, tuple):
-            key = unravel_key(key)
-            if not key:
-                raise RuntimeError(
-                    "key must be a NestedKey (a str or a possibly tuple of str)."
-                )
-            return key in self.keys(True, is_leaf=_is_leaf_nontensor)
-        raise RuntimeError(
-            "key must be a NestedKey (a str or a possibly tuple of str)."
-        )
-
-    def __getitem__(self, index: IndexType) -> T:
-        """Indexes all tensors according to the provided index.
-
-        The index can be a (nested) key or any valid shape index given the
-        tensordict batch size.
-
-        If the index is a nested key and the result is a :class:`~tensordict.NonTensorData`
-        object, the content of the non-tensor is returned.
-
-        Examples:
-            >>> td = TensorDict({"root": torch.arange(2), ("nested", "entry"): torch.arange(2)}, [2])
-            >>> td["root"]
-            torch.tensor([0, 1])
-            >>> td["nested", "entry"]
-            torch.tensor([0, 1])
-            >>> td[:1]
-            TensorDict(
-                fields={
-                    nested: TensorDict(
-                        fields={
-                            entry: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([1]),
-                        device=None,
-                        is_shared=False),
-                    root: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([1]),
-                device=None,
-                is_shared=False)
-        """
-        istuple = isinstance(index, tuple)
-        if istuple or isinstance(index, str):
-            # _unravel_key_to_tuple will return an empty tuple if the index isn't a NestedKey
-            idx_unravel = _unravel_key_to_tuple(index)
-            if idx_unravel:
-                result = self._get_tuple(idx_unravel, NO_DEFAULT)
-                if is_non_tensor(result):
-                    result_data = getattr(result, "data", NO_DEFAULT)
-                    if result_data is NO_DEFAULT:
-                        return result.tolist()
-                    return result_data
-                return result
-
-        if (istuple and not index) or (not istuple and index is Ellipsis):
-            # empty tuple returns self
-            return self
-        if not istuple:
-            if isinstance(index, int):
-                return self._index_tensordict(index)
-            # we only want tuple indices
-            index = (index,)
-        # # convert range/np.ndarray to tensor: this is not cheap
-        # index = tuple(
-        #     torch.tensor(idx) if isinstance(idx, (np.ndarray, range)) else idx
-        #     for idx in index
-        # )
-        if istuple and any(idx is Ellipsis for idx in index):
-            index = convert_ellipsis_to_idx(index, self.batch_size)
-        if all(isinstance(idx, slice) and idx == slice(None) for idx in index):
-            return self
-
-        return self._index_tensordict(index)
-
-    # this is necessary for data collectors for instance, otherwise indexing
-    # will always be achieved one element at a time.
-    __getitems__ = __getitem__
-
-    def _get_sub_tensordict(self, idx: IndexType) -> T:
-        """Returns a _SubTensorDict with the desired index."""
-        from tensordict._td import _SubTensorDict
-
-        return _SubTensorDict(source=self, idx=idx)
-
-    @abc.abstractmethod
-    def __setitem__(
-        self,
-        index: IndexType,
-        value: T | dict | numbers.Number | CompatibleType,
-    ) -> None:
-        ...
-
-    def __delitem__(self, key: NestedKey) -> T:
-        return self.del_(key)
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        from tensordict._torch_func import TD_HANDLED_FUNCTIONS
-
-        if kwargs is None:
-            kwargs = {}
-        if func not in TD_HANDLED_FUNCTIONS or not all(
-            issubclass(t, (Tensor, TensorDictBase)) for t in types
-        ):
-            return NotImplemented
-        return TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-
-    @abc.abstractmethod
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        """Checks if all values are True/non-null in the tensordict.
-
-        Args:
-            dim (int, optional): if ``None``, returns a boolean indicating
-                whether all tensors return `tensor.all() == True`
-                If integer, all is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        """Checks if any value is True/non-null in the tensordict.
-
-        Args:
-            dim (int, optional): if ``None``, returns a boolean indicating
-                whether all tensors return `tensor.any() == True`.
-                If integer, all is called upon the dimension specified if
-                and only if this dimension is compatible with
-                the tensordict shape.
-
-        """
-        ...
-
-    def mean(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the mean value of all elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the mean value of all leaves (if this can be computed).
-                If integer or tuple of integers, `mean` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            dtype (torch.dtype, optional) – the desired data type of returned tensor.
-                If specified, the input tensor is casted to dtype before the operation is performed.
-                This is useful for preventing data type overflows. Default: ``None``.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="mean", dim=dim, keepdim=keepdim, dtype=dtype
-        )
-
-    def nanmean(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the mean of all non-NaN elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the mean value of all leaves (if this can be computed).
-                If integer or tuple of integers, `mean` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            dtype (torch.dtype, optional) – the desired data type of returned tensor.
-                If specified, the input tensor is casted to dtype before the operation is performed.
-                This is useful for preventing data type overflows. Default: ``None``.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="nanmean", keepdim=keepdim, dim=dim, dtype=dtype
-        )
-
-    def prod(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the produce of values of all elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the prod value of all leaves (if this can be computed).
-                If integer or tuple of integers, `prod` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            dtype (torch.dtype, optional) – the desired data type of returned tensor.
-                If specified, the input tensor is casted to dtype before the operation is performed.
-                This is useful for preventing data type overflows. Default: ``None``.
-
-        """
-        result = self._cast_reduction(
-            reduction_name="prod", dim=dim, keepdim=False, tuple_ok=False, dtype=dtype
-        )
-        if keepdim:
-            if isinstance(dim, tuple):
-                dim = dim[0]
-            if dim not in (None, NO_DEFAULT):
-                result = result.unsqueeze(dim)
-            else:
-                result = result.reshape([1 for _ in self.shape])
-        return result
-
-    def sum(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the sum value of all elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the sum value of all leaves (if this can be computed).
-                If integer or tuple of integers, `sum` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            dtype (torch.dtype, optional) – the desired data type of returned tensor.
-                If specified, the input tensor is casted to dtype before the operation is performed.
-                This is useful for preventing data type overflows. Default: ``None``.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="sum", dim=dim, keepdim=keepdim, dtype=dtype
-        )
-
-    def nansum(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the sum of all non-NaN elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the sum value of all leaves (if this can be computed).
-                If integer or tuple of integers, `sum` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            dtype (torch.dtype, optional) – the desired data type of returned tensor.
-                If specified, the input tensor is casted to dtype before the operation is performed.
-                This is useful for preventing data type overflows. Default: ``None``.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="nansum", dim=dim, keepdim=keepdim, dtype=dtype
-        )
-
-    def std(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        correction: int = 1,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the standard deviation value of all elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the sum value of all leaves (if this can be computed).
-                If integer or tuple of integers, `std` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            correction (int): difference between the sample size and sample degrees of freedom.
-                Defaults to Bessel’s correction, correction=1.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="std",
-            dim=dim,
-            keepdim=keepdim,
-            correction=correction,
-        )
-
-    def var(
-        self,
-        dim: int | Tuple[int] = NO_DEFAULT,
-        keepdim: bool = NO_DEFAULT,
-        *,
-        correction: int = 1,
-    ) -> bool | TensorDictBase:  # noqa: D417
-        """Returns the variance value of all elements in the input tensordit.
-
-        Args:
-            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
-                tensordict containing the sum value of all leaves (if this can be computed).
-                If integer or tuple of integers, `var` is called upon the dimension specified if
-                and only if this dimension is compatible with the tensordict
-                shape.
-            keepdim (bool) – whether the output tensor has dim retained or not.
-
-        Keyword Args:
-            correction (int): difference between the sample size and sample degrees of freedom.
-                Defaults to Bessel’s correction, correction=1.
-
-        """
-        if dim is NO_DEFAULT and keepdim:
-            dim = None
-        return self._cast_reduction(
-            reduction_name="var",
-            dim=dim,
-            keepdim=keepdim,
-            correction=correction,
-        )
-
-    @abc.abstractmethod
-    def _cast_reduction(
-        self,
-        *,
-        reduction_name,
-        dim=NO_DEFAULT,
-        keepdim=NO_DEFAULT,
-        dtype,
-        tuple_ok=True,
-        **kwargs,
-    ):
-        ...
-
-    def auto_batch_size_(self, batch_dims: int | None = None) -> T:
-        """Sets the maximum batch-size for the tensordict, up to an optional batch_dims.
-
-        Args:
-            batch_dims (int, optional): if provided, the batch-size will be at
-                most ``batch_dims`` long.
-
-        Returns:
-            self
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>> td = TensorDict({"a": torch.randn(3, 4, 5), "b": {"c": torch.randn(3, 4, 6)}}, batch_size=[])
-            >>> td.auto_batch_size_()
-            >>> print(td.batch_size)
-            torch.Size([3, 4])
-            >>> td.auto_batch_size_(batch_dims=1)
-            >>> print(td.batch_size)
-            torch.Size([3])
-
-        """
-        _set_max_batch_size(self, batch_dims)
-        return self
-
-    @abc.abstractmethod
-    def from_dict_instance(
-        self, input_dict, batch_size=None, device=None, batch_dims=None
-    ):
-        """Instance method version of :meth:`~tensordict.TensorDict.from_dict`.
-
-        Unlike :meth:`~tensordict.TensorDict.from_dict`, this method will
-        attempt to keep the tensordict types within the existing tree (for
-        any existing leaf).
-
-        Examples:
-            >>> from tensordict import TensorDict, tensorclass
-            >>> import torch
-            >>>
-            >>> @tensorclass
-            >>> class MyClass:
-            ...     x: torch.Tensor
-            ...     y: int
-            >>>
-            >>> td = TensorDict({"a": torch.randn(()), "b": MyClass(x=torch.zeros(()), y=1)})
-            >>> print(td.from_dict_instance(td.to_dict()))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: MyClass(
-                        x=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                        y=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> print(td.from_dict(td.to_dict()))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                            y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        ...
-
-    @classmethod
-    def from_h5(cls, filename, mode="r"):
-        """Creates a PersistentTensorDict from a h5 file.
-
-        This function will automatically determine the batch-size for each nested
-        tensordict.
-
-        Args:
-            filename (str): the path to the h5 file.
-            mode (str, optional): reading mode. Defaults to ``"r"``.
-        """
-        from tensordict.persistent import PersistentTensorDict
-
-        return PersistentTensorDict.from_h5(filename, mode=mode)
-
-    # Module interaction
-    @classmethod
-    def from_module(
-        cls,
-        module,
-        as_module: bool = False,
-        lock: bool = True,
-        use_state_dict: bool = False,
-    ):
-        """Copies the params and buffers of a module in a tensordict.
-
-        Args:
-            module (nn.Module): the module to get the parameters from.
-            as_module (bool, optional): if ``True``, a :class:`~tensordict.nn.TensorDictParams`
-                instance will be returned which can be used to store parameters
-                within a :class:`torch.nn.Module`. Defaults to ``False``.
-            lock (bool, optional): if ``True``, the resulting tensordict will be locked.
-                Defaults to ``True``.
-            use_state_dict (bool, optional): if ``True``, the state-dict from the
-                module will be used and unflattened into a TensorDict with
-                the tree structure of the model. Defaults to ``False``.
-                .. note::
-                  This is particularly useful when state-dict hooks have to be
-                  used.
-
-        Examples:
-            >>> from torch import nn
-            >>> module = nn.TransformerDecoder(
-            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
-            ...     num_layers=1)
-            >>> params = TensorDict.from_module(module)
-            >>> print(params["layers", "0", "linear1"])
-            TensorDict(
-                fields={
-                    bias: Parameter(shape=torch.Size([2048]), device=cpu, dtype=torch.float32, is_shared=False),
-                    weight: Parameter(shape=torch.Size([2048, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-        """
-        ...
-
-    @classmethod
-    def from_modules(
-        cls,
-        *modules,
-        as_module: bool = False,
-        lock: bool = True,
-        use_state_dict: bool = False,
-        lazy_stack: bool = False,
-    ):
-        """Retrieves the parameters of several modules for ensebmle learning/feature of expects applications through vmap.
-
-        Args:
-            modules (sequence of nn.Module): the modules to get the parameters from.
-                If the modules differ in their structure, a lazy stack is needed
-                (see the ``lazy_stack`` argument below).
-
-        Keyword Args:
-            as_module (bool, optional): if ``True``, a :class:`~tensordict.nn.TensorDictParams`
-                instance will be returned which can be used to store parameters
-                within a :class:`torch.nn.Module`. Defaults to ``False``.
-            lock (bool, optional): if ``True``, the resulting tensordict will be locked.
-                Defaults to ``True``.
-            use_state_dict (bool, optional): if ``True``, the state-dict from the
-                module will be used and unflattened into a TensorDict with
-                the tree structure of the model. Defaults to ``False``.
-                .. note::
-                  This is particularly useful when state-dict hooks have to be
-                  used.
-            lazy_stack (bool, optional): whether parameters should be densly or
-                lazily stacked. Defaults to ``False`` (dense stack).
-
-                .. note:: ``lazy_stack`` and ``as_module`` are exclusive features.
-
-                .. warning::
-                    There is a crucial difference between lazy and non-lazy outputs
-                    in that non-lazy output will reinstantiate parameters with the
-                    desired batch-size, while ``lazy_stack`` will just represent
-                    the parameters as lazily stacked. This means that whilst the
-                    original parameters can safely be passed to an optimizer
-                    when ``lazy_stack=True``, the new parameters need to be passed
-                    when it is set to ``True``.
-
-                .. warning::
-                    Whilst it can be tempting to use a lazy stack to keep the
-                    orignal parameter references, remember that lazy stack
-                    perform a stack each time :meth:`~.get` is called. This will
-                    require memory (N times the size of the parameters, more if a
-                    graph is built) and time to be computed.
-                    It also means that the optimizer(s) will contain more
-                    parameters, and operations like :meth:`~torch.optim.Optimizer.step`
-                    or :meth:`~torch.optim.Optimizer.zero_grad` will take longer
-                    to be executed. In general, ``lazy_stack`` should be reserved
-                    to very few use cases.
-
-        Examples:
-            >>> from torch import nn
-            >>> from tensordict import TensorDict
-            >>> torch.manual_seed(0)
-            >>> empty_module = nn.Linear(3, 4, device="meta")
-            >>> n_models = 2
-            >>> modules = [nn.Linear(3, 4) for _ in range(n_models)]
-            >>> params = TensorDict.from_modules(*modules)
-            >>> print(params)
-            TensorDict(
-                fields={
-                    bias: Parameter(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                    weight: Parameter(shape=torch.Size([2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False)
-            >>> # example of batch execution
-            >>> def exec_module(params, x):
-            ...     with params.to_module(empty_module):
-            ...         return empty_module(x)
-            >>> x = torch.randn(3)
-            >>> y = torch.vmap(exec_module, (0, None))(params, x)
-            >>> assert y.shape == (n_models, 4)
-            >>> # since lazy_stack = False, backprop leaves the original params untouched
-            >>> y.sum().backward()
-            >>> assert params["weight"].grad.norm() > 0
-            >>> assert modules[0].weight.grad is None
-
-        With ``lazy_stack=True``, things are slightly different:
-
-            >>> params = TensorDict.from_modules(*modules, lazy_stack=True)
-            >>> print(params)
-            LazyStackedTensorDict(
-                fields={
-                    bias: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                    weight: Tensor(shape=torch.Size([2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                exclusive_fields={
-                },
-                batch_size=torch.Size([2]),
-                device=None,
-                is_shared=False,
-                stack_dim=0)
-            >>> # example of batch execution
-            >>> y = torch.vmap(exec_module, (0, None))(params, x)
-            >>> assert y.shape == (n_models, 4)
-            >>> y.sum().backward()
-            >>> assert modules[0].weight.grad is not None
-
-
-        """
-        param_list = [
-            cls.from_module(module, use_state_dict=use_state_dict) for module in modules
-        ]
-        if lazy_stack:
-            from tensordict._lazy import LazyStackedTensorDict
-
-            for param in param_list:
-                if any(
-                    isinstance(tensor, UninitializedTensorMixin)
-                    for tensor in param.values(True, True)
-                ):
-                    raise RuntimeError(
-                        "lasy_stack=True is not compatible with lazy modules."
-                    )
-            params = LazyStackedTensorDict.lazy_stack(param_list)
-        else:
-            with set_lazy_legacy(False), torch.no_grad():
-                params = torch.stack(param_list)
-
-            # Make sure params are params, buffers are buffers
-            def make_param(param, orig_param):
-                if isinstance(param, UninitializedTensorMixin):
-                    return param
-                if isinstance(orig_param, nn.Parameter):
-                    return nn.Parameter(param.detach(), orig_param.requires_grad)
-                return Buffer(param)
-
-            params = params._fast_apply(make_param, param_list[0], propagate_lock=True)
-        if as_module:
-            from tensordict.nn import TensorDictParams
-
-            params = TensorDictParams(params, no_convert=True)
-        if lock:
-            params.lock_()
-        return params
-
-    @as_decorator()
-    def to_module(
-        self,
-        module: nn.Module,
-        *,
-        inplace: bool | None = None,
-        return_swap: bool = True,
-        swap_dest=None,
-        use_state_dict: bool = False,
-        non_blocking: bool = False,
-        memo=None,  # deprecated
-    ):
-        """Writes the content of a TensorDictBase instance onto a given nn.Module attributes, recursively.
-
-        Args:
-            module (nn.Module): a module to write the parameters into.
-
-        Keyword Args:
-            inplace (bool, optional): if ``True``, the parameters or tensors
-                in the module are updated in-place. Defaults to ``True``.
-            return_swap (bool, optional): if ``True``, the old parameter configuration
-                will be returned. Defaults to ``False``.
-            swap_dest (TensorDictBase, optional): if ``return_swap`` is ``True``,
-                the tensordict where the swap should be written.
-            use_state_dict (bool, optional): if ``True``, state-dict API will be
-                used to load the parameters (including the state-dict hooks).
-                Defaults to ``False``.
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Examples:
-            >>> from torch import nn
-            >>> module = nn.TransformerDecoder(
-            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
-            ...     num_layers=1)
-            >>> params = TensorDict.from_module(module)
-            >>> params.zero_()
-            >>> params.to_module(module)
-            >>> assert (module.layers[0].linear1.weight == 0).all()
-        """
-        if memo is not None:
-            raise RuntimeError("memo cannot be passed to the public to_module anymore.")
-        hooks = getattr(
-            torch.nn.modules.module, "_global_parameter_registration_hooks", {}
-        )
-        memo = {"hooks": tuple(hooks.values())}
-        return self._to_module(
-            module=module,
-            inplace=inplace,
-            return_swap=return_swap,
-            swap_dest=swap_dest,
-            memo=memo,
-            use_state_dict=use_state_dict,
-            non_blocking=non_blocking,
-        )
-
-    @abc.abstractmethod
-    def _to_module(
-        self,
-        module,
-        *,
-        inplace: bool | None = None,
-        return_swap: bool = True,
-        swap_dest=None,
-        memo=None,
-        use_state_dict: bool = False,
-        non_blocking: bool = False,
-    ):
-        ...
-
-    # Shape functionality
-    @property
-    def shape(self) -> torch.Size:
-        """See :obj:`~tensordict.TensorDictBase.batch_size`."""
-        return self.batch_size
-
-    @property
-    @abc.abstractmethod
-    def batch_size(self) -> torch.Size:
-        """Shape (or batch_size) of a TensorDict.
-
-        The shape of a tensordict corresponds to the common first ``N``
-        dimensions of the tensors it contains, where ``N`` is an arbitrary
-        number.
-        The ``TensorDict`` shape is controlled by the user upon
-        initialization (ie, it is not inferred from the tensor shapes).
-
-        The ``batch_size`` can be edited dynamically if the new size is compatible
-        with the TensorDict content. For instance, setting the batch size to
-        an empty value is always allowed.
-
-        Returns:
-            a :obj:`~torch.Size` object describing the TensorDict batch size.
-
-        Examples:
-            >>> data = TensorDict({
-            ...     "key 0": torch.randn(3, 4),
-            ...     "key 1": torch.randn(3, 5),
-            ...     "nested": TensorDict({"key 0": torch.randn(3, 4)}, batch_size=[3, 4])},
-            ...     batch_size=[3])
-            >>> data.batch_size = () # resets the batch-size to an empty value
-        """
-        ...
-
-    def size(self, dim: int | None = None) -> torch.Size | int:
-        """Returns the size of the dimension indicated by ``dim``.
-
-        If ``dim`` is not specified, returns the ``batch_size`` attribute of the TensorDict.
-
-        """
-        if dim is None:
-            return self.batch_size
-        return self.batch_size[dim]
-
-    @property
-    def data(self):
-        """Returns a tensordict containing the .data attributes of the leaf tensors."""
-        return self._data()
-
-    @property
-    def grad(self):
-        """Returns a tensordict containing the .grad attributes of the leaf tensors."""
-        return self._grad()
-
-    @cache  # noqa
-    def _dtype(self):
-        dtype = None
-        for val in self.values(True, True):
-            val_dtype = getattr(val, "dtype", None)
-            if dtype is None and val_dtype is not None:
-                dtype = val_dtype
-            elif dtype is not None and val_dtype is not None and dtype != val_dtype:
-                return None
-        return dtype
-
-    @property
-    def dtype(self):
-        """Returns the dtype of the values in the tensordict, if it is unique."""
-        return self._dtype()
-
-    def _batch_size_setter(self, new_batch_size: torch.Size) -> None:
-        if new_batch_size == self.batch_size:
-            return
-        if self._lazy:
-            raise RuntimeError(
-                "modifying the batch size of a lazy representation of a "
-                "tensordict is not permitted. Consider instantiating the "
-                "tensordict first by calling `td = td.to_tensordict()` before "
-                "resetting the batch size."
-            )
-        if not isinstance(new_batch_size, torch.Size):
-            new_batch_size = torch.Size(new_batch_size)
-        for key, value in self.items():
-            if _is_tensor_collection(type(value)):
-                if len(value.batch_size) < len(new_batch_size):
-                    # document as edge case
-                    value.batch_size = new_batch_size
-                    self._set_str(
-                        key, value, inplace=True, validated=True, non_blocking=False
-                    )
-        self._check_new_batch_size(new_batch_size)
-        self._change_batch_size(new_batch_size)
-        if self._has_names():
-            # if the tensordict has dim names and the new batch-size has more dims,
-            # we can simply add empty names after the current ones.
-            # Otherwise, we discard the extra existing names.
-            names = self.names
-            if len(names) < len(new_batch_size):
-                self.names = names + [None] * (len(new_batch_size) - len(names))
-            else:
-                self.names = names[: self.batch_dims]
-
-    @property
-    def batch_dims(self) -> int:
-        """Length of the tensordict batch size.
-
-        Returns:
-            int describing the number of dimensions of the tensordict.
-
-        """
-        return len(self.batch_size)
-
-    def ndimension(self) -> int:
-        """See :meth:`~.batch_dims`."""
-        return self.batch_dims
-
-    @property
-    def ndim(self) -> int:
-        """See :meth:`~.batch_dims`."""
-        return self.batch_dims
-
-    def dim(self) -> int:
-        """See :meth:`~.batch_dims`."""
-        return self.batch_dims
-
-    def numel(self) -> int:
-        """Total number of elements in the batch.
-
-        Lower-bounded to 1, as a stack of two tensordict with empty shape will
-        have two elements, therefore we consider that a tensordict is at least
-        1-element big.
-        """
-        return max(1, self.batch_size.numel())
-
-    @property
-    def depth(self) -> int:
-        """Returns the depth - maximum number of levels - of a tensordict.
-
-        The minimum depth is 0 (no nested tensordict).
-        """
-        return self._depth()
-
-    @cache  # noqa: B019
-    def _depth(self):
-        depth = 0
-        for key in self.keys(True, True, is_leaf=_is_leaf_nontensor):
-            if isinstance(key, tuple):
-                depth = max(depth, len(key) - 1)
-        return depth
-
-    @overload
-    def expand(self, *shape: int) -> T:
-        ...
-
-    @overload
-    def expand(self, shape: torch.Size) -> T:
-        ...
-
-    @abc.abstractmethod
-    def expand(self, *args: int | torch.Size) -> T:
-        """Expands each tensor of the tensordict according to the :func:`~torch.expand` function, ignoring the feature dimensions.
-
-        Supports iterables to specify the shape.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'a': torch.zeros(3, 4, 5),
-            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
-            >>> td_expand = td.expand(10, 3, 4)
-            >>> assert td_expand.shape == torch.Size([10, 3, 4])
-            >>> assert td_expand.get("a").shape == torch.Size([10, 3, 4, 5])
-
-        """
-        ...
-
-    def expand_as(self, other: TensorDictBase | torch.Tensor) -> TensorDictBase:
-        """Broadcasts the shape of the tensordict to the shape of `other` and expands it accordingly.
-
-        If the input is a tensor collection (tensordict or tensorclass),
-        the leaves will be expanded on a one-to-one basis.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>> td0 = TensorDict({
-            ...     "a": torch.ones(3, 1, 4),
-            ...     "b": {"c": torch.ones(3, 2, 1, 4)}},
-            ...     batch_size=[3],
-            ... )
-            >>> td1 = TensorDict({
-            ...     "a": torch.zeros(2, 3, 5, 4),
-            ...     "b": {"c": torch.zeros(2, 3, 2, 6, 4)}},
-            ...     batch_size=[2, 3],
-            ... )
-            >>> expanded = td0.expand_as(td1)
-            >>> assert (expanded==1).all()
-            >>> print(expanded)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([2, 3, 5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([2, 3, 2, 6, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([2, 3]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([2, 3]),
-                device=None,
-                is_shared=False)
-
-        """
-        if _is_tensor_collection(type(other)):
-            return self.apply(
-                lambda x, y: x.expand_as(y), other, batch_size=other.batch_size
-            )
-        return self.expand(other.shape)
-
-    def unbind(self, dim: int) -> tuple[T, ...]:
-        """Returns a tuple of indexed tensordicts, unbound along the indicated dimension.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(12).reshape(3, 4),
-            ... }, batch_size=[3, 4])
-            >>> td0, td1, td2 = td.unbind(0)
-            >>> td0['x']
-            tensor([0, 1, 2, 3])
-            >>> td1['x']
-            tensor([4, 5, 6, 7])
-
-        """
-        batch_dims = self.batch_dims
-        if dim < -batch_dims or dim >= batch_dims:
-            raise RuntimeError(
-                f"the dimension provided ({dim}) is beyond the tensordict dimensions ({self.ndim})."
-            )
-        if dim < 0:
-            dim = batch_dims + dim
-        results = self._unbind(dim)
-        if self._is_memmap or self._is_shared:
-            for result in results:
-                result.lock_()
-        return results
-
-    @abc.abstractmethod
-    def _unbind(self, dim: int) -> tuple[T, ...]:
-        ...
-
-    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
-        """Splits a tensordict into the specified number of chunks, if possible.
-
-        Each chunk is a view of the input tensordict.
-
-        Args:
-            chunks (int): number of chunks to return
-            dim (int, optional): dimension along which to split the
-                tensordict. Default is 0.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(24).reshape(3, 4, 2),
-            ... }, batch_size=[3, 4])
-            >>> td0, td1 = td.chunk(dim=-1, chunks=2)
-            >>> td0['x']
-            tensor([[[ 0,  1],
-                     [ 2,  3]],
-                    [[ 8,  9],
-                     [10, 11]],
-                    [[16, 17],
-                     [18, 19]]])
-
-        """
-        if chunks < 1:
-            raise ValueError(
-                f"chunks must be a strictly positive integer, got {chunks}."
-            )
-        # fall back on split, using upper rounding
-        split_size = -(self.batch_size[dim] // -chunks)
-        return self.split(split_size, dim=dim)
-
-    @overload
-    def unsqueeze(self, dim: int) -> T:
-        ...
-
-    @as_decorator()
-    def unsqueeze(self, *args, **kwargs):
-        """Unsqueezes all tensors for a dimension comprised in between `-td.batch_dims` and `td.batch_dims` and returns them in a new tensordict.
-
-        Args:
-            dim (int): dimension along which to unsqueeze
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(24).reshape(3, 4, 2),
-            ... }, batch_size=[3, 4])
-            >>> td = td.unsqueeze(-2)
-            >>> td.shape
-            torch.Size([3, 1, 4])
-            >>> td.get("x").shape
-            torch.Size([3, 1, 4, 2])
-
-        This operation can be used as a context manager too. Changes to the original
-        tensordict will occur out-place, i.e. the content of the original tensors
-        will not be altered. This also assumes that the tensordict is not locked
-        (otherwise, unlocking the tensordict is necessary).
-
-            >>> td = TensorDict({
-            ...     'x': torch.arange(24).reshape(3, 4, 2),
-            ... }, batch_size=[3, 4])
-            >>> with td.unsqueeze(-2) as tds:
-            ...     tds.set("y", torch.zeros(3, 1, 4))
-            >>> assert td.get("y").shape == [3, 4]
-
-        """
-        _lazy_legacy = lazy_legacy()
-
-        if _lazy_legacy:
-            return self._legacy_unsqueeze(*args, **kwargs)
-        else:
-            result = self._unsqueeze(*args, **kwargs)
-            if result._is_memmap or result._is_shared:
-                result.lock_()
-            return result
-
-    @abc.abstractmethod
-    def _unsqueeze(self, dim):
-        ...
-
-    def _legacy_unsqueeze(self, dim: int) -> T:
-        if dim < 0:
-            dim = self.batch_dims + dim + 1
-
-        if (dim > self.batch_dims) or (dim < 0):
-            raise RuntimeError(
-                f"unsqueezing is allowed for dims comprised between "
-                f"`-td.batch_dims` and `td.batch_dims` only. Got "
-                f"dim={dim} with a batch size of {self.batch_size}."
-            )
-        from tensordict._lazy import _UnsqueezedTensorDict
-
-        return _UnsqueezedTensorDict(
-            source=self,
-            custom_op="unsqueeze",
-            inv_op="squeeze",
-            custom_op_kwargs={"dim": dim},
-            inv_op_kwargs={"dim": dim},
-        )
-
-    @overload
-    def squeeze(self, dim: int | None = None) -> T:
-        ...
-
-    @as_decorator()
-    def squeeze(self, *args, **kwargs):
-        """Squeezes all tensors for a dimension in between `-self.batch_dims+1` and `self.batch_dims-1` and returns them in a new tensordict.
-
-        Args:
-            dim (Optional[int]): dimension along which to squeeze. If dim is
-                ``None``, all singleton dimensions will be squeezed.
-                Defaults to ``None``.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(24).reshape(3, 1, 4, 2),
-            ... }, batch_size=[3, 1, 4])
-            >>> td = td.squeeze()
-            >>> td.shape
-            torch.Size([3, 4])
-            >>> td.get("x").shape
-            torch.Size([3, 4, 2])
-
-        This operation can be used as a context manager too. Changes to the original
-        tensordict will occur out-place, i.e. the content of the original tensors
-        will not be altered. This also assumes that the tensordict is not locked
-        (otherwise, unlocking the tensordict is necessary). This functionality is
-        *not* compatible with implicit squeezing.
-
-            >>> td = TensorDict({
-            ...     'x': torch.arange(24).reshape(3, 1, 4, 2),
-            ... }, batch_size=[3, 1, 4])
-            >>> with td.squeeze(1) as tds:
-            ...     tds.set("y", torch.zeros(3, 4))
-            >>> assert td.get("y").shape == [3, 1, 4]
-
-        """
-        _lazy_legacy = lazy_legacy()
-
-        if _lazy_legacy:
-            return self._legacy_squeeze(*args, **kwargs)
-        else:
-            result = self._squeeze(*args, **kwargs)
-            if result._is_memmap or result._is_shared:
-                result.lock_()
-            return result
-
-    @abc.abstractmethod
-    def _squeeze(self, dim=None):
-        ...
-
-    def _legacy_squeeze(self, dim: int | None = None) -> T:
-        from tensordict._lazy import _SqueezedTensorDict
-
-        if dim is None:
-            size = self.size()
-            if len(self.size()) == 1 or size.count(1) == 0:
-                return self
-            first_singleton_dim = size.index(1)
-
-            squeezed_dict = _SqueezedTensorDict(
-                source=self,
-                custom_op="squeeze",
-                inv_op="unsqueeze",
-                custom_op_kwargs={"dim": first_singleton_dim},
-                inv_op_kwargs={"dim": first_singleton_dim},
-            )
-            return squeezed_dict.squeeze(dim=None)
-
-        if dim < 0:
-            dim = self.batch_dims + dim
-
-        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
-            raise RuntimeError(
-                f"squeezing is allowed for dims comprised between 0 and "
-                f"td.batch_dims only. Got dim={dim} and batch_size"
-                f"={self.batch_size}."
-            )
-
-        if dim >= self.batch_dims or self.batch_size[dim] != 1:
-            return self
-
-        return _SqueezedTensorDict(
-            source=self,
-            custom_op="squeeze",
-            inv_op="unsqueeze",
-            custom_op_kwargs={"dim": dim},
-            inv_op_kwargs={"dim": dim},
-        )
-
-    @overload
-    def reshape(self, *shape: int):
-        ...
-
-    @overload
-    def reshape(self, shape: list | tuple):
-        ...
-
-    @abc.abstractmethod
-    def reshape(
-        self,
-        *args,
-        **kwargs,
-    ) -> T:
-        """Returns a contiguous, reshaped tensor of the desired shape.
-
-        Args:
-            *shape (int): new shape of the resulting tensordict.
-
-        Returns:
-            A TensorDict with reshaped keys
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(12).reshape(3, 4),
-            ... }, batch_size=[3, 4])
-            >>> td = td.reshape(12)
-            >>> print(td['x'])
-            torch.Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
-
-        """
-        ...
-
-    @classmethod
-    def stack(cls, input, dim=0, *, out=None):
-        """Stacks tensordicts into a single tensordict along the given dimension.
-
-        This call is equivalent to calling :func:`torch.stack` but is compatible with torch.compile.
-
-        """
-        from tensordict._torch_func import _stack
-
-        if not _is_tensor_collection(type(input[0])):
-            return torch.stack(input, dim, out=out)
-        return _stack(input, dim, out=out)
-
-    @classmethod
-    def cat(cls, input, dim=0, *, out=None):
-        """Concatenates tensordicts into a single tensordict along the given dimension.
-
-        This call is equivalent to calling :func:`torch.cat` but is compatible with torch.compile.
-
-        """
-        from tensordict._torch_func import _cat
-
-        if not _is_tensor_collection(type(input[0])):
-            return torch.cat(input, dim, out=out)
-        return _cat(input, dim, out=out)
-
-    @classmethod
-    def lazy_stack(cls, input, dim=0, *, out=None):
-        """Creates a lazy stack of tensordicts.
-
-        See :meth:`~tensordict.LazyStackTensorDict.lazy_stack` for details.
-        """
-        from tensordict._lazy import LazyStackedTensorDict
-
-        return LazyStackedTensorDict.lazy_stack(input, dim=dim, out=out)
-
-    @classmethod
-    def maybe_dense_stack(cls, input, dim=0, *, out=None):
-        """Attempts to make a dense stack of tensordicts, and falls back on lazy stack when required..
-
-        See :meth:`~tensordict.LazyStackTensorDict.maybe_dense_stack` for details.
-        """
-        from tensordict._lazy import LazyStackedTensorDict
-
-        return LazyStackedTensorDict.maybe_dense_stack(input, dim=dim, out=out)
-
-    @abc.abstractmethod
-    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
-        """Splits each tensor in the TensorDict with the specified size in the given dimension, like `torch.split`.
-
-        Returns a list of ``TensorDict`` instances with the view of split chunks of items.
-
-        Args:
-            split_size (int or List(int)): size of a single chunk or list of sizes for each chunk.
-            dim (int): dimension along which to split the tensor.
-
-        Returns:
-            A list of TensorDict with specified size in given dimension.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'x': torch.arange(12).reshape(3, 4),
-            ... }, batch_size=[3, 4])
-            >>> td0, td1 = td.split([1, 2], dim=0)
-            >>> print(td0['x'])
-            torch.Tensor([[0, 1, 2, 3]])
-        """
-        ...
-
-    def gather(self, dim: int, index: Tensor, out: T | None = None) -> T:
-        """Gathers values along an axis specified by `dim`.
-
-        Args:
-            dim (int): the dimension along which collect the elements
-            index (torch.Tensor): a long tensor which number of dimension matches
-                the one of the tensordict with only one dimension differring between
-                the two (the gathering dimension). Its elements refer to the
-                index to be gathered along the required dimension.
-            out (TensorDictBase, optional): a destination tensordict. It must
-                have the same shape as the index.
-
-        Examples:
-            >>> td = TensorDict(
-            ...     {"a": torch.randn(3, 4, 5),
-            ...      "b": TensorDict({"c": torch.zeros(3, 4, 5)}, [3, 4, 5])},
-            ...     [3, 4])
-            >>> index = torch.randint(4, (3, 2))
-            >>> td_gather = td.gather(dim=1, index=index)
-            >>> print(td_gather)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3, 2, 5]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3, 2]),
-                device=None,
-                is_shared=False)
-
-        Gather keeps the dimension names.
-
-        Examples:
-            >>> td.names = ["a", "b"]
-            >>> td_gather = td.gather(dim=1, index=index)
-            >>> td_gather.names
-            ["a", "b"]
-        """
-        return torch.gather(self, dim, index, out=out)
-
-    @overload
-    def view(self, *shape: int):
-        ...
-
-    @overload
-    def view(self, shape: torch.Size):
-        ...
-
-    @abc.abstractmethod
-    def _view(
-        self,
-        *args,
-        **kwargs,
-    ) -> T:
-        ...
-
-    @as_decorator()
-    def view(
-        self,
-        *shape: int,
-        size: list | tuple | torch.Size | None = None,
-    ):
-        """Returns a tensordict with views of the tensors according to a new shape, compatible with the tensordict batch_size.
-
-        Args:
-            *shape (int): new shape of the resulting tensordict.
-            size: iterable
-
-        Returns:
-            a new tensordict with the desired batch_size.
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3,4,5),
-            ...    'b': torch.zeros(3,4,10,1)}, batch_size=torch.Size([3, 4]))
-            >>> td_view = td.view(12)
-            >>> print(td_view.get("a").shape)  # torch.Size([12, 5])
-            >>> print(td_view.get("b").shape)  # torch.Size([12, 10, 1])
-            >>> td_view = td.view(-1, 4, 3)
-            >>> print(td_view.get("a").shape)  # torch.Size([1, 4, 3, 5])
-            >>> print(td_view.get("b").shape)  # torch.Size([1, 4, 3, 10, 1])
-
-        """
-        _lazy_legacy = lazy_legacy()
-
-        if _lazy_legacy:
-            return self._legacy_view(*shape, size=size)
-        else:
-            result = self._view(size=size) if size is not None else self._view(*shape)
-            if result._is_shared or result._is_memmap:
-                result.lock_()
-            return result
-
-    def _legacy_view(
-        self,
-        *shape: int,
-        size: list | tuple | torch.Size | None = None,
-    ) -> T:
-        if len(shape) == 0 and size is not None:
-            return self.view(*size)
-        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
-            return self.view(*shape[0])
-        elif not isinstance(shape, torch.Size):
-            shape = infer_size_impl(shape, self.numel())
-            shape = torch.Size(shape)
-        if shape == self.shape:
-            return self
-        from tensordict._lazy import _ViewedTensorDict
-
-        return _ViewedTensorDict(
-            source=self,
-            custom_op="view",
-            inv_op="view",
-            custom_op_kwargs={"size": shape},
-            inv_op_kwargs={"size": self.batch_size},
-        )
-
-    @as_decorator()
-    def transpose(self, dim0, dim1):
-        """Returns a tensordict that is a transposed version of input. The given dimensions ``dim0`` and ``dim1`` are swapped.
-
-        In-place or out-place modifications of the transposed tensordict will
-        impact the original tensordict too as the memory is shared and the operations
-        are mapped back on the original tensordict.
-
-        Examples:
-            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
-            >>> tensordict_transpose = tensordict.transpose(0, 1)
-            >>> print(tensordict_transpose.shape)
-            torch.Size([4, 3])
-            >>> tensordict_transpose.set("b",, torch.randn(4, 3))
-            >>> print(tensordict.get("b").shape)
-            torch.Size([3, 4])
-        """
-        _lazy_legacy = lazy_legacy()
-
-        if _lazy_legacy:
-            return self._legacy_transpose(dim0, dim1)
-        else:
-            ndim = self.ndim
-            if dim0 < 0:
-                dim0 = ndim + dim0
-            if dim1 < 0:
-                dim1 = ndim + dim1
-            if dim0 < 0 or dim1 < 0 or dim0 >= ndim or dim1 >= ndim:
-                raise ValueError(
-                    "dim0 and dim1 must be within the range of the number of dimensions."
-                )
-            dim0, dim1 = min(dim0, dim1), max(dim0, dim1)
-            if dim0 == dim1:
-                return self
-            result = self._transpose(dim0, dim1)
-            if result._is_shared or result._is_memmap:
-                result.lock_()
-            return result
-
-    @abc.abstractmethod
-    def _transpose(self, dim0, dim1):
-        ...
-
-    def _legacy_transpose(self, dim0, dim1):
-        if dim0 < 0:
-            dim0 = self.ndim + dim0
-        if dim1 < 0:
-            dim1 = self.ndim + dim1
-        if any((dim0 < 0, dim1 < 0)):
-            raise ValueError(
-                "The provided dimensions are incompatible with the tensordict batch-size."
-            )
-        if dim0 == dim1:
-            return self
-        from tensordict._lazy import _TransposedTensorDict
-
-        return _TransposedTensorDict(
-            source=self,
-            custom_op="transpose",
-            inv_op="transpose",
-            custom_op_kwargs={"dim0": dim0, "dim1": dim1},
-            inv_op_kwargs={"dim0": dim0, "dim1": dim1},
-        )
-
-    @overload
-    def permute(self, *dims: int):
-        ...
-
-    @overload
-    def permute(self, dims: list | tuple):
-        ...
-
-    @as_decorator()
-    def permute(self, *args, **kwargs):
-        """Returns a view of a tensordict with the batch dimensions permuted according to dims.
-
-        Args:
-            *dims_list (int): the new ordering of the batch dims of the tensordict. Alternatively,
-                a single iterable of integers can be provided.
-            dims (list of int): alternative way of calling permute(...).
-
-        Returns:
-            a new tensordict with the batch dimensions in the desired order.
-
-        Examples:
-            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
-            >>> print(tensordict.permute([1, 0]))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-            >>> print(tensordict.permute(1, 0))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-            >>> print(tensordict.permute(dims=[1, 0]))
-            PermutedTensorDict(
-                source=TensorDict(
-                    fields={
-                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
-                    batch_size=torch.Size([3, 4]),
-                    device=cpu,
-                    is_shared=False),
-                op=permute(dims=[1, 0]))
-        """
-        _lazy_legacy = lazy_legacy()
-
-        if _lazy_legacy:
-            return self._legacy_permute(*args, **kwargs)
-        else:
-            result = self._permute(*args, **kwargs)
-            if result._is_shared or result._is_memmap:
-                result.lock_()
-            return result
-
-    @abc.abstractmethod
-    def _permute(
-        self,
-        *args,
-        **kwargs,
-    ):
-        ...
-
-    def _legacy_permute(
-        self,
-        *dims_list: int,
-        dims: list[int] | None = None,
-    ) -> T:
-        if len(dims_list) == 0:
-            dims_list = dims
-        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
-            dims_list = dims_list[0]
-        if len(dims_list) != len(self.shape):
-            raise RuntimeError(
-                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
-            )
-
-        if not len(dims_list) and not self.batch_dims:
-            return self
-        if np.array_equal(dims_list, range(self.batch_dims)):
-            return self
-        min_dim, max_dim = -self.batch_dims, self.batch_dims - 1
-        seen = [False for dim in range(max_dim + 1)]
-        for idx in dims_list:
-            if idx < min_dim or idx > max_dim:
-                raise IndexError(
-                    f"dimension out of range (expected to be in range of [{min_dim}, {max_dim}], but got {idx})"
-                )
-            if seen[idx]:
-                raise RuntimeError("repeated dim in permute")
-            seen[idx] = True
-
-        from tensordict._lazy import _PermutedTensorDict
-
-        return _PermutedTensorDict(
-            source=self,
-            custom_op="permute",
-            inv_op="permute",
-            custom_op_kwargs={"dims": list(map(int, dims_list))},
-            inv_op_kwargs={"dims": list(map(int, dims_list))},
-        )
-
-    # Cache functionality
-    def _erase_cache(self):
-        self._cache = None
-
-    # Dim names functionality
-    @property
-    @abc.abstractmethod
-    def names(self):
-        """The dimension names of the tensordict.
-
-        The names can be set at construction time using the ``names`` argument.
-
-        See also :meth:`~.refine_names` for details on how to set the names after
-        construction.
-        """
-        ...
-
-    @abc.abstractmethod
-    def _erase_names(self):
-        """Erases the dimension names from a tensordict."""
-        ...
-
-    @abc.abstractmethod
-    def _rename_subtds(self, value):
-        """Renames all the sub-tensordicts dimension according to value.
-
-        If value has less dimensions than the TD, the rest is just assumed to be None.
-        """
-        ...
-
-    def _check_dim_name(self, name):
-        if name is None:
-            return False
-        if self._has_names() and name in self.names:
-            return True
-        for key in self.keys():
-            if _is_tensor_collection(self.entry_class(key)):
-                if self._get_str(key, NO_DEFAULT)._check_dim_name(name):
-                    return True
-        else:
-            return False
-
-    def refine_names(self, *names):
-        """Refines the dimension names of self according to names.
-
-        Refining is a special case of renaming that “lifts” unnamed dimensions.
-        A None dim can be refined to have any name; a named dim can only be
-        refined to have the same name.
-
-        Because named tensors can coexist with unnamed tensors, refining names
-        gives a nice way to write named-tensor-aware code that works with both
-        named and unnamed tensors.
-
-        names may contain up to one Ellipsis (...). The Ellipsis is expanded
-        greedily; it is expanded in-place to fill names to the same length as
-        self.dim() using names from the corresponding indices of self.names.
-
-        Returns: the same tensordict with dimensions named according to the input.
-
-        Examples:
-            >>> td = TensorDict({}, batch_size=[3, 4, 5, 6])
-            >>> tdr = td.refine_names(None, None, None, "d")
-            >>> assert tdr.names == [None, None, None, "d"]
-            >>> tdr = td.refine_names("a", None, None, "d")
-            >>> assert tdr.names == ["a", None, None, "d"]
-
-        """
-        # replace ellipsis if any
-        names_copy = copy(names)
-        if any(name is Ellipsis for name in names):
-            ellipsis_name = [NO_DEFAULT for _ in range(self.ndim - len(names) + 1)]
-            names = []
-            for name in names_copy:
-                if name is Ellipsis:
-                    names += ellipsis_name
-                else:
-                    names.append(name)
-        # check that the names that are set are either None or identical
-        curr_names = self.names
-        for i, name in enumerate(names):
-            if name is NO_DEFAULT:
-                # whatever value is ok
-                names[i] = curr_names[i]
-                continue
-            else:
-                if curr_names[i] is None:
-                    continue
-                if self.names[i] == name:
-                    continue
-                else:
-                    raise RuntimeError(
-                        f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
-                    )
-        self.names = names
-        # we also need to rename the sub-tensordicts
-        # self._rename_subtds(self.names)
-        return self
-
-    def rename(self, *names, **rename_map):
-        """Returns a clone of the tensordict with dimensions renamed.
-
-        Examples:
-            >>> td = TensorDict({}, batch_size=[1, 2, 3 ,4])
-            >>> td.names = list("abcd")
-            >>> td_rename = td.rename(c="g")
-            >>> assert td_rename.names == list("abgd")
-
-        """
-        clone = self.clone(recurse=False)
-        if len(names) == 1 and names[0] is None:
-            clone.names = None
-        if rename_map and names:
-            raise ValueError(
-                "Passed both a name map and a name list. Only one is accepted."
-            )
-        elif not rename_map and not names:
-            raise ValueError(
-                "Neither a name map nor a name list was passed. "
-                "Only one is accepted."
-            )
-        elif rename_map:
-            cnames = list(clone.names)
-            for i, name in enumerate(cnames):
-                new_name = rename_map.pop(name, NO_DEFAULT)
-                if new_name is not NO_DEFAULT:
-                    cnames[i] = new_name
-            clone.names = cnames
-            if rename_map:
-                raise ValueError(
-                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
-                )
-        else:
-            clone.names = names
-        return clone
-
-    def rename_(self, *names, **rename_map):
-        """Same as :meth:`~.rename`, but executes the renaming in-place.
-
-        Examples:
-            >>> td = TensorDict({}, batch_size=[1, 2, 3 ,4])
-            >>> td.names = list("abcd")
-            >>> assert td.rename_(c="g")
-            >>> assert td.names == list("abgd")
-        """
-        if len(names) == 1 and names[0] is None:
-            self.names = None
-        if rename_map and names:
-            raise ValueError(
-                "Passed both a name map and a name list. " "Only one is accepted."
-            )
-        elif not rename_map and not names and self.batch_dims:
-            raise ValueError(
-                "Neither a name map nor a name list was passed. "
-                "Only one is accepted."
-            )
-        elif rename_map:
-            cnames = list(self.names)
-            for i, name in enumerate(cnames):
-                new_name = rename_map.pop(name, NO_DEFAULT)
-                if new_name is not NO_DEFAULT:
-                    cnames[i] = new_name
-            if rename_map:
-                raise ValueError(
-                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
-                )
-            self.names = cnames
-        else:
-            self.names = names
-        return self
-
-    @abc.abstractmethod
-    def _has_names(self):
-        ...
-
-    # Device functionality: device is optional. If provided, it will enforce
-    # all data is on the same device
-    @property
-    @abc.abstractmethod
-    def device(self) -> torch.device | None:
-        """Device of a TensorDict.
-
-        If the TensorDict has a specified device, all
-        its tensors (incl. nested ones) must live on the same device.
-        If the TensorDict device is ``None``, different values can be located
-        on different devices.
-
-        Returns:
-            torch.device object indicating the device where the tensors
-            are placed, or None if TensorDict does not have a device.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     "cpu": torch.randn(3, device='cpu'),
-            ...     "cuda": torch.randn(3, device='cuda'),
-            ... }, batch_size=[], device=None)
-            >>> td['cpu'].device
-            device(type='cpu')
-            >>> td['cuda'].device
-            device(type='cuda')
-            >>> td = TensorDict({
-            ...     "x": torch.randn(3, device='cpu'),
-            ...     "y": torch.randn(3, device='cuda'),
-            ... }, batch_size=[], device='cuda')
-            >>> td['x'].device
-            device(type='cuda')
-            >>> td['y'].device
-            device(type='cuda')
-            >>> td = TensorDict({
-            ...     "x": torch.randn(3, device='cpu'),
-            ...     "y": TensorDict({'z': torch.randn(3, device='cpu')}, batch_size=[], device=None),
-            ... }, batch_size=[], device='cuda')
-            >>> td['x'].device
-            device(type='cuda')
-            >>> td['y'].device # nested tensordicts are also mapped onto the appropriate device.
-            device(type='cuda')
-            >>> td['y', 'x'].device
-            device(type='cuda')
-
-        """
-        ...
-
-    @device.setter
-    @abc.abstractmethod
-    def device(self, value: DeviceType) -> None:
-        ...
-
-    @lock_blocked
-    def clear(self) -> T:
-        """Erases the content of the tensordict."""
-        for key in list(self.keys()):
-            del self[key]
-        return self
-
-    @classmethod
-    def fromkeys(cls, keys: List[NestedKey], value: Any = 0):
-        """Creates a tensordict from a list of keys and a single value.
-
-        Args:
-            keys (list of NestedKey): An iterable specifying the keys of the new dictionary.
-            value (compatible type, optional): The value for all keys. Defaults to ``0``.
-        """
-        from tensordict._td import TensorDict
-
-        return TensorDict(dict.fromkeys(keys, value), batch_size=[])
-
-    @abc.abstractmethod
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        """Removes the item that was last inserted into the TensorDict.
-
-        ``popitem`` will only return non-nested values.
-        """
-        ...
-
-    def clear_device_(self) -> T:
-        """Clears the device of the tensordict.
-
-        Returns: self
-
-        """
-        self._device = None
-        for value in self.values():
-            if _is_tensor_collection(value.__class__):
-                value.clear_device_()
-        return self
-
-    @abc.abstractmethod
-    def pin_memory(self) -> T:
-        """Calls :meth:`~torch.Tensor.pin_memory` on the stored tensors."""
-        ...
-
-    def cpu(self) -> T:
-        """Casts a tensordict to CPU."""
-        return self.to("cpu")
-
-    def cuda(self, device: int = None) -> T:
-        """Casts a tensordict to a cuda device (if not already on it).
-
-        Args:
-            device (int, optional): if provided, the cuda device on which the
-                tensor should be cast.
-
-        """
-        if device is None:
-            return self.to(torch.device("cuda"))
-        return self.to(f"cuda:{device}")
-
-    @property
-    def is_cuda(self):
-        return self.device is not None and self.device.type == "cuda"
-
-    @property
-    def is_cpu(self):
-        return self.device is not None and self.device.type == "cpu"
-
-    # Serialization functionality
-    def state_dict(
-        self,
-        destination=None,
-        prefix="",
-        keep_vars=False,
-        flatten=False,
-    ) -> OrderedDict[str, Any]:
-        """Produces a state_dict from the tensordict.
-
-        The structure of the state-dict will still be nested, unless ``flatten`` is set to ``True``.
-
-        A tensordict state-dict contains all the tensors and meta-data needed
-        to rebuild the tensordict (names are currently not supported).
-
-        Args:
-            destination (dict, optional): If provided, the state of tensordict will
-                be updated into the dict and the same object is returned.
-                Otherwise, an ``OrderedDict`` will be created and returned.
-                Default: ``None``.
-            prefix (str, optional): a prefix added to tensor
-                names to compose the keys in state_dict. Default: ``''``.
-            keep_vars (bool, optional): by default the :class:`torch.Tensor` items
-                returned in the state dict are detached from autograd. If it's
-                set to ``True``, detaching will not be performed.
-                Default: ``False``.
-            flatten (bool, optional): whether the structure should be flattened
-                with the ``"."`` character or not.
-                Defaults to ``False``.
-
-        Examples:
-            >>> data = TensorDict({"1": 1, "2": 2, "3": {"3": 3}}, [])
-            >>> sd = data.state_dict()
-            >>> print(sd)
-            OrderedDict([('1', tensor(1)), ('2', tensor(2)), ('3', OrderedDict([('3', tensor(3)), ('__batch_size', torch.Size([])), ('__device', None)])), ('__batch_size', torch.Size([])), ('__device', None)])
-            >>> sd = data.state_dict(flatten=True)
-            OrderedDict([('1', tensor(1)), ('2', tensor(2)), ('3.3', tensor(3)), ('__batch_size', torch.Size([])), ('__device', None)])
-
-        """
-        out = collections.OrderedDict()
-        source = self
-        if flatten:
-            source = source.flatten_keys(".")
-        for key, item in source.items():
-            if not _is_tensor_collection(item.__class__):
-                if not keep_vars:
-                    out[prefix + key] = item.detach().clone()
-                else:
-                    out[prefix + key] = item
-            else:
-                out[prefix + key] = item.state_dict(keep_vars=keep_vars)
-        if "__batch_size" in out:
-            raise KeyError(
-                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
-            )
-        if "__device" in out:
-            raise KeyError(
-                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
-            )
-        out[prefix + "__batch_size"] = source.batch_size
-        out[prefix + "__device"] = source.device
-        if destination is not None:
-            destination.update(out)
-            return destination
-        return out
-
-    def load_state_dict(
-        self,
-        state_dict: OrderedDict[str, Any],
-        strict=True,
-        assign=False,
-        from_flatten=False,
-    ) -> T:
-        """Loads a state-dict, formatted as in :meth:`~.state_dict`, into the tensordict.
-
-        Args:
-            state_dict (OrderedDict): the state_dict of to be copied.
-            strict (bool, optional): whether to strictly enforce that the keys
-                in :attr:`state_dict` match the keys returned by this tensordict's
-                :meth:`torch.nn.Module.state_dict` function. Default: ``True``
-            assign (bool, optional): whether to assign items in the state
-                dictionary to their corresponding keys in the tensordict instead
-                of copying them inplace into the tensordict's current tensors.
-                When ``False``, the properties of the tensors in the current
-                module are preserved while when ``True``, the properties of the
-                Tensors in the state dict are preserved.
-                Default: ``False``
-            from_flatten (bool, optional): if ``True``, the input state_dict is
-                assumed to be flattened.
-                Defaults to ``False``.
-
-        Examples:
-            >>> data = TensorDict({"1": 1, "2": 2, "3": {"3": 3}}, [])
-            >>> data_zeroed = TensorDict({"1": 0, "2": 0, "3": {"3": 0}}, [])
-            >>> sd = data.state_dict()
-            >>> data_zeroed.load_state_dict(sd)
-            >>> print(data_zeroed["3", "3"])
-            tensor(3)
-            >>> # with flattening
-            >>> data_zeroed = TensorDict({"1": 0, "2": 0, "3": {"3": 0}}, [])
-            >>> data_zeroed.load_state_dict(data.state_dict(flatten=True), from_flatten=True)
-            >>> print(data_zeroed["3", "3"])
-            tensor(3)
-
-
-        """
-        if from_flatten:
-            self_flatten = self.flatten_keys(".")
-            self_flatten.load_state_dict(state_dict, strict=strict, assign=assign)
-            if not assign:
-                # modifications are done in-place so we should be fine returning self
-                return self
-            else:
-                # run a check over keys, if we any key with a '.' in name we're doomed
-                DOT_ERROR = "Cannot use load_state_dict(..., from_flatten=True, assign=True) when some keys contain a dot character."
-                for key in self.keys(True, True):
-                    if isinstance(key, tuple):
-                        for subkey in key:
-                            if "." in subkey:
-                                raise RuntimeError(DOT_ERROR)
-                    elif "." in key:
-                        raise RuntimeError(DOT_ERROR)
-                return self.update(self_flatten.unflatten_keys("."))
-
-        # copy since we'll be using pop
-        state_dict = copy(state_dict)
-        batch_size = state_dict.pop("__batch_size")
-        device = state_dict.pop("__device", None)
-
-        if strict and set(state_dict.keys()) != set(self.keys()):
-            set_sd = set(state_dict.keys())
-            set_td = set(self.keys())
-
-            # if there are keys in state-dict that point to an empty tensordict
-            # or if the local tensordicts are empty, we can skip
-            def _is_empty_dict(sd, key=None):
-                if key is not None:
-                    if not isinstance(sd[key], dict):
-                        return False
-                    return _is_empty_dict(sd[key])
-                for key, item in sd.items():
-                    if key in ("__batch_size", "__device"):
-                        continue
-                    if isinstance(item, dict):
-                        if not _is_empty_dict(item):
-                            return False
-                        continue
-                    return False
-                else:
-                    return True
-
-            def check_is_empty(target, key):
-                item = target.get(key)
-                if not is_tensor_collection(item) or not item.is_empty():
-                    return False
-                return True
-
-            if not all(check_is_empty(self, key) for key in set_td - set_sd) or not all(
-                _is_empty_dict(state_dict, key) for key in set_sd - set_td
-            ):
-                raise RuntimeError(
-                    "Cannot load state-dict because the key sets don't match: got "
-                    f"state_dict extra keys \n{set_sd - set_td}\n and tensordict extra keys\n{set_td - set_sd}\n"
-                )
-
-        self.batch_size = batch_size
-        if device is not None and self.device is not None and device != self.device:
-            raise RuntimeError("Loading data from another device is not yet supported.")
-
-        for key, item in state_dict.items():
-            if isinstance(item, dict):
-                dest = self.get(key, default=None)
-                if dest is None:
-                    dest = self.empty()
-                dest.load_state_dict(item, assign=assign, strict=strict)
-                self.set(
-                    key,
-                    dest,
-                    inplace=not assign,
-                )
-            else:
-                self.set(key, item, inplace=not assign)
-        return self
-
-    def is_shared(self) -> bool:
-        """Checks if tensordict is in shared memory.
-
-        If a TensorDict instance is in shared memory, it is locked (entries cannot
-        be renamed, removed or added). If a ``TensorDict`` is created with
-        tensors that are all in shared memory, this does __not__ mean that ``is_shared``
-        will return ``True`` (as a new tensor may or may not be in shared memory).
-        Only if one calls `tensordict.share_memory_()` or places the tensordict
-        on a device where the content is shared by default (eg, ``"cuda"``)
-        will the tensordict be considered in shared memory.
-
-        This is always ``True`` for tensordicts on a CUDA device.
-
-        """
-        if self.device and not self._is_memmap:
-            return self.device.type == "cuda" or self._is_shared
-        return self._is_shared
-
-    def is_memmap(self) -> bool:
-        """Checks if tensordict is memory-mapped.
-
-        If a TensorDict instance is memory-mapped, it is locked (entries cannot
-        be renamed, removed or added). If a ``TensorDict`` is created with
-        tensors that are all memory-mapped, this does __not__ mean that ``is_memmap``
-        will return ``True`` (as a new tensor may or may not be memory-mapped).
-        Only if one calls `tensordict.memmap_()` will the tensordict be
-        considered as memory-mapped.
-
-        This is always ``True`` for tensordicts on a CUDA device.
-
-        """
-        return self._is_memmap
-
-    @abc.abstractmethod
-    def share_memory_(self) -> T:
-        """Places all the tensors in shared memory.
-
-        The TensorDict is then locked, meaning that any writing operations that
-        isn't in-place will throw an exception (eg, rename, set or remove an
-        entry).
-        Conversely, once the tensordict is unlocked, the share_memory attribute
-        is turned to ``False``, because cross-process identity is not
-        guaranteed anymore.
-
-        Returns:
-            self
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None,
-        copy_existing: bool,
-        executor,
-        futures,
-        inplace,
-        like,
-        share_non_tensor,
-    ) -> T:
-        ...
-
-    @property
-    def saved_path(self):
-        """Returns the path where a memmap saved TensorDict is being stored.
-
-        This argument valishes as soon as is_memmap() returns ``False`` (e.g., when the tensordict is unlocked).
-        """
-        if self.is_memmap():
-            path = self._memmap_prefix
-            return path
-        raise AttributeError(
-            f"The tensordict has no saved path (memmap={self.is_memmap()}, path={self._memmap_prefix})."
-        )
-
-    def memmap_(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        *,
-        num_threads: int = 0,
-        return_early: bool = False,
-        share_non_tensor: bool = False,
-    ) -> T:
-        """Writes all tensors onto a corresponding memory-mapped Tensor, in-place.
-
-        Args:
-            prefix (str): directory prefix where the memory-mapped tensors will
-                be stored. The directory tree structure will mimic the tensordict's.
-            copy_existing (bool): If False (default), an exception will be raised if an
-                entry in the tensordict is already a tensor stored on disk
-                with an associated file, but is not saved in the correct
-                location according to prefix.
-                If ``True``, any existing Tensor will be copied to the new location.
-
-        Keyword Args:
-            num_threads (int, optional): the number of threads used to write the memmap
-                tensors. Defaults to `0`.
-            return_early (bool, optional): if ``True`` and ``num_threads>0``,
-                the method will return a future of the tensordict.
-            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
-                shared between the processes and writing operation (such as inplace update
-                or set) on any of the workers within a single node will update the value
-                on all other workers. If the number of non-tensor leaves is high (e.g.,
-                sharing large stacks of non-tensor data) this may result in OOM or similar
-                errors. Defaults to ``False``.
-
-        The TensorDict is then locked, meaning that any writing operations that
-        isn't in-place will throw an exception (eg, rename, set or remove an
-        entry).
-        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            self if ``return_early=False``, otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
-
-        Note:
-            Serialising in this fashion might be slow with deeply nested tensordicts, so
-            it is not recommended to call this method inside a training loop.
-        """
-        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
-        if num_threads > 1:
-            with (
-                ThreadPoolExecutor(max_workers=num_threads)
-                if not return_early
-                else contextlib.nullcontext()
-            ) as executor:
-                if return_early:
-                    executor = ThreadPoolExecutor(max_workers=num_threads)
-                futures = []
-                result = self._memmap_(
-                    prefix=prefix,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=True,
-                    like=False,
-                    share_non_tensor=share_non_tensor,
-                )
-                if not return_early:
-                    concurrent.futures.wait(futures)
-                    return result
-                else:
-                    return TensorDictFuture(futures, result)
-        return self._memmap_(
-            prefix=prefix,
-            copy_existing=copy_existing,
-            inplace=True,
-            futures=None,
-            executor=None,
-            like=False,
-            share_non_tensor=share_non_tensor,
-        ).lock_()
-
-    @abc.abstractmethod
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        """Creates an empty memory-mapped tensor given a shape and possibly a dtype.
-
-        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
-            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
-
-        Writing an existing entry will result in an error.
-
-        Args:
-            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
-                exception is raised.
-            shape (torch.Size or equivalent, torch.Tensor for nested tensors): the shape of the tensor to write.
-
-        Keyword arguments:
-            dtype (torch.dtype, optional): the dtype of the new tensor.
-
-        Returns:
-            A new memory mapped tensor.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        """Creates an empty memory-mapped tensor given a storage, a shape and possibly a dtype.
-
-        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
-            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
-
-        .. note:: If the storage has a filename associated, it must match the new filename for the file.
-            If it has not a filename associated but the tensordict has an associated path, this will result in an
-            exception.
-
-        Args:
-            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
-                exception is raised.
-            storage (torch.UntypedStorage): the storage to use for the new MemoryMappedTensor. Must be a physical memory
-                storage.
-            shape (torch.Size or equivalent, torch.Tensor for nested tensors): the shape of the tensor to write.
-
-        Keyword arguments:
-            dtype (torch.dtype, optional): the dtype of the new tensor.
-
-        Returns:
-            A new memory mapped tensor with the given storage.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        """Creates an empty memory-mapped tensor given a tensor.
-
-        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
-            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
-
-        This method always copies the storage content if ``copy_data`` is ``True`` (i.e., the storage is not shared).
-
-        Args:
-            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
-                exception is raised.
-            tensor (torch.Tensor): the tensor to replicate on physical memory.
-
-        Keyword arguments:
-            copy_data (bool, optionaL): if ``False``, the new tensor will share the metadata of the input such as
-                shape and dtype, but the content will be empty. Defaults to ``True``.
-
-        Returns:
-            A new memory mapped tensor with the given storage.
-
-        """
-        ...
-
-    def save(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        *,
-        num_threads: int = 0,
-        return_early: bool = False,
-        share_non_tensor: bool = False,
-    ) -> T:
-        """Saves the tensordict to disk.
-
-        This function is a proxy to :meth:`~.memmap`.
-        """
-        return self.memmap(
-            prefix=prefix,
-            copy_existing=copy_existing,
-            num_threads=num_threads,
-            return_early=return_early,
-            share_non_tensor=share_non_tensor,
-        )
-
-    dumps = save
-
-    def memmap(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        *,
-        num_threads: int = 0,
-        return_early: bool = False,
-        share_non_tensor: bool = False,
-    ) -> T:
-        """Writes all tensors onto a corresponding memory-mapped Tensor in a new tensordict.
-
-        Args:
-            prefix (str): directory prefix where the memory-mapped tensors will
-                be stored. The directory tree structure will mimic the tensordict's.
-            copy_existing (bool): If False (default), an exception will be raised if an
-                entry in the tensordict is already a tensor stored on disk
-                with an associated file, but is not saved in the correct
-                location according to prefix.
-                If ``True``, any existing Tensor will be copied to the new location.
-
-        Keyword Args:
-            num_threads (int, optional): the number of threads used to write the memmap
-                tensors. Defaults to `0`.
-            return_early (bool, optional): if ``True`` and ``num_threads>0``,
-                the method will return a future of the tensordict.
-            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
-                shared between the processes and writing operation (such as inplace update
-                or set) on any of the workers within a single node will update the value
-                on all other workers. If the number of non-tensor leaves is high (e.g.,
-                sharing large stacks of non-tensor data) this may result in OOM or similar
-                errors. Defaults to ``False``.
-
-        The TensorDict is then locked, meaning that any writing operations that
-        isn't in-place will throw an exception (eg, rename, set or remove an
-        entry).
-        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            A new tensordict with the tensors stored on disk if ``return_early=False``,
-            otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
-
-        Note:
-            Serialising in this fashion might be slow with deeply nested tensordicts, so
-            it is not recommended to call this method inside a training loop.
-        """
-        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
-
-        if num_threads > 1:
-            with (
-                ThreadPoolExecutor(max_workers=num_threads)
-                if not return_early
-                else contextlib.nullcontext()
-            ) as executor:
-                if return_early:
-                    executor = ThreadPoolExecutor(max_workers=num_threads)
-                futures = []
-                result = self._memmap_(
-                    prefix=prefix,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=False,
-                    like=False,
-                    share_non_tensor=share_non_tensor,
-                )
-                if not return_early:
-                    concurrent.futures.wait(futures)
-                    return result
-                else:
-                    return TensorDictFuture(futures, result)
-
-        return self._memmap_(
-            prefix=prefix,
-            copy_existing=copy_existing,
-            inplace=False,
-            executor=None,
-            like=False,
-            futures=None,
-            share_non_tensor=share_non_tensor,
-        ).lock_()
-
-    def memmap_like(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        *,
-        num_threads: int = 0,
-        return_early: bool = False,
-        share_non_tensor: bool = False,
-    ) -> T:
-        """Creates a contentless Memory-mapped tensordict with the same shapes as the original one.
-
-        Args:
-            prefix (str): directory prefix where the memory-mapped tensors will
-                be stored. The directory tree structure will mimic the tensordict's.
-            copy_existing (bool): If False (default), an exception will be raised if an
-                entry in the tensordict is already a tensor stored on disk
-                with an associated file, but is not saved in the correct
-                location according to prefix.
-                If ``True``, any existing Tensor will be copied to the new location.
-
-        Keyword Args:
-            num_threads (int, optional): the number of threads used to write the memmap
-                tensors. Defaults to `0`.
-            return_early (bool, optional): if ``True`` and ``num_threads>0``,
-                the method will return a future of the tensordict.
-            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
-                shared between the processes and writing operation (such as inplace update
-                or set) on any of the workers within a single node will update the value
-                on all other workers. If the number of non-tensor leaves is high (e.g.,
-                sharing large stacks of non-tensor data) this may result in OOM or similar
-                errors. Defaults to ``False``.
-
-        The TensorDict is then locked, meaning that any writing operations that
-        isn't in-place will throw an exception (eg, rename, set or remove an
-        entry).
-        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
-        because cross-process identity is not guaranteed anymore.
-
-        Returns:
-            A new ``TensorDict`` instance with data stored as memory-mapped tensors if ``return_early=False``,
-            otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
-
-        .. note:: This is the recommended method to write a set of large buffers
-            on disk, as :meth:`~.memmap_()` will copy the information, which can
-            be slow for large content.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     "a": torch.zeros((3, 64, 64), dtype=torch.uint8),
-            ...     "b": torch.zeros(1, dtype=torch.int64),
-            ... }, batch_size=[]).expand(1_000_000)  # expand does not allocate new memory
-            >>> buffer = td.memmap_like("/path/to/dataset")
-
-        """
-        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
-        if num_threads > 1:
-            with (
-                ThreadPoolExecutor(max_workers=num_threads)
-                if not return_early
-                else contextlib.nullcontext()
-            ) as executor:
-                if return_early:
-                    executor = ThreadPoolExecutor(max_workers=num_threads)
-                futures = []
-                # we create an empty copy of self
-                # This is because calling MMapTensor.from_tensor(mmap_tensor) does nothing
-                # if both are in filesystem
-                input = self.apply(
-                    lambda x: torch.empty((), device=x.device, dtype=x.dtype).expand(
-                        x.shape
-                    )
-                )
-                result = input._memmap_(
-                    prefix=prefix,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=False,
-                    like=True,
-                    share_non_tensor=share_non_tensor,
-                )
-                if not return_early:
-                    concurrent.futures.wait(futures)
-                    return result
-                else:
-                    return TensorDictFuture(futures, result)
-        input = self.apply(
-            lambda x: torch.empty((), device=x.device, dtype=x.dtype).expand(x.shape)
-        )
-        return input._memmap_(
-            prefix=prefix,
-            copy_existing=copy_existing,
-            inplace=False,
-            like=True,
-            executor=None,
-            futures=None,
-            share_non_tensor=share_non_tensor,
-        ).lock_()
-
-    @classmethod
-    def load(cls, prefix: str | Path, *args, **kwargs) -> T:
-        """Loads a tensordict from disk.
-
-        This class method is a proxy to :meth:`~.load_memmap`.
-        """
-        return cls.load_memmap(prefix, *args, **kwargs)
-
-    def load_(self, prefix: str | Path, *args, **kwargs):
-        """Loads a tensordict from disk within the current tensordict.
-
-        This class method is a proxy to :meth:`~.load_memmap_`.
-        """
-        return self.load_memmap_(prefix, *args, **kwargs)
-
-    @classmethod
-    def load_memmap(
-        cls,
-        prefix: str | Path,
-        device: torch.device | None = None,
-        non_blocking: bool = False,
-        *,
-        out: TensorDictBase | None = None,
-    ) -> T:
-        """Loads a memory-mapped tensordict from disk.
-
-        Args:
-            prefix (str or Path to folder): the path to the folder where the
-                saved tensordict should be fetched.
-            device (torch.device or equivalent, optional): if provided, the
-                data will be asynchronously cast to that device.
-                Supports `"meta"` device, in which case the data isn't loaded
-                but a set of empty "meta" tensors are created. This is
-                useful to get a sense of the total model size and structure
-                without actually opening any file.
-            non_blocking (bool, optional): if ``True``, synchronize won't be
-                called after loading tensors on device. Defaults to ``False``.
-            out (TensorDictBase, optional): optional tensordict where the data
-                should be written.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> td = TensorDict.fromkeys(["a", "b", "c", ("nested", "e")], 0)
-            >>> td.memmap("./saved_td")
-            >>> td_load = TensorDict.load_memmap("./saved_td")
-            >>> assert (td == td_load).all()
-
-        This method also allows loading nested tensordicts.
-
-            >>> nested = TensorDict.load_memmap("./saved_td/nested")
-            >>> assert nested["e"] == 0
-
-        A tensordict can also be loaded on "meta" device or, alternatively,
-        as a fake tensor:
-            >>> import tempfile
-            >>> td = TensorDict({"a": torch.zeros(()), "b": {"c": torch.zeros(())}})
-            >>> with tempfile.TemporaryDirectory() as path:
-            ...     td.save(path)
-            ...     td_load = TensorDict.load_memmap(path, device="meta")
-            ...     print("meta:", td_load)
-            ...     from torch._subclasses import FakeTensorMode
-            ...     with FakeTensorMode():
-            ...         td_load = TensorDict.load_memmap(path)
-            ...         print("fake:", td_load)
-            meta: TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=meta, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([]), device=meta, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=meta,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=meta,
-                is_shared=False)
-            fake: TensorDict(
-                fields={
-                    a: FakeTensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: FakeTensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=cpu,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=cpu,
-                is_shared=False)
-
-        """
-        prefix = Path(prefix)
-
-        metadata = _load_metadata(prefix)
-        type_name = metadata["_type"]
-        if type_name != str(cls):
-            import tensordict
-
-            for other_cls in tensordict.base._ACCEPTED_CLASSES:
-                if str(other_cls) == type_name:
-                    return other_cls._load_memmap(prefix, metadata)
-            else:
-                raise RuntimeError(
-                    f"Could not find name {type_name} in {tensordict.base._ACCEPTED_CLASSES}. "
-                    f"Did you call _register_tensor_class(cls) on {type_name}?"
-                )
-        if device is not None:
-            device = torch.device(device)
-        out = cls._load_memmap(prefix, metadata, device=device, out=out)
-        if not non_blocking and device is not None and device != torch.device("meta"):
-            out._sync_all()
-        return out
-
-    def load_memmap_(
-        self,
-        prefix: str | Path,
-    ):
-        """Loads the content of a memory-mapped tensordict within the tensordict where ``load_memmap_`` is called.
-
-        See :meth:`~tensordict.TensorDictBase.load_memmap` for more info.
-        """
-        is_memmap = self.is_memmap()
-        with self.unlock_() if is_memmap else contextlib.nullcontext():
-            self.load_memmap(prefix=prefix, device=self.device, out=self)
-        if is_memmap:
-            self.memmap_()
-        return self
-
-    def memmap_refresh_(self):
-        """Refreshes the content of the memory-mapped tensordict if it has a :attr:`~tensordict.TensorDict.saved_path`.
-
-        This method will raise an exception if no path is associated with it.
-
-        """
-        if not self.is_memmap() or self._memmap_prefix is None:
-            raise RuntimeError(
-                "Cannot refresh a TensorDict that is not memory mapped or has no path associated."
-            )
-        return self.load_memmap_(prefix=self.saved_path)
-
-    @classmethod
-    @abc.abstractmethod
-    def _load_memmap(
-        cls,
-        prefix: Path,
-        metadata: dict,
-        device: torch.device | None = None,
-        *,
-        out=None,
-    ):
-        ...
-
-    # Key functionality: set, get, set_, set_at_, update, update_
-    @abc.abstractmethod
-    def entry_class(self, key: NestedKey) -> type:
-        """Returns the class of an entry, possibly avoiding a call to `isinstance(td.get(key), type)`.
-
-        This method should be preferred to ``tensordict.get(key).shape`` whenever
-        :meth:`.get` can be expensive to execute.
-
-        """
-        ...
-
-    def set(
-        self,
-        key: NestedKey,
-        item: CompatibleType,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        **kwargs: Any,
-    ) -> T:
-        """Sets a new key-value pair.
-
-        Args:
-            key (str, tuple of str): name of the key to be set.
-            item (torch.Tensor or equivalent, TensorDictBase instance): value
-                to be stored in the tensordict.
-            inplace (bool, optional): if ``True`` and if a key matches an existing
-                key in the tensordict, then the update will occur in-place
-                for that key-value pair. If inplace is ``True`` and
-                the entry cannot be found, it will be added. For a more restrictive
-                in-place operation, use :meth:`~.set_` instead.
-                Defaults to ``False``.
-
-        Keyword Args:
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict({}, batch_size[3, 4])
-            >>> td.set("x", torch.randn(3, 4))
-            >>> y = torch.randn(3, 4, 5)
-            >>> td.set("y", y, inplace=True) # works, even if 'y' is not present yet
-            >>> td.set("y", torch.zeros_like(y), inplace=True)
-            >>> assert (y==0).all() # y values are overwritten
-            >>> td.set("y", torch.ones(5), inplace=True) # raises an exception as shapes mismatch
-
-        """
-        key = _unravel_key_to_tuple(key)
-        # inplace is loose here, but for set_ it is constraining. We translate it
-        # to None to tell _set_str and others to drop it if the key isn't found
-        inplace = BEST_ATTEMPT_INPLACE if inplace else False
-        return self._set_tuple(
-            key, item, inplace=inplace, validated=False, non_blocking=non_blocking
-        )
-
-    @abc.abstractmethod
-    def _set_str(
-        self,
-        key: str,
-        value: Any,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ):
-        ...
-
-    @abc.abstractmethod
-    def _set_tuple(self, key, value, *, inplace, validated, non_blocking: bool):
-        ...
-
-    @lock_blocked
-    def set_non_tensor(self, key: NestedKey, value: Any):
-        """Registers a non-tensor value in the tensordict using :class:`tensordict.tensorclass.NonTensorData`.
-
-        The value can be retrieved using :meth:`TensorDictBase.get_non_tensor`
-        or directly using `get`, which will return the :class:`tensordict.tensorclass.NonTensorData`
-        object.
-
-        return: self
-
-        Examples:
-            >>> data = TensorDict({}, batch_size=[])
-            >>> data.set_non_tensor(("nested", "the string"), "a string!")
-            >>> assert data.get_non_tensor(("nested", "the string")) == "a string!"
-            >>> # regular `get` works but returns a NonTensorData object
-            >>> data.get(("nested", "the string"))
-            NonTensorData(
-                data='a string!',
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        key = unravel_key(key)
-        return self._set_non_tensor(key, value)
-
-    def _set_non_tensor(self, key: NestedKey, value: Any):
-        if isinstance(key, tuple):
-            if len(key) == 1:
-                return self._set_non_tensor(key[0], value)
-            sub_td = self._get_str(key[0], None)
-            if sub_td is None:
-                sub_td = self._create_nested_str(key[0])
-            sub_td._set_non_tensor(key[1:], value)
-            return self
-        from tensordict.tensorclass import NonTensorData
-
-        self._set_str(
-            key,
-            NonTensorData(
-                value,
-                batch_size=self.batch_size,
-                device=self.device,
-                names=self.names if self._has_names() else None,
-            ),
-            validated=True,
-            inplace=False,
-            non_blocking=False,
-        )
-        return self
-
-    def get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
-        """Gets a non-tensor value, if it exists, or `default` if the non-tensor value is not found.
-
-        This method is robust to tensor/TensorDict values, meaning that if the
-        value gathered is a regular tensor it will be returned too (although
-        this method comes with some overhead and should not be used out of its
-        natural scope).
-
-        See :meth:`~tensordict.TensorDictBase.set_non_tensor` for more information
-        on how to set non-tensor values in a tensordict.
-
-        Args:
-            key (NestedKey): the location of the NonTensorData object.
-            default (Any, optional): the value to be returned if the key cannot
-                be found.
-
-        Returns: the content of the :class:`tensordict.tensorclass.NonTensorData`,
-            or the entry corresponding to the ``key`` if it isn't a
-            :class:`tensordict.tensorclass.NonTensorData` (or ``default`` if the
-            entry cannot be found).
-
-        Examples:
-            >>> data = TensorDict({}, batch_size=[])
-            >>> data.set_non_tensor(("nested", "the string"), "a string!")
-            >>> assert data.get_non_tensor(("nested", "the string")) == "a string!"
-            >>> # regular `get` works but returns a NonTensorData object
-            >>> data.get(("nested", "the string"))
-            NonTensorData(
-                data='a string!',
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        key = unravel_key(key)
-        return self._get_non_tensor(key, default=default)
-
-    def _get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
-        if isinstance(key, tuple):
-            if len(key) == 1:
-                return self._get_non_tensor(key[0], default=default)
-            subtd = self._get_str(key[0], default=default)
-            if subtd is default:
-                return subtd
-            return subtd._get_non_tensor(key[1:], default=default)
-        value = self._get_str(key, default=default)
-
-        if is_non_tensor(value):
-            data = getattr(value, "data", None)
-            if data is None:
-                return value.tolist()
-            return data
-        return value
-
-    def filter_non_tensor_data(self) -> T:
-        """Filters out all non-tensor-data."""
-
-        def _filter(x):
-            if not is_non_tensor(x):
-                if is_tensor_collection(x):
-                    return x.filter_non_tensor_data()
-                return x
-
-        return self._apply_nest(_filter, call_on_nested=True, filter_empty=False)
-
-    def _convert_inplace(self, inplace, key):
-        if inplace is not False:
-            has_key = key in self.keys()
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
-                )
-            inplace = has_key
-        return inplace
-
-    def set_at_(
-        self,
-        key: NestedKey,
-        value: CompatibleType,
-        index: IndexType,
-        *,
-        non_blocking: bool = False,
-    ) -> T:
-        """Sets the values in-place at the index indicated by ``index``.
-
-        Args:
-            key (str, tuple of str): key to be modified.
-            value (torch.Tensor): value to be set at the index `index`
-            index (int, tensor or tuple): index where to write the values.
-
-        Keyword Args:
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict({}, batch_size[3, 4])
-            >>> x = torch.randn(3, 4)
-            >>> td.set("x", x)
-            >>> td.set_at_("x", value=torch.ones(1, 4), index=slice(1))
-            >>> assert (x[0] == 1).all()
-        """
-        key = _unravel_key_to_tuple(key)
-        return self._set_at_tuple(
-            key, value, index, validated=False, non_blocking=non_blocking
-        )
-
-    @abc.abstractmethod
-    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
-        ...
-
-    @abc.abstractmethod
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
-        ...
-
-    def set_(
-        self,
-        key: NestedKey,
-        item: CompatibleType,
-        *,
-        non_blocking: bool = False,
-    ) -> T:
-        """Sets a value to an existing key while keeping the original storage.
-
-        Args:
-            key (str): name of the value
-            item (torch.Tensor or compatible type, TensorDictBase): value to
-                be stored in the tensordict
-
-        Keyword Args:
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict({}, batch_size[3, 4])
-            >>> x = torch.randn(3, 4)
-            >>> td.set("x", x)
-            >>> td.set_("x", torch.zeros_like(x))
-            >>> assert (x == 0).all()
-
-        """
-        key = _unravel_key_to_tuple(key)
-        return self._set_tuple(
-            key, item, inplace=True, validated=False, non_blocking=non_blocking
-        )
-
-    # Stack functionality
-    @abc.abstractmethod
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        """Stacks a list of values onto an existing key while keeping the original storage.
-
-        Args:
-            key (str): name of the value
-            list_item (list of torch.Tensor): value to be stacked and stored in the tensordict.
-            dim (int): dimension along which the tensors should be stacked.
-
-        Returns:
-            self
-
-        """
-        ...
-
-    def _stack_onto_at_(
-        self,
-        key: NestedKey,
-        list_item: list[CompatibleType],
-        dim: int,
-        idx: IndexType,
-    ) -> T:
-        """Similar to _stack_onto_ but on a specific index. Only works with regular TensorDicts."""
-        raise RuntimeError(
-            f"Cannot call _stack_onto_at_ with {self.__class__.__name__}. "
-            "Make sure your sub-classed tensordicts are turned into regular tensordicts by calling to_tensordict() "
-            "before calling __getindex__ and stack."
-        )
-
-    def _default_get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        if default is not NO_DEFAULT:
-            return default
-        else:
-            # raise KeyError
-            raise KeyError(
-                _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
-            )
-
-    def get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        """Gets the value stored with the input key.
-
-        Args:
-            key (str, tuple of str): key to be queried. If tuple of str it is
-                equivalent to chained calls of getattr.
-            default: default value if the key is not found in the tensordict.
-
-        Examples:
-            >>> td = TensorDict({"x": 1}, batch_size=[])
-            >>> td.get("x")
-            tensor(1)
-            >>> td.get("y", default=None)
-            None
-        """
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR.format(key))
-        return self._get_tuple(key, default=default)
-
-    @abc.abstractmethod
-    def _get_str(self, key, default):
-        ...
-
-    @abc.abstractmethod
-    def _get_tuple(self, key, default):
-        ...
-
-    def get_at(
-        self, key: NestedKey, index: IndexType, default: CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        """Get the value of a tensordict from the key `key` at the index `idx`.
-
-        Args:
-            key (str, tuple of str): key to be retrieved.
-            index (int, slice, torch.Tensor, iterable): index of the tensor.
-            default (torch.Tensor): default value to return if the key is
-                not present in the tensordict.
-
-        Returns:
-            indexed tensor.
-
-        Examples:
-            >>> td = TensorDict({"x": torch.arange(3)}, batch_size=[])
-            >>> td.get_at("x", index=1)
-            tensor(1)
-
-        """
-        # TODO: check that this works with masks, and add to docstring
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR.format(key))
-        # must be a tuple
-        return self._get_at_tuple(key, index, default)
-
-    def _get_at_str(self, key, idx, default):
-        out = self._get_str(key, default)
-        if out is default:
-            return out
-        return out[idx]
-
-    def _get_at_tuple(self, key, idx, default):
-        out = self._get_tuple(key, default)
-        if out is default:
-            return out
-        return out[idx]
-
-    def get_item_shape(self, key: NestedKey):
-        """Returns the shape of the entry, possibly avoiding recurring to :meth:`~.get`."""
-        return _shape(self.get(key))
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        """Updates the TensorDict with values from either a dictionary or another TensorDict.
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): input data to be written
-                in self.
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set.
-                Defaults to ``False``.
-            inplace (bool, optional): if ``True`` and if a key matches an existing
-                key in the tensordict, then the update will occur in-place
-                for that key-value pair. If the entry cannot be found, it will be
-                added. Defaults to ``False``.
-
-        Keyword Args:
-            keys_to_update (sequence of NestedKeys, optional): if provided, only
-                the list of keys in ``key_to_update`` will be updated.
-                This is aimed at avoiding calls to
-                ``data_dest.update(data_src.select(*keys_to_update))``.
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict({}, batch_size=[3])
-            >>> a = torch.randn(3)
-            >>> b = torch.randn(3, 4)
-            >>> other_td = TensorDict({"a": a, "b": b}, batch_size=[])
-            >>> td.update(other_td, inplace=True) # writes "a" and "b" even though they can't be found
-            >>> assert td['a'] is other_td['a']
-            >>> other_td = other_td.clone().zero_()
-            >>> td.update(other_td)
-            >>> assert td['a'] is not other_td['a']
-
-        """
-        if input_dict_or_td is self:
-            # no op
-            return self
-        if keys_to_update is not None:
-            if len(keys_to_update) == 0:
-                return self
-            keys_to_update = unravel_key_list(keys_to_update)
-        for key, value in input_dict_or_td.items():
-            key = _unravel_key_to_tuple(key)
-            firstkey, subkey = key[0], key[1:]
-            if keys_to_update and not any(
-                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
-                for ktu in keys_to_update
-            ):
-                continue
-            target = self._get_str(firstkey, None)
-            if clone and hasattr(value, "clone"):
-                value = value.clone()
-            elif clone:
-                value = tree_map(torch.clone, value)
-            # the key must be a string by now. Let's check if it is present
-            if target is not None:
-                if _is_tensor_collection(type(target)):
-                    if subkey:
-                        sub_keys_to_update = _prune_selected_keys(
-                            keys_to_update, firstkey
-                        )
-                        target.update(
-                            {subkey: value},
-                            inplace=inplace,
-                            clone=clone,
-                            keys_to_update=sub_keys_to_update,
-                            non_blocking=non_blocking,
-                        )
-                        continue
-                    elif isinstance(value, (dict,)) or _is_tensor_collection(
-                        value.__class__
-                    ):
-                        from tensordict._lazy import LazyStackedTensorDict
-
-                        if isinstance(value, LazyStackedTensorDict) and not isinstance(
-                            target, LazyStackedTensorDict
-                        ):
-                            sub_keys_to_update = _prune_selected_keys(
-                                keys_to_update, firstkey
-                            )
-                            self._set_tuple(
-                                key,
-                                LazyStackedTensorDict(
-                                    *target.unbind(value.stack_dim),
-                                    stack_dim=value.stack_dim,
-                                ).update(
-                                    value,
-                                    inplace=inplace,
-                                    clone=clone,
-                                    keys_to_update=sub_keys_to_update,
-                                    non_blocking=non_blocking,
-                                ),
-                                validated=True,
-                                inplace=False,
-                                non_blocking=non_blocking,
-                            )
-                        else:
-                            sub_keys_to_update = _prune_selected_keys(
-                                keys_to_update, firstkey
-                            )
-                            target.update(
-                                value,
-                                inplace=inplace,
-                                clone=clone,
-                                non_blocking=non_blocking,
-                                keys_to_update=sub_keys_to_update,
-                            )
-                        continue
-            self._set_tuple(
-                key,
-                value,
-                inplace=BEST_ATTEMPT_INPLACE if inplace else False,
-                validated=False,
-                non_blocking=non_blocking,
-            )
-        return self
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        """Updates the TensorDict in-place with values from either a dictionary or another TensorDict.
-
-        Unlike :meth:`~.update`, this function will throw an error if the key is unknown to ``self``.
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): input data to be written
-                in self.
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set. Defaults to ``False``.
-
-        Keyword Args:
-            keys_to_update (sequence of NestedKeys, optional): if provided, only
-                the list of keys in ``key_to_update`` will be updated.
-                This is aimed at avoiding calls to
-                ``data_dest.update_(data_src.select(*keys_to_update))``.
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> a = torch.randn(3)
-            >>> b = torch.randn(3, 4)
-            >>> td = TensorDict({"a": a, "b": b}, batch_size=[3])
-            >>> other_td = TensorDict({"a": a*0, "b": b*0}, batch_size=[])
-            >>> td.update_(other_td)
-            >>> assert td['a'] is not other_td['a']
-            >>> assert (td['a'] == other_td['a']).all()
-            >>> assert (td['a'] == 0).all()
-
-        """
-        if input_dict_or_td is self:
-            # no op
-            return self
-        if keys_to_update is not None:
-            if len(keys_to_update) == 0:
-                return self
-            keys_to_update = [_unravel_key_to_tuple(key) for key in keys_to_update]
-
-            named = True
-
-            def inplace_update(name, dest, source):
-                if source is None:
-                    return None
-                name = _unravel_key_to_tuple(name)
-                for key in keys_to_update:
-                    if key == name[: len(key)]:
-                        dest.copy_(source, non_blocking=non_blocking)
-
-        else:
-            named = False
-
-            def inplace_update(dest, source):
-                if source is None:
-                    return None
-                dest.copy_(source, non_blocking=non_blocking)
-
-        if not _is_tensor_collection(type(input_dict_or_td)):
-            from tensordict import TensorDict
-
-            input_dict_or_td = TensorDict.from_dict(
-                input_dict_or_td, batch_dims=self.batch_dims
-            )
-        self._apply_nest(
-            inplace_update,
-            input_dict_or_td,
-            nested_keys=True,
-            default=None,
-            filter_empty=True,
-            named=named,
-            is_leaf=_is_leaf_nontensor,
-        )
-        return self
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        idx: IndexType,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        """Updates the TensorDict in-place at the specified index with values from either a dictionary or another TensorDict.
-
-        Unlike  TensorDict.update, this function will throw an error if the key is unknown to the TensorDict.
-
-        Args:
-            input_dict_or_td (TensorDictBase or dict): input data to be written
-                in self.
-            idx (int, torch.Tensor, iterable, slice): index of the tensordict
-                where the update should occur.
-            clone (bool, optional): whether the tensors in the input (
-                tensor) dict should be cloned before being set. Default is
-                `False`.
-
-        Keyword Args:
-            keys_to_update (sequence of NestedKeys, optional): if provided, only
-                the list of keys in ``key_to_update`` will be updated.
-            non_blocking (bool, optional): if ``True`` and this copy is between
-                different devices, the copy may occur asynchronously with respect
-                to the host.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict({
-            ...     'a': torch.zeros(3, 4, 5),
-            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
-            >>> td.update_at_(
-            ...     TensorDict({
-            ...         'a': torch.ones(1, 4, 5),
-            ...         'b': torch.ones(1, 4, 10)}, batch_size=[1, 4]),
-            ...    slice(1, 2))
-            TensorDict(
-                fields={
-                    a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32),
-                    b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
-                batch_size=torch.Size([3, 4]),
-                device=None,
-                is_shared=False)
-            >>> assert (td[1] == 1).all()
-
-        """
-        if idx == ():
-            return self.update_(
-                input_dict_or_td=input_dict_or_td,
-                keys_to_update=keys_to_update,
-                clone=clone,
-                non_blocking=non_blocking,
-            )
-        if keys_to_update is not None:
-            if len(keys_to_update) == 0:
-                return self
-            keys_to_update = unravel_key_list(keys_to_update)
-        for key, value in input_dict_or_td.items():
-            firstkey, *nextkeys = _unravel_key_to_tuple(key)
-            if keys_to_update and not any(
-                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
-                for ktu in keys_to_update
-            ):
-                continue
-            if not isinstance(value, _ACCEPTED_CLASSES):
-                raise TypeError(
-                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
-                    f"but got {type(value)}"
-                )
-            if clone:
-                value = value.clone()
-            self.set_at_((firstkey, *nextkeys), value, idx, non_blocking=non_blocking)
-        return self
-
-    def replace(self, *args, **kwargs):
-        """Creates a shallow copy of the tensordict where entries have been replaced.
-
-        Accepts one unnamed argument which must be a dictionary of a :class:`~tensordict.TensorDictBase` subclass.
-        Additionaly, first-level entries can be updated with the named keyword arguments.
-
-        Returns:
-            a copy of ``self`` with updated entries if the input is non-empty. If an empty dict or no dict is provided
-            and the kwargs are empty, ``self`` is returned.
-
-        """
-        if args:
-            if len(args) > 1:
-                raise RuntimeError(
-                    "Only a single argument containing a dictionary-like "
-                    f"structure of entries to replace can be passed to replace. Received {len(args)} "
-                    f"arguments instead."
-                )
-            dict_to_replace = args[0]
-        else:
-            dict_to_replace = {}
-        if kwargs:
-            dict_to_replace.update(kwargs)
-        is_dict = isinstance(dict_to_replace, dict)
-        if is_dict:
-            if not dict_to_replace:
-                return self
-        else:
-            if not is_tensor_collection(dict_to_replace):
-                raise RuntimeError(
-                    f"Cannot use object type {type(dict_to_replace)} to update values in tensordict."
-                )
-            if dict_to_replace.is_empty():
-                return self
-        result = self.copy()
-        # using update makes sure that any optimization (e.g. for lazy stacks) is done properly
-        result.update(dict_to_replace)
-        return result
-
-    @lock_blocked
-    def create_nested(self, key):
-        """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
-
-        If the value already exists, it will be overwritten by this operation.
-        This operation is blocked in locked tensordicts.
-
-        Examples:
-            >>> data = TensorDict({}, [3, 4, 5])
-            >>> data.create_nested("root")
-            >>> data.create_nested(("some", "nested", "value"))
-            >>> print(data)
-            TensorDict(
-                fields={
-                    root: TensorDict(
-                        fields={
-                        },
-                        batch_size=torch.Size([3, 4, 5]),
-                        device=None,
-                        is_shared=False),
-                    some: TensorDict(
-                        fields={
-                            nested: TensorDict(
-                                fields={
-                                    value: TensorDict(
-                                        fields={
-                                        },
-                                        batch_size=torch.Size([3, 4, 5]),
-                                        device=None,
-                                        is_shared=False)},
-                                batch_size=torch.Size([3, 4, 5]),
-                                device=None,
-                                is_shared=False)},
-                        batch_size=torch.Size([3, 4, 5]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3, 4, 5]),
-                device=None,
-                is_shared=False)
-        """
-        key = _unravel_key_to_tuple(key)
-        self._create_nested_tuple(key)
-        return self
-
-    def _create_nested_str(self, key):
-        out = self.empty()
-        self._set_str(key, out, inplace=False, validated=True, non_blocking=False)
-        return out
-
-    def _create_nested_tuple(self, key):
-        td = self._create_nested_str(key[0])
-        if len(key) > 1:
-            td._create_nested_tuple(key[1:])
-
-    def copy_(self, tensordict: T, non_blocking: bool = False) -> T:
-        """See :obj:`TensorDictBase.update_`.
-
-        The non-blocking argument will be ignored and is just present for
-        compatibility with :func:`torch.Tensor.copy_`.
-        """
-        return self.update_(tensordict, non_blocking=non_blocking)
-
-    def copy_at_(self, tensordict: T, idx: IndexType, non_blocking: bool = False) -> T:
-        """See :obj:`TensorDictBase.update_at_`."""
-        return self.update_at_(tensordict, idx, non_blocking=non_blocking)
-
-    def is_empty(self) -> bool:
-        """Checks if the tensordict contains any leaf."""
-        for _ in self.keys(True, True):
-            return False
-        return True
-
-    # Dict features: setdefault, items, values, keys, ...
-    def setdefault(
-        self, key: NestedKey, default: CompatibleType, inplace: bool = False
-    ) -> CompatibleType:
-        """Insert the ``key`` entry with a value of ``default`` if ``key`` is not in the tensordict.
-
-        Return the value for ``key`` if ``key`` is in the tensordict, else ``default``.
-
-        Args:
-            key (str or nested key): the name of the value.
-            default (torch.Tensor or compatible type, TensorDictBase): value
-                to be stored in the tensordict if the key is not already present.
-
-        Returns:
-            The value of key in the tensordict. Will be default if the key was not
-            previously set.
-
-        Examples:
-            >>> td = TensorDict({}, batch_size=[3, 4])
-            >>> val = td.setdefault("a", torch.zeros(3, 4))
-            >>> assert (val == 0).all()
-            >>> val = td.setdefault("a", torch.ones(3, 4))
-            >>> assert (val == 0).all() # output is still 0
-
-        """
-        if key not in self.keys(include_nested=isinstance(key, tuple)):
-            self.set(key, default, inplace=inplace)
-        return self.get(key)
-
-    def items(
-        self, include_nested: bool = False, leaves_only: bool = False, is_leaf=None
-    ) -> Iterator[tuple[str, CompatibleType]]:
-        """Returns a generator of key-value pairs for the tensordict.
-
-        Args:
-            include_nested (bool, optional): if ``True``, nested values will be returned.
-                Defaults to ``False``.
-            leaves_only (bool, optional): if ``False``, only leaves will be
-                returned. Defaults to ``False``.
-            is_leaf: an optional callable that indicates if a class is to be considered a
-                leaf or not.
-
-        """
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-
-        # check the conditions once only
-        if include_nested and leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if not is_leaf(val.__class__):
-                    yield from (
-                        (_unravel_key_to_tuple((k, _key)), _val)
-                        for _key, _val in val.items(
-                            include_nested=include_nested,
-                            leaves_only=leaves_only,
-                            is_leaf=is_leaf,
-                        )
-                    )
-                else:
-                    yield k, val
-        elif include_nested:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                yield k, val
-                if not is_leaf(val.__class__):
-                    yield from (
-                        (_unravel_key_to_tuple((k, _key)), _val)
-                        for _key, _val in val.items(
-                            include_nested=include_nested,
-                            leaves_only=leaves_only,
-                            is_leaf=is_leaf,
-                        )
-                    )
-        elif leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if is_leaf(val.__class__):
-                    yield k, val
-        else:
-            for k in self.keys():
-                yield k, self._get_str(k, NO_DEFAULT)
-
-    def non_tensor_items(self, include_nested: bool = False):
-        """Returns all non-tensor leaves, maybe recursively."""
-        return tuple(
-            self.items(
-                include_nested,
-                leaves_only=True,
-                is_leaf=_is_non_tensor,
-            )
-        )
-
-    def values(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf=None,
-    ) -> Iterator[CompatibleType]:
-        """Returns a generator representing the values for the tensordict.
-
-        Args:
-            include_nested (bool, optional): if ``True``, nested values will be returned.
-                Defaults to ``False``.
-            leaves_only (bool, optional): if ``False``, only leaves will be
-                returned. Defaults to ``False``.
-            is_leaf: an optional callable that indicates if a class is to be considered a
-                leaf or not.
-
-        """
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-        # check the conditions once only
-        if include_nested and leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if not is_leaf(val.__class__):
-                    yield from val.values(
-                        include_nested=include_nested,
-                        leaves_only=leaves_only,
-                        is_leaf=is_leaf,
-                    )
-                else:
-                    yield val
-        elif include_nested:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                yield val
-                if not is_leaf(val.__class__):
-                    yield from val.values(
-                        include_nested=include_nested,
-                        leaves_only=leaves_only,
-                        is_leaf=is_leaf,
-                    )
-        elif leaves_only:
-            for k in self.keys():
-                val = self._get_str(k, NO_DEFAULT)
-                if is_leaf(val.__class__):
-                    yield val
-        else:
-            for k in self.keys():
-                yield self._get_str(k, NO_DEFAULT)
-
-    @cache  # noqa: B019
-    def _values_list(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-    ) -> List:
-        return list(
-            self.values(
-                include_nested=include_nested,
-                leaves_only=leaves_only,
-                is_leaf=_NESTED_TENSORS_AS_LISTS,
-            )
-        )
-
-    @cache  # noqa: B019
-    def _items_list(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        *,
-        collapse: bool = False,
-    ) -> Tuple[List, List]:
-        return tuple(
-            list(key_or_val)
-            for key_or_val in zip(
-                *self.items(
-                    include_nested=include_nested,
-                    leaves_only=leaves_only,
-                    is_leaf=_NESTED_TENSORS_AS_LISTS if not collapse else None,
-                )
-            )
-        )
-
-    @cache  # noqa: B019
-    def _grad(self):
-        result = self._fast_apply(lambda x: x.grad, propagate_lock=True)
-        return result
-
-    @cache  # noqa: B019
-    def _data(self):
-        result = self._fast_apply(lambda x: x.data, propagate_lock=True)
-        return result
-
-    @abc.abstractmethod
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] = None,
-    ):
-        """Returns a generator of tensordict keys.
-
-        Args:
-            include_nested (bool, optional): if ``True``, nested values will be returned.
-                Defaults to ``False``.
-            leaves_only (bool, optional): if ``False``, only leaves will be
-                returned. Defaults to ``False``.
-            is_leaf: an optional callable that indicates if a class is to be considered a
-                leaf or not.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> data = TensorDict({"0": 0, "1": {"2": 2}}, batch_size=[])
-            >>> data.keys()
-            ['0', '1']
-            >>> list(data.keys(leaves_only=True))
-            ['0']
-            >>> list(data.keys(include_nested=True, leaves_only=True))
-            ['0', '1', ('1', '2')]
-        """
-        ...
-
-    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        """Removes and returns a value from a tensordict.
-
-        If the value is not present and no default value is provided, a KeyError
-        is thrown.
-
-        Args:
-            key (str or nested key): the entry to look for.
-            default (Any, optional): the value to return if the key cannot be found.
-
-        Examples:
-            >>> td = TensorDict({"1": 1}, [])
-            >>> one = td.pop("1")
-            >>> assert one == 1
-            >>> none = td.pop("1", default=None)
-            >>> assert none is None
-        """
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR.format(key))
-        try:
-            # using try/except for get/del is suboptimal, but
-            # this is faster that checkink if key in self keys
-            out = self.get(key, default)
-            self.del_(key)
-        except KeyError as err:
-            # if default provided, 'out' value will return, else raise error
-            if default == NO_DEFAULT:
-                raise KeyError(
-                    f"You are trying to pop key `{key}` which is not in dict "
-                    f"without providing default value."
-                ) from err
-        return out
-
-    @property
-    @cache  # noqa: B019
-    def sorted_keys(self) -> list[NestedKey]:
-        """Returns the keys sorted in alphabetical order.
-
-        Does not support extra arguments.
-
-        If the TensorDict is locked, the keys are cached until the tensordict
-        is unlocked for faster execution.
-
-        """
-        return sorted(self.keys())
-
-    @as_decorator()
-    def flatten(self, start_dim=0, end_dim=-1):
-        """Flattens all the tensors of a tensordict.
-
-        Args:
-            start_dim (int): the first dim to flatten
-            end_dim (int): the last dim to flatten
-
-        Examples:
-            >>> td = TensorDict({
-            ...     "a": torch.arange(60).view(3, 4, 5),
-            ...     "b": torch.arange(12).view(3, 4)}, batch_size=[3, 4])
-            >>> td_flat = td.flatten(0, 1)
-            >>> td_flat.batch_size
-            torch.Size([12])
-            >>> td_flat["a"]
-            tensor([[ 0,  1,  2,  3,  4],
-                    [ 5,  6,  7,  8,  9],
-                    [10, 11, 12, 13, 14],
-                    [15, 16, 17, 18, 19],
-                    [20, 21, 22, 23, 24],
-                    [25, 26, 27, 28, 29],
-                    [30, 31, 32, 33, 34],
-                    [35, 36, 37, 38, 39],
-                    [40, 41, 42, 43, 44],
-                    [45, 46, 47, 48, 49],
-                    [50, 51, 52, 53, 54],
-                    [55, 56, 57, 58, 59]])
-            >>> td_flat["b"]
-            tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
-
-        """
-        if start_dim < 0:
-            start_dim = self.ndim + start_dim
-        if end_dim < 0:
-            end_dim = self.ndim + end_dim
-            if end_dim < 0:
-                raise ValueError(
-                    f"Incompatible end_dim {end_dim} for tensordict with shape {self.shape}."
-                )
-        if end_dim <= start_dim:
-            raise ValueError(
-                "The end dimension must be strictly greater than the start dim."
-            )
-
-        def flatten(tensor):
-            return torch.flatten(tensor, start_dim, end_dim)
-
-        nelt = prod(self.batch_size[start_dim : end_dim + 1])
-        if start_dim > 0:
-            batch_size = (
-                list(self.batch_size)[:start_dim]
-                + [nelt]
-                + list(self.batch_size[end_dim + 1 :])
-            )
-        else:
-            batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
-        # TODO: check that this works with nested tds of different batch size
-        out = self._fast_apply(flatten, batch_size=batch_size, propagate_lock=True)
-        if self._has_names():
-            names = [
-                name
-                for i, name in enumerate(self.names)
-                if (i < start_dim or i > end_dim)
-            ]
-            names.insert(start_dim, None)
-            out.names = names
-        return out
-
-    @as_decorator()
-    def unflatten(self, dim, unflattened_size):
-        """Unflattens a tensordict dim expanding it to a desired shape.
-
-        Args:
-            dim (int): specifies the dimension of the input tensor to be
-                unflattened.
-            unflattened_size (shape): is the new shape of the unflattened
-                dimension of the tensordict.
-
-        Examples:
-            >>> td = TensorDict({
-            ...     "a": torch.arange(60).view(3, 4, 5),
-            ...     "b": torch.arange(12).view(3, 4)},
-            ...     batch_size=[3, 4])
-            >>> td_flat = td.flatten(0, 1)
-            >>> td_unflat = td_flat.unflatten(0, [3, 4])
-            >>> assert (td == td_unflat).all()
-        """
-        if dim < 0:
-            dim = self.ndim + dim
-            if dim < 0:
-                raise ValueError(
-                    f"Incompatible dim {dim} for tensordict with shape {self.shape}."
-                )
-
-        def unflatten(tensor):
-            return torch.unflatten(
-                tensor,
-                dim,
-                unflattened_size,
-            )
-
-        if dim > 0:
-            batch_size = (
-                list(self.batch_size)[:dim]
-                + list(unflattened_size)
-                + list(self.batch_size[dim + 1 :])
-            )
-        else:
-            batch_size = list(unflattened_size) + list(self.batch_size[1:])
-        # TODO: check that this works with nested tds of different batch size
-        out = self._fast_apply(unflatten, batch_size=batch_size, propagate_lock=True)
-        if self._has_names():
-            names = copy(self.names)
-            for _ in range(len(unflattened_size) - 1):
-                names.insert(dim, None)
-            out.names = names
-        return out
-
-    @abc.abstractmethod
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> T:
-        """Renames a key with a new string and returns the same tensordict with the updated key name.
-
-        Args:
-            old_key (str or nested key): key to be renamed.
-            new_key (str or nested key): new name of the entry.
-            safe (bool, optional): if ``True``, an error is thrown when the new
-                key is already present in the TensorDict.
-
-        Returns:
-            self
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def del_(self, key: NestedKey) -> T:
-        """Deletes a key of the tensordict.
-
-        Args:
-            key (NestedKey): key to be deleted
-
-        Returns:
-            self
-
-        """
-        ...
-
-    # Distributed functionality
-    def gather_and_stack(
-        self, dst: int, group: "dist.ProcessGroup" | None = None
-    ) -> T | None:
-        """Gathers tensordicts from various workers and stacks them onto self in the destination worker.
-
-        Args:
-            dst (int): the rank of the destination worker where :func:`gather_and_stack` will be called.
-            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
-                will be used for communication. Otherwise, the default process group
-                will be used.
-                Defaults to ``None``.
-
-        Example:
-            >>> from torch import multiprocessing as mp
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>>
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     # Create a single tensordict to be sent to server
-            ...     td = TensorDict(
-            ...         {("a", "b"): torch.randn(2),
-            ...          "c": torch.randn(2)}, [2]
-            ...     )
-            ...     td.gather_and_stack(0)
-            ...
-            >>> def server():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     # Creates the destination tensordict on server.
-            ...     # The first dim must be equal to world_size-1
-            ...     td = TensorDict(
-            ...         {("a", "b"): torch.zeros(2),
-            ...          "c": torch.zeros(2)}, [2]
-            ...     ).expand(1, 2).contiguous()
-            ...     td.gather_and_stack(0)
-            ...     assert td["a", "b"] != 0
-            ...     print("yuppie")
-            ...
-            >>> if __name__ == "__main__":
-            ...     mp.set_start_method("spawn")
-            ...
-            ...     main_worker = mp.Process(target=server)
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-        """
-        output = (
-            [None for _ in range(dist.get_world_size(group=group))]
-            if dst == dist.get_rank(group=group)
-            else None
-        )
-        dist.gather_object(self, output, dst=dst, group=group)
-        if dst == dist.get_rank(group=group):
-            # remove self from output
-            output = [item for i, item in enumerate(output) if i != dst]
-            self.update(torch.stack(output, 0), inplace=True)
-            return self
-        return None
-
-    def send(
-        self,
-        dst: int,
-        *,
-        group: "dist.ProcessGroup" | None = None,
-        init_tag: int = 0,
-        pseudo_rand: bool = False,
-    ) -> None:  # noqa: D417
-        """Sends the content of a tensordict to a distant worker.
-
-        Args:
-            dst (int): the rank of the destination worker where the content
-                should be sent.
-
-        Keyword Args:
-            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
-                will be used for communication. Otherwise, the default process group
-                will be used.
-                Defaults to ``None``.
-            init_tag (int): the initial tag to be used to mark the tensors.
-                Note that this will be incremented by as much as the number of
-                tensors contained in the TensorDict.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                Defaults to ``False``.
-
-        Example:
-            >>> from torch import multiprocessing as mp
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>>
-            >>>
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.randn(2),
-            ...             "c": torch.randn(2, 3),
-            ...             "_": torch.ones(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.send(0)
-            ...
-            >>>
-            >>> def server(queue):
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.zeros(2),
-            ...             "c": torch.zeros(2, 3),
-            ...             "_": torch.zeros(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.recv(1)
-            ...     assert (td != 0).all()
-            ...     queue.put("yuppie")
-            ...
-            >>>
-            >>> if __name__=="__main__":
-            ...     queue = mp.Queue(1)
-            ...     main_worker = mp.Process(target=server, args=(queue,))
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...     out = queue.get(timeout=10)
-            ...     assert out == "yuppie"
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-
-        """
-        self._send(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
-
-    def _send(
-        self,
-        dst: int,
-        _tag: int = -1,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> int:
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if isinstance(value, Tensor):
-                pass
-            elif _is_tensor_collection(value.__class__):
-                _tag = value._send(dst, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
-                continue
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            dist.send(value, dst=dst, tag=_tag, group=group)
-
-        return _tag
-
-    def recv(
-        self,
-        src: int,
-        *,
-        group: "dist.ProcessGroup" | None = None,
-        init_tag: int = 0,
-        pseudo_rand: bool = False,
-    ) -> int:  # noqa: D417
-        """Receives the content of a tensordict and updates content with it.
-
-        Check the example in the `send` method for context.
-
-        Args:
-            src (int): the rank of the source worker.
-
-        Keyword Args:
-            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
-                will be used for communication. Otherwise, the default process group
-                will be used.
-                Defaults to ``None``.
-            init_tag (int): the ``init_tag`` used by the source worker.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                This value must match the one passed to :func:`send`.
-                Defaults to ``False``.
-        """
-        return self._recv(src, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
-
-    def _recv(
-        self,
-        src: int,
-        _tag: int = -1,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-        non_blocking: bool = False,
-    ) -> int:
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if isinstance(value, Tensor):
-                pass
-            elif _is_tensor_collection(value.__class__):
-                _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
-                continue
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            dist.recv(value, src=src, tag=_tag, group=group)
-            self._set_str(
-                key, value, inplace=True, validated=True, non_blocking=non_blocking
-            )
-
-        return _tag
-
-    def isend(
-        self,
-        dst: int,
-        *,
-        group: "dist.ProcessGroup" | None = None,
-        init_tag: int = 0,
-        pseudo_rand: bool = False,
-    ) -> int:  # noqa: D417
-        """Sends the content of the tensordict asynchronously.
-
-        Args:
-            dst (int): the rank of the destination worker where the content
-                should be sent.
-
-        Keyword Args:
-            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
-                will be used for communication. Otherwise, the default process group
-                will be used.
-                Defaults to ``None``.
-            init_tag (int): the initial tag to be used to mark the tensors.
-                Note that this will be incremented by as much as the number of
-                tensors contained in the TensorDict.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                Defaults to ``False``.
-
-        Example:
-            >>> import torch
-            >>> from tensordict import TensorDict
-            >>> from torch import multiprocessing as mp
-            >>> def client():
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=1,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.randn(2),
-            ...             "c": torch.randn(2, 3),
-            ...             "_": torch.ones(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     td.isend(0)
-            ...
-            >>>
-            >>> def server(queue, return_premature=True):
-            ...     torch.distributed.init_process_group(
-            ...         "gloo",
-            ...         rank=0,
-            ...         world_size=2,
-            ...         init_method=f"tcp://localhost:10003",
-            ...     )
-            ...     td = TensorDict(
-            ...         {
-            ...             ("a", "b"): torch.zeros(2),
-            ...             "c": torch.zeros(2, 3),
-            ...             "_": torch.zeros(2, 1, 5),
-            ...         },
-            ...         [2],
-            ...     )
-            ...     out = td.irecv(1, return_premature=return_premature)
-            ...     if return_premature:
-            ...         for fut in out:
-            ...             fut.wait()
-            ...     assert (td != 0).all()
-            ...     queue.put("yuppie")
-            ...
-            >>>
-            >>> if __name__ == "__main__":
-            ...     queue = mp.Queue(1)
-            ...     main_worker = mp.Process(
-            ...         target=server,
-            ...         args=(queue, )
-            ...         )
-            ...     secondary_worker = mp.Process(target=client)
-            ...
-            ...     main_worker.start()
-            ...     secondary_worker.start()
-            ...     out = queue.get(timeout=10)
-            ...     assert out == "yuppie"
-            ...     main_worker.join()
-            ...     secondary_worker.join()
-
-        """
-        return self._isend(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
-
-    def _isend(
-        self,
-        dst: int,
-        _tag: int = -1,
-        _futures: list[torch.Future] | None = None,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> int:
-        root = False
-        if _futures is None:
-            root = True
-            _futures = []
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _tag = value._isend(
-                    dst,
-                    _tag=_tag,
-                    pseudo_rand=pseudo_rand,
-                    _futures=_futures,
-                    group=group,
-                )
-                continue
-            elif isinstance(value, Tensor):
-                pass
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            _future = dist.isend(value, dst=dst, tag=_tag, group=group)
-            _futures.append(_future)
-        if root:
-            for _future in _futures:
-                _future.wait()
-        return _tag
-
-    def irecv(
-        self,
-        src: int,
-        *,
-        group: "dist.ProcessGroup" | None = None,
-        return_premature: bool = False,
-        init_tag: int = 0,
-        pseudo_rand: bool = False,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        """Receives the content of a tensordict and updates content with it asynchronously.
-
-        Check the example in the :meth:`~.isend` method for context.
-
-        Args:
-            src (int): the rank of the source worker.
-
-        Keyword Args:
-            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
-                will be used for communication. Otherwise, the default process group
-                will be used.
-                Defaults to ``None``.
-            return_premature (bool): if ``True``, returns a list of futures to wait
-                upon until the tensordict is updated. Defaults to ``False``,
-                i.e. waits until update is completed withing the call.
-            init_tag (int): the ``init_tag`` used by the source worker.
-            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
-                random, allowing to send multiple data from different nodes
-                without overlap. Notice that the generation of these pseudo-random
-                numbers is expensive (1e-5 sec/number), meaning that it could
-                slow down the runtime of your algorithm.
-                This value must match the one passed to :func:`isend`.
-                Defaults to ``False``.
-
-        Returns:
-            if ``return_premature=True``, a list of futures to wait
-                upon until the tensordict is updated.
-        """
-        return self._irecv(
-            src,
-            return_premature=return_premature,
-            _tag=init_tag - 1,
-            pseudo_rand=pseudo_rand,
-            group=group,
-        )
-
-    def _irecv(
-        self,
-        src: int,
-        return_premature: bool = False,
-        _tag: int = -1,
-        _future_list: list[torch.Future] = None,
-        pseudo_rand: bool = False,
-        group: "dist.ProcessGroup" | None = None,
-    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _tag, _future_list = value._irecv(
-                    src,
-                    _tag=_tag,
-                    _future_list=_future_list,
-                    pseudo_rand=pseudo_rand,
-                    group=group,
-                )
-                continue
-            elif isinstance(value, Tensor):
-                pass
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            if not pseudo_rand:
-                _tag += 1
-            else:
-                _tag = int_generator(_tag + 1)
-            _future_list.append(dist.irecv(value, src=src, tag=_tag, group=group))
-        if not root:
-            return _tag, _future_list
-        elif return_premature:
-            return _future_list
-        else:
-            for future in _future_list:
-                future.wait()
-            return
-
-    def reduce(
-        self,
-        dst,
-        op=None,
-        async_op=False,
-        return_premature=False,
-        group=None,
-    ):
-        """Reduces the tensordict across all machines.
-
-        Only the process with ``rank`` dst is going to receive the final result.
-
-        """
-        if op is None:
-            op = dist.ReduceOp.SUM
-        return self._reduce(dst, op, async_op, return_premature, group=group)
-
-    def _reduce(
-        self,
-        dst,
-        op=None,
-        async_op=False,
-        return_premature=False,
-        _future_list=None,
-        group=None,
-    ):
-        if op is None:
-            op = dist.ReduceOp.SUM
-        root = False
-        if _future_list is None:
-            _future_list = []
-            root = True
-        for key in self.sorted_keys:
-            value = self._get_str(key, NO_DEFAULT)
-            if _is_tensor_collection(value.__class__):
-                _future_list = value._reduce(
-                    dst=dst,
-                    op=op,
-                    async_op=async_op,
-                    _future_list=_future_list,
-                )
-                continue
-            elif isinstance(value, Tensor):
-                pass
-            else:
-                raise NotImplementedError(f"Type {type(value)} is not supported.")
-            _future_list.append(
-                dist.reduce(value, dst=dst, op=op, async_op=async_op, group=group)
-            )
-        if not root:
-            return _future_list
-        elif async_op and return_premature:
-            return _future_list
-        elif async_op:
-            for future in _future_list:
-                future.wait()
-            return
-
-    # Apply and map functionality
-    def apply_(self, fn: Callable, *others, **kwargs) -> T:
-        """Applies a callable to all values stored in the tensordict and re-writes them in-place.
-
-        Args:
-            fn (Callable): function to be applied to the tensors in the
-                tensordict.
-            *others (sequence of TensorDictBase, optional): the other
-                tensordicts to be used.
-
-        Keyword Args: See :meth:`~.apply`.
-
-        Returns:
-            self or a copy of self with the function applied
-
-        """
-        return self.apply(fn, *others, inplace=True, **kwargs)
-
-    def apply(
-        self,
-        fn: Callable,
-        *others: T,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        default: Any = NO_DEFAULT,
-        filter_empty: bool | None = None,
-        propagate_lock: bool = False,
-        call_on_nested: bool = False,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
-
-        The callable signature must be ``Callable[Tuple[Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
-
-        Args:
-            fn (Callable): function to be applied to the tensors in the
-                tensordict.
-            *others (TensorDictBase instances, optional): if provided, these
-                tensordict instances should have a structure matching the one
-                of self. The ``fn`` argument should receive as many
-                unnamed inputs as the number of tensordicts, including self.
-                If other tensordicts have missing entries, a default value
-                can be passed through the ``default`` keyword argument.
-
-        Keyword Args:
-            batch_size (sequence of int, optional): if provided,
-                the resulting TensorDict will have the desired batch_size.
-                The :obj:`batch_size` argument should match the batch_size after
-                the transformation. This is a keyword only argument.
-            device (torch.device, optional): the resulting device, if any.
-            names (list of str, optional): the new dimension names, in case the
-                batch_size is modified.
-            inplace (bool, optional): if True, changes are made in-place.
-                Default is False. This is a keyword only argument.
-            default (Any, optional): default value for missing entries in the
-                other tensordicts. If not provided, missing entries will
-                raise a `KeyError`.
-            filter_empty (bool, optional): if ``True``, empty tensordicts will be
-                filtered out. This also comes with a lower computational cost as
-                empty data structures won't be created and destroyed. Non-tensor data
-                is considered as a leaf and thereby will be kept in the tensordict even
-                if left untouched by the function.
-                Defaults to ``False`` for backward compatibility.
-            propagate_lock (bool, optional): if ``True``, a locked tensordict will produce
-                another locked tensordict. Defaults to ``False``.
-            call_on_nested (bool, optional): if ``True``, the function will be called on first-level tensors
-                and containers (TensorDict or tensorclass). In this scenario, ``func`` is responsible of
-                propagating its calls to nested levels. This allows a fine-grained behaviour
-                when propagating the calls to nested tensordicts.
-                If ``False``, the function will only be called on leaves, and ``apply`` will take care of dispatching
-                the function to all leaves.
-
-                    >>> td = TensorDict({"a": {"b": [0.0, 1.0]}, "c": [1.0, 2.0]})
-                    >>> def mean_tensor_only(val):
-                    ...     if is_tensor_collection(val):
-                    ...         raise RuntimeError("Unexpected!")
-                    ...     return val.mean()
-                    >>> td_mean = td.apply(mean_tensor_only)
-                    >>> def mean_any(val):
-                    ...     if is_tensor_collection(val):
-                    ...         # Recurse
-                    ...         return val.apply(mean_any, call_on_nested=True)
-                    ...     return val.mean()
-                    >>> td_mean = td.apply(mean_any, call_on_nested=True)
-            out (TensorDictBase, optional): a tensordict where to write the results. This can be used to avoid
-                creating a new tensordict:
-
-                    >>> td = TensorDict({"a": 0})
-                    >>> td.apply(lambda x: x+1, out=td)
-                    >>> assert (td==1).all()
-
-                .. warning:: If the operation executed on the tensordict requires multiple keys to be accessed for
-                    a single computation, providing an ``out`` argument equal to ``self`` can cause the operation
-                    to provide silently wrong results.
-                    For instance:
-
-                        >>> td = TensorDict({"a": 1, "b": 1})
-                        >>> td.apply(lambda x: x+td["a"])["b"] # Right!
-                        tensor(2)
-                        >>> td.apply(lambda x: x+td["a"], out=td)["b"] # Wrong!
-                        tensor(3)
-
-            **constructor_kwargs: additional keyword arguments to be passed to the
-                TensorDict constructor.
-
-        Returns:
-            a new tensordict with transformed_in tensors.
-
-        Example:
-            >>> td = TensorDict({
-            ...     "a": -torch.ones(3),
-            ...     "b": {"c": torch.ones(3)}},
-            ...     batch_size=[3])
-            >>> td_1 = td.apply(lambda x: x+1)
-            >>> assert (td_1["a"] == 0).all()
-            >>> assert (td_1["b", "c"] == 2).all()
-            >>> td_2 = td.apply(lambda x, y: x+y, td)
-            >>> assert (td_2["a"] == -2).all()
-            >>> assert (td_2["b", "c"] == 2).all()
-
-        .. note::
-            If ``None`` is returned by the function, the entry is ignored. This
-            can be used to filter the data in the tensordict:
-
-            >>> td = TensorDict({"1": 1, "2": 2, "b": {"2": 2, "1": 1}}, [])
-            >>> def filter(tensor):
-            ...     if tensor == 1:
-            ...         return tensor
-            >>> td.apply(filter)
-            TensorDict(
-                fields={
-                    1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        .. note::
-            The apply method will return an :class:`~tensordict.TensorDict` instance,
-            regardless of the input type. To keep the same type, one can execute
-
-            >>> out = td.clone(False).update(td.apply(...))
-
-
-        """
-        result = self._apply_nest(
-            fn,
-            *others,
-            batch_size=batch_size,
-            device=device,
-            names=names,
-            inplace=inplace,
-            checked=False,
-            default=default,
-            filter_empty=filter_empty,
-            call_on_nested=call_on_nested,
-            out=out,
-            **constructor_kwargs,
-        )
-        if propagate_lock and not inplace and self.is_locked and result is not None:
-            result.lock_()
-        return result
-
-    def named_apply(
-        self,
-        fn: Callable,
-        *others: T,
-        nested_keys: bool = False,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        default: Any = NO_DEFAULT,
-        filter_empty: bool | None = None,
-        propagate_lock: bool = False,
-        call_on_nested: bool = False,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        """Applies a key-conditioned callable to all values stored in the tensordict and sets them in a new atensordict.
-
-        The callable signature must be ``Callable[Tuple[str, Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
-
-        Args:
-            fn (Callable): function to be applied to the (name, tensor) pairs in the
-                tensordict. For each leaf, only its leaf name will be used (not
-                the full `NestedKey`).
-            *others (TensorDictBase instances, optional): if provided, these
-                tensordict instances should have a structure matching the one
-                of self. The ``fn`` argument should receive as many
-                unnamed inputs as the number of tensordicts, including self.
-                If other tensordicts have missing entries, a default value
-                can be passed through the ``default`` keyword argument.
-            nested_keys (bool, optional): if ``True``, the complete path
-                to the leaf will be used. Defaults to ``False``, i.e. only the last
-                string is passed to the function.
-            batch_size (sequence of int, optional): if provided,
-                the resulting TensorDict will have the desired batch_size.
-                The :obj:`batch_size` argument should match the batch_size after
-                the transformation. This is a keyword only argument.
-            device (torch.device, optional): the resulting device, if any.
-            names (list of str, optional): the new dimension names, in case the
-                batch_size is modified.
-            inplace (bool, optional): if True, changes are made in-place.
-                Default is False. This is a keyword only argument.
-            default (Any, optional): default value for missing entries in the
-                other tensordicts. If not provided, missing entries will
-                raise a `KeyError`.
-            filter_empty (bool, optional): if ``True``, empty tensordicts will be
-                filtered out. This also comes with a lower computational cost as
-                empty data structures won't be created and destroyed. Defaults to
-                ``False`` for backward compatibility.
-            propagate_lock (bool, optional): if ``True``, a locked tensordict will produce
-                another locked tensordict. Defaults to ``False``.
-            call_on_nested (bool, optional): if ``True``, the function will be called on first-level tensors
-                and containers (TensorDict or tensorclass). In this scenario, ``func`` is responsible of
-                propagating its calls to nested levels. This allows a fine-grained behaviour
-                when propagating the calls to nested tensordicts.
-                If ``False``, the function will only be called on leaves, and ``apply`` will take care of dispatching
-                the function to all leaves.
-
-                    >>> td = TensorDict({"a": {"b": [0.0, 1.0]}, "c": [1.0, 2.0]})
-                    >>> def mean_tensor_only(val):
-                    ...     if is_tensor_collection(val):
-                    ...         raise RuntimeError("Unexpected!")
-                    ...     return val.mean()
-                    >>> td_mean = td.apply(mean_tensor_only)
-                    >>> def mean_any(val):
-                    ...     if is_tensor_collection(val):
-                    ...         # Recurse
-                    ...         return val.apply(mean_any, call_on_nested=True)
-                    ...     return val.mean()
-                    >>> td_mean = td.apply(mean_any, call_on_nested=True)
-
-            out (TensorDictBase, optional): a tensordict where to write the results. This can be used to avoid
-                creating a new tensordict:
-
-                    >>> td = TensorDict({"a": 0})
-                    >>> td.apply(lambda x: x+1, out=td)
-                    >>> assert (td==1).all()
-
-                .. warning:: If the operation executed on the tensordict requires multiple keys to be accessed for
-                    a single computation, providing an ``out`` argument equal to ``self`` can cause the operation
-                    to provide silently wrong results.
-                    For instance:
-
-                        >>> td = TensorDict({"a": 1, "b": 1})
-                        >>> td.apply(lambda x: x+td["a"])["b"] # Right!
-                        tensor(2)
-                        >>> td.apply(lambda x: x+td["a"], out=td)["b"] # Wrong!
-                        tensor(3)
-
-            **constructor_kwargs: additional keyword arguments to be passed to the
-                TensorDict constructor.
-
-        Returns:
-            a new tensordict with transformed_in tensors.
-
-        Example:
-            >>> td = TensorDict({
-            ...     "a": -torch.ones(3),
-            ...     "nested": {"a": torch.ones(3), "b": torch.zeros(3)}},
-            ...     batch_size=[3])
-            >>> def name_filter(name, tensor):
-            ...     if name == "a":
-            ...         return tensor
-            >>> td.named_apply(name_filter)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    nested: TensorDict(
-                        fields={
-                            a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> def name_filter(name, *tensors):
-            ...     if name == "a":
-            ...         r = 0
-            ...         for tensor in tensors:
-            ...             r = r + tensor
-            ...         return tensor
-            >>> out = td.named_apply(name_filter, td)
-            >>> print(out)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    nested: TensorDict(
-                        fields={
-                            a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-            >>> print(out["a"])
-            tensor([-1., -1., -1.])
-
-        .. note::
-            If ``None`` is returned by the function, the entry is ignored. This
-            can be used to filter the data in the tensordict:
-
-            >>> td = TensorDict({"1": 1, "2": 2, "b": {"2": 2, "1": 1}}, [])
-            >>> def name_filter(name, tensor):
-            ...     if name == "1":
-            ...         return tensor
-            >>> td.named_apply(name_filter)
-            TensorDict(
-                fields={
-                    1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        result = self._apply_nest(
-            fn,
-            *others,
-            batch_size=batch_size,
-            device=device,
-            names=names,
-            inplace=inplace,
-            checked=False,
-            default=default,
-            named=True,
-            nested_keys=nested_keys,
-            filter_empty=filter_empty,
-            call_on_nested=call_on_nested,
-            **constructor_kwargs,
-        )
-        if propagate_lock and not inplace and self.is_locked and result is not None:
-            result.lock_()
-        return result
-
-    @abc.abstractmethod
-    def _apply_nest(
-        self,
-        fn: Callable,
-        *others: T,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        checked: bool = False,
-        call_on_nested: bool = False,
-        default: Any = NO_DEFAULT,
-        named: bool = False,
-        nested_keys: bool = False,
-        prefix: tuple = (),
-        filter_empty: bool | None = None,
-        is_leaf: Callable = None,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        ...
-
-    def _fast_apply(
-        self,
-        fn: Callable,
-        *others: T,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        call_on_nested: bool = False,
-        default: Any = NO_DEFAULT,
-        named: bool = False,
-        nested_keys: bool = False,
-        # filter_empty must be False because we use _fast_apply for all sorts of ops like expand etc
-        # and non-tensor data will disappear if we use True by default.
-        filter_empty: bool | None = False,
-        is_leaf: Callable = None,
-        propagate_lock: bool = False,
-        out: TensorDictBase | None = None,
-        **constructor_kwargs,
-    ) -> T | None:
-        """A faster apply method.
-
-        This method does not run any check after performing the func. This
-        means that one to make sure that the metadata of the resulting tensors
-        (device, shape etc.) match the :meth:`~.apply` ones.
-
-        """
-        result = self._apply_nest(
-            fn,
-            *others,
-            batch_size=batch_size,
-            device=device,
-            names=names,
-            inplace=inplace,
-            checked=True,
-            call_on_nested=call_on_nested,
-            named=named,
-            default=default,
-            nested_keys=nested_keys,
-            filter_empty=filter_empty,
-            is_leaf=is_leaf,
-            out=out,
-            **constructor_kwargs,
-        )
-        if propagate_lock and not inplace and self.is_locked and result is not None:
-            result.lock_()
-        return result
-
-    def map(
-        self,
-        fn: Callable[[TensorDictBase], TensorDictBase | None],
-        dim: int = 0,
-        num_workers: int | None = None,
-        *,
-        out: TensorDictBase | None = None,
-        chunksize: int | None = None,
-        num_chunks: int | None = None,
-        pool: mp.Pool | None = None,
-        generator: torch.Generator | None = None,
-        max_tasks_per_child: int | None = None,
-        worker_threads: int = 1,
-        index_with_generator: bool = False,
-        pbar: bool = False,
-        mp_start_method: str | None = None,
-    ):
-        """Maps a function to splits of the tensordict across one dimension.
-
-        This method will apply a function to a tensordict instance by chunking
-        it in tensordicts of equal size and dispatching the operations over the
-        desired number of workers.
-
-        The function signature should be ``Callabe[[TensorDict], Union[TensorDict, Tensor]]``.
-        The output must support the :func:`torch.cat` operation. The function
-        must be serializable.
-
-        Args:
-            fn (callable): function to apply to the tensordict.
-                Signatures similar to ``Callabe[[TensorDict], Union[TensorDict, Tensor]]``
-                are supported.
-            dim (int, optional): the dim along which the tensordict will be chunked.
-            num_workers (int, optional): the number of workers. Exclusive with ``pool``.
-                If none is provided, the number of workers will be set to the
-                number of cpus available.
-
-        Keyword Args:
-            out (TensorDictBase, optional): an optional container for the output.
-                Its batch-size along the ``dim`` provided must match ``self.ndim``.
-                If it is shared or memmap (:meth:`~.is_shared` or :meth:`~.is_memmap`
-                returns ``True``) it will be populated within the remote processes,
-                avoiding data inward transfers. Otherwise, the data from the ``self``
-                slice will be sent to the process, collected on the current process
-                and written inplace into ``out``.
-            chunksize (int, optional): The size of each chunk of data.
-                A ``chunksize`` of 0 will unbind the tensordict along the
-                desired dimension and restack it after the function is applied,
-                whereas ``chunksize>0`` will split the tensordict and call
-                :func:`torch.cat` on the resulting list of tensordicts.
-                If none is provided, the number of chunks will equate the number
-                of workers. For very large tensordicts, such large chunks
-                may not fit in memory for the operation to be done and
-                more chunks may be needed to make the operation practically
-                doable. This argument is exclusive with ``num_chunks``.
-            num_chunks (int, optional): the number of chunks to split the tensordict
-                into. If none is provided, the number of chunks will equate the number
-                of workers. For very large tensordicts, such large chunks
-                may not fit in memory for the operation to be done and
-                more chunks may be needed to make the operation practically
-                doable. This argument is exclusive with ``chunksize``.
-            pool (mp.Pool, optional): a multiprocess Pool instance to use
-                to execute the job. If none is provided, a pool will be created
-                within the ``map`` method.
-            generator (torch.Generator, optional): a generator to use for seeding.
-                A base seed will be generated from it, and each worker
-                of the pool will be seeded with the provided seed incremented
-                by a unique integer from ``0`` to ``num_workers``. If no generator
-                is provided, a random integer will be used as seed.
-                To work with unseeded workers, a pool should be created separately
-                and passed to :meth:`map` directly.
-                .. note::
-                  Caution should be taken when providing a low-valued seed as
-                  this can cause autocorrelation between experiments, example:
-                  if 8 workers are asked and the seed is 4, the workers seed will
-                  range from 4 to 11. If the seed is 5, the workers seed will range
-                  from 5 to 12. These two experiments will have an overlap of 7
-                  seeds, which can have unexpected effects on the results.
-
-                .. note::
-                  The goal of seeding the workers is to have independent seed on
-                  each worker, and NOT to have reproducible results across calls
-                  of the `map` method. In other words, two experiments may and
-                  probably will return different results as it is impossible to
-                  know which worker will pick which job. However, we can make sure
-                  that each worker has a different seed and that the pseudo-random
-                  operations on each will be uncorrelated.
-            max_tasks_per_child (int, optional): the maximum number of jobs picked
-                by every child process. Defaults to ``None``, i.e., no restriction
-                on the number of jobs.
-            worker_threads (int, optional): the number of threads for the workers.
-                Defaults to ``1``.
-            index_with_generator (bool, optional): if ``True``, the splitting / chunking
-                of the tensordict will be done during the query, sparing init time.
-                Note that :meth:`~.chunk` and :meth:`~.split` are much more
-                efficient than indexing (which is used within the generator)
-                so a gain of processing time at init time may have a negative
-                impact on the total runtime. Defaults to ``False``.
-            pbar (bool, optional): if ``True``, a progress bar will be displayed.
-                Requires tqdm to be available. Defaults to ``False``.
-            mp_start_method (str, optional): the start method for multiprocessing.
-                If not provided, the default start method will be used.
-                Accepted strings are ``"fork"`` and ``"spawn"``. Keep in mind that
-                ``"cuda"`` tensors cannot be shared between processes with the
-                ``"fork"`` start method. This is without effect if the ``pool``
-                is passed to the ``map`` method.
-
-        Examples:
-            >>> import torch
-            >>> from tensordict import TensorDict
-            >>>
-            >>> def process_data(data):
-            ...     data.set("y", data.get("x") + 1)
-            ...     return data
-            >>> if __name__ == "__main__":
-            ...     data = TensorDict({"x": torch.zeros(1, 1_000_000)}, [1, 1_000_000]).memmap_()
-            ...     data = data.map(process_data, dim=1)
-            ...     print(data["y"][:, :10])
-            ...
-            tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
-
-        .. note:: This method is particularily useful when working with large
-            datasets stored on disk (e.g. memory-mapped tensordicts) where
-            chunks will be zero-copied slices of the original data which can
-            be passed to the processes with virtually zero-cost. This allows
-            to tread very large datasets (eg. over a Tb big) to be processed
-            at little cost.
-
-        """
-        from torch import multiprocessing as mp
-
-        if pool is None:
-            if num_workers is None:
-                num_workers = mp.cpu_count()  # Get the number of CPU cores
-            if generator is None:
-                generator = torch.Generator()
-            seed = (
-                torch.empty((), dtype=torch.int64).random_(generator=generator).item()
-            )
-            if mp_start_method is not None:
-                ctx = mp.get_context(mp_start_method)
-            else:
-                ctx = mp.get_context()
-
-            queue = ctx.Queue(maxsize=num_workers)
-            for i in range(num_workers):
-                queue.put(i)
-            with ctx.Pool(
-                processes=num_workers,
-                initializer=_proc_init,
-                initargs=(seed, queue, worker_threads),
-                maxtasksperchild=max_tasks_per_child,
-            ) as pool:
-                return self.map(
-                    fn,
-                    dim=dim,
-                    chunksize=chunksize,
-                    num_chunks=num_chunks,
-                    pool=pool,
-                    pbar=pbar,
-                    out=out,
-                )
-        num_workers = pool._processes
-        dim_orig = dim
-        if dim < 0:
-            dim = self.ndim + dim
-        if dim < 0 or dim >= self.ndim:
-            raise ValueError(f"Got incompatible dimension {dim_orig}")
-
-        self_split = _split_tensordict(
-            self,
-            chunksize,
-            num_chunks,
-            num_workers,
-            dim,
-            use_generator=index_with_generator,
-        )
-        if not index_with_generator:
-            length = len(self_split)
-        else:
-            length = None
-        call_chunksize = 1
-
-        if out is not None and (out.is_shared() or out.is_memmap()):
-
-            def wrap_fn_with_out(fn, out):
-                @wraps(fn)
-                def newfn(item_and_out):
-                    item, out = item_and_out
-                    result = fn(item)
-                    out.update_(result)
-                    return
-
-                out_split = _split_tensordict(
-                    out,
-                    chunksize,
-                    num_chunks,
-                    num_workers,
-                    dim,
-                    use_generator=index_with_generator,
-                )
-                return _CloudpickleWrapper(newfn), zip(self_split, out_split)
-
-            fn, self_split = wrap_fn_with_out(fn, out)
-            out = None
-
-        imap = pool.imap(fn, self_split, call_chunksize)
-
-        if pbar and importlib.util.find_spec("tqdm", None) is not None:
-            import tqdm
-
-            imap = tqdm.tqdm(imap, total=length)
-
-        imaplist = []
-        start = 0
-        base_index = (slice(None),) * dim
-        for item in imap:
-            if item is not None:
-                if out is not None:
-                    if chunksize == 0:
-                        out[base_index + (start,)].update_(item)
-                        start += 1
-                    else:
-                        end = start + item.shape[dim]
-                        chunk = base_index + (slice(start, end),)
-                        out[chunk].update_(item)
-                        start = end
-                else:
-                    imaplist.append(item)
-        del imap
-
-        # support inplace modif
-        if imaplist:
-            if chunksize == 0:
-                from tensordict._lazy import LazyStackedTensorDict
-
-                # We want to be able to return whichever data structure
-                out = LazyStackedTensorDict.maybe_dense_stack(imaplist, dim)
-            else:
-                out = torch.cat(imaplist, dim)
-        return out
-
-    # point-wise arithmetic ops
-    def __add__(self, other: TensorDictBase | float) -> T:
-        return self.add(other)
-
-    def __iadd__(self, other: TensorDictBase | float) -> T:
-        return self.add_(other)
-
-    def __abs__(self):
-        return self.abs()
-
-    def __truediv__(self, other: TensorDictBase | float) -> T:
-        return self.div(other)
-
-    def __itruediv__(self, other: TensorDictBase | float) -> T:
-        return self.div_(other)
-
-    def __mul__(self, other: TensorDictBase | float) -> T:
-        return self.mul(other)
-
-    def __imul__(self, other: TensorDictBase | float) -> T:
-        return self.mul_(other)
-
-    def __sub__(self, other: TensorDictBase | float) -> T:
-        return self.sub(other)
-
-    def __isub__(self, other: TensorDictBase | float) -> T:
-        return self.sub_(other)
-
-    def __pow__(self, other: TensorDictBase | float) -> T:
-        return self.pow(other)
-
-    def __ipow__(self, other: TensorDictBase | float) -> T:
-        return self.pow_(other)
-
-    def abs(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_abs(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def abs_(self) -> T:
-        torch._foreach_abs_(self._values_list(True, True))
-        return self
-
-    def acos(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_acos(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def acos_(self) -> T:
-        torch._foreach_acos_(self._values_list(True, True))
-        return self
-
-    def exp(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_exp(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def exp_(self) -> T:
-        torch._foreach_exp_(self._values_list(True, True))
-        return self
-
-    def neg(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_neg(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def neg_(self) -> T:
-        torch._foreach_neg_(self._values_list(True, True))
-        return self
-
-    def reciprocal(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_reciprocal(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def reciprocal_(self) -> T:
-        torch._foreach_reciprocal_(self._values_list(True, True))
-        return self
-
-    def sigmoid(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_sigmoid(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sigmoid_(self) -> T:
-        torch._foreach_sigmoid_(self._values_list(True, True))
-        return self
-
-    def sign(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_sign(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sign_(self) -> T:
-        torch._foreach_sign_(self._values_list(True, True))
-        return self
-
-    def sin(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_sin(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sin_(self) -> T:
-        torch._foreach_sin_(self._values_list(True, True))
-        return self
-
-    def sinh(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_sinh(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sinh_(self) -> T:
-        torch._foreach_sinh_(self._values_list(True, True))
-        return self
-
-    def tan(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_tan(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def tan_(self) -> T:
-        torch._foreach_tan_(self._values_list(True, True))
-        return self
-
-    def tanh(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_tanh(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def tanh_(self) -> T:
-        torch._foreach_tanh_(self._values_list(True, True))
-        return self
-
-    def trunc(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_trunc(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def trunc_(self) -> T:
-        torch._foreach_trunc_(self._values_list(True, True))
-        return self
-
-    @implement_for("torch", None, "2.4")
-    def norm(
-        self,
-        out=None,
-        dtype: torch.dtype | None = None,
-    ):
-        keys, vals = self._items_list(True, True, collapse=True)
-        if dtype is not None:
-            raise RuntimeError("dtype must be None for torch <= 2.3")
-        vals = torch._foreach_norm(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            batch_size=[],
-            propagate_lock=True,
-        )
-
-    @implement_for("torch", "2.4")
-    def norm(  # noqa: F811
-        self,
-        out=None,
-        dtype: torch.dtype | None = None,
-    ):
-        keys, vals = self._items_list(True, True, collapse=True)
-        vals = torch._foreach_norm(vals, dtype=dtype)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            batch_size=[],
-            propagate_lock=True,
-        )
-
-    def lgamma(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_lgamma(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def lgamma_(self) -> T:
-        torch._foreach_lgamma_(self._values_list(True, True))
-        return self
-
-    def frac(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_frac(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def frac_(self) -> T:
-        torch._foreach_frac_(self._values_list(True, True))
-        return self
-
-    def expm1(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_expm1(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def expm1_(self) -> T:
-        torch._foreach_expm1_(self._values_list(True, True))
-        return self
-
-    def log(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_log(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def log_(self) -> T:
-        torch._foreach_log_(self._values_list(True, True))
-        return self
-
-    def log10(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_log10(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def log10_(self) -> T:
-        torch._foreach_log10_(self._values_list(True, True))
-        return self
-
-    def log1p(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_log1p(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def log1p_(self) -> T:
-        torch._foreach_log1p_(self._values_list(True, True))
-        return self
-
-    def log2(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_log2(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def log2_(self) -> T:
-        torch._foreach_log2_(self._values_list(True, True))
-        return self
-
-    def ceil(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_ceil(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def ceil_(self) -> T:
-        torch._foreach_ceil_(self._values_list(True, True))
-        return self
-
-    def floor(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_floor(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def floor_(self) -> T:
-        torch._foreach_floor_(self._values_list(True, True))
-        return self
-
-    def round(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_round(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def round_(self) -> T:
-        torch._foreach_round_(self._values_list(True, True))
-        return self
-
-    def erf(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_erf(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def erf_(self) -> T:
-        torch._foreach_erf_(self._values_list(True, True))
-        return self
-
-    def erfc(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_erfc(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def erfc_(self) -> T:
-        torch._foreach_erfc_(self._values_list(True, True))
-        return self
-
-    def asin(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_asin(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def asin_(self) -> T:
-        torch._foreach_asin_(self._values_list(True, True))
-        return self
-
-    def atan(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_atan(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def atan_(self) -> T:
-        torch._foreach_atan_(self._values_list(True, True))
-        return self
-
-    def cos(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_cos(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def cos_(self) -> T:
-        torch._foreach_cos_(self._values_list(True, True))
-        return self
-
-    def cosh(self) -> T:
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_cosh(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def cosh_(self) -> T:
-        torch._foreach_cosh_(self._values_list(True, True))
-        return self
-
-    def add(self, other: TensorDictBase | float, alpha: float | None = None):
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        if alpha is not None:
-            vals = torch._foreach_add(vals, other_val, alpha=alpha)
-        else:
-            vals = torch._foreach_add(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def add_(self, other: TensorDictBase | float, alpha: float | None = None):
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        if alpha is not None:
-            torch._foreach_add_(self._values_list(True, True), other_val, alpha=alpha)
-        else:
-            torch._foreach_add_(self._values_list(True, True), other_val)
-        return self
-
-    def lerp(self, end: TensorDictBase | float, weight: TensorDictBase | float):
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(end)):
-            end_val = end._values_list(True, True)
-        else:
-            end_val = end
-        if _is_tensor_collection(type(weight)):
-            weight_val = weight._values_list(True, True)
-        else:
-            weight_val = weight
-        vals = torch._foreach_lerp(vals, end_val, weight_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def lerp_(self, end: TensorDictBase | float, weight: TensorDictBase | float):
-        if _is_tensor_collection(type(end)):
-            end_val = end._values_list(True, True)
-        else:
-            end_val = end
-        if _is_tensor_collection(type(weight)):
-            weight_val = weight._values_list(True, True)
-        else:
-            weight_val = weight
-        torch._foreach_lerp_(self._values_list(True, True), end_val, weight_val)
-        return self
-
-    def addcdiv(self, other1, other2, value: float | None = 1):
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other1)):
-            other1_val = other1._values_list(True, True)
-        else:
-            other1_val = other1
-        if _is_tensor_collection(type(other2)):
-            other2_val = other2._values_list(True, True)
-        else:
-            other2_val = other2
-        vals = torch._foreach_addcdiv(vals, other1_val, other2_val, value=value)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def addcdiv_(self, other1, other2, value: float | None = 1):
-        if _is_tensor_collection(type(other1)):
-            other1_val = other1._values_list(True, True)
-        else:
-            other1_val = other1
-        if _is_tensor_collection(type(other2)):
-            other2_val = other2._values_list(True, True)
-        else:
-            other2_val = other2
-        torch._foreach_addcdiv_(
-            self._values_list(True, True), other1_val, other2_val, value=value
-        )
-        return self
-
-    def addcmul(self, other1, other2, value: float | None = 1):
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other1)):
-            other1_val = other1._values_list(True, True)
-        else:
-            other1_val = other1
-        if _is_tensor_collection(type(other2)):
-            other2_val = other2._values_list(True, True)
-        else:
-            other2_val = other2
-        vals = torch._foreach_addcmul(vals, other1_val, other2_val, value=value)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def addcmul_(self, other1, other2, value: float | None = 1):
-        if _is_tensor_collection(type(other1)):
-            other1_val = other1._values_list(True, True)
-        else:
-            other1_val = other1
-        if _is_tensor_collection(type(other2)):
-            other2_val = other2._values_list(True, True)
-        else:
-            other2_val = other2
-        torch._foreach_addcmul_(
-            self._values_list(True, True), other1_val, other2_val, value=value
-        )
-        return self
-
-    def sub(self, other: TensorDictBase | float, alpha: float | None = None):
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        if alpha is not None:
-            vals = torch._foreach_sub(vals, other_val, alpha=alpha)
-        else:
-            vals = torch._foreach_sub(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sub_(self, other: TensorDictBase | float, alpha: float | None = None):
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        if alpha is not None:
-            torch._foreach_sub_(self._values_list(True, True), other_val, alpha=alpha)
-        else:
-            torch._foreach_sub_(self._values_list(True, True), other_val)
-        return self
-
-    def mul_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_mul_(self._values_list(True, True), other_val)
-        return self
-
-    def mul(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_mul(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def maximum_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_maximum_(self._values_list(True, True), other_val)
-        return self
-
-    def maximum(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_maximum(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def minimum_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_minimum_(self._values_list(True, True), other_val)
-        return self
-
-    def minimum(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_minimum(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def clamp_max_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_clamp_max_(self._values_list(True, True), other_val)
-        return self
-
-    def clamp_max(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_clamp_max(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def clamp_min_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_clamp_min_(self._values_list(True, True), other_val)
-        return self
-
-    def clamp_min(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_clamp_min(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def pow_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_pow_(self._values_list(True, True), other_val)
-        return self
-
-    def pow(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_pow(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def div_(self, other: TensorDictBase | float) -> T:
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        torch._foreach_div_(self._values_list(True, True), other_val)
-        return self
-
-    def div(self, other: TensorDictBase | float) -> T:
-        keys, vals = self._items_list(True, True)
-        if _is_tensor_collection(type(other)):
-            other_val = other._values_list(True, True)
-        else:
-            other_val = other
-        vals = torch._foreach_div(vals, other_val)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    def sqrt_(self):
-        torch._foreach_sqrt_(self._values_list(True, True))
-        return self
-
-    def sqrt(self):
-        keys, vals = self._items_list(True, True)
-        vals = torch._foreach_sqrt(vals)
-        items = dict(zip(keys, vals))
-        return self._fast_apply(
-            lambda name, val: items[name],
-            named=True,
-            nested_keys=True,
-            is_leaf=_NESTED_TENSORS_AS_LISTS,
-            propagate_lock=True,
-        )
-
-    # Functorch compatibility
-    @abc.abstractmethod
-    @cache  # noqa: B019
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        ...
-
-    @abc.abstractmethod
-    @cache  # noqa: B019
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        ...
-
-    # Validation and checks
-    def _convert_to_tensor(self, array: np.ndarray) -> Tensor:
-        if isinstance(array, (float, int, np.ndarray, bool)):
-            pass
-        elif isinstance(array, np.bool_):
-            array = array.item()
-        elif isinstance(array, list):
-            array = np.asarray(array)
-        elif hasattr(array, "numpy"):
-            # tf.Tensor with no shape can't be converted otherwise
-            array = array.numpy()
-        try:
-            return torch.as_tensor(array, device=self.device)
-        except Exception:
-            from tensordict.tensorclass import NonTensorData
-
-            return NonTensorData(
-                array,
-                batch_size=self.batch_size,
-                device=self.device,
-                names=self.names if self._has_names() else None,
-            )
-
-    @abc.abstractmethod
-    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> T:
-        ...
-
-    def _check_batch_size(self) -> None:
-        batch_dims = self.batch_dims
-        for value in self.values():
-            if _is_tensor_collection(type(value)):
-                value._check_batch_size()
-            if _shape(value)[:batch_dims] != self.batch_size:
-                raise RuntimeError(
-                    f"batch_size are incongruent, got value with shape {_shape(value)}, "
-                    f"-- expected {self.batch_size}"
-                )
-
-    @abc.abstractmethod
-    def _check_is_shared(self) -> bool:
-        ...
-
-    def _check_new_batch_size(self, new_size: torch.Size) -> None:
-        batch_dims = len(new_size)
-        for key, tensor in self.items():
-            if _shape(tensor)[:batch_dims] != new_size:
-                raise RuntimeError(
-                    f"the tensor {key} has shape {_shape(tensor)} which "
-                    f"is incompatible with the batch-size {new_size}."
-                )
-
-    @abc.abstractmethod
-    def _check_device(self) -> None:
-        ...
-
-    def _validate_key(self, key: NestedKey) -> NestedKey:
-        key = _unravel_key_to_tuple(key)
-        if not key:
-            raise KeyError(_GENERIC_NESTED_ERR.format(key))
-        return key
-
-    def _validate_value(
-        self,
-        value: CompatibleType | dict[str, CompatibleType],
-        *,
-        check_shape: bool = True,
-    ) -> CompatibleType | dict[str, CompatibleType]:
-        cls = type(value)
-        is_tc = None
-        if issubclass(cls, dict):
-            value = self._convert_to_tensordict(value)
-            is_tc = True
-        elif not issubclass(cls, _ACCEPTED_CLASSES):
-            try:
-                value = self._convert_to_tensor(value)
-            except ValueError as err:
-                raise ValueError(
-                    f"TensorDict conversion only supports tensorclasses, tensordicts,"
-                    f" numeric scalars and tensors. Got {type(value)}"
-                ) from err
-        batch_size = self.batch_size
-        check_shape = check_shape and self.batch_size
-        if (
-            check_shape
-            and batch_size
-            and _shape(value)[: self.batch_dims] != batch_size
-        ):
-            # if TensorDict, let's try to map it to the desired shape
-            if is_tc is None:
-                is_tc = _is_tensor_collection(cls)
-            if is_tc:
-                # we must clone the value before not to corrupt the data passed to set()
-                value = value.clone(recurse=False)
-                value.batch_size = self.batch_size
-            else:
-                raise RuntimeError(
-                    f"batch dimension mismatch, got self.batch_size"
-                    f"={self.batch_size} and value.shape={_shape(value)}."
-                )
-        device = self.device
-        if device is not None and value.device != device:
-            value = value.to(device, non_blocking=True)
-        if check_shape:
-            if is_tc is None:
-                is_tc = _is_tensor_collection(cls)
-            if not is_tc:
-                return value
-            has_names = self._has_names()
-            # we do our best to match the dim names of the value and the
-            # container.
-            if has_names and value.names[: self.batch_dims] != self.names:
-                # we clone not to corrupt the value
-                value = value.clone(False).refine_names(*self.names)
-            elif not has_names and value._has_names():
-                self.names = value.names[: self.batch_dims]
-        return value
-
-    # Context manager functionality
-    @property
-    def _last_op_queue(self):
-        # this is used to keep track of the last operation when using
-        # the tensordict as a context manager.
-        last_op_queue = self.__dict__.get("__last_op_queue", None)
-        if last_op_queue is None:
-            last_op_queue = collections.deque()
-            self.__dict__["__last_op_queue"] = last_op_queue
-        return last_op_queue
-
-    def __enter__(self):
-        self._last_op_queue.append(self._last_op)
-        return self
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        # During exit, updates mustn't be made in-place as the source and dest
-        # storage location can be identical, resulting in a RuntimeError
-        if exc_type is not None and issubclass(exc_type, Exception):
-            return False
-        _last_op = self._last_op_queue.pop()
-        if _last_op is not None:
-            last_op, (args, kwargs, out) = _last_op
-            # TODO: transpose, flatten etc. as decorator should lock the content to make sure that no key is
-            #  added or deleted
-            if last_op == self.__class__.lock_.__name__:
-                return self.unlock_()
-            elif last_op == self.__class__.unlock_.__name__:
-                return self.lock_()
-            elif last_op == self.__class__.transpose.__name__:
-                dim0, dim1 = args
-                if not out.is_locked:
-                    return out.update(self.transpose(dim0, dim1), inplace=False)
-                else:
-                    return out.update_(self.transpose(dim0, dim1))
-            elif last_op == self.__class__.flatten.__name__:
-                if len(args) == 2:
-                    dim0, dim1 = args
-                elif len(args) == 1:
-                    dim0 = args[0]
-                    dim1 = kwargs.get("end_dim", -1)
-                else:
-                    dim0 = kwargs.get("start_dim", 0)
-                    dim1 = kwargs.get("end_dim", -1)
-                if dim1 < 0:
-                    dim1 = out.ndim + dim1
-                if dim0 < 0:
-                    dim0 = out.ndim + dim0
-
-                if not out.is_locked:
-                    return out.update(
-                        self.unflatten(dim0, out.shape[dim0 : dim1 + 1]), inplace=False
-                    )
-                else:
-                    return out.update_(self.unflatten(dim0, out.shape[dim0 : dim1 + 1]))
-
-            elif last_op == self.__class__.unflatten.__name__:
-                if args:
-                    dim0 = args[0]
-                    if len(args) > 1:
-                        unflattened_size = args[1]
-                    else:
-                        unflattened_size = kwargs.get("unflattened_size")
-                else:
-                    dim0 = kwargs.get("dim")
-                    unflattened_size = kwargs.get("unflattened_size")
-                if dim0 < 0:
-                    dim0 = out.ndim + dim0
-                dim1 = dim0 + len(unflattened_size) - 1
-                if not out.is_locked:
-                    return out.update(self.flatten(dim0, dim1), inplace=False)
-                else:
-                    return out.update_(self.flatten(dim0, dim1))
-
-            elif last_op == self.__class__.permute.__name__:
-                dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
-                dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
-                # inverse map
-                inv_dims_list = np.argsort(dims_list)
-                if not out.is_locked:
-                    return out.update(self.permute(inv_dims_list), inplace=False)
-                else:
-                    return out.update_(self.permute(inv_dims_list))
-            elif last_op == self.__class__.view.__name__:
-                if not out.is_locked:
-                    return out.update(self.view(out.shape), inplace=False)
-                else:
-                    return out.update_(self.view(out.shape))
-            elif last_op == self.__class__.unsqueeze.__name__:
-                if args:
-                    (dim,) = args
-                elif kwargs:
-                    dim = kwargs["dim"]
-                else:
-                    raise RuntimeError(
-                        "Cannot use td.unsqueeze() as a decorator if the dimension is implicit."
-                    )
-                if not out.is_locked:
-                    return out.update(self.squeeze(dim), inplace=False)
-                else:
-                    return out.update_(self.squeeze(dim))
-            elif last_op == self.__class__.squeeze.__name__:
-                if args:
-                    (dim,) = args
-                elif kwargs:
-                    dim = kwargs["dim"]
-                else:
-                    raise RuntimeError(
-                        "Cannot use td.squeeze() as a decorator if the dimension is implicit."
-                    )
-                if not out.is_locked:
-                    return out.update(self.unsqueeze(dim), inplace=False)
-                else:
-                    return out.update_(self.unsqueeze(dim))
-            elif last_op == self.__class__.to_module.__name__:
-                if is_tensor_collection(out):
-                    with out.unlock_():
-                        return self.to_module(*args, **kwargs, swap_dest=out)
-                else:
-                    raise RuntimeError(
-                        "to_module cannot be used as a decorator when return_swap=False."
-                    )
-            else:
-                raise NotImplementedError(f"Unrecognised function {last_op}.")
-        return self
-
-    # Clone, select, exclude, empty
-    def select(self, *keys: NestedKey, inplace: bool = False, strict: bool = True) -> T:
-        """Selects the keys of the tensordict and returns a new tensordict with only the selected keys.
-
-        The values are not copied: in-place modifications a tensor of either
-        of the original or new tensordict will result in a change in both
-        tensordicts.
-
-        Args:
-            *keys (str): keys to select
-            inplace (bool): if True, the tensordict is pruned in place.
-                Default is ``False``.
-            strict (bool, optional): whether selecting a key that is not present
-                will return an error or not. Default: :obj:`True`.
-
-        Returns:
-            A new tensordict (or the same if ``inplace=True``) with the selected keys only.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> td = TensorDict({"a": 0, "b": {"c": 1, "d": 2}}, [])
-            >>> td.select("a", ("b", "c"))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> td.select("a", "b")
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                            d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> td.select("this key does not exist", strict=False)
-            TensorDict(
-                fields={
-                },
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-        """
-        keys = unravel_key_list(keys)
-        result = self._select(*keys, inplace=inplace, strict=strict)
-        if not inplace and (result._is_memmap or result._is_shared):
-            result.lock_()
-        return result
-
-    @abc.abstractmethod
-    def _select(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        strict: bool = True,
-        set_shared: bool = True,
-    ) -> T:
-        ...
-
-    def exclude(self, *keys: NestedKey, inplace: bool = False) -> T:
-        """Excludes the keys of the tensordict and returns a new tensordict without these entries.
-
-        The values are not copied: in-place modifications a tensor of either
-        of the original or new tensordict will result in a change in both
-        tensordicts.
-
-        Args:
-            *keys (str): keys to exclude.
-            inplace (bool): if True, the tensordict is pruned in place.
-                Default is ``False``.
-
-        Returns:
-            A new tensordict (or the same if ``inplace=True``) without the excluded entries.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> td = TensorDict({"a": 0, "b": {"c": 1, "d": 2}}, [])
-            >>> td.exclude("a", ("b", "c"))
-            TensorDict(
-                fields={
-                    b: TensorDict(
-                        fields={
-                            d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> td.exclude("a", "b")
-            TensorDict(
-                fields={
-                },
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        keys = unravel_key_list(keys)
-        result = self._exclude(*keys, inplace=inplace)
-        if not inplace and (result._is_memmap or result._is_shared):
-            result.lock_()
-        return result
-
-    @abc.abstractmethod
-    def _exclude(
-        self,
-        *keys: NestedKey,
-        inplace: bool = False,
-        set_shared: bool = True,
-    ) -> T:
-        ...
-
-    def _maybe_set_shared_attributes(self, result, lock=False):
-        # We must use _is_shared to avoid having issues with CUDA tensordicts
-        if self._is_shared:
-            result._is_shared = True
-            if lock:
-                result.lock_()
-        elif self._is_memmap:
-            result._is_memmap = True
-            if lock:
-                result.lock_()
-
-    def to_tensordict(self) -> T:
-        """Returns a regular TensorDict instance from the TensorDictBase.
-
-        Returns:
-            a new TensorDict object containing the same values.
-
-        """
-        from tensordict import TensorDict
-
-        return TensorDict(
-            {
-                key: value.clone()
-                if not _is_tensor_collection(value.__class__)
-                else value
-                if is_non_tensor(value)
-                else value.to_tensordict()
-                for key, value in self.items(is_leaf=_is_leaf_nontensor)
-            },
-            device=self.device,
-            batch_size=self.batch_size,
-            names=self.names if self._has_names() else None,
-        )
-
-    def clone(self, recurse: bool = True, **kwargs) -> T:
-        """Clones a TensorDictBase subclass instance onto a new TensorDictBase subclass of the same type.
-
-        To create a TensorDict instance from any other TensorDictBase subtype, call the :meth:`~.to_tensordict` method
-        instead.
-
-        Args:
-            recurse (bool, optional): if ``True``, each tensor contained in the
-                TensorDict will be copied too. Otherwise only the TensorDict
-                tree structure will be copied. Defaults to ``True``.
-
-        .. note:: Unlike many other ops (pointwise arithmetic, shape operations, ...) ``clone`` does not inherit the
-            original lock attribute. This design choice is made such that a clone can be created to be modified,
-            which is the most frequent usage.
-
-        """
-        result = self._clone(recurse=recurse, **kwargs)
-        if not recurse and (result._is_shared or result._is_memmap):
-            result.lock_()
-        return result
-
-    @abc.abstractmethod
-    def _clone(self, recurse: bool = False):
-        ...
-
-    def copy(self):
-        """Return a shallow copy of the tensordict (ie, copies the structure but not the data).
-
-        Equivalent to `TensorDictBase.clone(recurse=False)`
-        """
-        return self.clone(recurse=False)
-
-    def to_padded_tensor(self, padding=0.0, mask_key: NestedKey | None = None):
-        """Converts all nested tensors to a padded version and adapts the batch-size accordingly.
-
-        Args:
-            padding (float): the padding value for the tensors in the tensordict.
-                Defaults to ``0.0``.
-            mask_key (NestedKey, optional): if provided, the key where a
-                mask for valid values will be written.
-                Will result in an error if the heterogeneous dimension
-                isn't part of the tensordict batch-size.
-                Defaults to ``None``
-
-        """
-        batch_size = self.batch_size
-        if any(shape == -1 for shape in batch_size):
-            new_batch_size = []
-        else:
-            new_batch_size = None
-            if mask_key is not None:
-                raise RuntimeError(
-                    "mask_key should only be provided if the "
-                    "heterogenous dimension is part of the batch-size."
-                )
-        padded_names = []
-
-        def to_padded(name, x):
-            if x.is_nested:
-                padded_names.append(name)
-                return torch.nested.to_padded_tensor(x, padding=padding)
-            return x
-
-        result = self._apply_nest(
-            to_padded,
-            batch_size=new_batch_size,
-            named=True,
-            nested_keys=True,
-        )
-        if new_batch_size is not None:
-            result = result.auto_batch_size_(batch_dims=self.batch_dims)
-
-            if mask_key:
-                # take the first of the padded keys
-                padded_key = padded_names[0]
-                # write the mask
-                val = self.get(padded_key)
-                val = torch.nested.to_padded_tensor(
-                    torch.ones_like(val, dtype=torch.bool), padding=False
-                )
-                if val.ndim > result.ndim:
-                    val = val.flatten(result.ndim, -1)[..., -1].clone()
-                result.set(mask_key, val)
-        return result
-
-    def as_tensor(self):
-        def as_tensor(tensor):
-            try:
-                return tensor.as_tensor()
-            except AttributeError:
-                return tensor
-
-        return self._fast_apply(as_tensor, propagate_lock=True)
-
-    def to_dict(self) -> dict[str, Any]:
-        """Returns a dictionary with key-value pairs matching those of the tensordict."""
-        return {
-            key: value.to_dict() if _is_tensor_collection(type(value)) else value
-            for key, value in self.items()
-        }
-
-    def numpy(self):
-        """Converts a tensordict to a (possibly nested) dictionary of numpy arrays.
-
-        Non-tensor data is exposed as such.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>> data = TensorDict({"a": {"b": torch.zeros(()), "c": "a string!"}})
-            >>> print(data)
-            TensorDict(
-                fields={
-                    a: TensorDict(
-                        fields={
-                            b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                            c: NonTensorData(data=a string!, batch_size=torch.Size([]), device=None)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> print(data.numpy())
-            {'a': {'b': array(0., dtype=float32), 'c': 'a string!'}}
-
-        """
-        as_dict = self.to_dict()
-
-        def to_numpy(x):
-            if isinstance(x, torch.Tensor):
-                if x.is_nested:
-                    return tuple(_x.numpy() for _x in x)
-                return x.numpy()
-            if hasattr(x, "numpy"):
-                return x.numpy()
-            return x
-
-        return torch.utils._pytree.tree_map(to_numpy, as_dict)
-
-    def to_namedtuple(self):
-        """Converts a tensordict to a namedtuple.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>> data = TensorDict({
-            ...     "a_tensor": torch.zeros((3)),
-            ...     "nested": {"a_tensor": torch.zeros((3)), "a_string": "zero!"}}, [3])
-            >>> data.to_namedtuple()
-            GenericDict(a_tensor=tensor([0., 0., 0.]), nested=GenericDict(a_tensor=tensor([0., 0., 0.]), a_string='zero!'))
-
-        """
-
-        def dict_to_namedtuple(dictionary):
-            for key, value in dictionary.items():
-                if isinstance(value, dict):
-                    dictionary[key] = dict_to_namedtuple(value)
-            return collections.namedtuple("GenericDict", dictionary.keys())(
-                **dictionary
-            )
-
-        return dict_to_namedtuple(self.to_dict())
-
-    @classmethod
-    def from_namedtuple(cls, named_tuple, *, auto_batch_size: bool = False):
-        """Converts a namedtuple to a TensorDict recursively.
-
-        Keyword Args:
-            auto_batch_size (bool, optional): if ``True``, the batch size will be computed automatically.
-                Defaults to ``False``.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> import torch
-            >>> data = TensorDict({
-            ...     "a_tensor": torch.zeros((3)),
-            ...     "nested": {"a_tensor": torch.zeros((3)), "a_string": "zero!"}}, [3])
-            >>> nt = data.to_namedtuple()
-            >>> print(nt)
-            GenericDict(a_tensor=tensor([0., 0., 0.]), nested=GenericDict(a_tensor=tensor([0., 0., 0.]), a_string='zero!'))
-            >>> TensorDict.from_namedtuple(nt, auto_batch_size=True)
-            TensorDict(
-                fields={
-                    a_tensor: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                    nested: TensorDict(
-                        fields={
-                            a_string: NonTensorData(data=zero!, batch_size=torch.Size([3]), device=None),
-                            a_tensor: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([3]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([3]),
-                device=None,
-                is_shared=False)
-
-        """
-        from tensordict import TensorDict
-
-        def is_namedtuple(obj):
-            """Check if obj is a namedtuple."""
-            return isinstance(obj, tuple) and hasattr(obj, "_fields")
-
-        def namedtuple_to_dict(namedtuple_obj):
-            if is_namedtuple(namedtuple_obj):
-                namedtuple_obj = namedtuple_obj._asdict()
-            for key, value in namedtuple_obj.items():
-                if is_namedtuple(value):
-                    namedtuple_obj[key] = namedtuple_to_dict(value)
-            return dict(namedtuple_obj)
-
-        result = TensorDict(namedtuple_to_dict(named_tuple))
-        if auto_batch_size:
-            result.auto_batch_size_()
-        return result
-
-    def to_h5(
-        self,
-        filename,
-        **kwargs,
-    ):
-        """Converts a tensordict to a PersistentTensorDict with the h5 backend.
-
-        Args:
-            filename (str or path): path to the h5 file.
-            device (torch.device or compatible, optional): the device where to
-                expect the tensor once they are returned. Defaults to ``None``
-                (on cpu by default).
-            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-        Returns:
-            A :class:`~.tensordict.PersitentTensorDict` instance linked to the newly created file.
-
-        Examples:
-            >>> import tempfile
-            >>> import timeit
-            >>>
-            >>> from tensordict import TensorDict, MemoryMappedTensor
-            >>> td = TensorDict({
-            ...     "a": MemoryMappedTensor.from_tensor(torch.zeros(()).expand(1_000_000)),
-            ...     "b": {"c": MemoryMappedTensor.from_tensor(torch.zeros(()).expand(1_000_000, 3))},
-            ... }, [1_000_000])
-            >>>
-            >>> file = tempfile.NamedTemporaryFile()
-            >>> td_h5 = td.to_h5(file.name, compression="gzip", compression_opts=9)
-            >>> print(td_h5)
-            PersistentTensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([1000000]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: PersistentTensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([1000000, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([1000000]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([1000000]),
-                device=None,
-                is_shared=False)
-
-
-        """
-        from tensordict.persistent import PersistentTensorDict
-
-        out = PersistentTensorDict.from_dict(
-            self,
-            filename=filename,
-            **kwargs,
-        )
-        if self._has_names():
-            out.names = self.names
-        return out
-
-    def empty(
-        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
-    ) -> T:  # noqa: D417
-        """Returns a new, empty tensordict with the same device and batch size.
-
-        Args:
-            recurse (bool, optional): if ``True``, the entire structure of the
-                ``TensorDict`` will be reproduced without content.
-                Otherwise, only the root will be duplicated.
-                Defaults to ``False``.
-
-        Keyword Args:
-            batch_size (torch.Size, optional): a new batch-size for the tensordict.
-            device (torch.device, optional): a new device.
-            names (list of str, optional): dimension names.
-
-        """
-        if not recurse:
-            result = self._select(set_shared=False)
-        else:
-            # simply exclude the leaves
-            result = self._exclude(*self.keys(True, True), set_shared=False)
-        if batch_size is not None:
-            result.batch_size = batch_size
-        if device is not NO_DEFAULT:
-            if device is None:
-                result.clear_device_()
-            else:
-                result = result.to(device)
-        if names is not None:
-            result.names = names
-        return result
-
-    # Filling
-    def zero_(self) -> T:
-        """Zeros all tensors in the tensordict in-place."""
-
-        def fn(item):
-            item.zero_()
-
-        self._fast_apply(fn=fn, call_on_nested=True, propagate_lock=True)
-        return self
-
-    def fill_(self, key: NestedKey, value: float | bool) -> T:
-        """Fills a tensor pointed by the key with a given scalar value.
-
-        Args:
-            key (str or nested key): entry to be filled.
-            value (Number or bool): value to use for the filling.
-
-        Returns:
-            self
-
-        """
-        key = _unravel_key_to_tuple(key)
-        data = self._get_tuple(key, NO_DEFAULT)
-        if _is_tensor_collection(type(data)):
-            data._fast_apply(lambda x: x.fill_(value), inplace=True)
-        else:
-            data = data.fill_(value)
-            self._set_tuple(key, data, inplace=True, validated=True, non_blocking=False)
-        return self
-
-    # Masking
-    @abc.abstractmethod
-    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
-        """Fills the values corresponding to the mask with the desired value.
-
-        Args:
-            mask (boolean torch.Tensor): mask of values to be filled. Shape
-                must match the tensordict batch-size.
-            value: value to used to fill the tensors.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...     batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> td.masked_fill_(mask, 1.0)
-            >>> td.get("a")
-            tensor([[1., 1., 1., 1.],
-                    [0., 0., 0., 0.],
-                    [0., 0., 0., 0.]])
-        """
-        ...
-
-    @abc.abstractmethod
-    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
-        """Out-of-place version of masked_fill.
-
-        Args:
-            mask (boolean torch.Tensor): mask of values to be filled. Shape
-                must match the tensordict batch-size.
-            value: value to used to fill the tensors.
-
-        Returns:
-            self
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...     batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> td1 = td.masked_fill(mask, 1.0)
-            >>> td1.get("a")
-            tensor([[1., 1., 1., 1.],
-                    [0., 0., 0., 0.],
-                    [0., 0., 0., 0.]])
-        """
-        ...
-
-    def where(self, condition, other, *, out=None, pad=None):  # noqa: D417
-        """Return a ``TensorDict`` of elements selected from either self or other, depending on condition.
-
-        Args:
-            condition (BoolTensor): When ``True`` (nonzero), yields ``self``,
-                otherwise yields ``other``.
-            other (TensorDictBase or Scalar): value (if ``other`` is a scalar)
-                or values selected at indices where condition is ``False``.
-
-        Keyword Args:
-            out (TensorDictBase, optional): the output ``TensorDictBase`` instance.
-            pad (scalar, optional): if provided, missing keys from the source
-                or destination tensordict will be written as `torch.where(mask, self, pad)`
-                or `torch.where(mask, pad, other)`. Defaults to ``None``, ie
-                missing keys are not tolerated.
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def masked_select(self, mask: Tensor) -> T:
-        """Masks all tensors of the TensorDict and return a new TensorDict instance with similar keys pointing to masked values.
-
-        Args:
-            mask (torch.Tensor): boolean mask to be used for the tensors.
-                Shape must match the TensorDict ``batch_size``.
-
-        Examples:
-            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
-            ...    batch_size=[3])
-            >>> mask = torch.tensor([True, False, False])
-            >>> td_mask = td.masked_select(mask)
-            >>> td_mask.get("a")
-            tensor([[0., 0., 0., 0.]])
-
-        """
-        ...
-
-    @abc.abstractmethod
-    def _change_batch_size(self, new_size: torch.Size) -> None:
-        ...
-
-    @abc.abstractmethod
-    def is_contiguous(self) -> bool:
-        """Returns a boolean indicating if all the tensors are contiguous."""
-        ...
-
-    @abc.abstractmethod
-    def contiguous(self) -> T:
-        """Returns a new tensordict of the same type with contiguous values (or self if values are already contiguous)."""
-        ...
-
-    @cache  # noqa: B019
-    def flatten_keys(
-        self,
-        separator: str = ".",
-        inplace: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> T:
-        """Converts a nested tensordict into a flat one, recursively.
-
-        The TensorDict type will be lost and the result will be a simple TensorDict instance.
-
-        Args:
-            separator (str, optional): the separator between the nested items.
-            inplace (bool, optional): if ``True``, the resulting tensordict will
-                have the same identity as the one where the call has been made.
-                Defaults to ``False``.
-            is_leaf (callable, optional): a callable over a class type returning
-                a bool indicating if this class has to be considered as a leaf.
-
-        Examples:
-            >>> data = TensorDict({"a": 1, ("b", "c"): 2, ("e", "f", "g"): 3}, batch_size=[])
-            >>> data.flatten_keys(separator=" - ")
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b - c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    e - f - g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This method and :meth:`~.unflatten_keys` are particularily useful when
-        handling state-dicts, as they make it possible to seamlessly convert
-        flat dictionaries into data structures that mimic the structure of the
-        model.
-
-        Examples:
-            >>> model = torch.nn.Sequential(torch.nn.Linear(3 ,4))
-            >>> ddp_model = torch.ao.quantization.QuantWrapper(model)
-            >>> state_dict = TensorDict(ddp_model.state_dict(), batch_size=[]).unflatten_keys(".")
-            >>> print(state_dict)
-            TensorDict(
-                fields={
-                    module: TensorDict(
-                        fields={
-                            0: TensorDict(
-                                fields={
-                                    bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                                    weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                                batch_size=torch.Size([]),
-                                device=None,
-                                is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> model_state_dict = state_dict.get("module")
-            >>> print(model_state_dict)
-            TensorDict(
-                fields={
-                    0: TensorDict(
-                        fields={
-                            bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                            weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> model.load_state_dict(dict(model_state_dict.flatten_keys(".")))
-        """
-        if inplace:
-            return self._flatten_keys_inplace(separator=separator, is_leaf=is_leaf)
-        return self._flatten_keys_outplace(separator=separator, is_leaf=is_leaf)
-
-    def _flatten_keys_outplace(self, separator, is_leaf):
-        if is_leaf is None:
-            is_leaf = _is_leaf_nontensor
-        all_leaves_all_vals = zip(
-            *self.items(include_nested=True, leaves_only=True, is_leaf=is_leaf)
-        )
-        try:
-            all_leaves, all_vals = all_leaves_all_vals
-        except ValueError:
-            return self.empty()
-        all_leaves_flat = [
-            key if isinstance(key, str) else separator.join(key) for key in all_leaves
-        ]
-
-        if len(set(all_leaves_flat)) < len(all_leaves_flat):
-            # find duplicates
-            seen = set()
-            conflicts = []
-            for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
-                if leaf_flat in seen:
-                    conflicts.append(leaf)
-                else:
-                    seen.add(leaf_flat)
-            raise KeyError(
-                f"Flattening keys in tensordict causes keys {conflicts} to collide."
-            )
-        result = self.empty()
-        _set_dict = getattr(result, "_set_dict", None)
-        if _set_dict is not None:
-            _set_dict(
-                dict(zip(all_leaves_flat, all_vals)),
-                validated=True,
-            )
-        else:
-            for val, leaf_flat in zip(all_vals, all_leaves_flat):
-                result._set_str(
-                    leaf_flat,
-                    val,
-                    validated=True,
-                    inplace=False,
-                    non_blocking=False,
-                )
-        # Uncomment if you want key operations to propagate the shared status
-        # self._maybe_set_shared_attributes(result)
-        # if result._is_shared or result._is_memmap:
-        #     result.lock_()
-        return result
-
-    def _flatten_keys_inplace(self, separator, is_leaf):
-        if is_leaf is None:
-            is_leaf = _is_leaf_nontensor
-        all_leaves = [
-            _unravel_key_to_tuple(key)
-            for key in self.keys(include_nested=True, leaves_only=True, is_leaf=is_leaf)
-        ]
-        all_leaves_flat = [separator.join(key) for key in all_leaves]
-        if len(set(all_leaves_flat)) < len(set(all_leaves)):
-            # find duplicates
-            seen = set()
-            conflicts = []
-            for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
-                if leaf_flat in seen:
-                    conflicts.append(leaf)
-                else:
-                    seen.add(leaf_flat)
-            raise KeyError(
-                f"Flattening keys in tensordict causes keys {conflicts} to collide."
-            )
-        # we will need to remove the empty tensordicts later on
-        root_keys = set(self.keys())
-        for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
-            self.rename_key_(leaf, leaf_flat)
-            if isinstance(leaf, str):
-                root_keys.discard(leaf)
-        self.exclude(*root_keys, inplace=True)
-        return self
-
-    @cache  # noqa: B019
-    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
-        """Converts a flat tensordict into a nested one, recursively.
-
-        The TensorDict type will be lost and the result will be a simple TensorDict instance.
-        The metadata of the nested tensordicts will be inferred from the root:
-        all instances across the data tree will share the same batch-size,
-        dimension names and device.
-
-        Args:
-            separator (str, optional): the separator between the nested items.
-            inplace (bool, optional): if ``True``, the resulting tensordict will
-                have the same identity as the one where the call has been made.
-                Defaults to ``False``.
-
-        Examples:
-            >>> data = TensorDict({"a": 1, "b - c": 2, "e - f - g": 3}, batch_size=[])
-            >>> data.unflatten_keys(separator=" - ")
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
-                    b: TensorDict(
-                        fields={
-                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False),
-                    e: TensorDict(
-                        fields={
-                            f: TensorDict(
-                                fields={
-                                    g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
-                                batch_size=torch.Size([]),
-                                device=None,
-                                is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This method and :meth:`~.unflatten_keys` are particularily useful when
-        handling state-dicts, as they make it possible to seamlessly convert
-        flat dictionaries into data structures that mimic the structure of the
-        model.
-
-        Examples:
-            >>> model = torch.nn.Sequential(torch.nn.Linear(3 ,4))
-            >>> ddp_model = torch.ao.quantization.QuantWrapper(model)
-            >>> state_dict = TensorDict(ddp_model.state_dict(), batch_size=[]).unflatten_keys(".")
-            >>> print(state_dict)
-            TensorDict(
-                fields={
-                    module: TensorDict(
-                        fields={
-                            0: TensorDict(
-                                fields={
-                                    bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                                    weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                                batch_size=torch.Size([]),
-                                device=None,
-                                is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> model_state_dict = state_dict.get("module")
-            >>> print(model_state_dict)
-            TensorDict(
-                fields={
-                    0: TensorDict(
-                        fields={
-                            bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                            weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                        batch_size=torch.Size([]),
-                        device=None,
-                        is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> model.load_state_dict(dict(model_state_dict.flatten_keys(".")))
-
-        """
-        if not inplace:
-            result = self._clone(recurse=False).unflatten_keys(
-                separator=separator, inplace=True
-            )
-            if result._is_shared or result._is_memmap:
-                result.lock_()
-            return result
-        else:
-            for key in list(self.keys()):
-                if separator in key:
-                    new_key = tuple(key.split(separator))
-                    try:
-                        self.rename_key_(key, new_key, safe=True)
-                    except KeyError:
-                        raise KeyError(
-                            f"Unflattening key(s) in tensordict will override an existing for unflattened key {new_key}."
-                        )
-            return self
-
-    @abc.abstractmethod
-    def _index_tensordict(
-        self,
-        index: IndexType,
-        new_batch_size: torch.Size | None = None,
-        names: List[str] | None = None,
-    ) -> T:
-        ...
-
-    # Locking functionality
-    @property
-    def is_locked(self) -> bool:
-        return self._is_locked
-
-    @is_locked.setter
-    def is_locked(self, value: bool) -> None:
-        if value:
-            self.lock_()
-        else:
-            self.unlock_()
-
-    def _propagate_lock(self, lock_parents_weakrefs=None):
-        """Registers the parent tensordict that handles the lock."""
-        self._is_locked = True
-        is_root = lock_parents_weakrefs is None
-        if is_root:
-            lock_parents_weakrefs = []
-        self._lock_parents_weakrefs = (
-            self._lock_parents_weakrefs + lock_parents_weakrefs
-        )
-        lock_parents_weakrefs = copy(lock_parents_weakrefs) + [weakref.ref(self)]
-        for value in self.values():
-            if _is_tensor_collection(type(value)):
-                value._propagate_lock(lock_parents_weakrefs)
-
-    @property
-    def _lock_parents_weakrefs(self):
-        _lock_parents_weakrefs = self.__dict__.get("__lock_parents_weakrefs", None)
-        if _lock_parents_weakrefs is None:
-            self.__dict__["__lock_parents_weakrefs"] = []
-            _lock_parents_weakrefs = self.__dict__["__lock_parents_weakrefs"]
-        return _lock_parents_weakrefs
-
-    @_lock_parents_weakrefs.setter
-    def _lock_parents_weakrefs(self, value: list):
-        self.__dict__["__lock_parents_weakrefs"] = value
-
-    @as_decorator("is_locked")
-    def lock_(self) -> T:
-        """Locks a tensordict for non in-place operations.
-
-        Functions such as :meth:`~.set`, :meth:`~.__setitem__`, :meth:`~.update`,
-        :meth:`~.rename_key_` or other operations that add or remove entries
-        will be blocked.
-
-        This method can be used as a decorator.
-
-        Example:
-            >>> from tensordict import TensorDict
-            >>> td = TensorDict({"a": 1, "b": 2, "c": 3}, batch_size=[])
-            >>> with td.lock_():
-            ...     assert td.is_locked
-            ...     try:
-            ...         td.set("d", 0) # error!
-            ...     except RuntimeError:
-            ...         print("td is locked!")
-            ...     try:
-            ...         del td["d"]
-            ...     except RuntimeError:
-            ...         print("td is locked!")
-            ...     try:
-            ...         td.rename_key_("a", "d")
-            ...     except RuntimeError:
-            ...         print("td is locked!")
-            ...     td.set("a", 0, inplace=True)  # No storage is added, moved or removed
-            ...     td.set_("a", 0) # No storage is added, moved or removed
-            ...     td.update({"a": 0}, inplace=True)  # No storage is added, moved or removed
-            ...     td.update_({"a": 0})  # No storage is added, moved or removed
-            >>> assert not td.is_locked
-        """
-        if self.is_locked:
-            return self
-        self._propagate_lock()
-        return self
-
-    @erase_cache
-    def _propagate_unlock(self):
-        # if we end up here, we can clear the graph associated with this td
-        self._is_locked = False
-
-        self._is_shared = False
-        self._is_memmap = False
-
-        sub_tds = []
-        for value in self.values():
-            if _is_tensor_collection(type(value)):
-                sub_tds.extend(value._propagate_unlock())
-                sub_tds.append(value)
-        return sub_tds
-
-    def _check_unlock(self):
-        for ref in self._lock_parents_weakrefs:
-            obj = ref()
-            # check if the locked parent exists and if it's locked
-            # we check _is_locked because it can be False or None in the case of Lazy stacks,
-            # but if we check obj.is_locked it will be True for this class.
-            if obj is not None and obj._is_locked:
-                raise RuntimeError(
-                    "Cannot unlock a tensordict that is part of a locked graph. "
-                    "Unlock the root tensordict first. If the tensordict is part of multiple graphs, "
-                    "group the graphs under a common tensordict an unlock this root. "
-                    f"self: {self}, obj: {obj}"
-                )
-        try:
-            self._lock_parents_weakrefs = []
-        except AttributeError:
-            # Some tds (eg, LazyStack) have an automated way of creating the _lock_parents_weakref
-            pass
-
-    @as_decorator("is_locked")
-    def unlock_(self) -> T:
-        """Unlocks a tensordict for non in-place operations.
-
-        Can be used as a decorator.
-
-        See :meth:`~.lock_` for more details.
-        """
-        try:
-            sub_tds = self._propagate_unlock()
-            for sub_td in sub_tds:
-                sub_td._check_unlock()
-            self._check_unlock()
-        except RuntimeError as err:
-            self.lock_()
-            raise err
-        return self
-
-    # Conversion (device or dtype)
-    @overload
-    def to(
-        self: T,
-        device: Optional[Union[int, device]] = ...,
-        dtype: Optional[Union[torch.device, str]] = ...,
-        non_blocking: bool = ...,
-    ) -> T:
-        ...
-
-    @overload
-    def to(self: T, dtype: Union[torch.device, str], non_blocking: bool = ...) -> T:
-        ...
-
-    @overload
-    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:
-        ...
-
-    @overload
-    def to(self: T, *, other: T, non_blocking: bool = ...) -> T:
-        ...
-
-    @overload
-    def to(self: T, *, batch_size: torch.Size) -> T:
-        ...
-
-    @abc.abstractmethod
-    def to(self, *args, **kwargs) -> T:
-        """Maps a TensorDictBase subclass either on another device, dtype or to another TensorDictBase subclass (if permitted).
-
-        Casting tensors to a new dtype is not allowed, as tensordicts are not bound to contain a single
-        tensor dtype.
-
-        Args:
-            device (torch.device, optional): the desired device of the tensordict.
-            dtype (torch.dtype, optional): the desired floating point or complex dtype of
-                the tensordict.
-            tensor (torch.Tensor, optional): Tensor whose dtype and device are the desired
-                dtype and device for all tensors in this TensorDict.
-
-        Keyword Args:
-            non_blocking (bool, optional): whether the operations should be blocking.
-            memory_format (torch.memory_format, optional): the desired memory
-                format for 4D parameters and buffers in this tensordict.
-            batch_size (torch.Size, optional): resulting batch-size of the
-                output tensordict.
-            other (TensorDictBase, optional): TensorDict instance whose dtype
-                and device are the desired dtype and device for all tensors
-                in this TensorDict.
-                .. note:: Since :class:`~tensordict.TensorDictBase` instances do not have
-                    a dtype, the dtype is gathered from the example leaves.
-                    If there are more than one dtype, then no dtype
-                    casting is undertook.
-
-        Returns:
-            a new tensordict instance if the device differs from the tensordict
-            device and/or if the dtype is passed. The same tensordict otherwise.
-            ``batch_size`` only modifications are done in-place.
-
-        Examples:
-            >>> data = TensorDict({"a": 1.0}, [], device=None)
-            >>> data_cuda = data.to("cuda:0")  # casts to cuda
-            >>> data_int = data.to(torch.int)  # casts to int
-            >>> data_cuda_int = data.to("cuda:0", torch.int)  # multiple casting
-            >>> data_cuda = data.to(torch.randn(3, device="cuda:0"))  # using an example tensor
-            >>> data_cuda = data.to(other=TensorDict({}, [], device="cuda:0"))  # using a tensordict example
-        """
-        ...
-
-    def _sync_all(self):
-        if _has_cuda:
-            if torch.cuda.is_initialized():
-                torch.cuda.synchronize()
-        elif _has_mps:
-            torch.mps.synchronize()
-
-    def is_floating_point(self):
-        for item in self.values(include_nested=True, leaves_only=True):
-            if not item.is_floating_point():
-                return False
-        else:
-            return True
-
-    def double(self):
-        r"""Casts all tensors to ``torch.bool``."""
-        return self._fast_apply(lambda x: x.double(), propagate_lock=True)
-
-    def float(self):
-        r"""Casts all tensors to ``torch.float``."""
-        return self._fast_apply(lambda x: x.float(), propagate_lock=True)
-
-    def int(self):
-        r"""Casts all tensors to ``torch.int``."""
-        return self._fast_apply(lambda x: x.int(), propagate_lock=True)
-
-    def bool(self):
-        r"""Casts all tensors to ``torch.bool``."""
-        return self._fast_apply(lambda x: x.bool(), propagate_lock=True)
-
-    def half(self):
-        r"""Casts all tensors to ``torch.half``."""
-        return self._fast_apply(lambda x: x.half(), propagate_lock=True)
-
-    def bfloat16(self):
-        r"""Casts all tensors to ``torch.bfloat16``."""
-        return self._fast_apply(lambda x: x.bfloat16(), propagate_lock=True)
-
-    def type(self, dst_type):
-        r"""Casts all tensors to :attr:`dst_type`.
-
-        Args:
-            dst_type (type or string): the desired type
-
-        """
-        return self._fast_apply(lambda x: x.type(dst_type))
-
-    # Gradient compatibility
-    @property
-    def requires_grad(self) -> bool:
-        return any(v.requires_grad for v in self.values())
-
-    @abc.abstractmethod
-    def detach_(self) -> T:
-        """Detach the tensors in the tensordict in-place.
-
-        Returns:
-            self.
-
-        """
-        ...
-
-    @cache  # noqa: B019
-    def detach(self) -> T:
-        """Detach the tensors in the tensordict.
-
-        Returns:
-            a new tensordict with no tensor requiring gradient.
-
-        """
-        return self._fast_apply(
-            lambda x: x.detach(),
-            propagate_lock=True,
-        )
-
-
-_ACCEPTED_CLASSES = (
-    Tensor,
-    TensorDictBase,
-)
-
-
-def _register_tensor_class(cls):
-    global _ACCEPTED_CLASSES
-    _ACCEPTED_CLASSES = set(_ACCEPTED_CLASSES)
-    _ACCEPTED_CLASSES.add(cls)
-    _ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
-
-
-def _is_tensor_collection(datatype):
-    try:
-        out = _TENSOR_COLLECTION_MEMO[datatype]
-    except KeyError:
-        if issubclass(datatype, TensorDictBase):
-            out = True
-        elif _is_tensorclass(datatype):
-            out = True
-        else:
-            out = False
-        _TENSOR_COLLECTION_MEMO[datatype] = out
-    return out
-
-
-def is_tensor_collection(datatype: type | Any) -> bool:
-    """Checks if a data object or a type is a tensor container from the tensordict lib.
-
-    Returns:
-        ``True`` if the input is a TensorDictBase subclass, a tensorclass or an istance of these.
-        ``False`` otherwise.
-
-    Examples:
-        >>> is_tensor_collection(TensorDictBase)  # True
-        >>> is_tensor_collection(TensorDict({}, []))  # True
-        >>> @tensorclass
-        ... class MyClass:
-        ...     pass
-        ...
-        >>> is_tensor_collection(MyClass)  # True
-        >>> is_tensor_collection(MyClass(batch_size=[]))  # True
-
-    """
-    # memoizing is 2x faster
-    if not isinstance(datatype, type):
-        datatype = type(datatype)
-    return _is_tensor_collection(datatype)
-
-
-def _default_is_leaf(cls: Type) -> bool:
-    return not _is_tensor_collection(cls)
-
-
-def _is_leaf_nontensor(cls: Type) -> bool:
-    if _is_tensor_collection(cls):
-        return _is_non_tensor(cls)
-    # if issubclass(cls, KeyedJaggedTensor):
-    #     return False
-    return issubclass(cls, torch.Tensor)
-
-
-def _load_metadata(prefix: Path):
-    filepath = prefix / "meta.json"
-    with open(filepath) as json_metadata:
-        metadata = json.load(json_metadata)
-    return metadata
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import abc
+import collections
+import concurrent.futures
+import contextlib
+import importlib
+import json
+import numbers
+import weakref
+from collections.abc import MutableMapping
+
+from concurrent.futures import ThreadPoolExecutor
+from copy import copy
+from functools import wraps
+from pathlib import Path
+from textwrap import indent
+from typing import (
+    Any,
+    Callable,
+    Generator,
+    Iterator,
+    List,
+    Optional,
+    OrderedDict,
+    overload,
+    Sequence,
+    Tuple,
+    Type,
+    TypeVar,
+    Union,
+)
+
+import numpy as np
+import torch
+
+from tensordict.memmap import MemoryMappedTensor
+from tensordict.utils import (
+    _CloudpickleWrapper,
+    _GENERIC_NESTED_ERR,
+    _get_shape_from_args,
+    _is_non_tensor,
+    _is_tensorclass,
+    _KEY_ERROR,
+    _proc_init,
+    _prune_selected_keys,
+    _set_max_batch_size,
+    _shape,
+    _split_tensordict,
+    _td_fields,
+    _unravel_key_to_tuple,
+    as_decorator,
+    Buffer,
+    cache,
+    convert_ellipsis_to_idx,
+    DeviceType,
+    erase_cache,
+    implement_for,
+    IndexType,
+    infer_size_impl,
+    int_generator,
+    is_non_tensor,
+    lazy_legacy,
+    lock_blocked,
+    NestedKey,
+    prod,
+    set_lazy_legacy,
+    TensorDictFuture,
+    unravel_key,
+    unravel_key_list,
+)
+from torch import distributed as dist, multiprocessing as mp, nn, Tensor
+from torch.nn.parameter import UninitializedTensorMixin
+from torch.utils._pytree import tree_map
+
+
+# NO_DEFAULT is used as a placeholder whenever the default is not provided.
+# Using None is not an option since `td.get(key, default=None)` is a valid usage.
+class _NoDefault:
+    def __new__(cls):
+        if not hasattr(cls, "instance"):
+            cls.instance = super(_NoDefault, cls).__new__(cls)
+        return cls.instance
+
+    def __bool__(self):
+        return False
+
+
+NO_DEFAULT = _NoDefault()
+
+
+class _NestedTensorsAsLists:
+    """Class used to iterate over leaves of lazily stacked tensordicts."""
+
+    def __new__(cls):
+        if not hasattr(cls, "instance"):
+            cls.instance = super(_NestedTensorsAsLists, cls).__new__(cls)
+        return cls.instance
+
+    def __bool__(self):
+        return False
+
+    def __call__(self, val):
+        return _default_is_leaf(val)
+
+
+_NESTED_TENSORS_AS_LISTS = _NestedTensorsAsLists()
+
+T = TypeVar("T", bound="TensorDictBase")
+
+
+class _BEST_ATTEMPT_INPLACE:
+    def __bool__(self):
+        # we use an exception to exit when running `inplace = BEST_ATTEMPT_INPLACE if inplace else False`
+        # more than once
+        raise NotImplementedError
+
+
+_has_mps = torch.backends.mps.is_available()
+_has_cuda = torch.cuda.is_available()
+
+BEST_ATTEMPT_INPLACE = _BEST_ATTEMPT_INPLACE()
+
+# some complex string used as separator to concatenate and split keys in
+# distributed frameworks
+CompatibleType = Union[
+    Tensor,
+]
+
+_STR_MIXED_INDEX_ERROR = "Received a mixed string-non string index. Only string-only or string-free indices are supported."
+
+_HEURISTIC_EXCLUDED = (Tensor, tuple, list, set, dict, np.ndarray)
+
+_TENSOR_COLLECTION_MEMO = {}
+
+
+class TensorDictBase(MutableMapping):
+    """TensorDictBase is an abstract parent class for TensorDicts, a torch.Tensor data container."""
+
+    _safe: bool = False
+    _lazy: bool = False
+    _inplace_set: bool = False
+    is_meta: bool = False
+    _is_locked: bool = False
+    _cache: bool = None
+    _is_non_tensor: bool = False
+    _memmap_prefix = None
+
+    def __bool__(self) -> bool:
+        raise RuntimeError("Converting a tensordict to boolean value is not permitted")
+
+    @abc.abstractmethod
+    def __ne__(self, other: object) -> T:
+        """NOT operation over two tensordicts, for evey key.
+
+        The two tensordicts must have the same key set.
+
+        Args:
+            other (TensorDictBase, dict, or float): the value to compare against.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __xor__(self, other: TensorDictBase | float):
+        """XOR operation over two tensordicts, for evey key.
+
+        The two tensordicts must have the same key set.
+
+        Args:
+            other (TensorDictBase, dict, or float): the value to compare against.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __or__(self, other: TensorDictBase | float) -> T:
+        """OR operation over two tensordicts, for evey key.
+
+        The two tensordicts must have the same key set.
+
+        Args:
+            other (TensorDictBase, dict, or float): the value to compare against.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __eq__(self, other: object) -> T:
+        """Compares two tensordicts against each other, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __ge__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "greater or equal" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __gt__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "greater than" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __le__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "lower or equal" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def __lt__(self, other: object) -> T:
+        """Compares two tensordicts against each other using the "lower than" operator, for every key. The two tensordicts must have the same key set.
+
+        Returns:
+            a new TensorDict instance with all tensors are boolean
+            tensors of the same shape as the original tensors.
+
+        """
+        ...
+
+    def __repr__(self) -> str:
+        fields = _td_fields(self)
+        field_str = indent(f"fields={{{fields}}}", 4 * " ")
+        batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+        device_str = indent(f"device={self.device}", 4 * " ")
+        is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+        string = ",\n".join([field_str, batch_size_str, device_str, is_shared_str])
+        return f"{type(self).__name__}(\n{string})"
+
+    def __iter__(self) -> Generator:
+        """Iterates over the first shape-dimension of the tensordict."""
+        if not self.batch_dims:
+            raise StopIteration
+        yield from self.unbind(0)
+
+    def __len__(self) -> int:
+        """Returns the length of first dimension, if there is, otherwise 0."""
+        return self.shape[0] if self.batch_dims else 0
+
+    def __contains__(self, key: NestedKey) -> bool:
+        if isinstance(key, str):
+            return key in self.keys()
+        if isinstance(key, tuple):
+            key = unravel_key(key)
+            if not key:
+                raise RuntimeError(
+                    "key must be a NestedKey (a str or a possibly tuple of str)."
+                )
+            return key in self.keys(True, is_leaf=_is_leaf_nontensor)
+        raise RuntimeError(
+            "key must be a NestedKey (a str or a possibly tuple of str)."
+        )
+
+    def __getitem__(self, index: IndexType) -> T:
+        """Indexes all tensors according to the provided index.
+
+        The index can be a (nested) key or any valid shape index given the
+        tensordict batch size.
+
+        If the index is a nested key and the result is a :class:`~tensordict.NonTensorData`
+        object, the content of the non-tensor is returned.
+
+        Examples:
+            >>> td = TensorDict({"root": torch.arange(2), ("nested", "entry"): torch.arange(2)}, [2])
+            >>> td["root"]
+            torch.tensor([0, 1])
+            >>> td["nested", "entry"]
+            torch.tensor([0, 1])
+            >>> td[:1]
+            TensorDict(
+                fields={
+                    nested: TensorDict(
+                        fields={
+                            entry: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([1]),
+                        device=None,
+                        is_shared=False),
+                    root: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([1]),
+                device=None,
+                is_shared=False)
+        """
+        istuple = isinstance(index, tuple)
+        if istuple or isinstance(index, str):
+            # _unravel_key_to_tuple will return an empty tuple if the index isn't a NestedKey
+            idx_unravel = _unravel_key_to_tuple(index)
+            if idx_unravel:
+                result = self._get_tuple(idx_unravel, NO_DEFAULT)
+                if is_non_tensor(result):
+                    result_data = getattr(result, "data", NO_DEFAULT)
+                    if result_data is NO_DEFAULT:
+                        return result.tolist()
+                    return result_data
+                return result
+
+        if (istuple and not index) or (not istuple and index is Ellipsis):
+            # empty tuple returns self
+            return self
+        if not istuple:
+            if isinstance(index, int):
+                return self._index_tensordict(index)
+            # we only want tuple indices
+            index = (index,)
+        # # convert range/np.ndarray to tensor: this is not cheap
+        # index = tuple(
+        #     torch.tensor(idx) if isinstance(idx, (np.ndarray, range)) else idx
+        #     for idx in index
+        # )
+        if istuple and any(idx is Ellipsis for idx in index):
+            index = convert_ellipsis_to_idx(index, self.batch_size)
+        if all(isinstance(idx, slice) and idx == slice(None) for idx in index):
+            return self
+
+        return self._index_tensordict(index)
+
+    # this is necessary for data collectors for instance, otherwise indexing
+    # will always be achieved one element at a time.
+    __getitems__ = __getitem__
+
+    def _get_sub_tensordict(self, idx: IndexType) -> T:
+        """Returns a _SubTensorDict with the desired index."""
+        from tensordict._td import _SubTensorDict
+
+        return _SubTensorDict(source=self, idx=idx)
+
+    @abc.abstractmethod
+    def __setitem__(
+        self,
+        index: IndexType,
+        value: T | dict | numbers.Number | CompatibleType,
+    ) -> None:
+        ...
+
+    def __delitem__(self, key: NestedKey) -> T:
+        return self.del_(key)
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        from tensordict._torch_func import TD_HANDLED_FUNCTIONS
+
+        if kwargs is None:
+            kwargs = {}
+        if func not in TD_HANDLED_FUNCTIONS or not all(
+            issubclass(t, (Tensor, TensorDictBase)) for t in types
+        ):
+            return NotImplemented
+        return TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+
+    @abc.abstractmethod
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        """Checks if all values are True/non-null in the tensordict.
+
+        Args:
+            dim (int, optional): if ``None``, returns a boolean indicating
+                whether all tensors return `tensor.all() == True`
+                If integer, all is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        """Checks if any value is True/non-null in the tensordict.
+
+        Args:
+            dim (int, optional): if ``None``, returns a boolean indicating
+                whether all tensors return `tensor.any() == True`.
+                If integer, all is called upon the dimension specified if
+                and only if this dimension is compatible with
+                the tensordict shape.
+
+        """
+        ...
+
+    def mean(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the mean value of all elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the mean value of all leaves (if this can be computed).
+                If integer or tuple of integers, `mean` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            dtype (torch.dtype, optional) – the desired data type of returned tensor.
+                If specified, the input tensor is casted to dtype before the operation is performed.
+                This is useful for preventing data type overflows. Default: ``None``.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="mean", dim=dim, keepdim=keepdim, dtype=dtype
+        )
+
+    def nanmean(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the mean of all non-NaN elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the mean value of all leaves (if this can be computed).
+                If integer or tuple of integers, `mean` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            dtype (torch.dtype, optional) – the desired data type of returned tensor.
+                If specified, the input tensor is casted to dtype before the operation is performed.
+                This is useful for preventing data type overflows. Default: ``None``.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="nanmean", keepdim=keepdim, dim=dim, dtype=dtype
+        )
+
+    def prod(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the produce of values of all elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the prod value of all leaves (if this can be computed).
+                If integer or tuple of integers, `prod` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            dtype (torch.dtype, optional) – the desired data type of returned tensor.
+                If specified, the input tensor is casted to dtype before the operation is performed.
+                This is useful for preventing data type overflows. Default: ``None``.
+
+        """
+        result = self._cast_reduction(
+            reduction_name="prod", dim=dim, keepdim=False, tuple_ok=False, dtype=dtype
+        )
+        if keepdim:
+            if isinstance(dim, tuple):
+                dim = dim[0]
+            if dim not in (None, NO_DEFAULT):
+                result = result.unsqueeze(dim)
+            else:
+                result = result.reshape([1 for _ in self.shape])
+        return result
+
+    def sum(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the sum value of all elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the sum value of all leaves (if this can be computed).
+                If integer or tuple of integers, `sum` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            dtype (torch.dtype, optional) – the desired data type of returned tensor.
+                If specified, the input tensor is casted to dtype before the operation is performed.
+                This is useful for preventing data type overflows. Default: ``None``.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="sum", dim=dim, keepdim=keepdim, dtype=dtype
+        )
+
+    def nansum(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the sum of all non-NaN elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the sum value of all leaves (if this can be computed).
+                If integer or tuple of integers, `sum` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            dtype (torch.dtype, optional) – the desired data type of returned tensor.
+                If specified, the input tensor is casted to dtype before the operation is performed.
+                This is useful for preventing data type overflows. Default: ``None``.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="nansum", dim=dim, keepdim=keepdim, dtype=dtype
+        )
+
+    def std(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        correction: int = 1,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the standard deviation value of all elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the sum value of all leaves (if this can be computed).
+                If integer or tuple of integers, `std` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            correction (int): difference between the sample size and sample degrees of freedom.
+                Defaults to Bessel’s correction, correction=1.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="std",
+            dim=dim,
+            keepdim=keepdim,
+            correction=correction,
+        )
+
+    def var(
+        self,
+        dim: int | Tuple[int] = NO_DEFAULT,
+        keepdim: bool = NO_DEFAULT,
+        *,
+        correction: int = 1,
+    ) -> bool | TensorDictBase:  # noqa: D417
+        """Returns the variance value of all elements in the input tensordit.
+
+        Args:
+            dim (int, tuple of int, optional): if ``None``, returns a dimensionless
+                tensordict containing the sum value of all leaves (if this can be computed).
+                If integer or tuple of integers, `var` is called upon the dimension specified if
+                and only if this dimension is compatible with the tensordict
+                shape.
+            keepdim (bool) – whether the output tensor has dim retained or not.
+
+        Keyword Args:
+            correction (int): difference between the sample size and sample degrees of freedom.
+                Defaults to Bessel’s correction, correction=1.
+
+        """
+        if dim is NO_DEFAULT and keepdim:
+            dim = None
+        return self._cast_reduction(
+            reduction_name="var",
+            dim=dim,
+            keepdim=keepdim,
+            correction=correction,
+        )
+
+    @abc.abstractmethod
+    def _cast_reduction(
+        self,
+        *,
+        reduction_name,
+        dim=NO_DEFAULT,
+        keepdim=NO_DEFAULT,
+        dtype,
+        tuple_ok=True,
+        **kwargs,
+    ):
+        ...
+
+    def auto_batch_size_(self, batch_dims: int | None = None) -> T:
+        """Sets the maximum batch-size for the tensordict, up to an optional batch_dims.
+
+        Args:
+            batch_dims (int, optional): if provided, the batch-size will be at
+                most ``batch_dims`` long.
+
+        Returns:
+            self
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>> td = TensorDict({"a": torch.randn(3, 4, 5), "b": {"c": torch.randn(3, 4, 6)}}, batch_size=[])
+            >>> td.auto_batch_size_()
+            >>> print(td.batch_size)
+            torch.Size([3, 4])
+            >>> td.auto_batch_size_(batch_dims=1)
+            >>> print(td.batch_size)
+            torch.Size([3])
+
+        """
+        _set_max_batch_size(self, batch_dims)
+        return self
+
+    @abc.abstractmethod
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        """Instance method version of :meth:`~tensordict.TensorDict.from_dict`.
+
+        Unlike :meth:`~tensordict.TensorDict.from_dict`, this method will
+        attempt to keep the tensordict types within the existing tree (for
+        any existing leaf).
+
+        Examples:
+            >>> from tensordict import TensorDict, tensorclass
+            >>> import torch
+            >>>
+            >>> @tensorclass
+            >>> class MyClass:
+            ...     x: torch.Tensor
+            ...     y: int
+            >>>
+            >>> td = TensorDict({"a": torch.randn(()), "b": MyClass(x=torch.zeros(()), y=1)})
+            >>> print(td.from_dict_instance(td.to_dict()))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: MyClass(
+                        x=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                        y=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> print(td.from_dict(td.to_dict()))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                            y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        ...
+
+    @classmethod
+    def from_h5(cls, filename, mode="r"):
+        """Creates a PersistentTensorDict from a h5 file.
+
+        This function will automatically determine the batch-size for each nested
+        tensordict.
+
+        Args:
+            filename (str): the path to the h5 file.
+            mode (str, optional): reading mode. Defaults to ``"r"``.
+        """
+        from tensordict.persistent import PersistentTensorDict
+
+        return PersistentTensorDict.from_h5(filename, mode=mode)
+
+    # Module interaction
+    @classmethod
+    def from_module(
+        cls,
+        module,
+        as_module: bool = False,
+        lock: bool = True,
+        use_state_dict: bool = False,
+    ):
+        """Copies the params and buffers of a module in a tensordict.
+
+        Args:
+            module (nn.Module): the module to get the parameters from.
+            as_module (bool, optional): if ``True``, a :class:`~tensordict.nn.TensorDictParams`
+                instance will be returned which can be used to store parameters
+                within a :class:`torch.nn.Module`. Defaults to ``False``.
+            lock (bool, optional): if ``True``, the resulting tensordict will be locked.
+                Defaults to ``True``.
+            use_state_dict (bool, optional): if ``True``, the state-dict from the
+                module will be used and unflattened into a TensorDict with
+                the tree structure of the model. Defaults to ``False``.
+                .. note::
+                  This is particularly useful when state-dict hooks have to be
+                  used.
+
+        Examples:
+            >>> from torch import nn
+            >>> module = nn.TransformerDecoder(
+            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
+            ...     num_layers=1)
+            >>> params = TensorDict.from_module(module)
+            >>> print(params["layers", "0", "linear1"])
+            TensorDict(
+                fields={
+                    bias: Parameter(shape=torch.Size([2048]), device=cpu, dtype=torch.float32, is_shared=False),
+                    weight: Parameter(shape=torch.Size([2048, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        ...
+
+    @classmethod
+    def from_modules(
+        cls,
+        *modules,
+        as_module: bool = False,
+        lock: bool = True,
+        use_state_dict: bool = False,
+        lazy_stack: bool = False,
+    ):
+        """Retrieves the parameters of several modules for ensebmle learning/feature of expects applications through vmap.
+
+        Args:
+            modules (sequence of nn.Module): the modules to get the parameters from.
+                If the modules differ in their structure, a lazy stack is needed
+                (see the ``lazy_stack`` argument below).
+
+        Keyword Args:
+            as_module (bool, optional): if ``True``, a :class:`~tensordict.nn.TensorDictParams`
+                instance will be returned which can be used to store parameters
+                within a :class:`torch.nn.Module`. Defaults to ``False``.
+            lock (bool, optional): if ``True``, the resulting tensordict will be locked.
+                Defaults to ``True``.
+            use_state_dict (bool, optional): if ``True``, the state-dict from the
+                module will be used and unflattened into a TensorDict with
+                the tree structure of the model. Defaults to ``False``.
+                .. note::
+                  This is particularly useful when state-dict hooks have to be
+                  used.
+            lazy_stack (bool, optional): whether parameters should be densly or
+                lazily stacked. Defaults to ``False`` (dense stack).
+
+                .. note:: ``lazy_stack`` and ``as_module`` are exclusive features.
+
+                .. warning::
+                    There is a crucial difference between lazy and non-lazy outputs
+                    in that non-lazy output will reinstantiate parameters with the
+                    desired batch-size, while ``lazy_stack`` will just represent
+                    the parameters as lazily stacked. This means that whilst the
+                    original parameters can safely be passed to an optimizer
+                    when ``lazy_stack=True``, the new parameters need to be passed
+                    when it is set to ``True``.
+
+                .. warning::
+                    Whilst it can be tempting to use a lazy stack to keep the
+                    orignal parameter references, remember that lazy stack
+                    perform a stack each time :meth:`~.get` is called. This will
+                    require memory (N times the size of the parameters, more if a
+                    graph is built) and time to be computed.
+                    It also means that the optimizer(s) will contain more
+                    parameters, and operations like :meth:`~torch.optim.Optimizer.step`
+                    or :meth:`~torch.optim.Optimizer.zero_grad` will take longer
+                    to be executed. In general, ``lazy_stack`` should be reserved
+                    to very few use cases.
+
+        Examples:
+            >>> from torch import nn
+            >>> from tensordict import TensorDict
+            >>> torch.manual_seed(0)
+            >>> empty_module = nn.Linear(3, 4, device="meta")
+            >>> n_models = 2
+            >>> modules = [nn.Linear(3, 4) for _ in range(n_models)]
+            >>> params = TensorDict.from_modules(*modules)
+            >>> print(params)
+            TensorDict(
+                fields={
+                    bias: Parameter(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                    weight: Parameter(shape=torch.Size([2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False)
+            >>> # example of batch execution
+            >>> def exec_module(params, x):
+            ...     with params.to_module(empty_module):
+            ...         return empty_module(x)
+            >>> x = torch.randn(3)
+            >>> y = torch.vmap(exec_module, (0, None))(params, x)
+            >>> assert y.shape == (n_models, 4)
+            >>> # since lazy_stack = False, backprop leaves the original params untouched
+            >>> y.sum().backward()
+            >>> assert params["weight"].grad.norm() > 0
+            >>> assert modules[0].weight.grad is None
+
+        With ``lazy_stack=True``, things are slightly different:
+
+            >>> params = TensorDict.from_modules(*modules, lazy_stack=True)
+            >>> print(params)
+            LazyStackedTensorDict(
+                fields={
+                    bias: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                    weight: Tensor(shape=torch.Size([2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                exclusive_fields={
+                },
+                batch_size=torch.Size([2]),
+                device=None,
+                is_shared=False,
+                stack_dim=0)
+            >>> # example of batch execution
+            >>> y = torch.vmap(exec_module, (0, None))(params, x)
+            >>> assert y.shape == (n_models, 4)
+            >>> y.sum().backward()
+            >>> assert modules[0].weight.grad is not None
+
+
+        """
+        param_list = [
+            cls.from_module(module, use_state_dict=use_state_dict) for module in modules
+        ]
+        if lazy_stack:
+            from tensordict._lazy import LazyStackedTensorDict
+
+            for param in param_list:
+                if any(
+                    isinstance(tensor, UninitializedTensorMixin)
+                    for tensor in param.values(True, True)
+                ):
+                    raise RuntimeError(
+                        "lasy_stack=True is not compatible with lazy modules."
+                    )
+            params = LazyStackedTensorDict.lazy_stack(param_list)
+        else:
+            with set_lazy_legacy(False), torch.no_grad():
+                params = torch.stack(param_list)
+
+            # Make sure params are params, buffers are buffers
+            def make_param(param, orig_param):
+                if isinstance(param, UninitializedTensorMixin):
+                    return param
+                if isinstance(orig_param, nn.Parameter):
+                    return nn.Parameter(param.detach(), orig_param.requires_grad)
+                return Buffer(param)
+
+            params = params._fast_apply(make_param, param_list[0], propagate_lock=True)
+        if as_module:
+            from tensordict.nn import TensorDictParams
+
+            params = TensorDictParams(params, no_convert=True)
+        if lock:
+            params.lock_()
+        return params
+
+    @as_decorator()
+    def to_module(
+        self,
+        module: nn.Module,
+        *,
+        inplace: bool | None = None,
+        return_swap: bool = True,
+        swap_dest=None,
+        use_state_dict: bool = False,
+        non_blocking: bool = False,
+        memo=None,  # deprecated
+    ):
+        """Writes the content of a TensorDictBase instance onto a given nn.Module attributes, recursively.
+
+        Args:
+            module (nn.Module): a module to write the parameters into.
+
+        Keyword Args:
+            inplace (bool, optional): if ``True``, the parameters or tensors
+                in the module are updated in-place. Defaults to ``True``.
+            return_swap (bool, optional): if ``True``, the old parameter configuration
+                will be returned. Defaults to ``False``.
+            swap_dest (TensorDictBase, optional): if ``return_swap`` is ``True``,
+                the tensordict where the swap should be written.
+            use_state_dict (bool, optional): if ``True``, state-dict API will be
+                used to load the parameters (including the state-dict hooks).
+                Defaults to ``False``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Examples:
+            >>> from torch import nn
+            >>> module = nn.TransformerDecoder(
+            ...     decoder_layer=nn.TransformerDecoderLayer(nhead=4, d_model=4),
+            ...     num_layers=1)
+            >>> params = TensorDict.from_module(module)
+            >>> params.zero_()
+            >>> params.to_module(module)
+            >>> assert (module.layers[0].linear1.weight == 0).all()
+        """
+        if memo is not None:
+            raise RuntimeError("memo cannot be passed to the public to_module anymore.")
+        hooks = getattr(
+            torch.nn.modules.module, "_global_parameter_registration_hooks", {}
+        )
+        memo = {"hooks": tuple(hooks.values())}
+        return self._to_module(
+            module=module,
+            inplace=inplace,
+            return_swap=return_swap,
+            swap_dest=swap_dest,
+            memo=memo,
+            use_state_dict=use_state_dict,
+            non_blocking=non_blocking,
+        )
+
+    @abc.abstractmethod
+    def _to_module(
+        self,
+        module,
+        *,
+        inplace: bool | None = None,
+        return_swap: bool = True,
+        swap_dest=None,
+        memo=None,
+        use_state_dict: bool = False,
+        non_blocking: bool = False,
+    ):
+        ...
+
+    # Shape functionality
+    @property
+    def shape(self) -> torch.Size:
+        """See :obj:`~tensordict.TensorDictBase.batch_size`."""
+        return self.batch_size
+
+    @property
+    @abc.abstractmethod
+    def batch_size(self) -> torch.Size:
+        """Shape (or batch_size) of a TensorDict.
+
+        The shape of a tensordict corresponds to the common first ``N``
+        dimensions of the tensors it contains, where ``N`` is an arbitrary
+        number.
+        The ``TensorDict`` shape is controlled by the user upon
+        initialization (ie, it is not inferred from the tensor shapes).
+
+        The ``batch_size`` can be edited dynamically if the new size is compatible
+        with the TensorDict content. For instance, setting the batch size to
+        an empty value is always allowed.
+
+        Returns:
+            a :obj:`~torch.Size` object describing the TensorDict batch size.
+
+        Examples:
+            >>> data = TensorDict({
+            ...     "key 0": torch.randn(3, 4),
+            ...     "key 1": torch.randn(3, 5),
+            ...     "nested": TensorDict({"key 0": torch.randn(3, 4)}, batch_size=[3, 4])},
+            ...     batch_size=[3])
+            >>> data.batch_size = () # resets the batch-size to an empty value
+        """
+        ...
+
+    def size(self, dim: int | None = None) -> torch.Size | int:
+        """Returns the size of the dimension indicated by ``dim``.
+
+        If ``dim`` is not specified, returns the ``batch_size`` attribute of the TensorDict.
+
+        """
+        if dim is None:
+            return self.batch_size
+        return self.batch_size[dim]
+
+    @property
+    def data(self):
+        """Returns a tensordict containing the .data attributes of the leaf tensors."""
+        return self._data()
+
+    @property
+    def grad(self):
+        """Returns a tensordict containing the .grad attributes of the leaf tensors."""
+        return self._grad()
+
+    @cache  # noqa
+    def _dtype(self):
+        dtype = None
+        for val in self.values(True, True):
+            val_dtype = getattr(val, "dtype", None)
+            if dtype is None and val_dtype is not None:
+                dtype = val_dtype
+            elif dtype is not None and val_dtype is not None and dtype != val_dtype:
+                return None
+        return dtype
+
+    @property
+    def dtype(self):
+        """Returns the dtype of the values in the tensordict, if it is unique."""
+        return self._dtype()
+
+    def _batch_size_setter(self, new_batch_size: torch.Size) -> None:
+        if new_batch_size == self.batch_size:
+            return
+        if self._lazy:
+            raise RuntimeError(
+                "modifying the batch size of a lazy representation of a "
+                "tensordict is not permitted. Consider instantiating the "
+                "tensordict first by calling `td = td.to_tensordict()` before "
+                "resetting the batch size."
+            )
+        if not isinstance(new_batch_size, torch.Size):
+            new_batch_size = torch.Size(new_batch_size)
+        for key, value in self.items():
+            if _is_tensor_collection(type(value)):
+                if len(value.batch_size) < len(new_batch_size):
+                    # document as edge case
+                    value.batch_size = new_batch_size
+                    self._set_str(
+                        key, value, inplace=True, validated=True, non_blocking=False
+                    )
+        self._check_new_batch_size(new_batch_size)
+        self._change_batch_size(new_batch_size)
+        if self._has_names():
+            # if the tensordict has dim names and the new batch-size has more dims,
+            # we can simply add empty names after the current ones.
+            # Otherwise, we discard the extra existing names.
+            names = self.names
+            if len(names) < len(new_batch_size):
+                self.names = names + [None] * (len(new_batch_size) - len(names))
+            else:
+                self.names = names[: self.batch_dims]
+
+    @property
+    def batch_dims(self) -> int:
+        """Length of the tensordict batch size.
+
+        Returns:
+            int describing the number of dimensions of the tensordict.
+
+        """
+        return len(self.batch_size)
+
+    def ndimension(self) -> int:
+        """See :meth:`~.batch_dims`."""
+        return self.batch_dims
+
+    @property
+    def ndim(self) -> int:
+        """See :meth:`~.batch_dims`."""
+        return self.batch_dims
+
+    def dim(self) -> int:
+        """See :meth:`~.batch_dims`."""
+        return self.batch_dims
+
+    def numel(self) -> int:
+        """Total number of elements in the batch.
+
+        Lower-bounded to 1, as a stack of two tensordict with empty shape will
+        have two elements, therefore we consider that a tensordict is at least
+        1-element big.
+        """
+        return max(1, self.batch_size.numel())
+
+    @property
+    def depth(self) -> int:
+        """Returns the depth - maximum number of levels - of a tensordict.
+
+        The minimum depth is 0 (no nested tensordict).
+        """
+        return self._depth()
+
+    @cache  # noqa: B019
+    def _depth(self):
+        depth = 0
+        for key in self.keys(True, True, is_leaf=_is_leaf_nontensor):
+            if isinstance(key, tuple):
+                depth = max(depth, len(key) - 1)
+        return depth
+
+    @overload
+    def expand(self, *shape: int) -> T:
+        ...
+
+    @overload
+    def expand(self, shape: torch.Size) -> T:
+        ...
+
+    @abc.abstractmethod
+    def expand(self, *args: int | torch.Size) -> T:
+        """Expands each tensor of the tensordict according to the :func:`~torch.expand` function, ignoring the feature dimensions.
+
+        Supports iterables to specify the shape.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'a': torch.zeros(3, 4, 5),
+            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
+            >>> td_expand = td.expand(10, 3, 4)
+            >>> assert td_expand.shape == torch.Size([10, 3, 4])
+            >>> assert td_expand.get("a").shape == torch.Size([10, 3, 4, 5])
+
+        """
+        ...
+
+    def expand_as(self, other: TensorDictBase | torch.Tensor) -> TensorDictBase:
+        """Broadcasts the shape of the tensordict to the shape of `other` and expands it accordingly.
+
+        If the input is a tensor collection (tensordict or tensorclass),
+        the leaves will be expanded on a one-to-one basis.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>> td0 = TensorDict({
+            ...     "a": torch.ones(3, 1, 4),
+            ...     "b": {"c": torch.ones(3, 2, 1, 4)}},
+            ...     batch_size=[3],
+            ... )
+            >>> td1 = TensorDict({
+            ...     "a": torch.zeros(2, 3, 5, 4),
+            ...     "b": {"c": torch.zeros(2, 3, 2, 6, 4)}},
+            ...     batch_size=[2, 3],
+            ... )
+            >>> expanded = td0.expand_as(td1)
+            >>> assert (expanded==1).all()
+            >>> print(expanded)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([2, 3, 5, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([2, 3, 2, 6, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([2, 3]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([2, 3]),
+                device=None,
+                is_shared=False)
+
+        """
+        if _is_tensor_collection(type(other)):
+            return self.apply(
+                lambda x, y: x.expand_as(y), other, batch_size=other.batch_size
+            )
+        return self.expand(other.shape)
+
+    def unbind(self, dim: int) -> tuple[T, ...]:
+        """Returns a tuple of indexed tensordicts, unbound along the indicated dimension.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(12).reshape(3, 4),
+            ... }, batch_size=[3, 4])
+            >>> td0, td1, td2 = td.unbind(0)
+            >>> td0['x']
+            tensor([0, 1, 2, 3])
+            >>> td1['x']
+            tensor([4, 5, 6, 7])
+
+        """
+        batch_dims = self.batch_dims
+        if dim < -batch_dims or dim >= batch_dims:
+            raise RuntimeError(
+                f"the dimension provided ({dim}) is beyond the tensordict dimensions ({self.ndim})."
+            )
+        if dim < 0:
+            dim = batch_dims + dim
+        results = self._unbind(dim)
+        if self._is_memmap or self._is_shared:
+            for result in results:
+                result.lock_()
+        return results
+
+    @abc.abstractmethod
+    def _unbind(self, dim: int) -> tuple[T, ...]:
+        ...
+
+    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
+        """Splits a tensordict into the specified number of chunks, if possible.
+
+        Each chunk is a view of the input tensordict.
+
+        Args:
+            chunks (int): number of chunks to return
+            dim (int, optional): dimension along which to split the
+                tensordict. Default is 0.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(24).reshape(3, 4, 2),
+            ... }, batch_size=[3, 4])
+            >>> td0, td1 = td.chunk(dim=-1, chunks=2)
+            >>> td0['x']
+            tensor([[[ 0,  1],
+                     [ 2,  3]],
+                    [[ 8,  9],
+                     [10, 11]],
+                    [[16, 17],
+                     [18, 19]]])
+
+        """
+        if chunks < 1:
+            raise ValueError(
+                f"chunks must be a strictly positive integer, got {chunks}."
+            )
+        # fall back on split, using upper rounding
+        split_size = -(self.batch_size[dim] // -chunks)
+        return self.split(split_size, dim=dim)
+
+    @overload
+    def unsqueeze(self, dim: int) -> T:
+        ...
+
+    @as_decorator()
+    def unsqueeze(self, *args, **kwargs):
+        """Unsqueezes all tensors for a dimension comprised in between `-td.batch_dims` and `td.batch_dims` and returns them in a new tensordict.
+
+        Args:
+            dim (int): dimension along which to unsqueeze
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(24).reshape(3, 4, 2),
+            ... }, batch_size=[3, 4])
+            >>> td = td.unsqueeze(-2)
+            >>> td.shape
+            torch.Size([3, 1, 4])
+            >>> td.get("x").shape
+            torch.Size([3, 1, 4, 2])
+
+        This operation can be used as a context manager too. Changes to the original
+        tensordict will occur out-place, i.e. the content of the original tensors
+        will not be altered. This also assumes that the tensordict is not locked
+        (otherwise, unlocking the tensordict is necessary).
+
+            >>> td = TensorDict({
+            ...     'x': torch.arange(24).reshape(3, 4, 2),
+            ... }, batch_size=[3, 4])
+            >>> with td.unsqueeze(-2) as tds:
+            ...     tds.set("y", torch.zeros(3, 1, 4))
+            >>> assert td.get("y").shape == [3, 4]
+
+        """
+        _lazy_legacy = lazy_legacy()
+
+        if _lazy_legacy:
+            return self._legacy_unsqueeze(*args, **kwargs)
+        else:
+            result = self._unsqueeze(*args, **kwargs)
+            if result._is_memmap or result._is_shared:
+                result.lock_()
+            return result
+
+    @abc.abstractmethod
+    def _unsqueeze(self, dim):
+        ...
+
+    def _legacy_unsqueeze(self, dim: int) -> T:
+        if dim < 0:
+            dim = self.batch_dims + dim + 1
+
+        if (dim > self.batch_dims) or (dim < 0):
+            raise RuntimeError(
+                f"unsqueezing is allowed for dims comprised between "
+                f"`-td.batch_dims` and `td.batch_dims` only. Got "
+                f"dim={dim} with a batch size of {self.batch_size}."
+            )
+        from tensordict._lazy import _UnsqueezedTensorDict
+
+        return _UnsqueezedTensorDict(
+            source=self,
+            custom_op="unsqueeze",
+            inv_op="squeeze",
+            custom_op_kwargs={"dim": dim},
+            inv_op_kwargs={"dim": dim},
+        )
+
+    @overload
+    def squeeze(self, dim: int | None = None) -> T:
+        ...
+
+    @as_decorator()
+    def squeeze(self, *args, **kwargs):
+        """Squeezes all tensors for a dimension in between `-self.batch_dims+1` and `self.batch_dims-1` and returns them in a new tensordict.
+
+        Args:
+            dim (Optional[int]): dimension along which to squeeze. If dim is
+                ``None``, all singleton dimensions will be squeezed.
+                Defaults to ``None``.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(24).reshape(3, 1, 4, 2),
+            ... }, batch_size=[3, 1, 4])
+            >>> td = td.squeeze()
+            >>> td.shape
+            torch.Size([3, 4])
+            >>> td.get("x").shape
+            torch.Size([3, 4, 2])
+
+        This operation can be used as a context manager too. Changes to the original
+        tensordict will occur out-place, i.e. the content of the original tensors
+        will not be altered. This also assumes that the tensordict is not locked
+        (otherwise, unlocking the tensordict is necessary). This functionality is
+        *not* compatible with implicit squeezing.
+
+            >>> td = TensorDict({
+            ...     'x': torch.arange(24).reshape(3, 1, 4, 2),
+            ... }, batch_size=[3, 1, 4])
+            >>> with td.squeeze(1) as tds:
+            ...     tds.set("y", torch.zeros(3, 4))
+            >>> assert td.get("y").shape == [3, 1, 4]
+
+        """
+        _lazy_legacy = lazy_legacy()
+
+        if _lazy_legacy:
+            return self._legacy_squeeze(*args, **kwargs)
+        else:
+            result = self._squeeze(*args, **kwargs)
+            if result._is_memmap or result._is_shared:
+                result.lock_()
+            return result
+
+    @abc.abstractmethod
+    def _squeeze(self, dim=None):
+        ...
+
+    def _legacy_squeeze(self, dim: int | None = None) -> T:
+        from tensordict._lazy import _SqueezedTensorDict
+
+        if dim is None:
+            size = self.size()
+            if len(self.size()) == 1 or size.count(1) == 0:
+                return self
+            first_singleton_dim = size.index(1)
+
+            squeezed_dict = _SqueezedTensorDict(
+                source=self,
+                custom_op="squeeze",
+                inv_op="unsqueeze",
+                custom_op_kwargs={"dim": first_singleton_dim},
+                inv_op_kwargs={"dim": first_singleton_dim},
+            )
+            return squeezed_dict.squeeze(dim=None)
+
+        if dim < 0:
+            dim = self.batch_dims + dim
+
+        if self.batch_dims and (dim >= self.batch_dims or dim < 0):
+            raise RuntimeError(
+                f"squeezing is allowed for dims comprised between 0 and "
+                f"td.batch_dims only. Got dim={dim} and batch_size"
+                f"={self.batch_size}."
+            )
+
+        if dim >= self.batch_dims or self.batch_size[dim] != 1:
+            return self
+
+        return _SqueezedTensorDict(
+            source=self,
+            custom_op="squeeze",
+            inv_op="unsqueeze",
+            custom_op_kwargs={"dim": dim},
+            inv_op_kwargs={"dim": dim},
+        )
+
+    @overload
+    def reshape(self, *shape: int):
+        ...
+
+    @overload
+    def reshape(self, shape: list | tuple):
+        ...
+
+    @abc.abstractmethod
+    def reshape(
+        self,
+        *args,
+        **kwargs,
+    ) -> T:
+        """Returns a contiguous, reshaped tensor of the desired shape.
+
+        Args:
+            *shape (int): new shape of the resulting tensordict.
+
+        Returns:
+            A TensorDict with reshaped keys
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(12).reshape(3, 4),
+            ... }, batch_size=[3, 4])
+            >>> td = td.reshape(12)
+            >>> print(td['x'])
+            torch.Tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11])
+
+        """
+        ...
+
+    @classmethod
+    def stack(cls, input, dim=0, *, out=None):
+        """Stacks tensordicts into a single tensordict along the given dimension.
+
+        This call is equivalent to calling :func:`torch.stack` but is compatible with torch.compile.
+
+        """
+        from tensordict._torch_func import _stack
+
+        if not _is_tensor_collection(type(input[0])):
+            return torch.stack(input, dim, out=out)
+        return _stack(input, dim, out=out)
+
+    @classmethod
+    def cat(cls, input, dim=0, *, out=None):
+        """Concatenates tensordicts into a single tensordict along the given dimension.
+
+        This call is equivalent to calling :func:`torch.cat` but is compatible with torch.compile.
+
+        """
+        from tensordict._torch_func import _cat
+
+        if not _is_tensor_collection(type(input[0])):
+            return torch.cat(input, dim, out=out)
+        return _cat(input, dim, out=out)
+
+    @classmethod
+    def lazy_stack(cls, input, dim=0, *, out=None):
+        """Creates a lazy stack of tensordicts.
+
+        See :meth:`~tensordict.LazyStackTensorDict.lazy_stack` for details.
+        """
+        from tensordict._lazy import LazyStackedTensorDict
+
+        return LazyStackedTensorDict.lazy_stack(input, dim=dim, out=out)
+
+    @classmethod
+    def maybe_dense_stack(cls, input, dim=0, *, out=None):
+        """Attempts to make a dense stack of tensordicts, and falls back on lazy stack when required..
+
+        See :meth:`~tensordict.LazyStackTensorDict.maybe_dense_stack` for details.
+        """
+        from tensordict._lazy import LazyStackedTensorDict
+
+        return LazyStackedTensorDict.maybe_dense_stack(input, dim=dim, out=out)
+
+    @abc.abstractmethod
+    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
+        """Splits each tensor in the TensorDict with the specified size in the given dimension, like `torch.split`.
+
+        Returns a list of ``TensorDict`` instances with the view of split chunks of items.
+
+        Args:
+            split_size (int or List(int)): size of a single chunk or list of sizes for each chunk.
+            dim (int): dimension along which to split the tensor.
+
+        Returns:
+            A list of TensorDict with specified size in given dimension.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'x': torch.arange(12).reshape(3, 4),
+            ... }, batch_size=[3, 4])
+            >>> td0, td1 = td.split([1, 2], dim=0)
+            >>> print(td0['x'])
+            torch.Tensor([[0, 1, 2, 3]])
+        """
+        ...
+
+    def gather(self, dim: int, index: Tensor, out: T | None = None) -> T:
+        """Gathers values along an axis specified by `dim`.
+
+        Args:
+            dim (int): the dimension along which collect the elements
+            index (torch.Tensor): a long tensor which number of dimension matches
+                the one of the tensordict with only one dimension differring between
+                the two (the gathering dimension). Its elements refer to the
+                index to be gathered along the required dimension.
+            out (TensorDictBase, optional): a destination tensordict. It must
+                have the same shape as the index.
+
+        Examples:
+            >>> td = TensorDict(
+            ...     {"a": torch.randn(3, 4, 5),
+            ...      "b": TensorDict({"c": torch.zeros(3, 4, 5)}, [3, 4, 5])},
+            ...     [3, 4])
+            >>> index = torch.randint(4, (3, 2))
+            >>> td_gather = td.gather(dim=1, index=index)
+            >>> print(td_gather)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([3, 2, 5]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3, 2, 5]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3, 2]),
+                device=None,
+                is_shared=False)
+
+        Gather keeps the dimension names.
+
+        Examples:
+            >>> td.names = ["a", "b"]
+            >>> td_gather = td.gather(dim=1, index=index)
+            >>> td_gather.names
+            ["a", "b"]
+        """
+        return torch.gather(self, dim, index, out=out)
+
+    @overload
+    def view(self, *shape: int):
+        ...
+
+    @overload
+    def view(self, shape: torch.Size):
+        ...
+
+    @abc.abstractmethod
+    def _view(
+        self,
+        *args,
+        **kwargs,
+    ) -> T:
+        ...
+
+    @as_decorator()
+    def view(
+        self,
+        *shape: int,
+        size: list | tuple | torch.Size | None = None,
+    ):
+        """Returns a tensordict with views of the tensors according to a new shape, compatible with the tensordict batch_size.
+
+        Args:
+            *shape (int): new shape of the resulting tensordict.
+            size: iterable
+
+        Returns:
+            a new tensordict with the desired batch_size.
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3,4,5),
+            ...    'b': torch.zeros(3,4,10,1)}, batch_size=torch.Size([3, 4]))
+            >>> td_view = td.view(12)
+            >>> print(td_view.get("a").shape)  # torch.Size([12, 5])
+            >>> print(td_view.get("b").shape)  # torch.Size([12, 10, 1])
+            >>> td_view = td.view(-1, 4, 3)
+            >>> print(td_view.get("a").shape)  # torch.Size([1, 4, 3, 5])
+            >>> print(td_view.get("b").shape)  # torch.Size([1, 4, 3, 10, 1])
+
+        """
+        _lazy_legacy = lazy_legacy()
+
+        if _lazy_legacy:
+            return self._legacy_view(*shape, size=size)
+        else:
+            result = self._view(size=size) if size is not None else self._view(*shape)
+            if result._is_shared or result._is_memmap:
+                result.lock_()
+            return result
+
+    def _legacy_view(
+        self,
+        *shape: int,
+        size: list | tuple | torch.Size | None = None,
+    ) -> T:
+        if len(shape) == 0 and size is not None:
+            return self.view(*size)
+        elif len(shape) == 1 and isinstance(shape[0], (list, tuple, torch.Size)):
+            return self.view(*shape[0])
+        elif not isinstance(shape, torch.Size):
+            shape = infer_size_impl(shape, self.numel())
+            shape = torch.Size(shape)
+        if shape == self.shape:
+            return self
+        from tensordict._lazy import _ViewedTensorDict
+
+        return _ViewedTensorDict(
+            source=self,
+            custom_op="view",
+            inv_op="view",
+            custom_op_kwargs={"size": shape},
+            inv_op_kwargs={"size": self.batch_size},
+        )
+
+    @as_decorator()
+    def transpose(self, dim0, dim1):
+        """Returns a tensordict that is a transposed version of input. The given dimensions ``dim0`` and ``dim1`` are swapped.
+
+        In-place or out-place modifications of the transposed tensordict will
+        impact the original tensordict too as the memory is shared and the operations
+        are mapped back on the original tensordict.
+
+        Examples:
+            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
+            >>> tensordict_transpose = tensordict.transpose(0, 1)
+            >>> print(tensordict_transpose.shape)
+            torch.Size([4, 3])
+            >>> tensordict_transpose.set("b",, torch.randn(4, 3))
+            >>> print(tensordict.get("b").shape)
+            torch.Size([3, 4])
+        """
+        _lazy_legacy = lazy_legacy()
+
+        if _lazy_legacy:
+            return self._legacy_transpose(dim0, dim1)
+        else:
+            ndim = self.ndim
+            if dim0 < 0:
+                dim0 = ndim + dim0
+            if dim1 < 0:
+                dim1 = ndim + dim1
+            if dim0 < 0 or dim1 < 0 or dim0 >= ndim or dim1 >= ndim:
+                raise ValueError(
+                    "dim0 and dim1 must be within the range of the number of dimensions."
+                )
+            dim0, dim1 = min(dim0, dim1), max(dim0, dim1)
+            if dim0 == dim1:
+                return self
+            result = self._transpose(dim0, dim1)
+            if result._is_shared or result._is_memmap:
+                result.lock_()
+            return result
+
+    @abc.abstractmethod
+    def _transpose(self, dim0, dim1):
+        ...
+
+    def _legacy_transpose(self, dim0, dim1):
+        if dim0 < 0:
+            dim0 = self.ndim + dim0
+        if dim1 < 0:
+            dim1 = self.ndim + dim1
+        if any((dim0 < 0, dim1 < 0)):
+            raise ValueError(
+                "The provided dimensions are incompatible with the tensordict batch-size."
+            )
+        if dim0 == dim1:
+            return self
+        from tensordict._lazy import _TransposedTensorDict
+
+        return _TransposedTensorDict(
+            source=self,
+            custom_op="transpose",
+            inv_op="transpose",
+            custom_op_kwargs={"dim0": dim0, "dim1": dim1},
+            inv_op_kwargs={"dim0": dim0, "dim1": dim1},
+        )
+
+    @overload
+    def permute(self, *dims: int):
+        ...
+
+    @overload
+    def permute(self, dims: list | tuple):
+        ...
+
+    @as_decorator()
+    def permute(self, *args, **kwargs):
+        """Returns a view of a tensordict with the batch dimensions permuted according to dims.
+
+        Args:
+            *dims_list (int): the new ordering of the batch dims of the tensordict. Alternatively,
+                a single iterable of integers can be provided.
+            dims (list of int): alternative way of calling permute(...).
+
+        Returns:
+            a new tensordict with the batch dimensions in the desired order.
+
+        Examples:
+            >>> tensordict = TensorDict({"a": torch.randn(3, 4, 5)}, [3, 4])
+            >>> print(tensordict.permute([1, 0]))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+            >>> print(tensordict.permute(1, 0))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+            >>> print(tensordict.permute(dims=[1, 0]))
+            PermutedTensorDict(
+                source=TensorDict(
+                    fields={
+                        a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32)},
+                    batch_size=torch.Size([3, 4]),
+                    device=cpu,
+                    is_shared=False),
+                op=permute(dims=[1, 0]))
+        """
+        _lazy_legacy = lazy_legacy()
+
+        if _lazy_legacy:
+            return self._legacy_permute(*args, **kwargs)
+        else:
+            result = self._permute(*args, **kwargs)
+            if result._is_shared or result._is_memmap:
+                result.lock_()
+            return result
+
+    @abc.abstractmethod
+    def _permute(
+        self,
+        *args,
+        **kwargs,
+    ):
+        ...
+
+    def _legacy_permute(
+        self,
+        *dims_list: int,
+        dims: list[int] | None = None,
+    ) -> T:
+        if len(dims_list) == 0:
+            dims_list = dims
+        elif len(dims_list) == 1 and not isinstance(dims_list[0], int):
+            dims_list = dims_list[0]
+        if len(dims_list) != len(self.shape):
+            raise RuntimeError(
+                f"number of dims don't match in permute (got {len(dims_list)}, expected {len(self.shape)}"
+            )
+
+        if not len(dims_list) and not self.batch_dims:
+            return self
+        if np.array_equal(dims_list, range(self.batch_dims)):
+            return self
+        min_dim, max_dim = -self.batch_dims, self.batch_dims - 1
+        seen = [False for dim in range(max_dim + 1)]
+        for idx in dims_list:
+            if idx < min_dim or idx > max_dim:
+                raise IndexError(
+                    f"dimension out of range (expected to be in range of [{min_dim}, {max_dim}], but got {idx})"
+                )
+            if seen[idx]:
+                raise RuntimeError("repeated dim in permute")
+            seen[idx] = True
+
+        from tensordict._lazy import _PermutedTensorDict
+
+        return _PermutedTensorDict(
+            source=self,
+            custom_op="permute",
+            inv_op="permute",
+            custom_op_kwargs={"dims": list(map(int, dims_list))},
+            inv_op_kwargs={"dims": list(map(int, dims_list))},
+        )
+
+    # Cache functionality
+    def _erase_cache(self):
+        self._cache = None
+
+    # Dim names functionality
+    @property
+    @abc.abstractmethod
+    def names(self):
+        """The dimension names of the tensordict.
+
+        The names can be set at construction time using the ``names`` argument.
+
+        See also :meth:`~.refine_names` for details on how to set the names after
+        construction.
+        """
+        ...
+
+    @abc.abstractmethod
+    def _erase_names(self):
+        """Erases the dimension names from a tensordict."""
+        ...
+
+    @abc.abstractmethod
+    def _rename_subtds(self, value):
+        """Renames all the sub-tensordicts dimension according to value.
+
+        If value has less dimensions than the TD, the rest is just assumed to be None.
+        """
+        ...
+
+    def _check_dim_name(self, name):
+        if name is None:
+            return False
+        if self._has_names() and name in self.names:
+            return True
+        for key in self.keys():
+            if _is_tensor_collection(self.entry_class(key)):
+                if self._get_str(key, NO_DEFAULT)._check_dim_name(name):
+                    return True
+        else:
+            return False
+
+    def refine_names(self, *names):
+        """Refines the dimension names of self according to names.
+
+        Refining is a special case of renaming that “lifts” unnamed dimensions.
+        A None dim can be refined to have any name; a named dim can only be
+        refined to have the same name.
+
+        Because named tensors can coexist with unnamed tensors, refining names
+        gives a nice way to write named-tensor-aware code that works with both
+        named and unnamed tensors.
+
+        names may contain up to one Ellipsis (...). The Ellipsis is expanded
+        greedily; it is expanded in-place to fill names to the same length as
+        self.dim() using names from the corresponding indices of self.names.
+
+        Returns: the same tensordict with dimensions named according to the input.
+
+        Examples:
+            >>> td = TensorDict({}, batch_size=[3, 4, 5, 6])
+            >>> tdr = td.refine_names(None, None, None, "d")
+            >>> assert tdr.names == [None, None, None, "d"]
+            >>> tdr = td.refine_names("a", None, None, "d")
+            >>> assert tdr.names == ["a", None, None, "d"]
+
+        """
+        # replace ellipsis if any
+        names_copy = copy(names)
+        if any(name is Ellipsis for name in names):
+            ellipsis_name = [NO_DEFAULT for _ in range(self.ndim - len(names) + 1)]
+            names = []
+            for name in names_copy:
+                if name is Ellipsis:
+                    names += ellipsis_name
+                else:
+                    names.append(name)
+        # check that the names that are set are either None or identical
+        curr_names = self.names
+        for i, name in enumerate(names):
+            if name is NO_DEFAULT:
+                # whatever value is ok
+                names[i] = curr_names[i]
+                continue
+            else:
+                if curr_names[i] is None:
+                    continue
+                if self.names[i] == name:
+                    continue
+                else:
+                    raise RuntimeError(
+                        f"refine_names: cannot coerce TensorDict names {self.names} with {names_copy}."
+                    )
+        self.names = names
+        # we also need to rename the sub-tensordicts
+        # self._rename_subtds(self.names)
+        return self
+
+    def rename(self, *names, **rename_map):
+        """Returns a clone of the tensordict with dimensions renamed.
+
+        Examples:
+            >>> td = TensorDict({}, batch_size=[1, 2, 3 ,4])
+            >>> td.names = list("abcd")
+            >>> td_rename = td.rename(c="g")
+            >>> assert td_rename.names == list("abgd")
+
+        """
+        clone = self.clone(recurse=False)
+        if len(names) == 1 and names[0] is None:
+            clone.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. Only one is accepted."
+            )
+        elif not rename_map and not names:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            cnames = list(clone.names)
+            for i, name in enumerate(cnames):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    cnames[i] = new_name
+            clone.names = cnames
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+        else:
+            clone.names = names
+        return clone
+
+    def rename_(self, *names, **rename_map):
+        """Same as :meth:`~.rename`, but executes the renaming in-place.
+
+        Examples:
+            >>> td = TensorDict({}, batch_size=[1, 2, 3 ,4])
+            >>> td.names = list("abcd")
+            >>> assert td.rename_(c="g")
+            >>> assert td.names == list("abgd")
+        """
+        if len(names) == 1 and names[0] is None:
+            self.names = None
+        if rename_map and names:
+            raise ValueError(
+                "Passed both a name map and a name list. " "Only one is accepted."
+            )
+        elif not rename_map and not names and self.batch_dims:
+            raise ValueError(
+                "Neither a name map nor a name list was passed. "
+                "Only one is accepted."
+            )
+        elif rename_map:
+            cnames = list(self.names)
+            for i, name in enumerate(cnames):
+                new_name = rename_map.pop(name, NO_DEFAULT)
+                if new_name is not NO_DEFAULT:
+                    cnames[i] = new_name
+            if rename_map:
+                raise ValueError(
+                    f"Some names to be renamed were not part of the tensordict names: {rename_map.keys()} vs {self.names}."
+                )
+            self.names = cnames
+        else:
+            self.names = names
+        return self
+
+    @abc.abstractmethod
+    def _has_names(self):
+        ...
+
+    # Device functionality: device is optional. If provided, it will enforce
+    # all data is on the same device
+    @property
+    @abc.abstractmethod
+    def device(self) -> torch.device | None:
+        """Device of a TensorDict.
+
+        If the TensorDict has a specified device, all
+        its tensors (incl. nested ones) must live on the same device.
+        If the TensorDict device is ``None``, different values can be located
+        on different devices.
+
+        Returns:
+            torch.device object indicating the device where the tensors
+            are placed, or None if TensorDict does not have a device.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     "cpu": torch.randn(3, device='cpu'),
+            ...     "cuda": torch.randn(3, device='cuda'),
+            ... }, batch_size=[], device=None)
+            >>> td['cpu'].device
+            device(type='cpu')
+            >>> td['cuda'].device
+            device(type='cuda')
+            >>> td = TensorDict({
+            ...     "x": torch.randn(3, device='cpu'),
+            ...     "y": torch.randn(3, device='cuda'),
+            ... }, batch_size=[], device='cuda')
+            >>> td['x'].device
+            device(type='cuda')
+            >>> td['y'].device
+            device(type='cuda')
+            >>> td = TensorDict({
+            ...     "x": torch.randn(3, device='cpu'),
+            ...     "y": TensorDict({'z': torch.randn(3, device='cpu')}, batch_size=[], device=None),
+            ... }, batch_size=[], device='cuda')
+            >>> td['x'].device
+            device(type='cuda')
+            >>> td['y'].device # nested tensordicts are also mapped onto the appropriate device.
+            device(type='cuda')
+            >>> td['y', 'x'].device
+            device(type='cuda')
+
+        """
+        ...
+
+    @device.setter
+    @abc.abstractmethod
+    def device(self, value: DeviceType) -> None:
+        ...
+
+    @lock_blocked
+    def clear(self) -> T:
+        """Erases the content of the tensordict."""
+        for key in list(self.keys()):
+            del self[key]
+        return self
+
+    @classmethod
+    def fromkeys(cls, keys: List[NestedKey], value: Any = 0):
+        """Creates a tensordict from a list of keys and a single value.
+
+        Args:
+            keys (list of NestedKey): An iterable specifying the keys of the new dictionary.
+            value (compatible type, optional): The value for all keys. Defaults to ``0``.
+        """
+        from tensordict._td import TensorDict
+
+        return TensorDict(dict.fromkeys(keys, value), batch_size=[])
+
+    @abc.abstractmethod
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        """Removes the item that was last inserted into the TensorDict.
+
+        ``popitem`` will only return non-nested values.
+        """
+        ...
+
+    def clear_device_(self) -> T:
+        """Clears the device of the tensordict.
+
+        Returns: self
+
+        """
+        self._device = None
+        for value in self.values():
+            if _is_tensor_collection(value.__class__):
+                value.clear_device_()
+        return self
+
+    @abc.abstractmethod
+    def pin_memory(self) -> T:
+        """Calls :meth:`~torch.Tensor.pin_memory` on the stored tensors."""
+        ...
+
+    def cpu(self) -> T:
+        """Casts a tensordict to CPU."""
+        return self.to("cpu")
+
+    def cuda(self, device: int = None) -> T:
+        """Casts a tensordict to a cuda device (if not already on it).
+
+        Args:
+            device (int, optional): if provided, the cuda device on which the
+                tensor should be cast.
+
+        """
+        if device is None:
+            return self.to(torch.device("cuda"))
+        return self.to(f"cuda:{device}")
+
+    @property
+    def is_cuda(self):
+        return self.device is not None and self.device.type == "cuda"
+
+    @property
+    def is_cpu(self):
+        return self.device is not None and self.device.type == "cpu"
+
+    # Serialization functionality
+    def state_dict(
+        self,
+        destination=None,
+        prefix="",
+        keep_vars=False,
+        flatten=False,
+    ) -> OrderedDict[str, Any]:
+        """Produces a state_dict from the tensordict.
+
+        The structure of the state-dict will still be nested, unless ``flatten`` is set to ``True``.
+
+        A tensordict state-dict contains all the tensors and meta-data needed
+        to rebuild the tensordict (names are currently not supported).
+
+        Args:
+            destination (dict, optional): If provided, the state of tensordict will
+                be updated into the dict and the same object is returned.
+                Otherwise, an ``OrderedDict`` will be created and returned.
+                Default: ``None``.
+            prefix (str, optional): a prefix added to tensor
+                names to compose the keys in state_dict. Default: ``''``.
+            keep_vars (bool, optional): by default the :class:`torch.Tensor` items
+                returned in the state dict are detached from autograd. If it's
+                set to ``True``, detaching will not be performed.
+                Default: ``False``.
+            flatten (bool, optional): whether the structure should be flattened
+                with the ``"."`` character or not.
+                Defaults to ``False``.
+
+        Examples:
+            >>> data = TensorDict({"1": 1, "2": 2, "3": {"3": 3}}, [])
+            >>> sd = data.state_dict()
+            >>> print(sd)
+            OrderedDict([('1', tensor(1)), ('2', tensor(2)), ('3', OrderedDict([('3', tensor(3)), ('__batch_size', torch.Size([])), ('__device', None)])), ('__batch_size', torch.Size([])), ('__device', None)])
+            >>> sd = data.state_dict(flatten=True)
+            OrderedDict([('1', tensor(1)), ('2', tensor(2)), ('3.3', tensor(3)), ('__batch_size', torch.Size([])), ('__device', None)])
+
+        """
+        out = collections.OrderedDict()
+        source = self
+        if flatten:
+            source = source.flatten_keys(".")
+        for key, item in source.items():
+            if not _is_tensor_collection(item.__class__):
+                if not keep_vars:
+                    out[prefix + key] = item.detach().clone()
+                else:
+                    out[prefix + key] = item
+            else:
+                out[prefix + key] = item.state_dict(keep_vars=keep_vars)
+        if "__batch_size" in out:
+            raise KeyError(
+                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
+            )
+        if "__device" in out:
+            raise KeyError(
+                "Cannot retrieve the state_dict of a TensorDict with `'__batch_size'` key"
+            )
+        out[prefix + "__batch_size"] = source.batch_size
+        out[prefix + "__device"] = source.device
+        if destination is not None:
+            destination.update(out)
+            return destination
+        return out
+
+    def load_state_dict(
+        self,
+        state_dict: OrderedDict[str, Any],
+        strict=True,
+        assign=False,
+        from_flatten=False,
+    ) -> T:
+        """Loads a state-dict, formatted as in :meth:`~.state_dict`, into the tensordict.
+
+        Args:
+            state_dict (OrderedDict): the state_dict of to be copied.
+            strict (bool, optional): whether to strictly enforce that the keys
+                in :attr:`state_dict` match the keys returned by this tensordict's
+                :meth:`torch.nn.Module.state_dict` function. Default: ``True``
+            assign (bool, optional): whether to assign items in the state
+                dictionary to their corresponding keys in the tensordict instead
+                of copying them inplace into the tensordict's current tensors.
+                When ``False``, the properties of the tensors in the current
+                module are preserved while when ``True``, the properties of the
+                Tensors in the state dict are preserved.
+                Default: ``False``
+            from_flatten (bool, optional): if ``True``, the input state_dict is
+                assumed to be flattened.
+                Defaults to ``False``.
+
+        Examples:
+            >>> data = TensorDict({"1": 1, "2": 2, "3": {"3": 3}}, [])
+            >>> data_zeroed = TensorDict({"1": 0, "2": 0, "3": {"3": 0}}, [])
+            >>> sd = data.state_dict()
+            >>> data_zeroed.load_state_dict(sd)
+            >>> print(data_zeroed["3", "3"])
+            tensor(3)
+            >>> # with flattening
+            >>> data_zeroed = TensorDict({"1": 0, "2": 0, "3": {"3": 0}}, [])
+            >>> data_zeroed.load_state_dict(data.state_dict(flatten=True), from_flatten=True)
+            >>> print(data_zeroed["3", "3"])
+            tensor(3)
+
+
+        """
+        if from_flatten:
+            self_flatten = self.flatten_keys(".")
+            self_flatten.load_state_dict(state_dict, strict=strict, assign=assign)
+            if not assign:
+                # modifications are done in-place so we should be fine returning self
+                return self
+            else:
+                # run a check over keys, if we any key with a '.' in name we're doomed
+                DOT_ERROR = "Cannot use load_state_dict(..., from_flatten=True, assign=True) when some keys contain a dot character."
+                for key in self.keys(True, True):
+                    if isinstance(key, tuple):
+                        for subkey in key:
+                            if "." in subkey:
+                                raise RuntimeError(DOT_ERROR)
+                    elif "." in key:
+                        raise RuntimeError(DOT_ERROR)
+                return self.update(self_flatten.unflatten_keys("."))
+
+        # copy since we'll be using pop
+        state_dict = copy(state_dict)
+        batch_size = state_dict.pop("__batch_size")
+        device = state_dict.pop("__device", None)
+
+        if strict and set(state_dict.keys()) != set(self.keys()):
+            set_sd = set(state_dict.keys())
+            set_td = set(self.keys())
+
+            # if there are keys in state-dict that point to an empty tensordict
+            # or if the local tensordicts are empty, we can skip
+            def _is_empty_dict(sd, key=None):
+                if key is not None:
+                    if not isinstance(sd[key], dict):
+                        return False
+                    return _is_empty_dict(sd[key])
+                for key, item in sd.items():
+                    if key in ("__batch_size", "__device"):
+                        continue
+                    if isinstance(item, dict):
+                        if not _is_empty_dict(item):
+                            return False
+                        continue
+                    return False
+                else:
+                    return True
+
+            def check_is_empty(target, key):
+                item = target.get(key)
+                if not is_tensor_collection(item) or not item.is_empty():
+                    return False
+                return True
+
+            if not all(check_is_empty(self, key) for key in set_td - set_sd) or not all(
+                _is_empty_dict(state_dict, key) for key in set_sd - set_td
+            ):
+                raise RuntimeError(
+                    "Cannot load state-dict because the key sets don't match: got "
+                    f"state_dict extra keys \n{set_sd - set_td}\n and tensordict extra keys\n{set_td - set_sd}\n"
+                )
+
+        self.batch_size = batch_size
+        if device is not None and self.device is not None and device != self.device:
+            raise RuntimeError("Loading data from another device is not yet supported.")
+
+        for key, item in state_dict.items():
+            if isinstance(item, dict):
+                dest = self.get(key, default=None)
+                if dest is None:
+                    dest = self.empty()
+                dest.load_state_dict(item, assign=assign, strict=strict)
+                self.set(
+                    key,
+                    dest,
+                    inplace=not assign,
+                )
+            else:
+                self.set(key, item, inplace=not assign)
+        return self
+
+    def is_shared(self) -> bool:
+        """Checks if tensordict is in shared memory.
+
+        If a TensorDict instance is in shared memory, it is locked (entries cannot
+        be renamed, removed or added). If a ``TensorDict`` is created with
+        tensors that are all in shared memory, this does __not__ mean that ``is_shared``
+        will return ``True`` (as a new tensor may or may not be in shared memory).
+        Only if one calls `tensordict.share_memory_()` or places the tensordict
+        on a device where the content is shared by default (eg, ``"cuda"``)
+        will the tensordict be considered in shared memory.
+
+        This is always ``True`` for tensordicts on a CUDA device.
+
+        """
+        if self.device and not self._is_memmap:
+            return self.device.type == "cuda" or self._is_shared
+        return self._is_shared
+
+    def is_memmap(self) -> bool:
+        """Checks if tensordict is memory-mapped.
+
+        If a TensorDict instance is memory-mapped, it is locked (entries cannot
+        be renamed, removed or added). If a ``TensorDict`` is created with
+        tensors that are all memory-mapped, this does __not__ mean that ``is_memmap``
+        will return ``True`` (as a new tensor may or may not be memory-mapped).
+        Only if one calls `tensordict.memmap_()` will the tensordict be
+        considered as memory-mapped.
+
+        This is always ``True`` for tensordicts on a CUDA device.
+
+        """
+        return self._is_memmap
+
+    @abc.abstractmethod
+    def share_memory_(self) -> T:
+        """Places all the tensors in shared memory.
+
+        The TensorDict is then locked, meaning that any writing operations that
+        isn't in-place will throw an exception (eg, rename, set or remove an
+        entry).
+        Conversely, once the tensordict is unlocked, the share_memory attribute
+        is turned to ``False``, because cross-process identity is not
+        guaranteed anymore.
+
+        Returns:
+            self
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None,
+        copy_existing: bool,
+        executor,
+        futures,
+        inplace,
+        like,
+        share_non_tensor,
+    ) -> T:
+        ...
+
+    @property
+    def saved_path(self):
+        """Returns the path where a memmap saved TensorDict is being stored.
+
+        This argument valishes as soon as is_memmap() returns ``False`` (e.g., when the tensordict is unlocked).
+        """
+        if self.is_memmap():
+            path = self._memmap_prefix
+            return path
+        raise AttributeError(
+            f"The tensordict has no saved path (memmap={self.is_memmap()}, path={self._memmap_prefix})."
+        )
+
+    def memmap_(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        *,
+        num_threads: int = 0,
+        return_early: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+        """Writes all tensors onto a corresponding memory-mapped Tensor, in-place.
+
+        Args:
+            prefix (str): directory prefix where the memory-mapped tensors will
+                be stored. The directory tree structure will mimic the tensordict's.
+            copy_existing (bool): If False (default), an exception will be raised if an
+                entry in the tensordict is already a tensor stored on disk
+                with an associated file, but is not saved in the correct
+                location according to prefix.
+                If ``True``, any existing Tensor will be copied to the new location.
+
+        Keyword Args:
+            num_threads (int, optional): the number of threads used to write the memmap
+                tensors. Defaults to `0`.
+            return_early (bool, optional): if ``True`` and ``num_threads>0``,
+                the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
+
+        The TensorDict is then locked, meaning that any writing operations that
+        isn't in-place will throw an exception (eg, rename, set or remove an
+        entry).
+        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            self if ``return_early=False``, otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
+
+        Note:
+            Serialising in this fashion might be slow with deeply nested tensordicts, so
+            it is not recommended to call this method inside a training loop.
+        """
+        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
+        if num_threads > 1:
+            with (
+                ThreadPoolExecutor(max_workers=num_threads)
+                if not return_early
+                else contextlib.nullcontext()
+            ) as executor:
+                if return_early:
+                    executor = ThreadPoolExecutor(max_workers=num_threads)
+                futures = []
+                result = self._memmap_(
+                    prefix=prefix,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=True,
+                    like=False,
+                    share_non_tensor=share_non_tensor,
+                )
+                if not return_early:
+                    concurrent.futures.wait(futures)
+                    return result
+                else:
+                    return TensorDictFuture(futures, result)
+        return self._memmap_(
+            prefix=prefix,
+            copy_existing=copy_existing,
+            inplace=True,
+            futures=None,
+            executor=None,
+            like=False,
+            share_non_tensor=share_non_tensor,
+        ).lock_()
+
+    @abc.abstractmethod
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        """Creates an empty memory-mapped tensor given a shape and possibly a dtype.
+
+        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
+            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
+
+        Writing an existing entry will result in an error.
+
+        Args:
+            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
+                exception is raised.
+            shape (torch.Size or equivalent, torch.Tensor for nested tensors): the shape of the tensor to write.
+
+        Keyword arguments:
+            dtype (torch.dtype, optional): the dtype of the new tensor.
+
+        Returns:
+            A new memory mapped tensor.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        """Creates an empty memory-mapped tensor given a storage, a shape and possibly a dtype.
+
+        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
+            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
+
+        .. note:: If the storage has a filename associated, it must match the new filename for the file.
+            If it has not a filename associated but the tensordict has an associated path, this will result in an
+            exception.
+
+        Args:
+            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
+                exception is raised.
+            storage (torch.UntypedStorage): the storage to use for the new MemoryMappedTensor. Must be a physical memory
+                storage.
+            shape (torch.Size or equivalent, torch.Tensor for nested tensors): the shape of the tensor to write.
+
+        Keyword arguments:
+            dtype (torch.dtype, optional): the dtype of the new tensor.
+
+        Returns:
+            A new memory mapped tensor with the given storage.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        """Creates an empty memory-mapped tensor given a tensor.
+
+        .. warning:: This method is not lock-safe by design. A memory-mapped TensorDict instance present on multiple nodes
+            will need to be updated using the method :meth:`~tensordict.TensorDictBase.memmap_refresh_`.
+
+        This method always copies the storage content if ``copy_data`` is ``True`` (i.e., the storage is not shared).
+
+        Args:
+            key (NestedKey): the key of the new entry to write. If the key is already present in the tensordict, an
+                exception is raised.
+            tensor (torch.Tensor): the tensor to replicate on physical memory.
+
+        Keyword arguments:
+            copy_data (bool, optionaL): if ``False``, the new tensor will share the metadata of the input such as
+                shape and dtype, but the content will be empty. Defaults to ``True``.
+
+        Returns:
+            A new memory mapped tensor with the given storage.
+
+        """
+        ...
+
+    def save(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        *,
+        num_threads: int = 0,
+        return_early: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+        """Saves the tensordict to disk.
+
+        This function is a proxy to :meth:`~.memmap`.
+        """
+        return self.memmap(
+            prefix=prefix,
+            copy_existing=copy_existing,
+            num_threads=num_threads,
+            return_early=return_early,
+            share_non_tensor=share_non_tensor,
+        )
+
+    dumps = save
+
+    def memmap(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        *,
+        num_threads: int = 0,
+        return_early: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+        """Writes all tensors onto a corresponding memory-mapped Tensor in a new tensordict.
+
+        Args:
+            prefix (str): directory prefix where the memory-mapped tensors will
+                be stored. The directory tree structure will mimic the tensordict's.
+            copy_existing (bool): If False (default), an exception will be raised if an
+                entry in the tensordict is already a tensor stored on disk
+                with an associated file, but is not saved in the correct
+                location according to prefix.
+                If ``True``, any existing Tensor will be copied to the new location.
+
+        Keyword Args:
+            num_threads (int, optional): the number of threads used to write the memmap
+                tensors. Defaults to `0`.
+            return_early (bool, optional): if ``True`` and ``num_threads>0``,
+                the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
+
+        The TensorDict is then locked, meaning that any writing operations that
+        isn't in-place will throw an exception (eg, rename, set or remove an
+        entry).
+        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            A new tensordict with the tensors stored on disk if ``return_early=False``,
+            otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
+
+        Note:
+            Serialising in this fashion might be slow with deeply nested tensordicts, so
+            it is not recommended to call this method inside a training loop.
+        """
+        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
+
+        if num_threads > 1:
+            with (
+                ThreadPoolExecutor(max_workers=num_threads)
+                if not return_early
+                else contextlib.nullcontext()
+            ) as executor:
+                if return_early:
+                    executor = ThreadPoolExecutor(max_workers=num_threads)
+                futures = []
+                result = self._memmap_(
+                    prefix=prefix,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=False,
+                    like=False,
+                    share_non_tensor=share_non_tensor,
+                )
+                if not return_early:
+                    concurrent.futures.wait(futures)
+                    return result
+                else:
+                    return TensorDictFuture(futures, result)
+
+        return self._memmap_(
+            prefix=prefix,
+            copy_existing=copy_existing,
+            inplace=False,
+            executor=None,
+            like=False,
+            futures=None,
+            share_non_tensor=share_non_tensor,
+        ).lock_()
+
+    def memmap_like(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        *,
+        num_threads: int = 0,
+        return_early: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+        """Creates a contentless Memory-mapped tensordict with the same shapes as the original one.
+
+        Args:
+            prefix (str): directory prefix where the memory-mapped tensors will
+                be stored. The directory tree structure will mimic the tensordict's.
+            copy_existing (bool): If False (default), an exception will be raised if an
+                entry in the tensordict is already a tensor stored on disk
+                with an associated file, but is not saved in the correct
+                location according to prefix.
+                If ``True``, any existing Tensor will be copied to the new location.
+
+        Keyword Args:
+            num_threads (int, optional): the number of threads used to write the memmap
+                tensors. Defaults to `0`.
+            return_early (bool, optional): if ``True`` and ``num_threads>0``,
+                the method will return a future of the tensordict.
+            share_non_tensor (bool, optional): if ``True``, the non-tensor data will be
+                shared between the processes and writing operation (such as inplace update
+                or set) on any of the workers within a single node will update the value
+                on all other workers. If the number of non-tensor leaves is high (e.g.,
+                sharing large stacks of non-tensor data) this may result in OOM or similar
+                errors. Defaults to ``False``.
+
+        The TensorDict is then locked, meaning that any writing operations that
+        isn't in-place will throw an exception (eg, rename, set or remove an
+        entry).
+        Once the tensordict is unlocked, the memory-mapped attribute is turned to ``False``,
+        because cross-process identity is not guaranteed anymore.
+
+        Returns:
+            A new ``TensorDict`` instance with data stored as memory-mapped tensors if ``return_early=False``,
+            otherwise a :class:`~tensordict.utils.TensorDictFuture` instance.
+
+        .. note:: This is the recommended method to write a set of large buffers
+            on disk, as :meth:`~.memmap_()` will copy the information, which can
+            be slow for large content.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     "a": torch.zeros((3, 64, 64), dtype=torch.uint8),
+            ...     "b": torch.zeros(1, dtype=torch.int64),
+            ... }, batch_size=[]).expand(1_000_000)  # expand does not allocate new memory
+            >>> buffer = td.memmap_like("/path/to/dataset")
+
+        """
+        prefix = Path(prefix) if prefix is not None else self._memmap_prefix
+        if num_threads > 1:
+            with (
+                ThreadPoolExecutor(max_workers=num_threads)
+                if not return_early
+                else contextlib.nullcontext()
+            ) as executor:
+                if return_early:
+                    executor = ThreadPoolExecutor(max_workers=num_threads)
+                futures = []
+                # we create an empty copy of self
+                # This is because calling MMapTensor.from_tensor(mmap_tensor) does nothing
+                # if both are in filesystem
+                input = self.apply(
+                    lambda x: torch.empty((), device=x.device, dtype=x.dtype).expand(
+                        x.shape
+                    )
+                )
+                result = input._memmap_(
+                    prefix=prefix,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=False,
+                    like=True,
+                    share_non_tensor=share_non_tensor,
+                )
+                if not return_early:
+                    concurrent.futures.wait(futures)
+                    return result
+                else:
+                    return TensorDictFuture(futures, result)
+        input = self.apply(
+            lambda x: torch.empty((), device=x.device, dtype=x.dtype).expand(x.shape)
+        )
+        return input._memmap_(
+            prefix=prefix,
+            copy_existing=copy_existing,
+            inplace=False,
+            like=True,
+            executor=None,
+            futures=None,
+            share_non_tensor=share_non_tensor,
+        ).lock_()
+
+    @classmethod
+    def load(cls, prefix: str | Path, *args, **kwargs) -> T:
+        """Loads a tensordict from disk.
+
+        This class method is a proxy to :meth:`~.load_memmap`.
+        """
+        return cls.load_memmap(prefix, *args, **kwargs)
+
+    def load_(self, prefix: str | Path, *args, **kwargs):
+        """Loads a tensordict from disk within the current tensordict.
+
+        This class method is a proxy to :meth:`~.load_memmap_`.
+        """
+        return self.load_memmap_(prefix, *args, **kwargs)
+
+    @classmethod
+    def load_memmap(
+        cls,
+        prefix: str | Path,
+        device: torch.device | None = None,
+        non_blocking: bool = False,
+        *,
+        out: TensorDictBase | None = None,
+    ) -> T:
+        """Loads a memory-mapped tensordict from disk.
+
+        Args:
+            prefix (str or Path to folder): the path to the folder where the
+                saved tensordict should be fetched.
+            device (torch.device or equivalent, optional): if provided, the
+                data will be asynchronously cast to that device.
+                Supports `"meta"` device, in which case the data isn't loaded
+                but a set of empty "meta" tensors are created. This is
+                useful to get a sense of the total model size and structure
+                without actually opening any file.
+            non_blocking (bool, optional): if ``True``, synchronize won't be
+                called after loading tensors on device. Defaults to ``False``.
+            out (TensorDictBase, optional): optional tensordict where the data
+                should be written.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> td = TensorDict.fromkeys(["a", "b", "c", ("nested", "e")], 0)
+            >>> td.memmap("./saved_td")
+            >>> td_load = TensorDict.load_memmap("./saved_td")
+            >>> assert (td == td_load).all()
+
+        This method also allows loading nested tensordicts.
+
+            >>> nested = TensorDict.load_memmap("./saved_td/nested")
+            >>> assert nested["e"] == 0
+
+        A tensordict can also be loaded on "meta" device or, alternatively,
+        as a fake tensor:
+            >>> import tempfile
+            >>> td = TensorDict({"a": torch.zeros(()), "b": {"c": torch.zeros(())}})
+            >>> with tempfile.TemporaryDirectory() as path:
+            ...     td.save(path)
+            ...     td_load = TensorDict.load_memmap(path, device="meta")
+            ...     print("meta:", td_load)
+            ...     from torch._subclasses import FakeTensorMode
+            ...     with FakeTensorMode():
+            ...         td_load = TensorDict.load_memmap(path)
+            ...         print("fake:", td_load)
+            meta: TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=meta, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([]), device=meta, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=meta,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=meta,
+                is_shared=False)
+            fake: TensorDict(
+                fields={
+                    a: FakeTensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: FakeTensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=cpu,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=cpu,
+                is_shared=False)
+
+        """
+        prefix = Path(prefix)
+
+        metadata = _load_metadata(prefix)
+        type_name = metadata["_type"]
+        if type_name != str(cls):
+            import tensordict
+
+            for other_cls in tensordict.base._ACCEPTED_CLASSES:
+                if str(other_cls) == type_name:
+                    return other_cls._load_memmap(prefix, metadata)
+            else:
+                raise RuntimeError(
+                    f"Could not find name {type_name} in {tensordict.base._ACCEPTED_CLASSES}. "
+                    f"Did you call _register_tensor_class(cls) on {type_name}?"
+                )
+        if device is not None:
+            device = torch.device(device)
+        out = cls._load_memmap(prefix, metadata, device=device, out=out)
+        if not non_blocking and device is not None and device != torch.device("meta"):
+            out._sync_all()
+        return out
+
+    def load_memmap_(
+        self,
+        prefix: str | Path,
+    ):
+        """Loads the content of a memory-mapped tensordict within the tensordict where ``load_memmap_`` is called.
+
+        See :meth:`~tensordict.TensorDictBase.load_memmap` for more info.
+        """
+        is_memmap = self.is_memmap()
+        with self.unlock_() if is_memmap else contextlib.nullcontext():
+            self.load_memmap(prefix=prefix, device=self.device, out=self)
+        if is_memmap:
+            self.memmap_()
+        return self
+
+    def memmap_refresh_(self):
+        """Refreshes the content of the memory-mapped tensordict if it has a :attr:`~tensordict.TensorDict.saved_path`.
+
+        This method will raise an exception if no path is associated with it.
+
+        """
+        if not self.is_memmap() or self._memmap_prefix is None:
+            raise RuntimeError(
+                "Cannot refresh a TensorDict that is not memory mapped or has no path associated."
+            )
+        return self.load_memmap_(prefix=self.saved_path)
+
+    @classmethod
+    @abc.abstractmethod
+    def _load_memmap(
+        cls,
+        prefix: Path,
+        metadata: dict,
+        device: torch.device | None = None,
+        *,
+        out=None,
+    ):
+        ...
+
+    # Key functionality: set, get, set_, set_at_, update, update_
+    @abc.abstractmethod
+    def entry_class(self, key: NestedKey) -> type:
+        """Returns the class of an entry, possibly avoiding a call to `isinstance(td.get(key), type)`.
+
+        This method should be preferred to ``tensordict.get(key).shape`` whenever
+        :meth:`.get` can be expensive to execute.
+
+        """
+        ...
+
+    def set(
+        self,
+        key: NestedKey,
+        item: CompatibleType,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        **kwargs: Any,
+    ) -> T:
+        """Sets a new key-value pair.
+
+        Args:
+            key (str, tuple of str): name of the key to be set.
+            item (torch.Tensor or equivalent, TensorDictBase instance): value
+                to be stored in the tensordict.
+            inplace (bool, optional): if ``True`` and if a key matches an existing
+                key in the tensordict, then the update will occur in-place
+                for that key-value pair. If inplace is ``True`` and
+                the entry cannot be found, it will be added. For a more restrictive
+                in-place operation, use :meth:`~.set_` instead.
+                Defaults to ``False``.
+
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict({}, batch_size[3, 4])
+            >>> td.set("x", torch.randn(3, 4))
+            >>> y = torch.randn(3, 4, 5)
+            >>> td.set("y", y, inplace=True) # works, even if 'y' is not present yet
+            >>> td.set("y", torch.zeros_like(y), inplace=True)
+            >>> assert (y==0).all() # y values are overwritten
+            >>> td.set("y", torch.ones(5), inplace=True) # raises an exception as shapes mismatch
+
+        """
+        key = _unravel_key_to_tuple(key)
+        # inplace is loose here, but for set_ it is constraining. We translate it
+        # to None to tell _set_str and others to drop it if the key isn't found
+        inplace = BEST_ATTEMPT_INPLACE if inplace else False
+        return self._set_tuple(
+            key, item, inplace=inplace, validated=False, non_blocking=non_blocking
+        )
+
+    @abc.abstractmethod
+    def _set_str(
+        self,
+        key: str,
+        value: Any,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
+        ...
+
+    @abc.abstractmethod
+    def _set_tuple(self, key, value, *, inplace, validated, non_blocking: bool):
+        ...
+
+    @lock_blocked
+    def set_non_tensor(self, key: NestedKey, value: Any):
+        """Registers a non-tensor value in the tensordict using :class:`tensordict.tensorclass.NonTensorData`.
+
+        The value can be retrieved using :meth:`TensorDictBase.get_non_tensor`
+        or directly using `get`, which will return the :class:`tensordict.tensorclass.NonTensorData`
+        object.
+
+        return: self
+
+        Examples:
+            >>> data = TensorDict({}, batch_size=[])
+            >>> data.set_non_tensor(("nested", "the string"), "a string!")
+            >>> assert data.get_non_tensor(("nested", "the string")) == "a string!"
+            >>> # regular `get` works but returns a NonTensorData object
+            >>> data.get(("nested", "the string"))
+            NonTensorData(
+                data='a string!',
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        key = unravel_key(key)
+        return self._set_non_tensor(key, value)
+
+    def _set_non_tensor(self, key: NestedKey, value: Any):
+        if isinstance(key, tuple):
+            if len(key) == 1:
+                return self._set_non_tensor(key[0], value)
+            sub_td = self._get_str(key[0], None)
+            if sub_td is None:
+                sub_td = self._create_nested_str(key[0])
+            sub_td._set_non_tensor(key[1:], value)
+            return self
+        from tensordict.tensorclass import NonTensorData
+
+        self._set_str(
+            key,
+            NonTensorData(
+                value,
+                batch_size=self.batch_size,
+                device=self.device,
+                names=self.names if self._has_names() else None,
+            ),
+            validated=True,
+            inplace=False,
+            non_blocking=False,
+        )
+        return self
+
+    def get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
+        """Gets a non-tensor value, if it exists, or `default` if the non-tensor value is not found.
+
+        This method is robust to tensor/TensorDict values, meaning that if the
+        value gathered is a regular tensor it will be returned too (although
+        this method comes with some overhead and should not be used out of its
+        natural scope).
+
+        See :meth:`~tensordict.TensorDictBase.set_non_tensor` for more information
+        on how to set non-tensor values in a tensordict.
+
+        Args:
+            key (NestedKey): the location of the NonTensorData object.
+            default (Any, optional): the value to be returned if the key cannot
+                be found.
+
+        Returns: the content of the :class:`tensordict.tensorclass.NonTensorData`,
+            or the entry corresponding to the ``key`` if it isn't a
+            :class:`tensordict.tensorclass.NonTensorData` (or ``default`` if the
+            entry cannot be found).
+
+        Examples:
+            >>> data = TensorDict({}, batch_size=[])
+            >>> data.set_non_tensor(("nested", "the string"), "a string!")
+            >>> assert data.get_non_tensor(("nested", "the string")) == "a string!"
+            >>> # regular `get` works but returns a NonTensorData object
+            >>> data.get(("nested", "the string"))
+            NonTensorData(
+                data='a string!',
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        key = unravel_key(key)
+        return self._get_non_tensor(key, default=default)
+
+    def _get_non_tensor(self, key: NestedKey, default=NO_DEFAULT):
+        if isinstance(key, tuple):
+            if len(key) == 1:
+                return self._get_non_tensor(key[0], default=default)
+            subtd = self._get_str(key[0], default=default)
+            if subtd is default:
+                return subtd
+            return subtd._get_non_tensor(key[1:], default=default)
+        value = self._get_str(key, default=default)
+
+        if is_non_tensor(value):
+            data = getattr(value, "data", None)
+            if data is None:
+                return value.tolist()
+            return data
+        return value
+
+    def filter_non_tensor_data(self) -> T:
+        """Filters out all non-tensor-data."""
+
+        def _filter(x):
+            if not is_non_tensor(x):
+                if is_tensor_collection(x):
+                    return x.filter_non_tensor_data()
+                return x
+
+        return self._apply_nest(_filter, call_on_nested=True, filter_empty=False)
+
+    def _convert_inplace(self, inplace, key):
+        if inplace is not False:
+            has_key = key in self.keys()
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
+                )
+            inplace = has_key
+        return inplace
+
+    def set_at_(
+        self,
+        key: NestedKey,
+        value: CompatibleType,
+        index: IndexType,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
+        """Sets the values in-place at the index indicated by ``index``.
+
+        Args:
+            key (str, tuple of str): key to be modified.
+            value (torch.Tensor): value to be set at the index `index`
+            index (int, tensor or tuple): index where to write the values.
+
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict({}, batch_size[3, 4])
+            >>> x = torch.randn(3, 4)
+            >>> td.set("x", x)
+            >>> td.set_at_("x", value=torch.ones(1, 4), index=slice(1))
+            >>> assert (x[0] == 1).all()
+        """
+        key = _unravel_key_to_tuple(key)
+        return self._set_at_tuple(
+            key, value, index, validated=False, non_blocking=non_blocking
+        )
+
+    @abc.abstractmethod
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking: bool):
+        ...
+
+    @abc.abstractmethod
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking: bool):
+        ...
+
+    def set_(
+        self,
+        key: NestedKey,
+        item: CompatibleType,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
+        """Sets a value to an existing key while keeping the original storage.
+
+        Args:
+            key (str): name of the value
+            item (torch.Tensor or compatible type, TensorDictBase): value to
+                be stored in the tensordict
+
+        Keyword Args:
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict({}, batch_size[3, 4])
+            >>> x = torch.randn(3, 4)
+            >>> td.set("x", x)
+            >>> td.set_("x", torch.zeros_like(x))
+            >>> assert (x == 0).all()
+
+        """
+        key = _unravel_key_to_tuple(key)
+        return self._set_tuple(
+            key, item, inplace=True, validated=False, non_blocking=non_blocking
+        )
+
+    # Stack functionality
+    @abc.abstractmethod
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        """Stacks a list of values onto an existing key while keeping the original storage.
+
+        Args:
+            key (str): name of the value
+            list_item (list of torch.Tensor): value to be stacked and stored in the tensordict.
+            dim (int): dimension along which the tensors should be stacked.
+
+        Returns:
+            self
+
+        """
+        ...
+
+    def _stack_onto_at_(
+        self,
+        key: NestedKey,
+        list_item: list[CompatibleType],
+        dim: int,
+        idx: IndexType,
+    ) -> T:
+        """Similar to _stack_onto_ but on a specific index. Only works with regular TensorDicts."""
+        raise RuntimeError(
+            f"Cannot call _stack_onto_at_ with {self.__class__.__name__}. "
+            "Make sure your sub-classed tensordicts are turned into regular tensordicts by calling to_tensordict() "
+            "before calling __getindex__ and stack."
+        )
+
+    def _default_get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        if default is not NO_DEFAULT:
+            return default
+        else:
+            # raise KeyError
+            raise KeyError(
+                _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
+            )
+
+    def get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        """Gets the value stored with the input key.
+
+        Args:
+            key (str, tuple of str): key to be queried. If tuple of str it is
+                equivalent to chained calls of getattr.
+            default: default value if the key is not found in the tensordict.
+
+        Examples:
+            >>> td = TensorDict({"x": 1}, batch_size=[])
+            >>> td.get("x")
+            tensor(1)
+            >>> td.get("y", default=None)
+            None
+        """
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR.format(key))
+        return self._get_tuple(key, default=default)
+
+    @abc.abstractmethod
+    def _get_str(self, key, default):
+        ...
+
+    @abc.abstractmethod
+    def _get_tuple(self, key, default):
+        ...
+
+    def get_at(
+        self, key: NestedKey, index: IndexType, default: CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        """Get the value of a tensordict from the key `key` at the index `idx`.
+
+        Args:
+            key (str, tuple of str): key to be retrieved.
+            index (int, slice, torch.Tensor, iterable): index of the tensor.
+            default (torch.Tensor): default value to return if the key is
+                not present in the tensordict.
+
+        Returns:
+            indexed tensor.
+
+        Examples:
+            >>> td = TensorDict({"x": torch.arange(3)}, batch_size=[])
+            >>> td.get_at("x", index=1)
+            tensor(1)
+
+        """
+        # TODO: check that this works with masks, and add to docstring
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR.format(key))
+        # must be a tuple
+        return self._get_at_tuple(key, index, default)
+
+    def _get_at_str(self, key, idx, default):
+        out = self._get_str(key, default)
+        if out is default:
+            return out
+        return out[idx]
+
+    def _get_at_tuple(self, key, idx, default):
+        out = self._get_tuple(key, default)
+        if out is default:
+            return out
+        return out[idx]
+
+    def get_item_shape(self, key: NestedKey):
+        """Returns the shape of the entry, possibly avoiding recurring to :meth:`~.get`."""
+        return _shape(self.get(key))
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        """Updates the TensorDict with values from either a dictionary or another TensorDict.
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): input data to be written
+                in self.
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set.
+                Defaults to ``False``.
+            inplace (bool, optional): if ``True`` and if a key matches an existing
+                key in the tensordict, then the update will occur in-place
+                for that key-value pair. If the entry cannot be found, it will be
+                added. Defaults to ``False``.
+
+        Keyword Args:
+            keys_to_update (sequence of NestedKeys, optional): if provided, only
+                the list of keys in ``key_to_update`` will be updated.
+                This is aimed at avoiding calls to
+                ``data_dest.update(data_src.select(*keys_to_update))``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict({}, batch_size=[3])
+            >>> a = torch.randn(3)
+            >>> b = torch.randn(3, 4)
+            >>> other_td = TensorDict({"a": a, "b": b}, batch_size=[])
+            >>> td.update(other_td, inplace=True) # writes "a" and "b" even though they can't be found
+            >>> assert td['a'] is other_td['a']
+            >>> other_td = other_td.clone().zero_()
+            >>> td.update(other_td)
+            >>> assert td['a'] is not other_td['a']
+
+        """
+        if input_dict_or_td is self:
+            # no op
+            return self
+        if keys_to_update is not None:
+            if len(keys_to_update) == 0:
+                return self
+            keys_to_update = unravel_key_list(keys_to_update)
+        for key, value in input_dict_or_td.items():
+            key = _unravel_key_to_tuple(key)
+            firstkey, subkey = key[0], key[1:]
+            if keys_to_update and not any(
+                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
+                for ktu in keys_to_update
+            ):
+                continue
+            target = self._get_str(firstkey, None)
+            if clone and hasattr(value, "clone"):
+                value = value.clone()
+            elif clone:
+                value = tree_map(torch.clone, value)
+            # the key must be a string by now. Let's check if it is present
+            if target is not None:
+                if _is_tensor_collection(type(target)):
+                    if subkey:
+                        sub_keys_to_update = _prune_selected_keys(
+                            keys_to_update, firstkey
+                        )
+                        target.update(
+                            {subkey: value},
+                            inplace=inplace,
+                            clone=clone,
+                            keys_to_update=sub_keys_to_update,
+                            non_blocking=non_blocking,
+                        )
+                        continue
+                    elif isinstance(value, (dict,)) or _is_tensor_collection(
+                        value.__class__
+                    ):
+                        from tensordict._lazy import LazyStackedTensorDict
+
+                        if isinstance(value, LazyStackedTensorDict) and not isinstance(
+                            target, LazyStackedTensorDict
+                        ):
+                            sub_keys_to_update = _prune_selected_keys(
+                                keys_to_update, firstkey
+                            )
+                            self._set_tuple(
+                                key,
+                                LazyStackedTensorDict(
+                                    *target.unbind(value.stack_dim),
+                                    stack_dim=value.stack_dim,
+                                ).update(
+                                    value,
+                                    inplace=inplace,
+                                    clone=clone,
+                                    keys_to_update=sub_keys_to_update,
+                                    non_blocking=non_blocking,
+                                ),
+                                validated=True,
+                                inplace=False,
+                                non_blocking=non_blocking,
+                            )
+                        else:
+                            sub_keys_to_update = _prune_selected_keys(
+                                keys_to_update, firstkey
+                            )
+                            target.update(
+                                value,
+                                inplace=inplace,
+                                clone=clone,
+                                non_blocking=non_blocking,
+                                keys_to_update=sub_keys_to_update,
+                            )
+                        continue
+            self._set_tuple(
+                key,
+                value,
+                inplace=BEST_ATTEMPT_INPLACE if inplace else False,
+                validated=False,
+                non_blocking=non_blocking,
+            )
+        return self
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        """Updates the TensorDict in-place with values from either a dictionary or another TensorDict.
+
+        Unlike :meth:`~.update`, this function will throw an error if the key is unknown to ``self``.
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): input data to be written
+                in self.
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set. Defaults to ``False``.
+
+        Keyword Args:
+            keys_to_update (sequence of NestedKeys, optional): if provided, only
+                the list of keys in ``key_to_update`` will be updated.
+                This is aimed at avoiding calls to
+                ``data_dest.update_(data_src.select(*keys_to_update))``.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> a = torch.randn(3)
+            >>> b = torch.randn(3, 4)
+            >>> td = TensorDict({"a": a, "b": b}, batch_size=[3])
+            >>> other_td = TensorDict({"a": a*0, "b": b*0}, batch_size=[])
+            >>> td.update_(other_td)
+            >>> assert td['a'] is not other_td['a']
+            >>> assert (td['a'] == other_td['a']).all()
+            >>> assert (td['a'] == 0).all()
+
+        """
+        if input_dict_or_td is self:
+            # no op
+            return self
+        if keys_to_update is not None:
+            if len(keys_to_update) == 0:
+                return self
+            keys_to_update = [_unravel_key_to_tuple(key) for key in keys_to_update]
+
+            named = True
+
+            def inplace_update(name, dest, source):
+                if source is None:
+                    return None
+                name = _unravel_key_to_tuple(name)
+                for key in keys_to_update:
+                    if key == name[: len(key)]:
+                        dest.copy_(source, non_blocking=non_blocking)
+
+        else:
+            named = False
+
+            def inplace_update(dest, source):
+                if source is None:
+                    return None
+                dest.copy_(source, non_blocking=non_blocking)
+
+        if not _is_tensor_collection(type(input_dict_or_td)):
+            from tensordict import TensorDict
+
+            input_dict_or_td = TensorDict.from_dict(
+                input_dict_or_td, batch_dims=self.batch_dims
+            )
+        self._apply_nest(
+            inplace_update,
+            input_dict_or_td,
+            nested_keys=True,
+            default=None,
+            filter_empty=True,
+            named=named,
+            is_leaf=_is_leaf_nontensor,
+        )
+        return self
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        idx: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        """Updates the TensorDict in-place at the specified index with values from either a dictionary or another TensorDict.
+
+        Unlike  TensorDict.update, this function will throw an error if the key is unknown to the TensorDict.
+
+        Args:
+            input_dict_or_td (TensorDictBase or dict): input data to be written
+                in self.
+            idx (int, torch.Tensor, iterable, slice): index of the tensordict
+                where the update should occur.
+            clone (bool, optional): whether the tensors in the input (
+                tensor) dict should be cloned before being set. Default is
+                `False`.
+
+        Keyword Args:
+            keys_to_update (sequence of NestedKeys, optional): if provided, only
+                the list of keys in ``key_to_update`` will be updated.
+            non_blocking (bool, optional): if ``True`` and this copy is between
+                different devices, the copy may occur asynchronously with respect
+                to the host.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict({
+            ...     'a': torch.zeros(3, 4, 5),
+            ...     'b': torch.zeros(3, 4, 10)}, batch_size=[3, 4])
+            >>> td.update_at_(
+            ...     TensorDict({
+            ...         'a': torch.ones(1, 4, 5),
+            ...         'b': torch.ones(1, 4, 10)}, batch_size=[1, 4]),
+            ...    slice(1, 2))
+            TensorDict(
+                fields={
+                    a: Tensor(torch.Size([3, 4, 5]), dtype=torch.float32),
+                    b: Tensor(torch.Size([3, 4, 10]), dtype=torch.float32)},
+                batch_size=torch.Size([3, 4]),
+                device=None,
+                is_shared=False)
+            >>> assert (td[1] == 1).all()
+
+        """
+        if idx == ():
+            return self.update_(
+                input_dict_or_td=input_dict_or_td,
+                keys_to_update=keys_to_update,
+                clone=clone,
+                non_blocking=non_blocking,
+            )
+        if keys_to_update is not None:
+            if len(keys_to_update) == 0:
+                return self
+            keys_to_update = unravel_key_list(keys_to_update)
+        for key, value in input_dict_or_td.items():
+            firstkey, *nextkeys = _unravel_key_to_tuple(key)
+            if keys_to_update and not any(
+                firstkey == ktu if isinstance(ktu, str) else firstkey == ktu[0]
+                for ktu in keys_to_update
+            ):
+                continue
+            if not isinstance(value, _ACCEPTED_CLASSES):
+                raise TypeError(
+                    f"Expected value to be one of types {_ACCEPTED_CLASSES} "
+                    f"but got {type(value)}"
+                )
+            if clone:
+                value = value.clone()
+            self.set_at_((firstkey, *nextkeys), value, idx, non_blocking=non_blocking)
+        return self
+
+    def replace(self, *args, **kwargs):
+        """Creates a shallow copy of the tensordict where entries have been replaced.
+
+        Accepts one unnamed argument which must be a dictionary of a :class:`~tensordict.TensorDictBase` subclass.
+        Additionaly, first-level entries can be updated with the named keyword arguments.
+
+        Returns:
+            a copy of ``self`` with updated entries if the input is non-empty. If an empty dict or no dict is provided
+            and the kwargs are empty, ``self`` is returned.
+
+        """
+        if args:
+            if len(args) > 1:
+                raise RuntimeError(
+                    "Only a single argument containing a dictionary-like "
+                    f"structure of entries to replace can be passed to replace. Received {len(args)} "
+                    f"arguments instead."
+                )
+            dict_to_replace = args[0]
+        else:
+            dict_to_replace = {}
+        if kwargs:
+            dict_to_replace.update(kwargs)
+        is_dict = isinstance(dict_to_replace, dict)
+        if is_dict:
+            if not dict_to_replace:
+                return self
+        else:
+            if not is_tensor_collection(dict_to_replace):
+                raise RuntimeError(
+                    f"Cannot use object type {type(dict_to_replace)} to update values in tensordict."
+                )
+            if dict_to_replace.is_empty():
+                return self
+        result = self.copy()
+        # using update makes sure that any optimization (e.g. for lazy stacks) is done properly
+        result.update(dict_to_replace)
+        return result
+
+    @lock_blocked
+    def create_nested(self, key):
+        """Creates a nested tensordict of the same shape, device and dim names as the current tensordict.
+
+        If the value already exists, it will be overwritten by this operation.
+        This operation is blocked in locked tensordicts.
+
+        Examples:
+            >>> data = TensorDict({}, [3, 4, 5])
+            >>> data.create_nested("root")
+            >>> data.create_nested(("some", "nested", "value"))
+            >>> print(data)
+            TensorDict(
+                fields={
+                    root: TensorDict(
+                        fields={
+                        },
+                        batch_size=torch.Size([3, 4, 5]),
+                        device=None,
+                        is_shared=False),
+                    some: TensorDict(
+                        fields={
+                            nested: TensorDict(
+                                fields={
+                                    value: TensorDict(
+                                        fields={
+                                        },
+                                        batch_size=torch.Size([3, 4, 5]),
+                                        device=None,
+                                        is_shared=False)},
+                                batch_size=torch.Size([3, 4, 5]),
+                                device=None,
+                                is_shared=False)},
+                        batch_size=torch.Size([3, 4, 5]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3, 4, 5]),
+                device=None,
+                is_shared=False)
+        """
+        key = _unravel_key_to_tuple(key)
+        self._create_nested_tuple(key)
+        return self
+
+    def _create_nested_str(self, key):
+        out = self.empty()
+        self._set_str(key, out, inplace=False, validated=True, non_blocking=False)
+        return out
+
+    def _create_nested_tuple(self, key):
+        td = self._create_nested_str(key[0])
+        if len(key) > 1:
+            td._create_nested_tuple(key[1:])
+
+    def copy_(self, tensordict: T, non_blocking: bool = False) -> T:
+        """See :obj:`TensorDictBase.update_`.
+
+        The non-blocking argument will be ignored and is just present for
+        compatibility with :func:`torch.Tensor.copy_`.
+        """
+        return self.update_(tensordict, non_blocking=non_blocking)
+
+    def copy_at_(self, tensordict: T, idx: IndexType, non_blocking: bool = False) -> T:
+        """See :obj:`TensorDictBase.update_at_`."""
+        return self.update_at_(tensordict, idx, non_blocking=non_blocking)
+
+    def is_empty(self) -> bool:
+        """Checks if the tensordict contains any leaf."""
+        for _ in self.keys(True, True):
+            return False
+        return True
+
+    # Dict features: setdefault, items, values, keys, ...
+    def setdefault(
+        self, key: NestedKey, default: CompatibleType, inplace: bool = False
+    ) -> CompatibleType:
+        """Insert the ``key`` entry with a value of ``default`` if ``key`` is not in the tensordict.
+
+        Return the value for ``key`` if ``key`` is in the tensordict, else ``default``.
+
+        Args:
+            key (str or nested key): the name of the value.
+            default (torch.Tensor or compatible type, TensorDictBase): value
+                to be stored in the tensordict if the key is not already present.
+
+        Returns:
+            The value of key in the tensordict. Will be default if the key was not
+            previously set.
+
+        Examples:
+            >>> td = TensorDict({}, batch_size=[3, 4])
+            >>> val = td.setdefault("a", torch.zeros(3, 4))
+            >>> assert (val == 0).all()
+            >>> val = td.setdefault("a", torch.ones(3, 4))
+            >>> assert (val == 0).all() # output is still 0
+
+        """
+        if key not in self.keys(include_nested=isinstance(key, tuple)):
+            self.set(key, default, inplace=inplace)
+        return self.get(key)
+
+    def items(
+        self, include_nested: bool = False, leaves_only: bool = False, is_leaf=None
+    ) -> Iterator[tuple[str, CompatibleType]]:
+        """Returns a generator of key-value pairs for the tensordict.
+
+        Args:
+            include_nested (bool, optional): if ``True``, nested values will be returned.
+                Defaults to ``False``.
+            leaves_only (bool, optional): if ``False``, only leaves will be
+                returned. Defaults to ``False``.
+            is_leaf: an optional callable that indicates if a class is to be considered a
+                leaf or not.
+
+        """
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not is_leaf(val.__class__):
+                    yield from (
+                        (_unravel_key_to_tuple((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested,
+                            leaves_only=leaves_only,
+                            is_leaf=is_leaf,
+                        )
+                    )
+                else:
+                    yield k, val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield k, val
+                if not is_leaf(val.__class__):
+                    yield from (
+                        (_unravel_key_to_tuple((k, _key)), _val)
+                        for _key, _val in val.items(
+                            include_nested=include_nested,
+                            leaves_only=leaves_only,
+                            is_leaf=is_leaf,
+                        )
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if is_leaf(val.__class__):
+                    yield k, val
+        else:
+            for k in self.keys():
+                yield k, self._get_str(k, NO_DEFAULT)
+
+    def non_tensor_items(self, include_nested: bool = False):
+        """Returns all non-tensor leaves, maybe recursively."""
+        return tuple(
+            self.items(
+                include_nested,
+                leaves_only=True,
+                is_leaf=_is_non_tensor,
+            )
+        )
+
+    def values(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf=None,
+    ) -> Iterator[CompatibleType]:
+        """Returns a generator representing the values for the tensordict.
+
+        Args:
+            include_nested (bool, optional): if ``True``, nested values will be returned.
+                Defaults to ``False``.
+            leaves_only (bool, optional): if ``False``, only leaves will be
+                returned. Defaults to ``False``.
+            is_leaf: an optional callable that indicates if a class is to be considered a
+                leaf or not.
+
+        """
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+        # check the conditions once only
+        if include_nested and leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if not is_leaf(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested,
+                        leaves_only=leaves_only,
+                        is_leaf=is_leaf,
+                    )
+                else:
+                    yield val
+        elif include_nested:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                yield val
+                if not is_leaf(val.__class__):
+                    yield from val.values(
+                        include_nested=include_nested,
+                        leaves_only=leaves_only,
+                        is_leaf=is_leaf,
+                    )
+        elif leaves_only:
+            for k in self.keys():
+                val = self._get_str(k, NO_DEFAULT)
+                if is_leaf(val.__class__):
+                    yield val
+        else:
+            for k in self.keys():
+                yield self._get_str(k, NO_DEFAULT)
+
+    @cache  # noqa: B019
+    def _values_list(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+    ) -> List:
+        return list(
+            self.values(
+                include_nested=include_nested,
+                leaves_only=leaves_only,
+                is_leaf=_NESTED_TENSORS_AS_LISTS,
+            )
+        )
+
+    @cache  # noqa: B019
+    def _items_list(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        *,
+        collapse: bool = False,
+    ) -> Tuple[List, List]:
+        return tuple(
+            list(key_or_val)
+            for key_or_val in zip(
+                *self.items(
+                    include_nested=include_nested,
+                    leaves_only=leaves_only,
+                    is_leaf=_NESTED_TENSORS_AS_LISTS if not collapse else None,
+                )
+            )
+        )
+
+    @cache  # noqa: B019
+    def _grad(self):
+        result = self._fast_apply(lambda x: x.grad, propagate_lock=True)
+        return result
+
+    @cache  # noqa: B019
+    def _data(self):
+        result = self._fast_apply(lambda x: x.data, propagate_lock=True)
+        return result
+
+    @abc.abstractmethod
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] = None,
+    ):
+        """Returns a generator of tensordict keys.
+
+        Args:
+            include_nested (bool, optional): if ``True``, nested values will be returned.
+                Defaults to ``False``.
+            leaves_only (bool, optional): if ``False``, only leaves will be
+                returned. Defaults to ``False``.
+            is_leaf: an optional callable that indicates if a class is to be considered a
+                leaf or not.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> data = TensorDict({"0": 0, "1": {"2": 2}}, batch_size=[])
+            >>> data.keys()
+            ['0', '1']
+            >>> list(data.keys(leaves_only=True))
+            ['0']
+            >>> list(data.keys(include_nested=True, leaves_only=True))
+            ['0', '1', ('1', '2')]
+        """
+        ...
+
+    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        """Removes and returns a value from a tensordict.
+
+        If the value is not present and no default value is provided, a KeyError
+        is thrown.
+
+        Args:
+            key (str or nested key): the entry to look for.
+            default (Any, optional): the value to return if the key cannot be found.
+
+        Examples:
+            >>> td = TensorDict({"1": 1}, [])
+            >>> one = td.pop("1")
+            >>> assert one == 1
+            >>> none = td.pop("1", default=None)
+            >>> assert none is None
+        """
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR.format(key))
+        try:
+            # using try/except for get/del is suboptimal, but
+            # this is faster that checkink if key in self keys
+            out = self.get(key, default)
+            self.del_(key)
+        except KeyError as err:
+            # if default provided, 'out' value will return, else raise error
+            if default == NO_DEFAULT:
+                raise KeyError(
+                    f"You are trying to pop key `{key}` which is not in dict "
+                    f"without providing default value."
+                ) from err
+        return out
+
+    @property
+    @cache  # noqa: B019
+    def sorted_keys(self) -> list[NestedKey]:
+        """Returns the keys sorted in alphabetical order.
+
+        Does not support extra arguments.
+
+        If the TensorDict is locked, the keys are cached until the tensordict
+        is unlocked for faster execution.
+
+        """
+        return sorted(self.keys())
+
+    @as_decorator()
+    def flatten(self, start_dim=0, end_dim=-1):
+        """Flattens all the tensors of a tensordict.
+
+        Args:
+            start_dim (int): the first dim to flatten
+            end_dim (int): the last dim to flatten
+
+        Examples:
+            >>> td = TensorDict({
+            ...     "a": torch.arange(60).view(3, 4, 5),
+            ...     "b": torch.arange(12).view(3, 4)}, batch_size=[3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_flat.batch_size
+            torch.Size([12])
+            >>> td_flat["a"]
+            tensor([[ 0,  1,  2,  3,  4],
+                    [ 5,  6,  7,  8,  9],
+                    [10, 11, 12, 13, 14],
+                    [15, 16, 17, 18, 19],
+                    [20, 21, 22, 23, 24],
+                    [25, 26, 27, 28, 29],
+                    [30, 31, 32, 33, 34],
+                    [35, 36, 37, 38, 39],
+                    [40, 41, 42, 43, 44],
+                    [45, 46, 47, 48, 49],
+                    [50, 51, 52, 53, 54],
+                    [55, 56, 57, 58, 59]])
+            >>> td_flat["b"]
+            tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
+
+        """
+        if start_dim < 0:
+            start_dim = self.ndim + start_dim
+        if end_dim < 0:
+            end_dim = self.ndim + end_dim
+            if end_dim < 0:
+                raise ValueError(
+                    f"Incompatible end_dim {end_dim} for tensordict with shape {self.shape}."
+                )
+        if end_dim <= start_dim:
+            raise ValueError(
+                "The end dimension must be strictly greater than the start dim."
+            )
+
+        def flatten(tensor):
+            return torch.flatten(tensor, start_dim, end_dim)
+
+        nelt = prod(self.batch_size[start_dim : end_dim + 1])
+        if start_dim > 0:
+            batch_size = (
+                list(self.batch_size)[:start_dim]
+                + [nelt]
+                + list(self.batch_size[end_dim + 1 :])
+            )
+        else:
+            batch_size = [nelt] + list(self.batch_size[end_dim + 1 :])
+        # TODO: check that this works with nested tds of different batch size
+        out = self._fast_apply(flatten, batch_size=batch_size, propagate_lock=True)
+        if self._has_names():
+            names = [
+                name
+                for i, name in enumerate(self.names)
+                if (i < start_dim or i > end_dim)
+            ]
+            names.insert(start_dim, None)
+            out.names = names
+        return out
+
+    @as_decorator()
+    def unflatten(self, dim, unflattened_size):
+        """Unflattens a tensordict dim expanding it to a desired shape.
+
+        Args:
+            dim (int): specifies the dimension of the input tensor to be
+                unflattened.
+            unflattened_size (shape): is the new shape of the unflattened
+                dimension of the tensordict.
+
+        Examples:
+            >>> td = TensorDict({
+            ...     "a": torch.arange(60).view(3, 4, 5),
+            ...     "b": torch.arange(12).view(3, 4)},
+            ...     batch_size=[3, 4])
+            >>> td_flat = td.flatten(0, 1)
+            >>> td_unflat = td_flat.unflatten(0, [3, 4])
+            >>> assert (td == td_unflat).all()
+        """
+        if dim < 0:
+            dim = self.ndim + dim
+            if dim < 0:
+                raise ValueError(
+                    f"Incompatible dim {dim} for tensordict with shape {self.shape}."
+                )
+
+        def unflatten(tensor):
+            return torch.unflatten(
+                tensor,
+                dim,
+                unflattened_size,
+            )
+
+        if dim > 0:
+            batch_size = (
+                list(self.batch_size)[:dim]
+                + list(unflattened_size)
+                + list(self.batch_size[dim + 1 :])
+            )
+        else:
+            batch_size = list(unflattened_size) + list(self.batch_size[1:])
+        # TODO: check that this works with nested tds of different batch size
+        out = self._fast_apply(unflatten, batch_size=batch_size, propagate_lock=True)
+        if self._has_names():
+            names = copy(self.names)
+            for _ in range(len(unflattened_size) - 1):
+                names.insert(dim, None)
+            out.names = names
+        return out
+
+    @abc.abstractmethod
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> T:
+        """Renames a key with a new string and returns the same tensordict with the updated key name.
+
+        Args:
+            old_key (str or nested key): key to be renamed.
+            new_key (str or nested key): new name of the entry.
+            safe (bool, optional): if ``True``, an error is thrown when the new
+                key is already present in the TensorDict.
+
+        Returns:
+            self
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def del_(self, key: NestedKey) -> T:
+        """Deletes a key of the tensordict.
+
+        Args:
+            key (NestedKey): key to be deleted
+
+        Returns:
+            self
+
+        """
+        ...
+
+    # Distributed functionality
+    def gather_and_stack(
+        self, dst: int, group: "dist.ProcessGroup" | None = None
+    ) -> T | None:
+        """Gathers tensordicts from various workers and stacks them onto self in the destination worker.
+
+        Args:
+            dst (int): the rank of the destination worker where :func:`gather_and_stack` will be called.
+            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
+                will be used for communication. Otherwise, the default process group
+                will be used.
+                Defaults to ``None``.
+
+        Example:
+            >>> from torch import multiprocessing as mp
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>>
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     # Create a single tensordict to be sent to server
+            ...     td = TensorDict(
+            ...         {("a", "b"): torch.randn(2),
+            ...          "c": torch.randn(2)}, [2]
+            ...     )
+            ...     td.gather_and_stack(0)
+            ...
+            >>> def server():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     # Creates the destination tensordict on server.
+            ...     # The first dim must be equal to world_size-1
+            ...     td = TensorDict(
+            ...         {("a", "b"): torch.zeros(2),
+            ...          "c": torch.zeros(2)}, [2]
+            ...     ).expand(1, 2).contiguous()
+            ...     td.gather_and_stack(0)
+            ...     assert td["a", "b"] != 0
+            ...     print("yuppie")
+            ...
+            >>> if __name__ == "__main__":
+            ...     mp.set_start_method("spawn")
+            ...
+            ...     main_worker = mp.Process(target=server)
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+        """
+        output = (
+            [None for _ in range(dist.get_world_size(group=group))]
+            if dst == dist.get_rank(group=group)
+            else None
+        )
+        dist.gather_object(self, output, dst=dst, group=group)
+        if dst == dist.get_rank(group=group):
+            # remove self from output
+            output = [item for i, item in enumerate(output) if i != dst]
+            self.update(torch.stack(output, 0), inplace=True)
+            return self
+        return None
+
+    def send(
+        self,
+        dst: int,
+        *,
+        group: "dist.ProcessGroup" | None = None,
+        init_tag: int = 0,
+        pseudo_rand: bool = False,
+    ) -> None:  # noqa: D417
+        """Sends the content of a tensordict to a distant worker.
+
+        Args:
+            dst (int): the rank of the destination worker where the content
+                should be sent.
+
+        Keyword Args:
+            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
+                will be used for communication. Otherwise, the default process group
+                will be used.
+                Defaults to ``None``.
+            init_tag (int): the initial tag to be used to mark the tensors.
+                Note that this will be incremented by as much as the number of
+                tensors contained in the TensorDict.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                Defaults to ``False``.
+
+        Example:
+            >>> from torch import multiprocessing as mp
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>>
+            >>>
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.randn(2),
+            ...             "c": torch.randn(2, 3),
+            ...             "_": torch.ones(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.send(0)
+            ...
+            >>>
+            >>> def server(queue):
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.zeros(2),
+            ...             "c": torch.zeros(2, 3),
+            ...             "_": torch.zeros(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.recv(1)
+            ...     assert (td != 0).all()
+            ...     queue.put("yuppie")
+            ...
+            >>>
+            >>> if __name__=="__main__":
+            ...     queue = mp.Queue(1)
+            ...     main_worker = mp.Process(target=server, args=(queue,))
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...     out = queue.get(timeout=10)
+            ...     assert out == "yuppie"
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+
+        """
+        self._send(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
+
+    def _send(
+        self,
+        dst: int,
+        _tag: int = -1,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> int:
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if isinstance(value, Tensor):
+                pass
+            elif _is_tensor_collection(value.__class__):
+                _tag = value._send(dst, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
+                continue
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            dist.send(value, dst=dst, tag=_tag, group=group)
+
+        return _tag
+
+    def recv(
+        self,
+        src: int,
+        *,
+        group: "dist.ProcessGroup" | None = None,
+        init_tag: int = 0,
+        pseudo_rand: bool = False,
+    ) -> int:  # noqa: D417
+        """Receives the content of a tensordict and updates content with it.
+
+        Check the example in the `send` method for context.
+
+        Args:
+            src (int): the rank of the source worker.
+
+        Keyword Args:
+            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
+                will be used for communication. Otherwise, the default process group
+                will be used.
+                Defaults to ``None``.
+            init_tag (int): the ``init_tag`` used by the source worker.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                This value must match the one passed to :func:`send`.
+                Defaults to ``False``.
+        """
+        return self._recv(src, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
+
+    def _recv(
+        self,
+        src: int,
+        _tag: int = -1,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+        non_blocking: bool = False,
+    ) -> int:
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if isinstance(value, Tensor):
+                pass
+            elif _is_tensor_collection(value.__class__):
+                _tag = value._recv(src, _tag=_tag, pseudo_rand=pseudo_rand, group=group)
+                continue
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            dist.recv(value, src=src, tag=_tag, group=group)
+            self._set_str(
+                key, value, inplace=True, validated=True, non_blocking=non_blocking
+            )
+
+        return _tag
+
+    def isend(
+        self,
+        dst: int,
+        *,
+        group: "dist.ProcessGroup" | None = None,
+        init_tag: int = 0,
+        pseudo_rand: bool = False,
+    ) -> int:  # noqa: D417
+        """Sends the content of the tensordict asynchronously.
+
+        Args:
+            dst (int): the rank of the destination worker where the content
+                should be sent.
+
+        Keyword Args:
+            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
+                will be used for communication. Otherwise, the default process group
+                will be used.
+                Defaults to ``None``.
+            init_tag (int): the initial tag to be used to mark the tensors.
+                Note that this will be incremented by as much as the number of
+                tensors contained in the TensorDict.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                Defaults to ``False``.
+
+        Example:
+            >>> import torch
+            >>> from tensordict import TensorDict
+            >>> from torch import multiprocessing as mp
+            >>> def client():
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=1,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.randn(2),
+            ...             "c": torch.randn(2, 3),
+            ...             "_": torch.ones(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     td.isend(0)
+            ...
+            >>>
+            >>> def server(queue, return_premature=True):
+            ...     torch.distributed.init_process_group(
+            ...         "gloo",
+            ...         rank=0,
+            ...         world_size=2,
+            ...         init_method=f"tcp://localhost:10003",
+            ...     )
+            ...     td = TensorDict(
+            ...         {
+            ...             ("a", "b"): torch.zeros(2),
+            ...             "c": torch.zeros(2, 3),
+            ...             "_": torch.zeros(2, 1, 5),
+            ...         },
+            ...         [2],
+            ...     )
+            ...     out = td.irecv(1, return_premature=return_premature)
+            ...     if return_premature:
+            ...         for fut in out:
+            ...             fut.wait()
+            ...     assert (td != 0).all()
+            ...     queue.put("yuppie")
+            ...
+            >>>
+            >>> if __name__ == "__main__":
+            ...     queue = mp.Queue(1)
+            ...     main_worker = mp.Process(
+            ...         target=server,
+            ...         args=(queue, )
+            ...         )
+            ...     secondary_worker = mp.Process(target=client)
+            ...
+            ...     main_worker.start()
+            ...     secondary_worker.start()
+            ...     out = queue.get(timeout=10)
+            ...     assert out == "yuppie"
+            ...     main_worker.join()
+            ...     secondary_worker.join()
+
+        """
+        return self._isend(dst, _tag=init_tag - 1, pseudo_rand=pseudo_rand, group=group)
+
+    def _isend(
+        self,
+        dst: int,
+        _tag: int = -1,
+        _futures: list[torch.Future] | None = None,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> int:
+        root = False
+        if _futures is None:
+            root = True
+            _futures = []
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _tag = value._isend(
+                    dst,
+                    _tag=_tag,
+                    pseudo_rand=pseudo_rand,
+                    _futures=_futures,
+                    group=group,
+                )
+                continue
+            elif isinstance(value, Tensor):
+                pass
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            _future = dist.isend(value, dst=dst, tag=_tag, group=group)
+            _futures.append(_future)
+        if root:
+            for _future in _futures:
+                _future.wait()
+        return _tag
+
+    def irecv(
+        self,
+        src: int,
+        *,
+        group: "dist.ProcessGroup" | None = None,
+        return_premature: bool = False,
+        init_tag: int = 0,
+        pseudo_rand: bool = False,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        """Receives the content of a tensordict and updates content with it asynchronously.
+
+        Check the example in the :meth:`~.isend` method for context.
+
+        Args:
+            src (int): the rank of the source worker.
+
+        Keyword Args:
+            group (torch.distributed.ProcessGroup, optional): if set, the specified process group
+                will be used for communication. Otherwise, the default process group
+                will be used.
+                Defaults to ``None``.
+            return_premature (bool): if ``True``, returns a list of futures to wait
+                upon until the tensordict is updated. Defaults to ``False``,
+                i.e. waits until update is completed withing the call.
+            init_tag (int): the ``init_tag`` used by the source worker.
+            pseudo_rand (bool): if True, the sequence of tags will be pseudo-
+                random, allowing to send multiple data from different nodes
+                without overlap. Notice that the generation of these pseudo-random
+                numbers is expensive (1e-5 sec/number), meaning that it could
+                slow down the runtime of your algorithm.
+                This value must match the one passed to :func:`isend`.
+                Defaults to ``False``.
+
+        Returns:
+            if ``return_premature=True``, a list of futures to wait
+                upon until the tensordict is updated.
+        """
+        return self._irecv(
+            src,
+            return_premature=return_premature,
+            _tag=init_tag - 1,
+            pseudo_rand=pseudo_rand,
+            group=group,
+        )
+
+    def _irecv(
+        self,
+        src: int,
+        return_premature: bool = False,
+        _tag: int = -1,
+        _future_list: list[torch.Future] = None,
+        pseudo_rand: bool = False,
+        group: "dist.ProcessGroup" | None = None,
+    ) -> tuple[int, list[torch.Future]] | list[torch.Future] | None:
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _tag, _future_list = value._irecv(
+                    src,
+                    _tag=_tag,
+                    _future_list=_future_list,
+                    pseudo_rand=pseudo_rand,
+                    group=group,
+                )
+                continue
+            elif isinstance(value, Tensor):
+                pass
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            if not pseudo_rand:
+                _tag += 1
+            else:
+                _tag = int_generator(_tag + 1)
+            _future_list.append(dist.irecv(value, src=src, tag=_tag, group=group))
+        if not root:
+            return _tag, _future_list
+        elif return_premature:
+            return _future_list
+        else:
+            for future in _future_list:
+                future.wait()
+            return
+
+    def reduce(
+        self,
+        dst,
+        op=None,
+        async_op=False,
+        return_premature=False,
+        group=None,
+    ):
+        """Reduces the tensordict across all machines.
+
+        Only the process with ``rank`` dst is going to receive the final result.
+
+        """
+        if op is None:
+            op = dist.ReduceOp.SUM
+        return self._reduce(dst, op, async_op, return_premature, group=group)
+
+    def _reduce(
+        self,
+        dst,
+        op=None,
+        async_op=False,
+        return_premature=False,
+        _future_list=None,
+        group=None,
+    ):
+        if op is None:
+            op = dist.ReduceOp.SUM
+        root = False
+        if _future_list is None:
+            _future_list = []
+            root = True
+        for key in self.sorted_keys:
+            value = self._get_str(key, NO_DEFAULT)
+            if _is_tensor_collection(value.__class__):
+                _future_list = value._reduce(
+                    dst=dst,
+                    op=op,
+                    async_op=async_op,
+                    _future_list=_future_list,
+                )
+                continue
+            elif isinstance(value, Tensor):
+                pass
+            else:
+                raise NotImplementedError(f"Type {type(value)} is not supported.")
+            _future_list.append(
+                dist.reduce(value, dst=dst, op=op, async_op=async_op, group=group)
+            )
+        if not root:
+            return _future_list
+        elif async_op and return_premature:
+            return _future_list
+        elif async_op:
+            for future in _future_list:
+                future.wait()
+            return
+
+    # Apply and map functionality
+    def apply_(self, fn: Callable, *others, **kwargs) -> T:
+        """Applies a callable to all values stored in the tensordict and re-writes them in-place.
+
+        Args:
+            fn (Callable): function to be applied to the tensors in the
+                tensordict.
+            *others (sequence of TensorDictBase, optional): the other
+                tensordicts to be used.
+
+        Keyword Args: See :meth:`~.apply`.
+
+        Returns:
+            self or a copy of self with the function applied
+
+        """
+        return self.apply(fn, *others, inplace=True, **kwargs)
+
+    def apply(
+        self,
+        fn: Callable,
+        *others: T,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
+        propagate_lock: bool = False,
+        call_on_nested: bool = False,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        """Applies a callable to all values stored in the tensordict and sets them in a new tensordict.
+
+        The callable signature must be ``Callable[Tuple[Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
+
+        Args:
+            fn (Callable): function to be applied to the tensors in the
+                tensordict.
+            *others (TensorDictBase instances, optional): if provided, these
+                tensordict instances should have a structure matching the one
+                of self. The ``fn`` argument should receive as many
+                unnamed inputs as the number of tensordicts, including self.
+                If other tensordicts have missing entries, a default value
+                can be passed through the ``default`` keyword argument.
+
+        Keyword Args:
+            batch_size (sequence of int, optional): if provided,
+                the resulting TensorDict will have the desired batch_size.
+                The :obj:`batch_size` argument should match the batch_size after
+                the transformation. This is a keyword only argument.
+            device (torch.device, optional): the resulting device, if any.
+            names (list of str, optional): the new dimension names, in case the
+                batch_size is modified.
+            inplace (bool, optional): if True, changes are made in-place.
+                Default is False. This is a keyword only argument.
+            default (Any, optional): default value for missing entries in the
+                other tensordicts. If not provided, missing entries will
+                raise a `KeyError`.
+            filter_empty (bool, optional): if ``True``, empty tensordicts will be
+                filtered out. This also comes with a lower computational cost as
+                empty data structures won't be created and destroyed. Non-tensor data
+                is considered as a leaf and thereby will be kept in the tensordict even
+                if left untouched by the function.
+                Defaults to ``False`` for backward compatibility.
+            propagate_lock (bool, optional): if ``True``, a locked tensordict will produce
+                another locked tensordict. Defaults to ``False``.
+            call_on_nested (bool, optional): if ``True``, the function will be called on first-level tensors
+                and containers (TensorDict or tensorclass). In this scenario, ``func`` is responsible of
+                propagating its calls to nested levels. This allows a fine-grained behaviour
+                when propagating the calls to nested tensordicts.
+                If ``False``, the function will only be called on leaves, and ``apply`` will take care of dispatching
+                the function to all leaves.
+
+                    >>> td = TensorDict({"a": {"b": [0.0, 1.0]}, "c": [1.0, 2.0]})
+                    >>> def mean_tensor_only(val):
+                    ...     if is_tensor_collection(val):
+                    ...         raise RuntimeError("Unexpected!")
+                    ...     return val.mean()
+                    >>> td_mean = td.apply(mean_tensor_only)
+                    >>> def mean_any(val):
+                    ...     if is_tensor_collection(val):
+                    ...         # Recurse
+                    ...         return val.apply(mean_any, call_on_nested=True)
+                    ...     return val.mean()
+                    >>> td_mean = td.apply(mean_any, call_on_nested=True)
+            out (TensorDictBase, optional): a tensordict where to write the results. This can be used to avoid
+                creating a new tensordict:
+
+                    >>> td = TensorDict({"a": 0})
+                    >>> td.apply(lambda x: x+1, out=td)
+                    >>> assert (td==1).all()
+
+                .. warning:: If the operation executed on the tensordict requires multiple keys to be accessed for
+                    a single computation, providing an ``out`` argument equal to ``self`` can cause the operation
+                    to provide silently wrong results.
+                    For instance:
+
+                        >>> td = TensorDict({"a": 1, "b": 1})
+                        >>> td.apply(lambda x: x+td["a"])["b"] # Right!
+                        tensor(2)
+                        >>> td.apply(lambda x: x+td["a"], out=td)["b"] # Wrong!
+                        tensor(3)
+
+            **constructor_kwargs: additional keyword arguments to be passed to the
+                TensorDict constructor.
+
+        Returns:
+            a new tensordict with transformed_in tensors.
+
+        Example:
+            >>> td = TensorDict({
+            ...     "a": -torch.ones(3),
+            ...     "b": {"c": torch.ones(3)}},
+            ...     batch_size=[3])
+            >>> td_1 = td.apply(lambda x: x+1)
+            >>> assert (td_1["a"] == 0).all()
+            >>> assert (td_1["b", "c"] == 2).all()
+            >>> td_2 = td.apply(lambda x, y: x+y, td)
+            >>> assert (td_2["a"] == -2).all()
+            >>> assert (td_2["b", "c"] == 2).all()
+
+        .. note::
+            If ``None`` is returned by the function, the entry is ignored. This
+            can be used to filter the data in the tensordict:
+
+            >>> td = TensorDict({"1": 1, "2": 2, "b": {"2": 2, "1": 1}}, [])
+            >>> def filter(tensor):
+            ...     if tensor == 1:
+            ...         return tensor
+            >>> td.apply(filter)
+            TensorDict(
+                fields={
+                    1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        .. note::
+            The apply method will return an :class:`~tensordict.TensorDict` instance,
+            regardless of the input type. To keep the same type, one can execute
+
+            >>> out = td.clone(False).update(td.apply(...))
+
+
+        """
+        result = self._apply_nest(
+            fn,
+            *others,
+            batch_size=batch_size,
+            device=device,
+            names=names,
+            inplace=inplace,
+            checked=False,
+            default=default,
+            filter_empty=filter_empty,
+            call_on_nested=call_on_nested,
+            out=out,
+            **constructor_kwargs,
+        )
+        if propagate_lock and not inplace and self.is_locked and result is not None:
+            result.lock_()
+        return result
+
+    def named_apply(
+        self,
+        fn: Callable,
+        *others: T,
+        nested_keys: bool = False,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
+        propagate_lock: bool = False,
+        call_on_nested: bool = False,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        """Applies a key-conditioned callable to all values stored in the tensordict and sets them in a new atensordict.
+
+        The callable signature must be ``Callable[Tuple[str, Tensor, ...], Optional[Union[Tensor, TensorDictBase]]]``.
+
+        Args:
+            fn (Callable): function to be applied to the (name, tensor) pairs in the
+                tensordict. For each leaf, only its leaf name will be used (not
+                the full `NestedKey`).
+            *others (TensorDictBase instances, optional): if provided, these
+                tensordict instances should have a structure matching the one
+                of self. The ``fn`` argument should receive as many
+                unnamed inputs as the number of tensordicts, including self.
+                If other tensordicts have missing entries, a default value
+                can be passed through the ``default`` keyword argument.
+            nested_keys (bool, optional): if ``True``, the complete path
+                to the leaf will be used. Defaults to ``False``, i.e. only the last
+                string is passed to the function.
+            batch_size (sequence of int, optional): if provided,
+                the resulting TensorDict will have the desired batch_size.
+                The :obj:`batch_size` argument should match the batch_size after
+                the transformation. This is a keyword only argument.
+            device (torch.device, optional): the resulting device, if any.
+            names (list of str, optional): the new dimension names, in case the
+                batch_size is modified.
+            inplace (bool, optional): if True, changes are made in-place.
+                Default is False. This is a keyword only argument.
+            default (Any, optional): default value for missing entries in the
+                other tensordicts. If not provided, missing entries will
+                raise a `KeyError`.
+            filter_empty (bool, optional): if ``True``, empty tensordicts will be
+                filtered out. This also comes with a lower computational cost as
+                empty data structures won't be created and destroyed. Defaults to
+                ``False`` for backward compatibility.
+            propagate_lock (bool, optional): if ``True``, a locked tensordict will produce
+                another locked tensordict. Defaults to ``False``.
+            call_on_nested (bool, optional): if ``True``, the function will be called on first-level tensors
+                and containers (TensorDict or tensorclass). In this scenario, ``func`` is responsible of
+                propagating its calls to nested levels. This allows a fine-grained behaviour
+                when propagating the calls to nested tensordicts.
+                If ``False``, the function will only be called on leaves, and ``apply`` will take care of dispatching
+                the function to all leaves.
+
+                    >>> td = TensorDict({"a": {"b": [0.0, 1.0]}, "c": [1.0, 2.0]})
+                    >>> def mean_tensor_only(val):
+                    ...     if is_tensor_collection(val):
+                    ...         raise RuntimeError("Unexpected!")
+                    ...     return val.mean()
+                    >>> td_mean = td.apply(mean_tensor_only)
+                    >>> def mean_any(val):
+                    ...     if is_tensor_collection(val):
+                    ...         # Recurse
+                    ...         return val.apply(mean_any, call_on_nested=True)
+                    ...     return val.mean()
+                    >>> td_mean = td.apply(mean_any, call_on_nested=True)
+
+            out (TensorDictBase, optional): a tensordict where to write the results. This can be used to avoid
+                creating a new tensordict:
+
+                    >>> td = TensorDict({"a": 0})
+                    >>> td.apply(lambda x: x+1, out=td)
+                    >>> assert (td==1).all()
+
+                .. warning:: If the operation executed on the tensordict requires multiple keys to be accessed for
+                    a single computation, providing an ``out`` argument equal to ``self`` can cause the operation
+                    to provide silently wrong results.
+                    For instance:
+
+                        >>> td = TensorDict({"a": 1, "b": 1})
+                        >>> td.apply(lambda x: x+td["a"])["b"] # Right!
+                        tensor(2)
+                        >>> td.apply(lambda x: x+td["a"], out=td)["b"] # Wrong!
+                        tensor(3)
+
+            **constructor_kwargs: additional keyword arguments to be passed to the
+                TensorDict constructor.
+
+        Returns:
+            a new tensordict with transformed_in tensors.
+
+        Example:
+            >>> td = TensorDict({
+            ...     "a": -torch.ones(3),
+            ...     "nested": {"a": torch.ones(3), "b": torch.zeros(3)}},
+            ...     batch_size=[3])
+            >>> def name_filter(name, tensor):
+            ...     if name == "a":
+            ...         return tensor
+            >>> td.named_apply(name_filter)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    nested: TensorDict(
+                        fields={
+                            a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> def name_filter(name, *tensors):
+            ...     if name == "a":
+            ...         r = 0
+            ...         for tensor in tensors:
+            ...             r = r + tensor
+            ...         return tensor
+            >>> out = td.named_apply(name_filter, td)
+            >>> print(out)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    nested: TensorDict(
+                        fields={
+                            a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+            >>> print(out["a"])
+            tensor([-1., -1., -1.])
+
+        .. note::
+            If ``None`` is returned by the function, the entry is ignored. This
+            can be used to filter the data in the tensordict:
+
+            >>> td = TensorDict({"1": 1, "2": 2, "b": {"2": 2, "1": 1}}, [])
+            >>> def name_filter(name, tensor):
+            ...     if name == "1":
+            ...         return tensor
+            >>> td.named_apply(name_filter)
+            TensorDict(
+                fields={
+                    1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        result = self._apply_nest(
+            fn,
+            *others,
+            batch_size=batch_size,
+            device=device,
+            names=names,
+            inplace=inplace,
+            checked=False,
+            default=default,
+            named=True,
+            nested_keys=nested_keys,
+            filter_empty=filter_empty,
+            call_on_nested=call_on_nested,
+            **constructor_kwargs,
+        )
+        if propagate_lock and not inplace and self.is_locked and result is not None:
+            result.lock_()
+        return result
+
+    @abc.abstractmethod
+    def _apply_nest(
+        self,
+        fn: Callable,
+        *others: T,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        checked: bool = False,
+        call_on_nested: bool = False,
+        default: Any = NO_DEFAULT,
+        named: bool = False,
+        nested_keys: bool = False,
+        prefix: tuple = (),
+        filter_empty: bool | None = None,
+        is_leaf: Callable = None,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        ...
+
+    def _fast_apply(
+        self,
+        fn: Callable,
+        *others: T,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        call_on_nested: bool = False,
+        default: Any = NO_DEFAULT,
+        named: bool = False,
+        nested_keys: bool = False,
+        # filter_empty must be False because we use _fast_apply for all sorts of ops like expand etc
+        # and non-tensor data will disappear if we use True by default.
+        filter_empty: bool | None = False,
+        is_leaf: Callable = None,
+        propagate_lock: bool = False,
+        out: TensorDictBase | None = None,
+        **constructor_kwargs,
+    ) -> T | None:
+        """A faster apply method.
+
+        This method does not run any check after performing the func. This
+        means that one to make sure that the metadata of the resulting tensors
+        (device, shape etc.) match the :meth:`~.apply` ones.
+
+        """
+        result = self._apply_nest(
+            fn,
+            *others,
+            batch_size=batch_size,
+            device=device,
+            names=names,
+            inplace=inplace,
+            checked=True,
+            call_on_nested=call_on_nested,
+            named=named,
+            default=default,
+            nested_keys=nested_keys,
+            filter_empty=filter_empty,
+            is_leaf=is_leaf,
+            out=out,
+            **constructor_kwargs,
+        )
+        if propagate_lock and not inplace and self.is_locked and result is not None:
+            result.lock_()
+        return result
+
+    def map(
+        self,
+        fn: Callable[[TensorDictBase], TensorDictBase | None],
+        dim: int = 0,
+        num_workers: int | None = None,
+        *,
+        out: TensorDictBase | None = None,
+        chunksize: int | None = None,
+        num_chunks: int | None = None,
+        pool: mp.Pool | None = None,
+        generator: torch.Generator | None = None,
+        max_tasks_per_child: int | None = None,
+        worker_threads: int = 1,
+        index_with_generator: bool = False,
+        pbar: bool = False,
+        mp_start_method: str | None = None,
+    ):
+        """Maps a function to splits of the tensordict across one dimension.
+
+        This method will apply a function to a tensordict instance by chunking
+        it in tensordicts of equal size and dispatching the operations over the
+        desired number of workers.
+
+        The function signature should be ``Callabe[[TensorDict], Union[TensorDict, Tensor]]``.
+        The output must support the :func:`torch.cat` operation. The function
+        must be serializable.
+
+        Args:
+            fn (callable): function to apply to the tensordict.
+                Signatures similar to ``Callabe[[TensorDict], Union[TensorDict, Tensor]]``
+                are supported.
+            dim (int, optional): the dim along which the tensordict will be chunked.
+            num_workers (int, optional): the number of workers. Exclusive with ``pool``.
+                If none is provided, the number of workers will be set to the
+                number of cpus available.
+
+        Keyword Args:
+            out (TensorDictBase, optional): an optional container for the output.
+                Its batch-size along the ``dim`` provided must match ``self.ndim``.
+                If it is shared or memmap (:meth:`~.is_shared` or :meth:`~.is_memmap`
+                returns ``True``) it will be populated within the remote processes,
+                avoiding data inward transfers. Otherwise, the data from the ``self``
+                slice will be sent to the process, collected on the current process
+                and written inplace into ``out``.
+            chunksize (int, optional): The size of each chunk of data.
+                A ``chunksize`` of 0 will unbind the tensordict along the
+                desired dimension and restack it after the function is applied,
+                whereas ``chunksize>0`` will split the tensordict and call
+                :func:`torch.cat` on the resulting list of tensordicts.
+                If none is provided, the number of chunks will equate the number
+                of workers. For very large tensordicts, such large chunks
+                may not fit in memory for the operation to be done and
+                more chunks may be needed to make the operation practically
+                doable. This argument is exclusive with ``num_chunks``.
+            num_chunks (int, optional): the number of chunks to split the tensordict
+                into. If none is provided, the number of chunks will equate the number
+                of workers. For very large tensordicts, such large chunks
+                may not fit in memory for the operation to be done and
+                more chunks may be needed to make the operation practically
+                doable. This argument is exclusive with ``chunksize``.
+            pool (mp.Pool, optional): a multiprocess Pool instance to use
+                to execute the job. If none is provided, a pool will be created
+                within the ``map`` method.
+            generator (torch.Generator, optional): a generator to use for seeding.
+                A base seed will be generated from it, and each worker
+                of the pool will be seeded with the provided seed incremented
+                by a unique integer from ``0`` to ``num_workers``. If no generator
+                is provided, a random integer will be used as seed.
+                To work with unseeded workers, a pool should be created separately
+                and passed to :meth:`map` directly.
+                .. note::
+                  Caution should be taken when providing a low-valued seed as
+                  this can cause autocorrelation between experiments, example:
+                  if 8 workers are asked and the seed is 4, the workers seed will
+                  range from 4 to 11. If the seed is 5, the workers seed will range
+                  from 5 to 12. These two experiments will have an overlap of 7
+                  seeds, which can have unexpected effects on the results.
+
+                .. note::
+                  The goal of seeding the workers is to have independent seed on
+                  each worker, and NOT to have reproducible results across calls
+                  of the `map` method. In other words, two experiments may and
+                  probably will return different results as it is impossible to
+                  know which worker will pick which job. However, we can make sure
+                  that each worker has a different seed and that the pseudo-random
+                  operations on each will be uncorrelated.
+            max_tasks_per_child (int, optional): the maximum number of jobs picked
+                by every child process. Defaults to ``None``, i.e., no restriction
+                on the number of jobs.
+            worker_threads (int, optional): the number of threads for the workers.
+                Defaults to ``1``.
+            index_with_generator (bool, optional): if ``True``, the splitting / chunking
+                of the tensordict will be done during the query, sparing init time.
+                Note that :meth:`~.chunk` and :meth:`~.split` are much more
+                efficient than indexing (which is used within the generator)
+                so a gain of processing time at init time may have a negative
+                impact on the total runtime. Defaults to ``False``.
+            pbar (bool, optional): if ``True``, a progress bar will be displayed.
+                Requires tqdm to be available. Defaults to ``False``.
+            mp_start_method (str, optional): the start method for multiprocessing.
+                If not provided, the default start method will be used.
+                Accepted strings are ``"fork"`` and ``"spawn"``. Keep in mind that
+                ``"cuda"`` tensors cannot be shared between processes with the
+                ``"fork"`` start method. This is without effect if the ``pool``
+                is passed to the ``map`` method.
+
+        Examples:
+            >>> import torch
+            >>> from tensordict import TensorDict
+            >>>
+            >>> def process_data(data):
+            ...     data.set("y", data.get("x") + 1)
+            ...     return data
+            >>> if __name__ == "__main__":
+            ...     data = TensorDict({"x": torch.zeros(1, 1_000_000)}, [1, 1_000_000]).memmap_()
+            ...     data = data.map(process_data, dim=1)
+            ...     print(data["y"][:, :10])
+            ...
+            tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])
+
+        .. note:: This method is particularily useful when working with large
+            datasets stored on disk (e.g. memory-mapped tensordicts) where
+            chunks will be zero-copied slices of the original data which can
+            be passed to the processes with virtually zero-cost. This allows
+            to tread very large datasets (eg. over a Tb big) to be processed
+            at little cost.
+
+        """
+        from torch import multiprocessing as mp
+
+        if pool is None:
+            if num_workers is None:
+                num_workers = mp.cpu_count()  # Get the number of CPU cores
+            if generator is None:
+                generator = torch.Generator()
+            seed = (
+                torch.empty((), dtype=torch.int64).random_(generator=generator).item()
+            )
+            if mp_start_method is not None:
+                ctx = mp.get_context(mp_start_method)
+            else:
+                ctx = mp.get_context()
+
+            queue = ctx.Queue(maxsize=num_workers)
+            for i in range(num_workers):
+                queue.put(i)
+            with ctx.Pool(
+                processes=num_workers,
+                initializer=_proc_init,
+                initargs=(seed, queue, worker_threads),
+                maxtasksperchild=max_tasks_per_child,
+            ) as pool:
+                return self.map(
+                    fn,
+                    dim=dim,
+                    chunksize=chunksize,
+                    num_chunks=num_chunks,
+                    pool=pool,
+                    pbar=pbar,
+                    out=out,
+                )
+        num_workers = pool._processes
+        dim_orig = dim
+        if dim < 0:
+            dim = self.ndim + dim
+        if dim < 0 or dim >= self.ndim:
+            raise ValueError(f"Got incompatible dimension {dim_orig}")
+
+        self_split = _split_tensordict(
+            self,
+            chunksize,
+            num_chunks,
+            num_workers,
+            dim,
+            use_generator=index_with_generator,
+        )
+        if not index_with_generator:
+            length = len(self_split)
+        else:
+            length = None
+        call_chunksize = 1
+
+        if out is not None and (out.is_shared() or out.is_memmap()):
+
+            def wrap_fn_with_out(fn, out):
+                @wraps(fn)
+                def newfn(item_and_out):
+                    item, out = item_and_out
+                    result = fn(item)
+                    out.update_(result)
+                    return
+
+                out_split = _split_tensordict(
+                    out,
+                    chunksize,
+                    num_chunks,
+                    num_workers,
+                    dim,
+                    use_generator=index_with_generator,
+                )
+                return _CloudpickleWrapper(newfn), zip(self_split, out_split)
+
+            fn, self_split = wrap_fn_with_out(fn, out)
+            out = None
+
+        imap = pool.imap(fn, self_split, call_chunksize)
+
+        if pbar and importlib.util.find_spec("tqdm", None) is not None:
+            import tqdm
+
+            imap = tqdm.tqdm(imap, total=length)
+
+        imaplist = []
+        start = 0
+        base_index = (slice(None),) * dim
+        for item in imap:
+            if item is not None:
+                if out is not None:
+                    if chunksize == 0:
+                        out[base_index + (start,)].update_(item)
+                        start += 1
+                    else:
+                        end = start + item.shape[dim]
+                        chunk = base_index + (slice(start, end),)
+                        out[chunk].update_(item)
+                        start = end
+                else:
+                    imaplist.append(item)
+        del imap
+
+        # support inplace modif
+        if imaplist:
+            if chunksize == 0:
+                from tensordict._lazy import LazyStackedTensorDict
+
+                # We want to be able to return whichever data structure
+                out = LazyStackedTensorDict.maybe_dense_stack(imaplist, dim)
+            else:
+                out = torch.cat(imaplist, dim)
+        return out
+
+    # point-wise arithmetic ops
+    def __add__(self, other: TensorDictBase | float) -> T:
+        return self.add(other)
+
+    def __iadd__(self, other: TensorDictBase | float) -> T:
+        return self.add_(other)
+
+    def __abs__(self):
+        return self.abs()
+
+    def __truediv__(self, other: TensorDictBase | float) -> T:
+        return self.div(other)
+
+    def __itruediv__(self, other: TensorDictBase | float) -> T:
+        return self.div_(other)
+
+    def __mul__(self, other: TensorDictBase | float) -> T:
+        return self.mul(other)
+
+    def __imul__(self, other: TensorDictBase | float) -> T:
+        return self.mul_(other)
+
+    def __sub__(self, other: TensorDictBase | float) -> T:
+        return self.sub(other)
+
+    def __isub__(self, other: TensorDictBase | float) -> T:
+        return self.sub_(other)
+
+    def __pow__(self, other: TensorDictBase | float) -> T:
+        return self.pow(other)
+
+    def __ipow__(self, other: TensorDictBase | float) -> T:
+        return self.pow_(other)
+
+    def abs(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_abs(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def abs_(self) -> T:
+        torch._foreach_abs_(self._values_list(True, True))
+        return self
+
+    def acos(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_acos(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def acos_(self) -> T:
+        torch._foreach_acos_(self._values_list(True, True))
+        return self
+
+    def exp(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_exp(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def exp_(self) -> T:
+        torch._foreach_exp_(self._values_list(True, True))
+        return self
+
+    def neg(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_neg(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def neg_(self) -> T:
+        torch._foreach_neg_(self._values_list(True, True))
+        return self
+
+    def reciprocal(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_reciprocal(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def reciprocal_(self) -> T:
+        torch._foreach_reciprocal_(self._values_list(True, True))
+        return self
+
+    def sigmoid(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sigmoid(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sigmoid_(self) -> T:
+        torch._foreach_sigmoid_(self._values_list(True, True))
+        return self
+
+    def sign(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sign(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sign_(self) -> T:
+        torch._foreach_sign_(self._values_list(True, True))
+        return self
+
+    def sin(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sin(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sin_(self) -> T:
+        torch._foreach_sin_(self._values_list(True, True))
+        return self
+
+    def sinh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sinh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sinh_(self) -> T:
+        torch._foreach_sinh_(self._values_list(True, True))
+        return self
+
+    def tan(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_tan(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def tan_(self) -> T:
+        torch._foreach_tan_(self._values_list(True, True))
+        return self
+
+    def tanh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_tanh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def tanh_(self) -> T:
+        torch._foreach_tanh_(self._values_list(True, True))
+        return self
+
+    def trunc(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_trunc(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def trunc_(self) -> T:
+        torch._foreach_trunc_(self._values_list(True, True))
+        return self
+
+    @implement_for("torch", None, "2.4")
+    def norm(
+        self,
+        out=None,
+        dtype: torch.dtype | None = None,
+    ):
+        keys, vals = self._items_list(True, True, collapse=True)
+        if dtype is not None:
+            raise RuntimeError("dtype must be None for torch <= 2.3")
+        vals = torch._foreach_norm(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            batch_size=[],
+            propagate_lock=True,
+        )
+
+    @implement_for("torch", "2.4")
+    def norm(  # noqa: F811
+        self,
+        out=None,
+        dtype: torch.dtype | None = None,
+    ):
+        keys, vals = self._items_list(True, True, collapse=True)
+        vals = torch._foreach_norm(vals, dtype=dtype)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            batch_size=[],
+            propagate_lock=True,
+        )
+
+    def lgamma(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_lgamma(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def lgamma_(self) -> T:
+        torch._foreach_lgamma_(self._values_list(True, True))
+        return self
+
+    def frac(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_frac(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def frac_(self) -> T:
+        torch._foreach_frac_(self._values_list(True, True))
+        return self
+
+    def expm1(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_expm1(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def expm1_(self) -> T:
+        torch._foreach_expm1_(self._values_list(True, True))
+        return self
+
+    def log(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def log_(self) -> T:
+        torch._foreach_log_(self._values_list(True, True))
+        return self
+
+    def log10(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log10(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def log10_(self) -> T:
+        torch._foreach_log10_(self._values_list(True, True))
+        return self
+
+    def log1p(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log1p(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def log1p_(self) -> T:
+        torch._foreach_log1p_(self._values_list(True, True))
+        return self
+
+    def log2(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_log2(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def log2_(self) -> T:
+        torch._foreach_log2_(self._values_list(True, True))
+        return self
+
+    def ceil(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_ceil(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def ceil_(self) -> T:
+        torch._foreach_ceil_(self._values_list(True, True))
+        return self
+
+    def floor(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_floor(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def floor_(self) -> T:
+        torch._foreach_floor_(self._values_list(True, True))
+        return self
+
+    def round(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_round(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def round_(self) -> T:
+        torch._foreach_round_(self._values_list(True, True))
+        return self
+
+    def erf(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_erf(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def erf_(self) -> T:
+        torch._foreach_erf_(self._values_list(True, True))
+        return self
+
+    def erfc(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_erfc(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def erfc_(self) -> T:
+        torch._foreach_erfc_(self._values_list(True, True))
+        return self
+
+    def asin(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_asin(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def asin_(self) -> T:
+        torch._foreach_asin_(self._values_list(True, True))
+        return self
+
+    def atan(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_atan(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def atan_(self) -> T:
+        torch._foreach_atan_(self._values_list(True, True))
+        return self
+
+    def cos(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_cos(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def cos_(self) -> T:
+        torch._foreach_cos_(self._values_list(True, True))
+        return self
+
+    def cosh(self) -> T:
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_cosh(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def cosh_(self) -> T:
+        torch._foreach_cosh_(self._values_list(True, True))
+        return self
+
+    def add(self, other: TensorDictBase | float, alpha: float | None = None):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            vals = torch._foreach_add(vals, other_val, alpha=alpha)
+        else:
+            vals = torch._foreach_add(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def add_(self, other: TensorDictBase | float, alpha: float | None = None):
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            torch._foreach_add_(self._values_list(True, True), other_val, alpha=alpha)
+        else:
+            torch._foreach_add_(self._values_list(True, True), other_val)
+        return self
+
+    def lerp(self, end: TensorDictBase | float, weight: TensorDictBase | float):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(end)):
+            end_val = end._values_list(True, True)
+        else:
+            end_val = end
+        if _is_tensor_collection(type(weight)):
+            weight_val = weight._values_list(True, True)
+        else:
+            weight_val = weight
+        vals = torch._foreach_lerp(vals, end_val, weight_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def lerp_(self, end: TensorDictBase | float, weight: TensorDictBase | float):
+        if _is_tensor_collection(type(end)):
+            end_val = end._values_list(True, True)
+        else:
+            end_val = end
+        if _is_tensor_collection(type(weight)):
+            weight_val = weight._values_list(True, True)
+        else:
+            weight_val = weight
+        torch._foreach_lerp_(self._values_list(True, True), end_val, weight_val)
+        return self
+
+    def addcdiv(self, other1, other2, value: float | None = 1):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        vals = torch._foreach_addcdiv(vals, other1_val, other2_val, value=value)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def addcdiv_(self, other1, other2, value: float | None = 1):
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        torch._foreach_addcdiv_(
+            self._values_list(True, True), other1_val, other2_val, value=value
+        )
+        return self
+
+    def addcmul(self, other1, other2, value: float | None = 1):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        vals = torch._foreach_addcmul(vals, other1_val, other2_val, value=value)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def addcmul_(self, other1, other2, value: float | None = 1):
+        if _is_tensor_collection(type(other1)):
+            other1_val = other1._values_list(True, True)
+        else:
+            other1_val = other1
+        if _is_tensor_collection(type(other2)):
+            other2_val = other2._values_list(True, True)
+        else:
+            other2_val = other2
+        torch._foreach_addcmul_(
+            self._values_list(True, True), other1_val, other2_val, value=value
+        )
+        return self
+
+    def sub(self, other: TensorDictBase | float, alpha: float | None = None):
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            vals = torch._foreach_sub(vals, other_val, alpha=alpha)
+        else:
+            vals = torch._foreach_sub(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sub_(self, other: TensorDictBase | float, alpha: float | None = None):
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        if alpha is not None:
+            torch._foreach_sub_(self._values_list(True, True), other_val, alpha=alpha)
+        else:
+            torch._foreach_sub_(self._values_list(True, True), other_val)
+        return self
+
+    def mul_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_mul_(self._values_list(True, True), other_val)
+        return self
+
+    def mul(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_mul(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def maximum_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_maximum_(self._values_list(True, True), other_val)
+        return self
+
+    def maximum(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_maximum(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def minimum_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_minimum_(self._values_list(True, True), other_val)
+        return self
+
+    def minimum(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_minimum(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def clamp_max_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_clamp_max_(self._values_list(True, True), other_val)
+        return self
+
+    def clamp_max(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_clamp_max(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def clamp_min_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_clamp_min_(self._values_list(True, True), other_val)
+        return self
+
+    def clamp_min(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_clamp_min(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def pow_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_pow_(self._values_list(True, True), other_val)
+        return self
+
+    def pow(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_pow(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def div_(self, other: TensorDictBase | float) -> T:
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        torch._foreach_div_(self._values_list(True, True), other_val)
+        return self
+
+    def div(self, other: TensorDictBase | float) -> T:
+        keys, vals = self._items_list(True, True)
+        if _is_tensor_collection(type(other)):
+            other_val = other._values_list(True, True)
+        else:
+            other_val = other
+        vals = torch._foreach_div(vals, other_val)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    def sqrt_(self):
+        torch._foreach_sqrt_(self._values_list(True, True))
+        return self
+
+    def sqrt(self):
+        keys, vals = self._items_list(True, True)
+        vals = torch._foreach_sqrt(vals)
+        items = dict(zip(keys, vals))
+        return self._fast_apply(
+            lambda name, val: items[name],
+            named=True,
+            nested_keys=True,
+            is_leaf=_NESTED_TENSORS_AS_LISTS,
+            propagate_lock=True,
+        )
+
+    # Functorch compatibility
+    @abc.abstractmethod
+    @cache  # noqa: B019
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        ...
+
+    @abc.abstractmethod
+    @cache  # noqa: B019
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        ...
+
+    # Validation and checks
+    def _convert_to_tensor(self, array: np.ndarray) -> Tensor:
+        if isinstance(array, (float, int, np.ndarray, bool)):
+            pass
+        elif isinstance(array, np.bool_):
+            array = array.item()
+        elif isinstance(array, list):
+            array = np.asarray(array)
+        elif hasattr(array, "numpy"):
+            # tf.Tensor with no shape can't be converted otherwise
+            array = array.numpy()
+        try:
+            return torch.as_tensor(array, device=self.device)
+        except Exception:
+            from tensordict.tensorclass import NonTensorData
+
+            return NonTensorData(
+                array,
+                batch_size=self.batch_size,
+                device=self.device,
+                names=self.names if self._has_names() else None,
+            )
+
+    @abc.abstractmethod
+    def _convert_to_tensordict(self, dict_value: dict[str, Any]) -> T:
+        ...
+
+    def _check_batch_size(self) -> None:
+        batch_dims = self.batch_dims
+        for value in self.values():
+            if _is_tensor_collection(type(value)):
+                value._check_batch_size()
+            if _shape(value)[:batch_dims] != self.batch_size:
+                raise RuntimeError(
+                    f"batch_size are incongruent, got value with shape {_shape(value)}, "
+                    f"-- expected {self.batch_size}"
+                )
+
+    @abc.abstractmethod
+    def _check_is_shared(self) -> bool:
+        ...
+
+    def _check_new_batch_size(self, new_size: torch.Size) -> None:
+        batch_dims = len(new_size)
+        for key, tensor in self.items():
+            if _shape(tensor)[:batch_dims] != new_size:
+                raise RuntimeError(
+                    f"the tensor {key} has shape {_shape(tensor)} which "
+                    f"is incompatible with the batch-size {new_size}."
+                )
+
+    @abc.abstractmethod
+    def _check_device(self) -> None:
+        ...
+
+    def _validate_key(self, key: NestedKey) -> NestedKey:
+        key = _unravel_key_to_tuple(key)
+        if not key:
+            raise KeyError(_GENERIC_NESTED_ERR.format(key))
+        return key
+
+    def _validate_value(
+        self,
+        value: CompatibleType | dict[str, CompatibleType],
+        *,
+        check_shape: bool = True,
+    ) -> CompatibleType | dict[str, CompatibleType]:
+        cls = type(value)
+        is_tc = None
+        if issubclass(cls, dict):
+            value = self._convert_to_tensordict(value)
+            is_tc = True
+        elif not issubclass(cls, _ACCEPTED_CLASSES):
+            try:
+                value = self._convert_to_tensor(value)
+            except ValueError as err:
+                raise ValueError(
+                    f"TensorDict conversion only supports tensorclasses, tensordicts,"
+                    f" numeric scalars and tensors. Got {type(value)}"
+                ) from err
+        batch_size = self.batch_size
+        check_shape = check_shape and self.batch_size
+        if (
+            check_shape
+            and batch_size
+            and _shape(value)[: self.batch_dims] != batch_size
+        ):
+            # if TensorDict, let's try to map it to the desired shape
+            if is_tc is None:
+                is_tc = _is_tensor_collection(cls)
+            if is_tc:
+                # we must clone the value before not to corrupt the data passed to set()
+                value = value.clone(recurse=False)
+                value.batch_size = self.batch_size
+            else:
+                raise RuntimeError(
+                    f"batch dimension mismatch, got self.batch_size"
+                    f"={self.batch_size} and value.shape={_shape(value)}."
+                )
+        device = self.device
+        if device is not None and value.device != device:
+            value = value.to(device, non_blocking=True)
+        if check_shape:
+            if is_tc is None:
+                is_tc = _is_tensor_collection(cls)
+            if not is_tc:
+                return value
+            has_names = self._has_names()
+            # we do our best to match the dim names of the value and the
+            # container.
+            if has_names and value.names[: self.batch_dims] != self.names:
+                # we clone not to corrupt the value
+                value = value.clone(False).refine_names(*self.names)
+            elif not has_names and value._has_names():
+                self.names = value.names[: self.batch_dims]
+        return value
+
+    # Context manager functionality
+    @property
+    def _last_op_queue(self):
+        # this is used to keep track of the last operation when using
+        # the tensordict as a context manager.
+        last_op_queue = self.__dict__.get("__last_op_queue", None)
+        if last_op_queue is None:
+            last_op_queue = collections.deque()
+            self.__dict__["__last_op_queue"] = last_op_queue
+        return last_op_queue
+
+    def __enter__(self):
+        self._last_op_queue.append(self._last_op)
+        return self
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        # During exit, updates mustn't be made in-place as the source and dest
+        # storage location can be identical, resulting in a RuntimeError
+        if exc_type is not None and issubclass(exc_type, Exception):
+            return False
+        _last_op = self._last_op_queue.pop()
+        if _last_op is not None:
+            last_op, (args, kwargs, out) = _last_op
+            # TODO: transpose, flatten etc. as decorator should lock the content to make sure that no key is
+            #  added or deleted
+            if last_op == self.__class__.lock_.__name__:
+                return self.unlock_()
+            elif last_op == self.__class__.unlock_.__name__:
+                return self.lock_()
+            elif last_op == self.__class__.transpose.__name__:
+                dim0, dim1 = args
+                if not out.is_locked:
+                    return out.update(self.transpose(dim0, dim1), inplace=False)
+                else:
+                    return out.update_(self.transpose(dim0, dim1))
+            elif last_op == self.__class__.flatten.__name__:
+                if len(args) == 2:
+                    dim0, dim1 = args
+                elif len(args) == 1:
+                    dim0 = args[0]
+                    dim1 = kwargs.get("end_dim", -1)
+                else:
+                    dim0 = kwargs.get("start_dim", 0)
+                    dim1 = kwargs.get("end_dim", -1)
+                if dim1 < 0:
+                    dim1 = out.ndim + dim1
+                if dim0 < 0:
+                    dim0 = out.ndim + dim0
+
+                if not out.is_locked:
+                    return out.update(
+                        self.unflatten(dim0, out.shape[dim0 : dim1 + 1]), inplace=False
+                    )
+                else:
+                    return out.update_(self.unflatten(dim0, out.shape[dim0 : dim1 + 1]))
+
+            elif last_op == self.__class__.unflatten.__name__:
+                if args:
+                    dim0 = args[0]
+                    if len(args) > 1:
+                        unflattened_size = args[1]
+                    else:
+                        unflattened_size = kwargs.get("unflattened_size")
+                else:
+                    dim0 = kwargs.get("dim")
+                    unflattened_size = kwargs.get("unflattened_size")
+                if dim0 < 0:
+                    dim0 = out.ndim + dim0
+                dim1 = dim0 + len(unflattened_size) - 1
+                if not out.is_locked:
+                    return out.update(self.flatten(dim0, dim1), inplace=False)
+                else:
+                    return out.update_(self.flatten(dim0, dim1))
+
+            elif last_op == self.__class__.permute.__name__:
+                dims_list = _get_shape_from_args(*args, kwarg_name="dims", **kwargs)
+                dims_list = [dim if dim >= 0 else self.ndim + dim for dim in dims_list]
+                # inverse map
+                inv_dims_list = np.argsort(dims_list)
+                if not out.is_locked:
+                    return out.update(self.permute(inv_dims_list), inplace=False)
+                else:
+                    return out.update_(self.permute(inv_dims_list))
+            elif last_op == self.__class__.view.__name__:
+                if not out.is_locked:
+                    return out.update(self.view(out.shape), inplace=False)
+                else:
+                    return out.update_(self.view(out.shape))
+            elif last_op == self.__class__.unsqueeze.__name__:
+                if args:
+                    (dim,) = args
+                elif kwargs:
+                    dim = kwargs["dim"]
+                else:
+                    raise RuntimeError(
+                        "Cannot use td.unsqueeze() as a decorator if the dimension is implicit."
+                    )
+                if not out.is_locked:
+                    return out.update(self.squeeze(dim), inplace=False)
+                else:
+                    return out.update_(self.squeeze(dim))
+            elif last_op == self.__class__.squeeze.__name__:
+                if args:
+                    (dim,) = args
+                elif kwargs:
+                    dim = kwargs["dim"]
+                else:
+                    raise RuntimeError(
+                        "Cannot use td.squeeze() as a decorator if the dimension is implicit."
+                    )
+                if not out.is_locked:
+                    return out.update(self.unsqueeze(dim), inplace=False)
+                else:
+                    return out.update_(self.unsqueeze(dim))
+            elif last_op == self.__class__.to_module.__name__:
+                if is_tensor_collection(out):
+                    with out.unlock_():
+                        return self.to_module(*args, **kwargs, swap_dest=out)
+                else:
+                    raise RuntimeError(
+                        "to_module cannot be used as a decorator when return_swap=False."
+                    )
+            else:
+                raise NotImplementedError(f"Unrecognised function {last_op}.")
+        return self
+
+    # Clone, select, exclude, empty
+    def select(self, *keys: NestedKey, inplace: bool = False, strict: bool = True) -> T:
+        """Selects the keys of the tensordict and returns a new tensordict with only the selected keys.
+
+        The values are not copied: in-place modifications a tensor of either
+        of the original or new tensordict will result in a change in both
+        tensordicts.
+
+        Args:
+            *keys (str): keys to select
+            inplace (bool): if True, the tensordict is pruned in place.
+                Default is ``False``.
+            strict (bool, optional): whether selecting a key that is not present
+                will return an error or not. Default: :obj:`True`.
+
+        Returns:
+            A new tensordict (or the same if ``inplace=True``) with the selected keys only.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> td = TensorDict({"a": 0, "b": {"c": 1, "d": 2}}, [])
+            >>> td.select("a", ("b", "c"))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> td.select("a", "b")
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                            d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> td.select("this key does not exist", strict=False)
+            TensorDict(
+                fields={
+                },
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        keys = unravel_key_list(keys)
+        result = self._select(*keys, inplace=inplace, strict=strict)
+        if not inplace and (result._is_memmap or result._is_shared):
+            result.lock_()
+        return result
+
+    @abc.abstractmethod
+    def _select(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        strict: bool = True,
+        set_shared: bool = True,
+    ) -> T:
+        ...
+
+    def exclude(self, *keys: NestedKey, inplace: bool = False) -> T:
+        """Excludes the keys of the tensordict and returns a new tensordict without these entries.
+
+        The values are not copied: in-place modifications a tensor of either
+        of the original or new tensordict will result in a change in both
+        tensordicts.
+
+        Args:
+            *keys (str): keys to exclude.
+            inplace (bool): if True, the tensordict is pruned in place.
+                Default is ``False``.
+
+        Returns:
+            A new tensordict (or the same if ``inplace=True``) without the excluded entries.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> td = TensorDict({"a": 0, "b": {"c": 1, "d": 2}}, [])
+            >>> td.exclude("a", ("b", "c"))
+            TensorDict(
+                fields={
+                    b: TensorDict(
+                        fields={
+                            d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> td.exclude("a", "b")
+            TensorDict(
+                fields={
+                },
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        keys = unravel_key_list(keys)
+        result = self._exclude(*keys, inplace=inplace)
+        if not inplace and (result._is_memmap or result._is_shared):
+            result.lock_()
+        return result
+
+    @abc.abstractmethod
+    def _exclude(
+        self,
+        *keys: NestedKey,
+        inplace: bool = False,
+        set_shared: bool = True,
+    ) -> T:
+        ...
+
+    def _maybe_set_shared_attributes(self, result, lock=False):
+        # We must use _is_shared to avoid having issues with CUDA tensordicts
+        if self._is_shared:
+            result._is_shared = True
+            if lock:
+                result.lock_()
+        elif self._is_memmap:
+            result._is_memmap = True
+            if lock:
+                result.lock_()
+
+    def to_tensordict(self) -> T:
+        """Returns a regular TensorDict instance from the TensorDictBase.
+
+        Returns:
+            a new TensorDict object containing the same values.
+
+        """
+        from tensordict import TensorDict
+
+        return TensorDict(
+            {
+                key: value.clone()
+                if not _is_tensor_collection(value.__class__)
+                else value
+                if is_non_tensor(value)
+                else value.to_tensordict()
+                for key, value in self.items(is_leaf=_is_leaf_nontensor)
+            },
+            device=self.device,
+            batch_size=self.batch_size,
+            names=self.names if self._has_names() else None,
+        )
+
+    def clone(self, recurse: bool = True, **kwargs) -> T:
+        """Clones a TensorDictBase subclass instance onto a new TensorDictBase subclass of the same type.
+
+        To create a TensorDict instance from any other TensorDictBase subtype, call the :meth:`~.to_tensordict` method
+        instead.
+
+        Args:
+            recurse (bool, optional): if ``True``, each tensor contained in the
+                TensorDict will be copied too. Otherwise only the TensorDict
+                tree structure will be copied. Defaults to ``True``.
+
+        .. note:: Unlike many other ops (pointwise arithmetic, shape operations, ...) ``clone`` does not inherit the
+            original lock attribute. This design choice is made such that a clone can be created to be modified,
+            which is the most frequent usage.
+
+        """
+        result = self._clone(recurse=recurse, **kwargs)
+        if not recurse and (result._is_shared or result._is_memmap):
+            result.lock_()
+        return result
+
+    @abc.abstractmethod
+    def _clone(self, recurse: bool = False):
+        ...
+
+    def copy(self):
+        """Return a shallow copy of the tensordict (ie, copies the structure but not the data).
+
+        Equivalent to `TensorDictBase.clone(recurse=False)`
+        """
+        return self.clone(recurse=False)
+
+    def to_padded_tensor(self, padding=0.0, mask_key: NestedKey | None = None):
+        """Converts all nested tensors to a padded version and adapts the batch-size accordingly.
+
+        Args:
+            padding (float): the padding value for the tensors in the tensordict.
+                Defaults to ``0.0``.
+            mask_key (NestedKey, optional): if provided, the key where a
+                mask for valid values will be written.
+                Will result in an error if the heterogeneous dimension
+                isn't part of the tensordict batch-size.
+                Defaults to ``None``
+
+        """
+        batch_size = self.batch_size
+        if any(shape == -1 for shape in batch_size):
+            new_batch_size = []
+        else:
+            new_batch_size = None
+            if mask_key is not None:
+                raise RuntimeError(
+                    "mask_key should only be provided if the "
+                    "heterogenous dimension is part of the batch-size."
+                )
+        padded_names = []
+
+        def to_padded(name, x):
+            if x.is_nested:
+                padded_names.append(name)
+                return torch.nested.to_padded_tensor(x, padding=padding)
+            return x
+
+        result = self._apply_nest(
+            to_padded,
+            batch_size=new_batch_size,
+            named=True,
+            nested_keys=True,
+        )
+        if new_batch_size is not None:
+            result = result.auto_batch_size_(batch_dims=self.batch_dims)
+
+            if mask_key:
+                # take the first of the padded keys
+                padded_key = padded_names[0]
+                # write the mask
+                val = self.get(padded_key)
+                val = torch.nested.to_padded_tensor(
+                    torch.ones_like(val, dtype=torch.bool), padding=False
+                )
+                if val.ndim > result.ndim:
+                    val = val.flatten(result.ndim, -1)[..., -1].clone()
+                result.set(mask_key, val)
+        return result
+
+    def as_tensor(self):
+        def as_tensor(tensor):
+            try:
+                return tensor.as_tensor()
+            except AttributeError:
+                return tensor
+
+        return self._fast_apply(as_tensor, propagate_lock=True)
+
+    def to_dict(self) -> dict[str, Any]:
+        """Returns a dictionary with key-value pairs matching those of the tensordict."""
+        return {
+            key: value.to_dict() if _is_tensor_collection(type(value)) else value
+            for key, value in self.items()
+        }
+
+    def numpy(self):
+        """Converts a tensordict to a (possibly nested) dictionary of numpy arrays.
+
+        Non-tensor data is exposed as such.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>> data = TensorDict({"a": {"b": torch.zeros(()), "c": "a string!"}})
+            >>> print(data)
+            TensorDict(
+                fields={
+                    a: TensorDict(
+                        fields={
+                            b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                            c: NonTensorData(data=a string!, batch_size=torch.Size([]), device=None)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> print(data.numpy())
+            {'a': {'b': array(0., dtype=float32), 'c': 'a string!'}}
+
+        """
+        as_dict = self.to_dict()
+
+        def to_numpy(x):
+            if isinstance(x, torch.Tensor):
+                if x.is_nested:
+                    return tuple(_x.numpy() for _x in x)
+                return x.numpy()
+            if hasattr(x, "numpy"):
+                return x.numpy()
+            return x
+
+        return torch.utils._pytree.tree_map(to_numpy, as_dict)
+
+    def to_namedtuple(self):
+        """Converts a tensordict to a namedtuple.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>> data = TensorDict({
+            ...     "a_tensor": torch.zeros((3)),
+            ...     "nested": {"a_tensor": torch.zeros((3)), "a_string": "zero!"}}, [3])
+            >>> data.to_namedtuple()
+            GenericDict(a_tensor=tensor([0., 0., 0.]), nested=GenericDict(a_tensor=tensor([0., 0., 0.]), a_string='zero!'))
+
+        """
+
+        def dict_to_namedtuple(dictionary):
+            for key, value in dictionary.items():
+                if isinstance(value, dict):
+                    dictionary[key] = dict_to_namedtuple(value)
+            return collections.namedtuple("GenericDict", dictionary.keys())(
+                **dictionary
+            )
+
+        return dict_to_namedtuple(self.to_dict())
+
+    @classmethod
+    def from_namedtuple(cls, named_tuple, *, auto_batch_size: bool = False):
+        """Converts a namedtuple to a TensorDict recursively.
+
+        Keyword Args:
+            auto_batch_size (bool, optional): if ``True``, the batch size will be computed automatically.
+                Defaults to ``False``.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> import torch
+            >>> data = TensorDict({
+            ...     "a_tensor": torch.zeros((3)),
+            ...     "nested": {"a_tensor": torch.zeros((3)), "a_string": "zero!"}}, [3])
+            >>> nt = data.to_namedtuple()
+            >>> print(nt)
+            GenericDict(a_tensor=tensor([0., 0., 0.]), nested=GenericDict(a_tensor=tensor([0., 0., 0.]), a_string='zero!'))
+            >>> TensorDict.from_namedtuple(nt, auto_batch_size=True)
+            TensorDict(
+                fields={
+                    a_tensor: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                    nested: TensorDict(
+                        fields={
+                            a_string: NonTensorData(data=zero!, batch_size=torch.Size([3]), device=None),
+                            a_tensor: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([3]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([3]),
+                device=None,
+                is_shared=False)
+
+        """
+        from tensordict import TensorDict
+
+        def is_namedtuple(obj):
+            """Check if obj is a namedtuple."""
+            return isinstance(obj, tuple) and hasattr(obj, "_fields")
+
+        def namedtuple_to_dict(namedtuple_obj):
+            if is_namedtuple(namedtuple_obj):
+                namedtuple_obj = namedtuple_obj._asdict()
+            for key, value in namedtuple_obj.items():
+                if is_namedtuple(value):
+                    namedtuple_obj[key] = namedtuple_to_dict(value)
+            return dict(namedtuple_obj)
+
+        result = TensorDict(namedtuple_to_dict(named_tuple))
+        if auto_batch_size:
+            result.auto_batch_size_()
+        return result
+
+    def to_h5(
+        self,
+        filename,
+        **kwargs,
+    ):
+        """Converts a tensordict to a PersistentTensorDict with the h5 backend.
+
+        Args:
+            filename (str or path): path to the h5 file.
+            device (torch.device or compatible, optional): the device where to
+                expect the tensor once they are returned. Defaults to ``None``
+                (on cpu by default).
+            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+        Returns:
+            A :class:`~.tensordict.PersitentTensorDict` instance linked to the newly created file.
+
+        Examples:
+            >>> import tempfile
+            >>> import timeit
+            >>>
+            >>> from tensordict import TensorDict, MemoryMappedTensor
+            >>> td = TensorDict({
+            ...     "a": MemoryMappedTensor.from_tensor(torch.zeros(()).expand(1_000_000)),
+            ...     "b": {"c": MemoryMappedTensor.from_tensor(torch.zeros(()).expand(1_000_000, 3))},
+            ... }, [1_000_000])
+            >>>
+            >>> file = tempfile.NamedTemporaryFile()
+            >>> td_h5 = td.to_h5(file.name, compression="gzip", compression_opts=9)
+            >>> print(td_h5)
+            PersistentTensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([1000000]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: PersistentTensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([1000000, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([1000000]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([1000000]),
+                device=None,
+                is_shared=False)
+
+
+        """
+        from tensordict.persistent import PersistentTensorDict
+
+        out = PersistentTensorDict.from_dict(
+            self,
+            filename=filename,
+            **kwargs,
+        )
+        if self._has_names():
+            out.names = self.names
+        return out
+
+    def empty(
+        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
+    ) -> T:  # noqa: D417
+        """Returns a new, empty tensordict with the same device and batch size.
+
+        Args:
+            recurse (bool, optional): if ``True``, the entire structure of the
+                ``TensorDict`` will be reproduced without content.
+                Otherwise, only the root will be duplicated.
+                Defaults to ``False``.
+
+        Keyword Args:
+            batch_size (torch.Size, optional): a new batch-size for the tensordict.
+            device (torch.device, optional): a new device.
+            names (list of str, optional): dimension names.
+
+        """
+        if not recurse:
+            result = self._select(set_shared=False)
+        else:
+            # simply exclude the leaves
+            result = self._exclude(*self.keys(True, True), set_shared=False)
+        if batch_size is not None:
+            result.batch_size = batch_size
+        if device is not NO_DEFAULT:
+            if device is None:
+                result.clear_device_()
+            else:
+                result = result.to(device)
+        if names is not None:
+            result.names = names
+        return result
+
+    # Filling
+    def zero_(self) -> T:
+        """Zeros all tensors in the tensordict in-place."""
+
+        def fn(item):
+            item.zero_()
+
+        self._fast_apply(fn=fn, call_on_nested=True, propagate_lock=True)
+        return self
+
+    def fill_(self, key: NestedKey, value: float | bool) -> T:
+        """Fills a tensor pointed by the key with a given scalar value.
+
+        Args:
+            key (str or nested key): entry to be filled.
+            value (Number or bool): value to use for the filling.
+
+        Returns:
+            self
+
+        """
+        key = _unravel_key_to_tuple(key)
+        data = self._get_tuple(key, NO_DEFAULT)
+        if _is_tensor_collection(type(data)):
+            data._fast_apply(lambda x: x.fill_(value), inplace=True)
+        else:
+            data = data.fill_(value)
+            self._set_tuple(key, data, inplace=True, validated=True, non_blocking=False)
+        return self
+
+    # Masking
+    @abc.abstractmethod
+    def masked_fill_(self, mask: Tensor, value: float | bool) -> T:
+        """Fills the values corresponding to the mask with the desired value.
+
+        Args:
+            mask (boolean torch.Tensor): mask of values to be filled. Shape
+                must match the tensordict batch-size.
+            value: value to used to fill the tensors.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...     batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> td.masked_fill_(mask, 1.0)
+            >>> td.get("a")
+            tensor([[1., 1., 1., 1.],
+                    [0., 0., 0., 0.],
+                    [0., 0., 0., 0.]])
+        """
+        ...
+
+    @abc.abstractmethod
+    def masked_fill(self, mask: Tensor, value: float | bool) -> T:
+        """Out-of-place version of masked_fill.
+
+        Args:
+            mask (boolean torch.Tensor): mask of values to be filled. Shape
+                must match the tensordict batch-size.
+            value: value to used to fill the tensors.
+
+        Returns:
+            self
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...     batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> td1 = td.masked_fill(mask, 1.0)
+            >>> td1.get("a")
+            tensor([[1., 1., 1., 1.],
+                    [0., 0., 0., 0.],
+                    [0., 0., 0., 0.]])
+        """
+        ...
+
+    def where(self, condition, other, *, out=None, pad=None):  # noqa: D417
+        """Return a ``TensorDict`` of elements selected from either self or other, depending on condition.
+
+        Args:
+            condition (BoolTensor): When ``True`` (nonzero), yields ``self``,
+                otherwise yields ``other``.
+            other (TensorDictBase or Scalar): value (if ``other`` is a scalar)
+                or values selected at indices where condition is ``False``.
+
+        Keyword Args:
+            out (TensorDictBase, optional): the output ``TensorDictBase`` instance.
+            pad (scalar, optional): if provided, missing keys from the source
+                or destination tensordict will be written as `torch.where(mask, self, pad)`
+                or `torch.where(mask, pad, other)`. Defaults to ``None``, ie
+                missing keys are not tolerated.
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def masked_select(self, mask: Tensor) -> T:
+        """Masks all tensors of the TensorDict and return a new TensorDict instance with similar keys pointing to masked values.
+
+        Args:
+            mask (torch.Tensor): boolean mask to be used for the tensors.
+                Shape must match the TensorDict ``batch_size``.
+
+        Examples:
+            >>> td = TensorDict(source={'a': torch.zeros(3, 4)},
+            ...    batch_size=[3])
+            >>> mask = torch.tensor([True, False, False])
+            >>> td_mask = td.masked_select(mask)
+            >>> td_mask.get("a")
+            tensor([[0., 0., 0., 0.]])
+
+        """
+        ...
+
+    @abc.abstractmethod
+    def _change_batch_size(self, new_size: torch.Size) -> None:
+        ...
+
+    @abc.abstractmethod
+    def is_contiguous(self) -> bool:
+        """Returns a boolean indicating if all the tensors are contiguous."""
+        ...
+
+    @abc.abstractmethod
+    def contiguous(self) -> T:
+        """Returns a new tensordict of the same type with contiguous values (or self if values are already contiguous)."""
+        ...
+
+    @cache  # noqa: B019
+    def flatten_keys(
+        self,
+        separator: str = ".",
+        inplace: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> T:
+        """Converts a nested tensordict into a flat one, recursively.
+
+        The TensorDict type will be lost and the result will be a simple TensorDict instance.
+
+        Args:
+            separator (str, optional): the separator between the nested items.
+            inplace (bool, optional): if ``True``, the resulting tensordict will
+                have the same identity as the one where the call has been made.
+                Defaults to ``False``.
+            is_leaf (callable, optional): a callable over a class type returning
+                a bool indicating if this class has to be considered as a leaf.
+
+        Examples:
+            >>> data = TensorDict({"a": 1, ("b", "c"): 2, ("e", "f", "g"): 3}, batch_size=[])
+            >>> data.flatten_keys(separator=" - ")
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b - c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    e - f - g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This method and :meth:`~.unflatten_keys` are particularily useful when
+        handling state-dicts, as they make it possible to seamlessly convert
+        flat dictionaries into data structures that mimic the structure of the
+        model.
+
+        Examples:
+            >>> model = torch.nn.Sequential(torch.nn.Linear(3 ,4))
+            >>> ddp_model = torch.ao.quantization.QuantWrapper(model)
+            >>> state_dict = TensorDict(ddp_model.state_dict(), batch_size=[]).unflatten_keys(".")
+            >>> print(state_dict)
+            TensorDict(
+                fields={
+                    module: TensorDict(
+                        fields={
+                            0: TensorDict(
+                                fields={
+                                    bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                                    weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                                batch_size=torch.Size([]),
+                                device=None,
+                                is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> model_state_dict = state_dict.get("module")
+            >>> print(model_state_dict)
+            TensorDict(
+                fields={
+                    0: TensorDict(
+                        fields={
+                            bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                            weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> model.load_state_dict(dict(model_state_dict.flatten_keys(".")))
+        """
+        if inplace:
+            return self._flatten_keys_inplace(separator=separator, is_leaf=is_leaf)
+        return self._flatten_keys_outplace(separator=separator, is_leaf=is_leaf)
+
+    def _flatten_keys_outplace(self, separator, is_leaf):
+        if is_leaf is None:
+            is_leaf = _is_leaf_nontensor
+        all_leaves_all_vals = zip(
+            *self.items(include_nested=True, leaves_only=True, is_leaf=is_leaf)
+        )
+        try:
+            all_leaves, all_vals = all_leaves_all_vals
+        except ValueError:
+            return self.empty()
+        all_leaves_flat = [
+            key if isinstance(key, str) else separator.join(key) for key in all_leaves
+        ]
+
+        if len(set(all_leaves_flat)) < len(all_leaves_flat):
+            # find duplicates
+            seen = set()
+            conflicts = []
+            for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
+                if leaf_flat in seen:
+                    conflicts.append(leaf)
+                else:
+                    seen.add(leaf_flat)
+            raise KeyError(
+                f"Flattening keys in tensordict causes keys {conflicts} to collide."
+            )
+        result = self.empty()
+        _set_dict = getattr(result, "_set_dict", None)
+        if _set_dict is not None:
+            _set_dict(
+                dict(zip(all_leaves_flat, all_vals)),
+                validated=True,
+            )
+        else:
+            for val, leaf_flat in zip(all_vals, all_leaves_flat):
+                result._set_str(
+                    leaf_flat,
+                    val,
+                    validated=True,
+                    inplace=False,
+                    non_blocking=False,
+                )
+        # Uncomment if you want key operations to propagate the shared status
+        # self._maybe_set_shared_attributes(result)
+        # if result._is_shared or result._is_memmap:
+        #     result.lock_()
+        return result
+
+    def _flatten_keys_inplace(self, separator, is_leaf):
+        if is_leaf is None:
+            is_leaf = _is_leaf_nontensor
+        all_leaves = [
+            _unravel_key_to_tuple(key)
+            for key in self.keys(include_nested=True, leaves_only=True, is_leaf=is_leaf)
+        ]
+        all_leaves_flat = [separator.join(key) for key in all_leaves]
+        if len(set(all_leaves_flat)) < len(set(all_leaves)):
+            # find duplicates
+            seen = set()
+            conflicts = []
+            for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
+                if leaf_flat in seen:
+                    conflicts.append(leaf)
+                else:
+                    seen.add(leaf_flat)
+            raise KeyError(
+                f"Flattening keys in tensordict causes keys {conflicts} to collide."
+            )
+        # we will need to remove the empty tensordicts later on
+        root_keys = set(self.keys())
+        for leaf, leaf_flat in zip(all_leaves, all_leaves_flat):
+            self.rename_key_(leaf, leaf_flat)
+            if isinstance(leaf, str):
+                root_keys.discard(leaf)
+        self.exclude(*root_keys, inplace=True)
+        return self
+
+    @cache  # noqa: B019
+    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
+        """Converts a flat tensordict into a nested one, recursively.
+
+        The TensorDict type will be lost and the result will be a simple TensorDict instance.
+        The metadata of the nested tensordicts will be inferred from the root:
+        all instances across the data tree will share the same batch-size,
+        dimension names and device.
+
+        Args:
+            separator (str, optional): the separator between the nested items.
+            inplace (bool, optional): if ``True``, the resulting tensordict will
+                have the same identity as the one where the call has been made.
+                Defaults to ``False``.
+
+        Examples:
+            >>> data = TensorDict({"a": 1, "b - c": 2, "e - f - g": 3}, batch_size=[])
+            >>> data.unflatten_keys(separator=" - ")
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),
+                    b: TensorDict(
+                        fields={
+                            c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False),
+                    e: TensorDict(
+                        fields={
+                            f: TensorDict(
+                                fields={
+                                    g: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},
+                                batch_size=torch.Size([]),
+                                device=None,
+                                is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This method and :meth:`~.unflatten_keys` are particularily useful when
+        handling state-dicts, as they make it possible to seamlessly convert
+        flat dictionaries into data structures that mimic the structure of the
+        model.
+
+        Examples:
+            >>> model = torch.nn.Sequential(torch.nn.Linear(3 ,4))
+            >>> ddp_model = torch.ao.quantization.QuantWrapper(model)
+            >>> state_dict = TensorDict(ddp_model.state_dict(), batch_size=[]).unflatten_keys(".")
+            >>> print(state_dict)
+            TensorDict(
+                fields={
+                    module: TensorDict(
+                        fields={
+                            0: TensorDict(
+                                fields={
+                                    bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                                    weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                                batch_size=torch.Size([]),
+                                device=None,
+                                is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> model_state_dict = state_dict.get("module")
+            >>> print(model_state_dict)
+            TensorDict(
+                fields={
+                    0: TensorDict(
+                        fields={
+                            bias: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                            weight: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                        batch_size=torch.Size([]),
+                        device=None,
+                        is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> model.load_state_dict(dict(model_state_dict.flatten_keys(".")))
+
+        """
+        if not inplace:
+            result = self._clone(recurse=False).unflatten_keys(
+                separator=separator, inplace=True
+            )
+            if result._is_shared or result._is_memmap:
+                result.lock_()
+            return result
+        else:
+            for key in list(self.keys()):
+                if separator in key:
+                    new_key = tuple(key.split(separator))
+                    try:
+                        self.rename_key_(key, new_key, safe=True)
+                    except KeyError:
+                        raise KeyError(
+                            f"Unflattening key(s) in tensordict will override an existing for unflattened key {new_key}."
+                        )
+            return self
+
+    @abc.abstractmethod
+    def _index_tensordict(
+        self,
+        index: IndexType,
+        new_batch_size: torch.Size | None = None,
+        names: List[str] | None = None,
+    ) -> T:
+        ...
+
+    # Locking functionality
+    @property
+    def is_locked(self) -> bool:
+        return self._is_locked
+
+    @is_locked.setter
+    def is_locked(self, value: bool) -> None:
+        if value:
+            self.lock_()
+        else:
+            self.unlock_()
+
+    def _propagate_lock(self, lock_parents_weakrefs=None):
+        """Registers the parent tensordict that handles the lock."""
+        self._is_locked = True
+        is_root = lock_parents_weakrefs is None
+        if is_root:
+            lock_parents_weakrefs = []
+        self._lock_parents_weakrefs = (
+            self._lock_parents_weakrefs + lock_parents_weakrefs
+        )
+        lock_parents_weakrefs = copy(lock_parents_weakrefs) + [weakref.ref(self)]
+        for value in self.values():
+            if _is_tensor_collection(type(value)):
+                value._propagate_lock(lock_parents_weakrefs)
+
+    @property
+    def _lock_parents_weakrefs(self):
+        _lock_parents_weakrefs = self.__dict__.get("__lock_parents_weakrefs", None)
+        if _lock_parents_weakrefs is None:
+            self.__dict__["__lock_parents_weakrefs"] = []
+            _lock_parents_weakrefs = self.__dict__["__lock_parents_weakrefs"]
+        return _lock_parents_weakrefs
+
+    @_lock_parents_weakrefs.setter
+    def _lock_parents_weakrefs(self, value: list):
+        self.__dict__["__lock_parents_weakrefs"] = value
+
+    @as_decorator("is_locked")
+    def lock_(self) -> T:
+        """Locks a tensordict for non in-place operations.
+
+        Functions such as :meth:`~.set`, :meth:`~.__setitem__`, :meth:`~.update`,
+        :meth:`~.rename_key_` or other operations that add or remove entries
+        will be blocked.
+
+        This method can be used as a decorator.
+
+        Example:
+            >>> from tensordict import TensorDict
+            >>> td = TensorDict({"a": 1, "b": 2, "c": 3}, batch_size=[])
+            >>> with td.lock_():
+            ...     assert td.is_locked
+            ...     try:
+            ...         td.set("d", 0) # error!
+            ...     except RuntimeError:
+            ...         print("td is locked!")
+            ...     try:
+            ...         del td["d"]
+            ...     except RuntimeError:
+            ...         print("td is locked!")
+            ...     try:
+            ...         td.rename_key_("a", "d")
+            ...     except RuntimeError:
+            ...         print("td is locked!")
+            ...     td.set("a", 0, inplace=True)  # No storage is added, moved or removed
+            ...     td.set_("a", 0) # No storage is added, moved or removed
+            ...     td.update({"a": 0}, inplace=True)  # No storage is added, moved or removed
+            ...     td.update_({"a": 0})  # No storage is added, moved or removed
+            >>> assert not td.is_locked
+        """
+        if self.is_locked:
+            return self
+        self._propagate_lock()
+        return self
+
+    @erase_cache
+    def _propagate_unlock(self):
+        # if we end up here, we can clear the graph associated with this td
+        self._is_locked = False
+
+        self._is_shared = False
+        self._is_memmap = False
+
+        sub_tds = []
+        for value in self.values():
+            if _is_tensor_collection(type(value)):
+                sub_tds.extend(value._propagate_unlock())
+                sub_tds.append(value)
+        return sub_tds
+
+    def _check_unlock(self):
+        for ref in self._lock_parents_weakrefs:
+            obj = ref()
+            # check if the locked parent exists and if it's locked
+            # we check _is_locked because it can be False or None in the case of Lazy stacks,
+            # but if we check obj.is_locked it will be True for this class.
+            if obj is not None and obj._is_locked:
+                raise RuntimeError(
+                    "Cannot unlock a tensordict that is part of a locked graph. "
+                    "Unlock the root tensordict first. If the tensordict is part of multiple graphs, "
+                    "group the graphs under a common tensordict an unlock this root. "
+                    f"self: {self}, obj: {obj}"
+                )
+        try:
+            self._lock_parents_weakrefs = []
+        except AttributeError:
+            # Some tds (eg, LazyStack) have an automated way of creating the _lock_parents_weakref
+            pass
+
+    @as_decorator("is_locked")
+    def unlock_(self) -> T:
+        """Unlocks a tensordict for non in-place operations.
+
+        Can be used as a decorator.
+
+        See :meth:`~.lock_` for more details.
+        """
+        try:
+            sub_tds = self._propagate_unlock()
+            for sub_td in sub_tds:
+                sub_td._check_unlock()
+            self._check_unlock()
+        except RuntimeError as err:
+            self.lock_()
+            raise err
+        return self
+
+    # Conversion (device or dtype)
+    @overload
+    def to(
+        self: T,
+        device: Optional[Union[int, device]] = ...,
+        dtype: Optional[Union[torch.device, str]] = ...,
+        non_blocking: bool = ...,
+    ) -> T:
+        ...
+
+    @overload
+    def to(self: T, dtype: Union[torch.device, str], non_blocking: bool = ...) -> T:
+        ...
+
+    @overload
+    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:
+        ...
+
+    @overload
+    def to(self: T, *, other: T, non_blocking: bool = ...) -> T:
+        ...
+
+    @overload
+    def to(self: T, *, batch_size: torch.Size) -> T:
+        ...
+
+    @abc.abstractmethod
+    def to(self, *args, **kwargs) -> T:
+        """Maps a TensorDictBase subclass either on another device, dtype or to another TensorDictBase subclass (if permitted).
+
+        Casting tensors to a new dtype is not allowed, as tensordicts are not bound to contain a single
+        tensor dtype.
+
+        Args:
+            device (torch.device, optional): the desired device of the tensordict.
+            dtype (torch.dtype, optional): the desired floating point or complex dtype of
+                the tensordict.
+            tensor (torch.Tensor, optional): Tensor whose dtype and device are the desired
+                dtype and device for all tensors in this TensorDict.
+
+        Keyword Args:
+            non_blocking (bool, optional): whether the operations should be blocking.
+            memory_format (torch.memory_format, optional): the desired memory
+                format for 4D parameters and buffers in this tensordict.
+            batch_size (torch.Size, optional): resulting batch-size of the
+                output tensordict.
+            other (TensorDictBase, optional): TensorDict instance whose dtype
+                and device are the desired dtype and device for all tensors
+                in this TensorDict.
+                .. note:: Since :class:`~tensordict.TensorDictBase` instances do not have
+                    a dtype, the dtype is gathered from the example leaves.
+                    If there are more than one dtype, then no dtype
+                    casting is undertook.
+
+        Returns:
+            a new tensordict instance if the device differs from the tensordict
+            device and/or if the dtype is passed. The same tensordict otherwise.
+            ``batch_size`` only modifications are done in-place.
+
+        Examples:
+            >>> data = TensorDict({"a": 1.0}, [], device=None)
+            >>> data_cuda = data.to("cuda:0")  # casts to cuda
+            >>> data_int = data.to(torch.int)  # casts to int
+            >>> data_cuda_int = data.to("cuda:0", torch.int)  # multiple casting
+            >>> data_cuda = data.to(torch.randn(3, device="cuda:0"))  # using an example tensor
+            >>> data_cuda = data.to(other=TensorDict({}, [], device="cuda:0"))  # using a tensordict example
+        """
+        ...
+
+    def _sync_all(self):
+        if _has_cuda:
+            if torch.cuda.is_initialized():
+                torch.cuda.synchronize()
+        elif _has_mps:
+            torch.mps.synchronize()
+
+    def is_floating_point(self):
+        for item in self.values(include_nested=True, leaves_only=True):
+            if not item.is_floating_point():
+                return False
+        else:
+            return True
+
+    def double(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self._fast_apply(lambda x: x.double(), propagate_lock=True)
+
+    def float(self):
+        r"""Casts all tensors to ``torch.float``."""
+        return self._fast_apply(lambda x: x.float(), propagate_lock=True)
+
+    def int(self):
+        r"""Casts all tensors to ``torch.int``."""
+        return self._fast_apply(lambda x: x.int(), propagate_lock=True)
+
+    def bool(self):
+        r"""Casts all tensors to ``torch.bool``."""
+        return self._fast_apply(lambda x: x.bool(), propagate_lock=True)
+
+    def half(self):
+        r"""Casts all tensors to ``torch.half``."""
+        return self._fast_apply(lambda x: x.half(), propagate_lock=True)
+
+    def bfloat16(self):
+        r"""Casts all tensors to ``torch.bfloat16``."""
+        return self._fast_apply(lambda x: x.bfloat16(), propagate_lock=True)
+
+    def type(self, dst_type):
+        r"""Casts all tensors to :attr:`dst_type`.
+
+        Args:
+            dst_type (type or string): the desired type
+
+        """
+        return self._fast_apply(lambda x: x.type(dst_type))
+
+    # Gradient compatibility
+    @property
+    def requires_grad(self) -> bool:
+        return any(v.requires_grad for v in self.values())
+
+    @abc.abstractmethod
+    def detach_(self) -> T:
+        """Detach the tensors in the tensordict in-place.
+
+        Returns:
+            self.
+
+        """
+        ...
+
+    @cache  # noqa: B019
+    def detach(self) -> T:
+        """Detach the tensors in the tensordict.
+
+        Returns:
+            a new tensordict with no tensor requiring gradient.
+
+        """
+        return self._fast_apply(
+            lambda x: x.detach(),
+            propagate_lock=True,
+        )
+
+
+_ACCEPTED_CLASSES = (
+    Tensor,
+    TensorDictBase,
+)
+
+
+def _register_tensor_class(cls):
+    global _ACCEPTED_CLASSES
+    _ACCEPTED_CLASSES = set(_ACCEPTED_CLASSES)
+    _ACCEPTED_CLASSES.add(cls)
+    _ACCEPTED_CLASSES = tuple(_ACCEPTED_CLASSES)
+
+
+def _is_tensor_collection(datatype):
+    try:
+        out = _TENSOR_COLLECTION_MEMO[datatype]
+    except KeyError:
+        if issubclass(datatype, TensorDictBase):
+            out = True
+        elif _is_tensorclass(datatype):
+            out = True
+        else:
+            out = False
+        _TENSOR_COLLECTION_MEMO[datatype] = out
+    return out
+
+
+def is_tensor_collection(datatype: type | Any) -> bool:
+    """Checks if a data object or a type is a tensor container from the tensordict lib.
+
+    Returns:
+        ``True`` if the input is a TensorDictBase subclass, a tensorclass or an istance of these.
+        ``False`` otherwise.
+
+    Examples:
+        >>> is_tensor_collection(TensorDictBase)  # True
+        >>> is_tensor_collection(TensorDict({}, []))  # True
+        >>> @tensorclass
+        ... class MyClass:
+        ...     pass
+        ...
+        >>> is_tensor_collection(MyClass)  # True
+        >>> is_tensor_collection(MyClass(batch_size=[]))  # True
+
+    """
+    # memoizing is 2x faster
+    if not isinstance(datatype, type):
+        datatype = type(datatype)
+    return _is_tensor_collection(datatype)
+
+
+def _default_is_leaf(cls: Type) -> bool:
+    return not _is_tensor_collection(cls)
+
+
+def _is_leaf_nontensor(cls: Type) -> bool:
+    if _is_tensor_collection(cls):
+        return _is_non_tensor(cls)
+    # if issubclass(cls, KeyedJaggedTensor):
+    #     return False
+    return issubclass(cls, torch.Tensor)
+
+
+def _load_metadata(prefix: Path):
+    filepath = prefix / "meta.json"
+    with open(filepath) as json_metadata:
+        metadata = json.load(json_metadata)
+    return metadata
```

## tensordict/functional.py

 * *Ordering differences only*

```diff
@@ -1,405 +1,405 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import warnings
-
-from typing import Sequence
-
-import torch
-
-from tensordict._lazy import LazyStackedTensorDict
-from tensordict._td import TensorDict
-from tensordict.base import (
-    _is_tensor_collection,
-    CompatibleType,
-    NestedKey,
-    T,
-    TensorDictBase,
-)
-from tensordict.utils import _check_keys, _shape, DeviceType, is_non_tensor, unravel_key
-
-
-def pad(tensordict: T, pad_size: Sequence[int], value: float = 0.0) -> T:
-    """Pads all tensors in a tensordict along the batch dimensions with a constant value, returning a new tensordict.
-
-    Args:
-         tensordict (TensorDict): The tensordict to pad
-         pad_size (Sequence[int]): The padding size by which to pad some batch
-            dimensions of the tensordict, starting from the first dimension and
-            moving forward. [len(pad_size) / 2] dimensions of the batch size will
-            be padded. For example to pad only the first dimension, pad has the form
-            (padding_left, padding_right). To pad two dimensions,
-            (padding_left, padding_right, padding_top, padding_bottom) and so on.
-            pad_size must be even and less than or equal to twice the number of batch dimensions.
-         value (float, optional): The fill value to pad by, default 0.0
-
-    Returns:
-        A new TensorDict padded along the batch dimensions
-
-    Examples:
-        >>> from tensordict import TensorDict, pad
-        >>> import torch
-        >>> td = TensorDict({'a': torch.ones(3, 4, 1),
-        ...     'b': torch.ones(3, 4, 1, 1)}, batch_size=[3, 4])
-        >>> dim0_left, dim0_right, dim1_left, dim1_right = [0, 1, 0, 2]
-        >>> padded_td = pad(td, [dim0_left, dim0_right, dim1_left, dim1_right], value=0.0)
-        >>> print(padded_td.batch_size)
-        torch.Size([4, 6])
-        >>> print(padded_td.get("a").shape)
-        torch.Size([4, 6, 1])
-        >>> print(padded_td.get("b").shape)
-        torch.Size([4, 6, 1, 1])
-
-    """
-    if len(pad_size) > 2 * len(tensordict.batch_size):
-        raise RuntimeError(
-            "The length of pad_size must be <= 2 * the number of batch dimensions"
-        )
-
-    if len(pad_size) % 2:
-        raise RuntimeError("pad_size must have an even number of dimensions")
-
-    new_batch_size = list(tensordict.batch_size)
-    for i in range(len(pad_size)):
-        new_batch_size[i // 2] += pad_size[i]
-
-    reverse_pad = pad_size[::-1]
-    for i in range(0, len(reverse_pad), 2):
-        reverse_pad[i], reverse_pad[i + 1] = reverse_pad[i + 1], reverse_pad[i]
-
-    out = TensorDict(
-        {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
-    )
-    for key, tensor in tensordict.items():
-        cur_pad = reverse_pad
-        if len(pad_size) < len(_shape(tensor)) * 2:
-            cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
-
-        if _is_tensor_collection(tensor.__class__):
-            padded = pad(tensor, pad_size, value)
-        else:
-            padded = torch.nn.functional.pad(tensor, cur_pad, value=value)
-        out.set(key, padded)
-
-    return out
-
-
-def pad_sequence(
-    list_of_tensordicts: Sequence[T],
-    batch_first: bool | None = None,
-    pad_dim: int = 0,
-    padding_value: float = 0.0,
-    out: T | None = None,
-    device: DeviceType | None = None,
-    return_mask: bool | NestedKey = False,
-) -> T:
-    """Pads a list of tensordicts in order for them to be stacked together in a contiguous format.
-
-    Args:
-        list_of_tensordicts (List[TensorDictBase]): the list of instances to pad and stack.
-        pad_dim (int, optional): the ``pad_dim`` indicates the dimension to pad all the keys in the tensordict.
-            Defaults to ``0``.
-        padding_value (number, optional): the padding value. Defaults to ``0.0``.
-        out (TensorDictBase, optional): if provided, the destination where the data will be
-            written.
-        return_mask (bool or NestedKey, optional): if ``True``, a "masks" entry will be returned. If ``return_mask`` is a nested key (string or tuple of strings), it will be return the masks and be used as the key for the masks entry.
-            It contains a tensordict with the same structure as the stacked tensordict where every entry contains the mask of valid values with size ``torch.Size([stack_len, *new_shape])``,
-            where `new_shape[pad_dim] = max_seq_length` and the rest of the `new_shape` matches the previous shape of the contained tensors.
-
-    Examples:
-        >>> list_td = [
-        ...     TensorDict({"a": torch.zeros((3, 8)), "b": torch.zeros((6, 8))}, batch_size=[]),
-        ...     TensorDict({"a": torch.zeros((5, 8)), "b": torch.zeros((6, 8))}, batch_size=[]),
-        ...     ]
-        >>> padded_td = pad_sequence(list_td, return_mask=True)
-        >>> print(padded_td)
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([2, 4, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: Tensor(shape=torch.Size([2, 5, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                masks: TensorDict(
-                    fields={
-                        a: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.bool, is_shared=False),
-                        b: Tensor(shape=torch.Size([2, 6]), device=cpu, dtype=torch.bool, is_shared=False)},
-                    batch_size=torch.Size([2]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([2]),
-            device=None,
-            is_shared=False)
-    """
-    if device is not None:
-        warnings.warn(
-            "The device argument is ignored by this function and will be removed in v0.5. To cast your"
-            " result to a different device, call `tensordict.to(device)` instead."
-        )
-    if batch_first is not None:
-        warnings.warn(
-            "The batch_first argument is deprecated and will be removed in a future release. The output will always be batch_first.",
-            category=DeprecationWarning,
-        )
-
-    if not list_of_tensordicts:
-        raise RuntimeError("list_of_tensordicts cannot be empty")
-
-    masks_key = "masks"
-    if not isinstance(return_mask, bool):
-        masks_key = unravel_key(return_mask)
-        return_mask = True
-
-    # check that all tensordict match
-    update_batch_size = True
-    max_seq_length = float("-inf")
-    keys = _check_keys(list_of_tensordicts, leaves_only=True, include_nested=True)
-    list_of_dicts = [{} for _ in range(len(list_of_tensordicts))]
-    keys_copy = list(keys)
-    for i, td in enumerate(list_of_tensordicts):
-
-        for key in keys:
-            item = td.get(key)
-            list_of_dicts[i][key] = item
-            if is_non_tensor(item):
-                continue
-            tensor_shape = item.shape
-
-            if len(tensor_shape) == 0:
-                raise RuntimeError("Cannot pad scalars")
-
-            pos_pad_dim = pad_dim if pad_dim >= 0 else len(tensor_shape) + pad_dim
-
-            # track the maximum sequence length to update batch_size accordingly
-            if tensor_shape[pos_pad_dim] > max_seq_length:
-                max_seq_length = tensor_shape[pos_pad_dim]
-
-            # The mask should always contain the batch_size of the TensorDict
-            mask_shape = td.shape
-
-            # if the pad_dim is past the batch_size of the TensorDict, we need to add the new dimension to the mask
-            if pos_pad_dim >= td.ndim:
-                mask_shape += torch.Size([tensor_shape[pos_pad_dim]])
-                update_batch_size = False
-
-            if return_mask:
-                mask_key = unravel_key((masks_key, key))
-                list_of_dicts[i][mask_key] = torch.ones(mask_shape, dtype=torch.bool)
-                keys_copy.append(mask_key)
-
-    keys = keys_copy
-
-    old_batch_size = list(list_of_tensordicts[0].batch_size)
-    if update_batch_size and len(old_batch_size) > 0:
-        old_batch_size[pad_dim] = max_seq_length
-    shape = [
-        len(list_of_tensordicts),
-    ] + old_batch_size
-
-    if out is None:
-        out = list_of_tensordicts[0].empty(recurse=True).reshape(torch.Size(shape))
-
-    for key in keys:
-        try:
-            item0 = list_of_dicts[0][key]
-            if is_non_tensor(item0):
-                out.set(key, torch.stack([d[key] for d in list_of_dicts]))
-                continue
-            tensor_shape = item0.shape
-            pos_pad_dim = (
-                (pad_dim if pad_dim >= 0 else len(tensor_shape) + pad_dim)
-                if len(tensor_shape) > 1
-                else 0  # handles the case when the masks are 1-dimensional
-            )
-            out.set(
-                key,
-                torch.nn.utils.rnn.pad_sequence(
-                    [d[key].transpose(0, pos_pad_dim) for d in list_of_dicts],
-                    batch_first=True,
-                    padding_value=padding_value,
-                ).transpose(1, pos_pad_dim + 1),
-                inplace=True,
-            )
-        except Exception as err:
-            raise RuntimeError(f"pad_sequence failed for key {key}") from err
-    return out
-
-
-def merge_tensordicts(*tensordicts: T) -> T:
-    """Merges tensordicts together."""
-    if len(tensordicts) < 2:
-        raise RuntimeError(
-            f"at least 2 tensordicts must be provided, got" f" {len(tensordicts)}"
-        )
-    d = tensordicts[0].to_dict()
-    batch_size = tensordicts[0].batch_size
-    for td in tensordicts[1:]:
-        d.update(td.to_dict())
-        if td.batch_dims < len(batch_size):
-            batch_size = td.batch_size
-    return TensorDict(d, batch_size, device=td.device, _run_checks=False)
-
-
-def dense_stack_tds(
-    td_list: Sequence[TensorDictBase] | LazyStackedTensorDict,
-    dim: int = None,
-) -> T:
-    """Densely stack a list of :class:`~tensordict.TensorDictBase` objects (or a :class:`~tensordict.LazyStackedTensorDict`) given that they have the same structure.
-
-    This function is called with a list of :class:`~tensordict.TensorDictBase` (either passed directly or obtrained from
-    a :class:`~tensordict.LazyStackedTensorDict`).
-    Instead of calling ``torch.stack(td_list)``, which would return a :class:`~tensordict.LazyStackedTensorDict`,
-    this function expands the first element of the input list and stacks the input list onto that element.
-    This works only when all the elements of the input list have the same structure.
-    The :class:`~tensordict.TensorDictBase` returned will have the same type of the elements of the input list.
-
-    This function is useful when some of the :class:`~tensordict.TensorDictBase` objects that need to be stacked
-    are :class:`~tensordict.LazyStackedTensorDict` or have :class:`~tensordict.LazyStackedTensorDict`
-    among entries (or nested entries).
-    In those cases, calling ``torch.stack(td_list).to_tensordict()`` is infeasible.
-    Thus, this function provides an alternative for densely stacking the list provided.
-
-    Args:
-        td_list (List of TensorDictBase or LazyStackedTensorDict): the tds to stack.
-        dim (int, optional): the dimension to stack them.
-            If td_list is a LazyStackedTensorDict, it will be retrieved automatically.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict import dense_stack_tds
-        >>> from tensordict.tensordict import assert_allclose_td
-        >>> td0 = TensorDict({"a": torch.zeros(3)},[])
-        >>> td1 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(2)},[])
-        >>> td_lazy = torch.stack([td0, td1], dim=0)
-        >>> td_container = TensorDict({"lazy": td_lazy}, [])
-        >>> td_container_clone = td_container.clone()
-        >>> td_stack = torch.stack([td_container, td_container_clone], dim=0)
-        >>> td_stack
-        LazyStackedTensorDict(
-            fields={
-                lazy: LazyStackedTensorDict(
-                    fields={
-                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    exclusive_fields={
-                    },
-                    batch_size=torch.Size([2, 2]),
-                    device=None,
-                    is_shared=False,
-                    stack_dim=0)},
-            exclusive_fields={
-            },
-            batch_size=torch.Size([2]),
-            device=None,
-            is_shared=False,
-            stack_dim=0)
-        >>> td_stack = dense_stack_tds(td_stack) # Automatically use the LazyStackedTensorDict stack_dim
-        TensorDict(
-            fields={
-                lazy: LazyStackedTensorDict(
-                    fields={
-                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    exclusive_fields={
-                        1 ->
-                            b: Tensor(shape=torch.Size([2, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([2, 2]),
-                    device=None,
-                    is_shared=False,
-                    stack_dim=1)},
-            batch_size=torch.Size([2]),
-            device=None,
-            is_shared=False)
-        # Note that
-        # (1) td_stack is now a TensorDict
-        # (2) this has pushed the stack_dim of "lazy" (0 -> 1)
-        # (3) this has revealed the exclusive keys.
-        >>> assert_allclose_td(td_stack, dense_stack_tds([td_container, td_container_clone], dim=0))
-        # This shows it is the same to pass a list or a LazyStackedTensorDict
-
-    """
-    if isinstance(td_list, LazyStackedTensorDict):
-        dim = td_list.stack_dim
-        td_list = td_list.tensordicts
-    elif isinstance(td_list, TensorDict):
-        # then it is already dense
-        return td_list
-    elif dim is None:
-        raise ValueError(
-            "If a list of tensordicts is provided, stack_dim must not be None"
-        )
-    shape = list(td_list[0].shape)
-    shape.insert(dim, len(td_list))
-
-    out = td_list[0].unsqueeze(dim).expand(shape).clone()
-    return torch.stack(td_list, dim=dim, out=out)
-
-
-def make_tensordict(
-    input_dict: dict[str, CompatibleType] | None = None,
-    batch_size: Sequence[int] | torch.Size | int | None = None,
-    device: DeviceType | None = None,
-    **kwargs: CompatibleType,  # source
-) -> TensorDict:
-    """Returns a TensorDict created from the keyword arguments or an input dictionary.
-
-    If ``batch_size`` is not specified, returns the maximum batch size possible.
-
-    This function works on nested dictionaries too, or can be used to determine the
-    batch-size of a nested tensordict.
-
-    Args:
-        input_dict (dictionary, optional): a dictionary to use as a data source
-            (nested keys compatible).
-        **kwargs (TensorDict or torch.Tensor): keyword arguments as data source
-            (incompatible with nested keys).
-        batch_size (iterable of int, optional): a batch size for the tensordict.
-        device (torch.device or compatible type, optional): a device for the TensorDict.
-
-    Examples:
-        >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
-        >>> print(make_tensordict(input_dict))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> # alternatively
-        >>> td = make_tensordict(**input_dict)
-        >>> # nested dict: the nested TensorDict can have a different batch-size
-        >>> # as long as its leading dims match.
-        >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
-        >>> print(make_tensordict(input_dict))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: TensorDict(
-                    fields={
-                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([3, 4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> # we can also use this to work out the batch sie of a tensordict
-        >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
-        >>> print(make_tensordict(input_td))
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: TensorDict(
-                    fields={
-                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([3, 4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-    """
-    if input_dict is not None:
-        kwargs.update(input_dict)
-    return TensorDict.from_dict(kwargs, batch_size=batch_size, device=device)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import warnings
+
+from typing import Sequence
+
+import torch
+
+from tensordict._lazy import LazyStackedTensorDict
+from tensordict._td import TensorDict
+from tensordict.base import (
+    _is_tensor_collection,
+    CompatibleType,
+    NestedKey,
+    T,
+    TensorDictBase,
+)
+from tensordict.utils import _check_keys, _shape, DeviceType, is_non_tensor, unravel_key
+
+
+def pad(tensordict: T, pad_size: Sequence[int], value: float = 0.0) -> T:
+    """Pads all tensors in a tensordict along the batch dimensions with a constant value, returning a new tensordict.
+
+    Args:
+         tensordict (TensorDict): The tensordict to pad
+         pad_size (Sequence[int]): The padding size by which to pad some batch
+            dimensions of the tensordict, starting from the first dimension and
+            moving forward. [len(pad_size) / 2] dimensions of the batch size will
+            be padded. For example to pad only the first dimension, pad has the form
+            (padding_left, padding_right). To pad two dimensions,
+            (padding_left, padding_right, padding_top, padding_bottom) and so on.
+            pad_size must be even and less than or equal to twice the number of batch dimensions.
+         value (float, optional): The fill value to pad by, default 0.0
+
+    Returns:
+        A new TensorDict padded along the batch dimensions
+
+    Examples:
+        >>> from tensordict import TensorDict, pad
+        >>> import torch
+        >>> td = TensorDict({'a': torch.ones(3, 4, 1),
+        ...     'b': torch.ones(3, 4, 1, 1)}, batch_size=[3, 4])
+        >>> dim0_left, dim0_right, dim1_left, dim1_right = [0, 1, 0, 2]
+        >>> padded_td = pad(td, [dim0_left, dim0_right, dim1_left, dim1_right], value=0.0)
+        >>> print(padded_td.batch_size)
+        torch.Size([4, 6])
+        >>> print(padded_td.get("a").shape)
+        torch.Size([4, 6, 1])
+        >>> print(padded_td.get("b").shape)
+        torch.Size([4, 6, 1, 1])
+
+    """
+    if len(pad_size) > 2 * len(tensordict.batch_size):
+        raise RuntimeError(
+            "The length of pad_size must be <= 2 * the number of batch dimensions"
+        )
+
+    if len(pad_size) % 2:
+        raise RuntimeError("pad_size must have an even number of dimensions")
+
+    new_batch_size = list(tensordict.batch_size)
+    for i in range(len(pad_size)):
+        new_batch_size[i // 2] += pad_size[i]
+
+    reverse_pad = pad_size[::-1]
+    for i in range(0, len(reverse_pad), 2):
+        reverse_pad[i], reverse_pad[i + 1] = reverse_pad[i + 1], reverse_pad[i]
+
+    out = TensorDict(
+        {}, torch.Size(new_batch_size), device=tensordict.device, _run_checks=False
+    )
+    for key, tensor in tensordict.items():
+        cur_pad = reverse_pad
+        if len(pad_size) < len(_shape(tensor)) * 2:
+            cur_pad = [0] * (len(_shape(tensor)) * 2 - len(pad_size)) + reverse_pad
+
+        if _is_tensor_collection(tensor.__class__):
+            padded = pad(tensor, pad_size, value)
+        else:
+            padded = torch.nn.functional.pad(tensor, cur_pad, value=value)
+        out.set(key, padded)
+
+    return out
+
+
+def pad_sequence(
+    list_of_tensordicts: Sequence[T],
+    batch_first: bool | None = None,
+    pad_dim: int = 0,
+    padding_value: float = 0.0,
+    out: T | None = None,
+    device: DeviceType | None = None,
+    return_mask: bool | NestedKey = False,
+) -> T:
+    """Pads a list of tensordicts in order for them to be stacked together in a contiguous format.
+
+    Args:
+        list_of_tensordicts (List[TensorDictBase]): the list of instances to pad and stack.
+        pad_dim (int, optional): the ``pad_dim`` indicates the dimension to pad all the keys in the tensordict.
+            Defaults to ``0``.
+        padding_value (number, optional): the padding value. Defaults to ``0.0``.
+        out (TensorDictBase, optional): if provided, the destination where the data will be
+            written.
+        return_mask (bool or NestedKey, optional): if ``True``, a "masks" entry will be returned. If ``return_mask`` is a nested key (string or tuple of strings), it will be return the masks and be used as the key for the masks entry.
+            It contains a tensordict with the same structure as the stacked tensordict where every entry contains the mask of valid values with size ``torch.Size([stack_len, *new_shape])``,
+            where `new_shape[pad_dim] = max_seq_length` and the rest of the `new_shape` matches the previous shape of the contained tensors.
+
+    Examples:
+        >>> list_td = [
+        ...     TensorDict({"a": torch.zeros((3, 8)), "b": torch.zeros((6, 8))}, batch_size=[]),
+        ...     TensorDict({"a": torch.zeros((5, 8)), "b": torch.zeros((6, 8))}, batch_size=[]),
+        ...     ]
+        >>> padded_td = pad_sequence(list_td, return_mask=True)
+        >>> print(padded_td)
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([2, 4, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: Tensor(shape=torch.Size([2, 5, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                masks: TensorDict(
+                    fields={
+                        a: Tensor(shape=torch.Size([2, 4]), device=cpu, dtype=torch.bool, is_shared=False),
+                        b: Tensor(shape=torch.Size([2, 6]), device=cpu, dtype=torch.bool, is_shared=False)},
+                    batch_size=torch.Size([2]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([2]),
+            device=None,
+            is_shared=False)
+    """
+    if device is not None:
+        warnings.warn(
+            "The device argument is ignored by this function and will be removed in v0.5. To cast your"
+            " result to a different device, call `tensordict.to(device)` instead."
+        )
+    if batch_first is not None:
+        warnings.warn(
+            "The batch_first argument is deprecated and will be removed in a future release. The output will always be batch_first.",
+            category=DeprecationWarning,
+        )
+
+    if not list_of_tensordicts:
+        raise RuntimeError("list_of_tensordicts cannot be empty")
+
+    masks_key = "masks"
+    if not isinstance(return_mask, bool):
+        masks_key = unravel_key(return_mask)
+        return_mask = True
+
+    # check that all tensordict match
+    update_batch_size = True
+    max_seq_length = float("-inf")
+    keys = _check_keys(list_of_tensordicts, leaves_only=True, include_nested=True)
+    list_of_dicts = [{} for _ in range(len(list_of_tensordicts))]
+    keys_copy = list(keys)
+    for i, td in enumerate(list_of_tensordicts):
+
+        for key in keys:
+            item = td.get(key)
+            list_of_dicts[i][key] = item
+            if is_non_tensor(item):
+                continue
+            tensor_shape = item.shape
+
+            if len(tensor_shape) == 0:
+                raise RuntimeError("Cannot pad scalars")
+
+            pos_pad_dim = pad_dim if pad_dim >= 0 else len(tensor_shape) + pad_dim
+
+            # track the maximum sequence length to update batch_size accordingly
+            if tensor_shape[pos_pad_dim] > max_seq_length:
+                max_seq_length = tensor_shape[pos_pad_dim]
+
+            # The mask should always contain the batch_size of the TensorDict
+            mask_shape = td.shape
+
+            # if the pad_dim is past the batch_size of the TensorDict, we need to add the new dimension to the mask
+            if pos_pad_dim >= td.ndim:
+                mask_shape += torch.Size([tensor_shape[pos_pad_dim]])
+                update_batch_size = False
+
+            if return_mask:
+                mask_key = unravel_key((masks_key, key))
+                list_of_dicts[i][mask_key] = torch.ones(mask_shape, dtype=torch.bool)
+                keys_copy.append(mask_key)
+
+    keys = keys_copy
+
+    old_batch_size = list(list_of_tensordicts[0].batch_size)
+    if update_batch_size and len(old_batch_size) > 0:
+        old_batch_size[pad_dim] = max_seq_length
+    shape = [
+        len(list_of_tensordicts),
+    ] + old_batch_size
+
+    if out is None:
+        out = list_of_tensordicts[0].empty(recurse=True).reshape(torch.Size(shape))
+
+    for key in keys:
+        try:
+            item0 = list_of_dicts[0][key]
+            if is_non_tensor(item0):
+                out.set(key, torch.stack([d[key] for d in list_of_dicts]))
+                continue
+            tensor_shape = item0.shape
+            pos_pad_dim = (
+                (pad_dim if pad_dim >= 0 else len(tensor_shape) + pad_dim)
+                if len(tensor_shape) > 1
+                else 0  # handles the case when the masks are 1-dimensional
+            )
+            out.set(
+                key,
+                torch.nn.utils.rnn.pad_sequence(
+                    [d[key].transpose(0, pos_pad_dim) for d in list_of_dicts],
+                    batch_first=True,
+                    padding_value=padding_value,
+                ).transpose(1, pos_pad_dim + 1),
+                inplace=True,
+            )
+        except Exception as err:
+            raise RuntimeError(f"pad_sequence failed for key {key}") from err
+    return out
+
+
+def merge_tensordicts(*tensordicts: T) -> T:
+    """Merges tensordicts together."""
+    if len(tensordicts) < 2:
+        raise RuntimeError(
+            f"at least 2 tensordicts must be provided, got" f" {len(tensordicts)}"
+        )
+    d = tensordicts[0].to_dict()
+    batch_size = tensordicts[0].batch_size
+    for td in tensordicts[1:]:
+        d.update(td.to_dict())
+        if td.batch_dims < len(batch_size):
+            batch_size = td.batch_size
+    return TensorDict(d, batch_size, device=td.device, _run_checks=False)
+
+
+def dense_stack_tds(
+    td_list: Sequence[TensorDictBase] | LazyStackedTensorDict,
+    dim: int = None,
+) -> T:
+    """Densely stack a list of :class:`~tensordict.TensorDictBase` objects (or a :class:`~tensordict.LazyStackedTensorDict`) given that they have the same structure.
+
+    This function is called with a list of :class:`~tensordict.TensorDictBase` (either passed directly or obtrained from
+    a :class:`~tensordict.LazyStackedTensorDict`).
+    Instead of calling ``torch.stack(td_list)``, which would return a :class:`~tensordict.LazyStackedTensorDict`,
+    this function expands the first element of the input list and stacks the input list onto that element.
+    This works only when all the elements of the input list have the same structure.
+    The :class:`~tensordict.TensorDictBase` returned will have the same type of the elements of the input list.
+
+    This function is useful when some of the :class:`~tensordict.TensorDictBase` objects that need to be stacked
+    are :class:`~tensordict.LazyStackedTensorDict` or have :class:`~tensordict.LazyStackedTensorDict`
+    among entries (or nested entries).
+    In those cases, calling ``torch.stack(td_list).to_tensordict()`` is infeasible.
+    Thus, this function provides an alternative for densely stacking the list provided.
+
+    Args:
+        td_list (List of TensorDictBase or LazyStackedTensorDict): the tds to stack.
+        dim (int, optional): the dimension to stack them.
+            If td_list is a LazyStackedTensorDict, it will be retrieved automatically.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict import dense_stack_tds
+        >>> from tensordict.tensordict import assert_allclose_td
+        >>> td0 = TensorDict({"a": torch.zeros(3)},[])
+        >>> td1 = TensorDict({"a": torch.zeros(4), "b": torch.zeros(2)},[])
+        >>> td_lazy = torch.stack([td0, td1], dim=0)
+        >>> td_container = TensorDict({"lazy": td_lazy}, [])
+        >>> td_container_clone = td_container.clone()
+        >>> td_stack = torch.stack([td_container, td_container_clone], dim=0)
+        >>> td_stack
+        LazyStackedTensorDict(
+            fields={
+                lazy: LazyStackedTensorDict(
+                    fields={
+                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    exclusive_fields={
+                    },
+                    batch_size=torch.Size([2, 2]),
+                    device=None,
+                    is_shared=False,
+                    stack_dim=0)},
+            exclusive_fields={
+            },
+            batch_size=torch.Size([2]),
+            device=None,
+            is_shared=False,
+            stack_dim=0)
+        >>> td_stack = dense_stack_tds(td_stack) # Automatically use the LazyStackedTensorDict stack_dim
+        TensorDict(
+            fields={
+                lazy: LazyStackedTensorDict(
+                    fields={
+                        a: Tensor(shape=torch.Size([2, 2, -1]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    exclusive_fields={
+                        1 ->
+                            b: Tensor(shape=torch.Size([2, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([2, 2]),
+                    device=None,
+                    is_shared=False,
+                    stack_dim=1)},
+            batch_size=torch.Size([2]),
+            device=None,
+            is_shared=False)
+        # Note that
+        # (1) td_stack is now a TensorDict
+        # (2) this has pushed the stack_dim of "lazy" (0 -> 1)
+        # (3) this has revealed the exclusive keys.
+        >>> assert_allclose_td(td_stack, dense_stack_tds([td_container, td_container_clone], dim=0))
+        # This shows it is the same to pass a list or a LazyStackedTensorDict
+
+    """
+    if isinstance(td_list, LazyStackedTensorDict):
+        dim = td_list.stack_dim
+        td_list = td_list.tensordicts
+    elif isinstance(td_list, TensorDict):
+        # then it is already dense
+        return td_list
+    elif dim is None:
+        raise ValueError(
+            "If a list of tensordicts is provided, stack_dim must not be None"
+        )
+    shape = list(td_list[0].shape)
+    shape.insert(dim, len(td_list))
+
+    out = td_list[0].unsqueeze(dim).expand(shape).clone()
+    return torch.stack(td_list, dim=dim, out=out)
+
+
+def make_tensordict(
+    input_dict: dict[str, CompatibleType] | None = None,
+    batch_size: Sequence[int] | torch.Size | int | None = None,
+    device: DeviceType | None = None,
+    **kwargs: CompatibleType,  # source
+) -> TensorDict:
+    """Returns a TensorDict created from the keyword arguments or an input dictionary.
+
+    If ``batch_size`` is not specified, returns the maximum batch size possible.
+
+    This function works on nested dictionaries too, or can be used to determine the
+    batch-size of a nested tensordict.
+
+    Args:
+        input_dict (dictionary, optional): a dictionary to use as a data source
+            (nested keys compatible).
+        **kwargs (TensorDict or torch.Tensor): keyword arguments as data source
+            (incompatible with nested keys).
+        batch_size (iterable of int, optional): a batch size for the tensordict.
+        device (torch.device or compatible type, optional): a device for the TensorDict.
+
+    Examples:
+        >>> input_dict = {"a": torch.randn(3, 4), "b": torch.randn(3)}
+        >>> print(make_tensordict(input_dict))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> # alternatively
+        >>> td = make_tensordict(**input_dict)
+        >>> # nested dict: the nested TensorDict can have a different batch-size
+        >>> # as long as its leading dims match.
+        >>> input_dict = {"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}
+        >>> print(make_tensordict(input_dict))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: TensorDict(
+                    fields={
+                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([3, 4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> # we can also use this to work out the batch sie of a tensordict
+        >>> input_td = TensorDict({"a": torch.randn(3), "b": {"c": torch.randn(3, 4)}}, [])
+        >>> print(make_tensordict(input_td))
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: TensorDict(
+                    fields={
+                        c: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([3, 4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+    """
+    if input_dict is not None:
+        kwargs.update(input_dict)
+    return TensorDict.from_dict(kwargs, batch_size=batch_size, device=device)
```

## tensordict/memmap.py

 * *Ordering differences only*

```diff
@@ -1,1077 +1,1077 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-
-import mmap
-import os
-
-import sys
-import tempfile
-from multiprocessing import util
-from multiprocessing.context import reduction
-from pathlib import Path
-from typing import Any, Callable, overload
-
-import numpy as np
-import torch
-
-from tensordict.utils import _shape, implement_for
-
-from torch.multiprocessing.reductions import ForkingPickler
-
-NESTED_TENSOR_ERR = (
-    "The PyTorch version isn't compatible with memmap "
-    "nested tensors. Please upgrade to a more recent "
-    "version."
-)
-
-
-class MemoryMappedTensor(torch.Tensor):
-    """A Memory-mapped Tensor.
-
-    Supports filenames or file handlers.
-
-    The main advantage of MemoryMappedTensor resides in its serialization methods,
-    which ensure that the tensor is passed through queues or RPC remote calls without
-    any copy.
-
-    .. note::
-      When used within RPC settings, the filepath should be accessible to both nodes.
-      If it isn't the behaviour of passing a MemoryMappedTensor from one worker
-      to another is undefined.
-
-    MemoryMappedTensor supports multiple construction methods.
-
-    Examples:
-          >>> # from an existing tensor
-          >>> tensor = torch.randn(3)
-          >>> with tempfile.NamedTemporaryFile() as file:
-          ...     memmap_tensor = MemoryMappedTensor.from_tensor(tensor, filename=file.name)
-          ...     assert memmap_tensor.filename is not None
-          >>> # if no filename is passed, a handler is used
-          >>> tensor = torch.randn(3)
-          >>> memmap_tensor = MemoryMappedTensor.from_tensor(tensor, filename=file.name)
-          >>> assert memmap_tensor.filename is None
-          >>> # one can create an empty tensor too
-          >>> with tempfile.NamedTemporaryFile() as file:
-          ...     memmap_tensor_empty = MemoryMappedTensor.empty_like(tensor, filename=file.name)
-          >>> with tempfile.NamedTemporaryFile() as file:
-          ...     memmap_tensor_zero = MemoryMappedTensor.zeros_like(tensor, filename=file.name)
-          >>> with tempfile.NamedTemporaryFile() as file:
-          ...     memmap_tensor = MemoryMappedTensor.ones_like(tensor, filename=file.name)
-    """
-
-    _filename: str | Path = None
-    _handler: _FileHandler = None
-    _clear: bool
-    index: Any
-    parent_shape: torch.Size
-
-    def __new__(
-        cls,
-        source,
-        *,
-        dtype=None,
-        shape=None,
-        index=None,
-        device=None,
-        handler=None,
-        filename=None,
-    ):
-        if device is not None and torch.device(device).type != "cpu":
-            raise ValueError(f"{cls} device must be cpu!")
-        if isinstance(source, str):
-            if filename is not None:
-                raise TypeError("Duplicated filename argument.")
-            filename = source
-            source = None
-        if filename is not None:
-            if dtype is not None:
-                raise TypeError("Cannot pass new dtype if source is provided.")
-            result = cls.from_tensor(
-                torch.as_tensor(source),
-                filename=filename,
-                # dtype=dtype,
-                shape=shape,
-                # index=index,
-            )
-            if index is not None:
-                return result[index]
-            return result
-        elif isinstance(source, torch.StorageBase):
-            return cls.from_storage(
-                source,
-                dtype=dtype,
-                shape=shape,
-                index=index,
-                device=device,
-                handler=handler,
-                filename=filename,
-            )
-        elif handler is not None:
-            return cls.from_handler(
-                handler,
-                dtype,
-                shape,
-                index,
-            )
-        return super().__new__(cls, source)
-
-    def __init__(
-        self,
-        source,
-        *,
-        handler=None,
-        dtype=None,
-        shape=None,
-        device=None,
-        filename=None,
-    ):
-        ...
-
-    __torch_function__ = torch._C._disabled_torch_function_impl
-
-    @classmethod
-    def from_tensor(
-        cls,
-        input,
-        *,
-        filename=None,
-        existsok=False,
-        copy_existing=False,
-        copy_data=True,
-        shape=None,
-    ):
-        """Creates a MemoryMappedTensor with the same content as another tensor.
-
-        If the tensor is already a MemoryMappedTensor the original tensor is
-        returned if the `filename` argument is `None` or if the two paths match.
-        In all other cases, a new :class:`MemoryMappedTensor` is produced.
-
-        Args:
-            input (torch.Tensor): the tensor which content must be copied onto
-                the MemoryMappedTensor.
-            filename (path to a file): the path to the file where the tensor
-                should be stored. If none is provided, a file handler is used
-                instead.
-            existsok (bool, optional): if ``True``, the file will overwrite
-                an existing file. Defaults to ``False``.
-            copy_existing (bool, optional): if ``True`` and the provided input
-                is a MemoryMappedTensor with an associated filename, copying
-                the content to the new location is permitted. Otherwise, an
-                exception is thrown. This behaviour exists to prevent
-                inadvertently duplicating data on disk.
-            copy_data (bool, optional): if ``True``, the content of the tensor
-                will be copied on the storage. Defaults to ``True``.
-            shape (torch.Size or torch.Tensor): a shape to override the tensor
-                shape. If a tensor is passed, it must represent the nested shapes of a
-                nested tensor.
-        """
-        if isinstance(input, MemoryMappedTensor):
-            if (filename is None and input._filename is None) or (
-                input._filename is not None
-                and filename is not None
-                and Path(filename).absolute() == Path(input.filename).absolute()
-            ):
-                # either location was not specified, or memmap is already in the
-                # correct location, so just return the MemmapTensor unmodified
-                return input
-            elif not copy_existing and (
-                input._filename is not None
-                and filename is not None
-                and Path(filename).absolute() != Path(input.filename).absolute()
-            ):
-                raise RuntimeError(
-                    f"A filename was provided but the tensor already has a file associated "
-                    f"({input.filename}). "
-                    f"To copy the tensor onto the new location, pass copy_existing=True."
-                )
-        elif isinstance(input, np.ndarray):
-            raise TypeError(
-                "Convert input to torch.Tensor before calling MemoryMappedTensor.from_tensor."
-            )
-        if input.requires_grad:
-            raise RuntimeError(
-                "MemoryMappedTensor.from_tensor is incompatible with tensor.requires_grad."
-            )
-        if shape is None:
-            shape = _shape(input, nested_shape=True)
-        if isinstance(shape, torch.Tensor):
-            shape_numel = shape.prod(-1).sum()
-        elif isinstance(shape, torch.Size):
-            shape_numel = shape.numel()
-        else:
-            shape_numel = torch.Size(shape).numel()
-        if filename is None:
-            if input.dtype.is_floating_point:
-                size = torch.finfo(input.dtype).bits // 8 * shape_numel
-            elif input.dtype.is_complex:
-                raise ValueError(
-                    "Complex-valued tensors are not supported by MemoryMappedTensor."
-                )
-            elif input.dtype == torch.bool:
-                size = shape_numel
-            else:
-                # assume integer
-                size = torch.iinfo(input.dtype).bits // 8 * shape_numel
-            handler = _FileHandler(size)
-            if isinstance(shape, torch.Tensor):
-                func_offset_stride = getattr(
-                    torch, "_nested_compute_contiguous_strides_offsets", None
-                )
-                if func_offset_stride is not None:
-                    offsets_strides = func_offset_stride(shape)
-                else:
-                    raise RuntimeError(NESTED_TENSOR_ERR)
-                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
-                if copy_data:
-                    result.untyped_storage().copy_(input.untyped_storage())
-                result = torch._nested_view_from_buffer(
-                    result,
-                    shape,
-                    *offsets_strides,
-                )
-            else:
-                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
-                result = result.view(shape)
-            result = cls(result)
-        else:
-            handler = None
-            if not existsok and os.path.exists(str(filename)):
-                raise RuntimeError(f"The file {filename} already exists.")
-            result = torch.from_file(
-                str(filename), shared=True, dtype=input.dtype, size=shape_numel
-            )
-            if isinstance(shape, torch.Tensor):
-                func_offset_stride = getattr(
-                    torch, "_nested_compute_contiguous_strides_offsets", None
-                )
-                if func_offset_stride is not None:
-                    offsets_strides = func_offset_stride(shape)
-                else:
-                    raise RuntimeError(NESTED_TENSOR_ERR)
-                if copy_data:
-                    result.untyped_storage().copy_(input.untyped_storage())
-                result = torch._nested_view_from_buffer(
-                    result,
-                    shape,
-                    *offsets_strides,
-                )
-            else:
-                result = result.view(shape)
-            result = cls(result)
-        result._handler = handler
-        result.filename = filename
-        result.index = None
-        result.parent_shape = shape
-        if copy_data:
-            if hasattr(input, "full_tensor"):
-                # for DTensors, cheaper than importing DTensor every time
-                input = input.full_tensor()
-            if not result.is_nested:
-                result.copy_(input)
-        return result
-
-    @classmethod
-    def from_storage(
-        cls,
-        storage,
-        *,
-        shape=None,
-        dtype=None,
-        device=None,
-        index=None,
-        filename=None,
-        handler=None,
-    ):
-        if storage.filename is not None:
-            if filename is None:
-                filename = storage.filename
-            elif str(storage.filename) != str(filename):
-                raise RuntimeError(
-                    "Providing a storage with an associated filename that differs from the filename argument is not permitted unless filename=None. "
-                    f"Got filename={str(filename)}, storage.filename={str(storage.filename)}"
-                )
-        tensor = torch.tensor(storage, dtype=dtype, device=device)
-        if shape is not None:
-            if isinstance(shape, torch.Tensor):
-                func_offset_stride = getattr(
-                    torch, "_nested_compute_contiguous_strides_offsets", None
-                )
-                if func_offset_stride is not None:
-                    offsets_strides = func_offset_stride(shape)
-                else:
-                    raise RuntimeError(
-                        "The PyTorch version isn't compatible with memmap "
-                        "nested tensors. Please upgrade to a more recent "
-                        "version."
-                    )
-                tensor = torch._nested_view_from_buffer(
-                    tensor,
-                    shape,
-                    *offsets_strides,
-                )
-            else:
-                tensor = tensor.view(shape)
-
-        tensor = cls(tensor)
-        if filename is not None:
-            tensor.filename = filename
-        elif handler is not None:
-            tensor._handler = handler
-        if index is not None:
-            return tensor[index]
-        return tensor
-
-    @property
-    def filename(self):
-        """The filename of the tensor, if it has one.
-
-        Raises an exception otherwise.
-        """
-        filename = self._filename
-        if filename is None:
-            raise RuntimeError("The MemoryMappedTensor has no file associated.")
-        return filename
-
-    @filename.setter
-    def filename(self, value):
-        if value is None and self._filename is None:
-            return
-        value = str(Path(value).absolute())
-        if self._filename is not None and value != self._filename:
-            raise RuntimeError(
-                "the MemoryMappedTensor has already a filename associated."
-            )
-        self._filename = value
-
-    @classmethod
-    def empty_like(cls, input, *, filename=None):
-        # noqa: D417
-        """Creates a tensor with no content but the same shape and dtype as the input tensor.
-
-        Args:
-            input (torch.Tensor): the tensor to use as an example.
-
-        Keyword Args:
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-        """
-        return cls.from_tensor(
-            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
-            filename=filename,
-            copy_data=False,
-        )
-
-    @classmethod
-    def full_like(cls, input, fill_value, *, filename=None):
-        # noqa: D417
-        """Creates a tensor with a single content indicated by the `fill_value` argument, but the same shape and dtype as the input tensor.
-
-        Args:
-            input (torch.Tensor): the tensor to use as an example.
-            fill_value (float or equivalent): content of the tensor.
-
-        Keyword Args:
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-        """
-        return cls.from_tensor(
-            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
-            filename=filename,
-            copy_data=False,
-        ).fill_(fill_value)
-
-    @classmethod
-    def zeros_like(cls, input, *, filename=None):
-        # noqa: D417
-        """Creates a tensor with a 0-filled content, but the same shape and dtype as the input tensor.
-
-        Args:
-            input (torch.Tensor): the tensor to use as an example.
-
-        Keyword Args:
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-        """
-        return cls.from_tensor(
-            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
-            filename=filename,
-            copy_data=False,
-        ).fill_(0.0)
-
-    @classmethod
-    def ones_like(cls, input, *, filename=None):
-        # noqa: D417
-        """Creates a tensor with a 1-filled content, but the same shape and dtype as the input tensor.
-
-        Args:
-            input (torch.Tensor): the tensor to use as an example.
-
-        Keyword Args:
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-        """
-        return cls.from_tensor(
-            torch.ones((), dtype=input.dtype, device=input.device).expand_as(input),
-            filename=filename,
-            copy_data=False,
-        ).fill_(1.0)
-
-    @classmethod
-    @overload
-    def ones(cls, *size, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    @overload
-    def ones(cls, shape, *, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    def ones(cls, *args, **kwargs):
-        # noqa: D417
-        """Creates a tensor with a 1-filled content, specific shape, dtype and filename.
-
-        Args:
-            shape (integers or torch.Size): the shape of the tensor.
-
-        Keyword Args:
-            dtype (torch.dtype): the dtype of the tensor.
-            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
-                are accepted, any other device will raise an exception.
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-            existsok (bool, optional): whether it is ok to overwrite an existing file.
-                Defaults to ``False``.
-        """
-        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
-        if device is not None:
-            device = torch.device(device)
-            if device.type != "cpu":
-                raise RuntimeError("Only CPU tensors are supported.")
-        result = torch.ones((), dtype=dtype, device=device)
-        if isinstance(shape, torch.Tensor):
-            return cls.empty(
-                shape, device=device, dtype=dtype, filename=filename
-            ).fill_(1)
-        if shape:
-            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
-                shape = torch.Size(shape[0])
-            else:
-                shape = torch.Size(shape)
-            result = result.expand(shape)
-        return cls.from_tensor(
-            result,
-            filename=filename,
-            existsok=kwargs.pop("existsok", False),
-        )
-
-    @classmethod
-    @overload
-    def zeros(cls, *size, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    @overload
-    def zeros(cls, shape, *, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    def zeros(cls, *args, **kwargs):
-        # noqa: D417
-        """Creates a tensor with a 0-filled content, specific shape, dtype and filename.
-
-        Args:
-            shape (integers or torch.Size): the shape of the tensor.
-
-        Keyword Args:
-            dtype (torch.dtype): the dtype of the tensor.
-            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
-                are accepted, any other device will raise an exception.
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-            existsok (bool, optional): whether it is ok to overwrite an existing file.
-                Defaults to ``False``.
-        """
-        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
-        if device is not None:
-            device = torch.device(device)
-            if device.type != "cpu":
-                raise RuntimeError("Only CPU tensors are supported.")
-        if isinstance(shape, torch.Tensor):
-            return cls.empty(
-                shape, device=device, dtype=dtype, filename=filename
-            ).fill_(0)
-        result = torch.zeros((), dtype=dtype, device=device)
-        if shape:
-            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
-                shape = torch.Size(shape[0])
-            else:
-                shape = torch.Size(shape)
-            result = result.expand(shape)
-        result = cls.from_tensor(
-            result,
-            filename=filename,
-            existsok=kwargs.pop("existsok", False),
-        )
-        return result
-
-    @classmethod
-    @overload
-    def empty(cls, *size, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    @overload
-    def empty(cls, shape, *, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    def empty(cls, *args, **kwargs):
-        # noqa: D417
-        """Creates a tensor with empty content, specific shape, dtype and filename.
-
-        Args:
-            shape (integers or torch.Size): the shape of the tensor.
-
-        Keyword Args:
-            dtype (torch.dtype): the dtype of the tensor.
-            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
-                are accepted, any other device will raise an exception.
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-            existsok (bool, optional): whether it is ok to overwrite an existing file.
-                Defaults to ``False``.
-        """
-        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
-        if device is not None:
-            device = torch.device(device)
-            if device.type != "cpu":
-                raise RuntimeError("Only CPU tensors are supported.")
-        result = torch.zeros((), dtype=dtype, device=device)
-        if isinstance(shape, torch.Tensor):
-            # nested tensor
-            shape_numel = shape.prod(-1).sum()
-
-            if filename is None:
-                if dtype.is_floating_point:
-                    size = torch.finfo(dtype).bits // 8 * shape_numel
-                elif dtype.is_complex:
-                    raise ValueError(
-                        "Complex-valued tensors are not supported by MemoryMappedTensor."
-                    )
-                elif dtype == torch.bool:
-                    size = shape_numel
-                else:
-                    # assume integer
-                    size = torch.iinfo(dtype).bits // 8 * shape_numel
-                handler = _FileHandler(size)
-
-                # buffer
-                func_offset_stride = getattr(
-                    torch, "_nested_compute_contiguous_strides_offsets", None
-                )
-                if func_offset_stride is not None:
-                    offsets_strides = func_offset_stride(shape)
-                else:
-                    raise RuntimeError(NESTED_TENSOR_ERR)
-                result = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
-                result = torch._nested_view_from_buffer(
-                    result,
-                    shape,
-                    *offsets_strides,
-                )
-                result = cls(result)
-                result._handler = handler
-                return result
-            else:
-                result = torch.from_file(
-                    str(filename), shared=True, dtype=dtype, size=shape_numel
-                )
-                func_offset_stride = getattr(
-                    torch, "_nested_compute_contiguous_strides_offsets", None
-                )
-                if func_offset_stride is not None:
-                    offsets_strides = func_offset_stride(shape)
-                else:
-                    raise RuntimeError(NESTED_TENSOR_ERR)
-                result = torch._nested_view_from_buffer(
-                    result,
-                    shape,
-                    *offsets_strides,
-                )
-                result = cls(result)
-                result.filename = filename
-                return result
-            return result
-
-        if shape:
-            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
-                shape = torch.Size(shape[0])
-            else:
-                shape = torch.Size(shape)
-            result = result.expand(shape)
-        result = cls.from_tensor(
-            result,
-            filename=filename,
-            copy_data=False,
-            existsok=kwargs.pop("existsok", False),
-        )
-        return result
-
-    @classmethod
-    def empty_nested(cls, *args, **kwargs):
-        # noqa: D417
-        """Creates a tensor with empty content, specific shape, dtype and filename.
-
-        Args:
-            shape (nested_shape): the shapes of the tensors.
-
-        Keyword Args:
-            dtype (torch.dtype): the dtype of the tensor.
-            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
-                are accepted, any other device will raise an exception.
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-            existsok (bool, optional): whether it is ok to overwrite an existing file.
-                Defaults to ``False``.
-        """
-        shape = kwargs.pop("shape", args[0])
-        args = (torch.Size([]), *args)
-        _, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
-        if device is not None:
-            device = torch.device(device)
-            if device.type != "cpu":
-                raise RuntimeError("Only CPU tensors are supported.")
-        result = torch.zeros((), dtype=dtype, device=device)
-        if shape:
-            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
-                shape = torch.Size(shape[0])
-            else:
-                shape = torch.Size(shape)
-            result = result.expand(shape)
-        result = cls.from_tensor(
-            result,
-            filename=filename,
-            copy_data=False,
-            existsok=kwargs.pop("existsok", False),
-        )
-        return result
-
-    @classmethod
-    @overload
-    def full(cls, *size, fill_value, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    @overload
-    def full(cls, shape, *, fill_value, dtype=None, device=None, filename=None):
-        ...
-
-    @classmethod
-    def full(cls, *args, **kwargs):
-        # noqa: D417
-        """Creates a tensor with a single content specified by `fill_value`, specific shape, dtype and filename.
-
-        Args:
-            shape (integers or torch.Size): the shape of the tensor.
-
-        Keyword Args:
-            fill_value (float or equivalent): content of the tensor.
-            dtype (torch.dtype): the dtype of the tensor.
-            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
-                are accepted, any other device will raise an exception.
-            filename (path or equivalent): the path to the file, if any. If none
-                is provided, a handler is used.
-            existsok (bool, optional): whether it is ok to overwrite an existing file.
-                Defaults to ``False``.
-        """
-        shape, device, dtype, fill_value, filename = _proc_args_const(*args, **kwargs)
-        if device is not None:
-            device = torch.device(device)
-            if device.type != "cpu":
-                raise RuntimeError("Only CPU tensors are supported.")
-        result = torch.zeros((), dtype=dtype, device=device).fill_(fill_value)
-        if shape:
-            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
-                shape = torch.Size(shape[0])
-            else:
-                shape = torch.Size(shape)
-            result = result.expand(shape)
-        return cls.from_tensor(
-            result, filename=filename, existsok=kwargs.pop("existsok", False)
-        )
-
-    @classmethod
-    def from_filename(cls, filename, dtype, shape, index=None):
-        # noqa: D417
-        """Loads a MemoryMappedTensor from a given filename.
-
-        Args:
-            filename (path or equivalent): the path to the file.
-            dtype (torch.dtype): the dtype of the tensor.
-            shape (torch.Size or torch.Tensor): the shape of the tensor. If
-                a tensor is provided, it is assumed that the tensor is a nested_tensor
-                instance.
-            index (torch-compatible index type): an index to use to build the
-                tensor.
-
-        """
-        writable = _is_writable(filename)
-
-        if isinstance(shape, torch.Tensor):
-            func_offset_stride = getattr(
-                torch, "_nested_compute_contiguous_strides_offsets", None
-            )
-            if func_offset_stride is not None:
-                offsets_strides = func_offset_stride(shape)
-            else:
-                raise RuntimeError(
-                    "The PyTorch version isn't compatible with memmap "
-                    "nested tensors. Please upgrade to a more recent "
-                    "version."
-                )
-            if writable:
-                tensor = torch.from_file(
-                    str(filename),
-                    shared=True,
-                    dtype=dtype,
-                    size=shape.prod(-1).sum().int(),
-                )
-            else:
-                with open(str(filename), "rb") as f:
-                    mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
-                    tensor = torch.frombuffer(mm, dtype=dtype)
-                    # mm.close()
-            tensor = torch._nested_view_from_buffer(
-                tensor,
-                shape,
-                *offsets_strides,
-            )
-        else:
-            shape = torch.Size(shape)
-            # whether the file already existed
-            if writable:
-                tensor = torch.from_file(
-                    str(filename), shared=True, dtype=dtype, size=shape.numel()
-                )
-            else:
-                with open(str(filename), "rb") as f:
-                    mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
-                    tensor = torch.frombuffer(mm, dtype=dtype)
-            tensor = tensor.view(shape)
-
-        if index is not None:
-            tensor = tensor[index]
-        out = cls(tensor)
-        out.filename = filename
-        out._handler = None
-        out.index = index
-        out.parent_shape = shape
-        return out
-
-    @classmethod
-    def from_handler(cls, handler, dtype, shape, index=None):
-        # noqa: D417
-        """Loads a MemoryMappedTensor from a given handler.
-
-        Args:
-            handler (compatible file handler): the handler for the tensor.
-            dtype (torch.dtype): the dtype of the tensor.
-            shape (torch.Size or torch.Tensor): the shape of the tensor. If
-                a tensor is provided, it is assumed that the tensor is a nested_tensor
-                instance.
-            index (torch-compatible index type, optional): an index to use to build the
-                tensor.
-
-        """
-        out = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
-        if isinstance(shape, torch.Tensor):
-            func_offset_stride = getattr(
-                torch, "_nested_compute_contiguous_strides_offsets", None
-            )
-            if func_offset_stride is not None:
-                offsets_strides = func_offset_stride(shape)
-            else:
-                raise RuntimeError(
-                    "The PyTorch version isn't compatible with memmap "
-                    "nested tensors. Please upgrade to a more recent "
-                    "version."
-                )
-            out = torch._nested_view_from_buffer(
-                out,
-                shape,
-                *offsets_strides,
-            )
-        else:
-            shape = torch.Size(shape)
-            out = torch.reshape(out, shape)
-
-        if index is not None:
-            out = out[index]
-        out = cls(out)
-        out.filename = None
-        out._handler = handler
-        out.index = index
-        out.parent_shape = shape
-        return out
-
-    @property
-    def _tensor(self):
-        # for bc-compatibility with MemmapTensor, to be deprecated in v0.4
-        return self
-
-    def __setstate__(self, state):
-        if "filename" in state:
-            self.__dict__ = type(self).from_filename(**state).__dict__
-        else:
-            self.__dict__ = type(self).from_handler(**state).__dict__
-
-    def __getstate__(self):
-        if getattr(self, "_handler", None) is not None:
-            return {
-                "handler": self._handler,
-                "dtype": self.dtype,
-                "shape": self.parent_shape,
-                "index": self.index,
-            }
-        elif getattr(self, "_filename", None) is not None:
-            return {
-                "filename": self._filename,
-                "dtype": self.dtype,
-                "shape": self.parent_shape,
-                "index": self.index,
-            }
-        else:
-            raise RuntimeError("Could not find handler or filename.")
-
-    def __reduce_ex__(self, protocol):
-        return self.__reduce__()
-
-    def __reduce__(self):
-        if getattr(self, "_handler", None) is not None:
-            return type(self).from_handler, (
-                self._handler,
-                self.dtype,
-                self.parent_shape,
-                self.index,
-            )
-        elif getattr(self, "_filename", None) is not None:
-            return type(self).from_filename, (
-                self._filename,
-                self.dtype,
-                self.parent_shape,
-                self.index,
-            )
-        else:
-            raise RuntimeError("Could not find handler or filename.")
-
-    @implement_for("torch", "2.0", None)
-    def __getitem__(self, item):
-        try:
-            out = super().__getitem__(item)
-        except ValueError as err:
-            if "is unbound" in str(err):
-                raise ValueError(
-                    "Using first class dimension indices with MemoryMappedTensor "
-                    "isn't supported at the moment."
-                ) from err
-            raise
-        if out.untyped_storage().data_ptr() == self.untyped_storage().data_ptr():
-            out = self._index_wrap(out, item)
-        return out
-
-    @implement_for("torch", None, "2.0")
-    def __getitem__(self, item):  # noqa: F811
-        try:
-            out = super().__getitem__(item)
-        except ValueError as err:
-            if "is unbound" in str(err):
-                raise ValueError(
-                    "Using first class dimension indices with MemoryMappedTensor "
-                    "isn't supported at the moment."
-                ) from err
-            raise
-        if out.storage().data_ptr() == self.storage().data_ptr():
-            out = self._index_wrap(out, item)
-        return out
-
-    def _index_wrap(self, tensor, item, check=False):
-        if check:
-            if tensor.storage().data_ptr() == self.storage().data_ptr():
-                return self._index_wrap(tensor, item)
-            return tensor
-        tensor = MemoryMappedTensor(tensor)
-        tensor._handler = getattr(self, "_handler", None)
-        tensor.filename = getattr(self, "_filename", None)
-        tensor.index = item
-        tensor.parent_shape = getattr(self, "parent_shape", None)
-        return tensor
-
-    def unbind(self, dim):
-        out = super().unbind(dim)
-        if dim < 0:
-            dim = self.ndim + dim
-        index_base = (slice(None),) * dim
-        return tuple(
-            self._index_wrap(_out, index_base + (i,)) for i, _out in enumerate(out)
-        )
-
-    def chunk(self, chunks, dim=0):
-        out = super().chunk(chunks, dim)
-        return tuple(self._index_wrap(chunk, None, check=True) for chunk in out)
-
-
-#####################
-# File handler
-# borrowed from mp.heap
-
-if sys.platform == "win32":
-    import _winapi
-
-    class _FileHandler:
-        _rand = tempfile._RandomNameSequence()
-
-        def __init__(self, size):
-            self.size = size
-            for _ in range(100):
-                name = "pym-%d-%s" % (os.getpid(), next(self._rand))
-                buf = mmap.mmap(-1, size, tagname=name)
-                if _winapi.GetLastError() == 0:
-                    break
-                # We have reopened a preexisting mmap.
-                buf.close()
-            else:
-                raise FileExistsError("Cannot find name for new mmap")
-            self.name = name
-            self.buffer = buf
-            self._state = (self.size, self.name)
-
-        def __getstate__(self):
-            from multiprocessing.context import assert_spawning
-
-            assert_spawning(self)
-            return self._state
-
-        def __setstate__(self, state):
-            self.size, self.name = self._state = state
-            # Reopen existing mmap
-            self.buffer = mmap.mmap(-1, self.size, tagname=self.name)
-            # XXX Temporarily preventing buildbot failures while determining
-            # XXX the correct long-term fix. See issue 23060
-            # assert _winapi.GetLastError() == _winapi.ERROR_ALREADY_EXISTS
-
-else:
-
-    class _FileHandler:
-        if sys.platform == "linux":
-            _dir_candidates = ["/dev/shm"]
-        else:
-            _dir_candidates = []
-
-        def __init__(self, size, fd=-1):
-            self.size = size
-            self.fd = fd
-            if fd == -1:
-                self.fd, name = tempfile.mkstemp(
-                    prefix="pym-%d-" % os.getpid(), dir=self._choose_dir(size)
-                )
-                os.unlink(name)
-                util.Finalize(self, os.close, (self.fd,))
-                os.ftruncate(self.fd, size)
-            self.buffer = mmap.mmap(self.fd, self.size)
-
-        def _choose_dir(self, size):
-            # Choose a non-storage backed directory if possible,
-            # to improve performance
-            for d in self._dir_candidates:
-                st = os.statvfs(d)
-                if st.f_bavail * st.f_frsize >= size:  # enough free space?
-                    return d
-            return util.get_temp_dir()
-
-    def _reduce_handler(handler):
-        if handler.fd == -1:
-            raise ValueError(
-                "Handler is unpicklable because "
-                "forking was enabled when it was created"
-            )
-        return _rebuild_handler, (handler.size, reduction.DupFd(handler.fd))
-
-    def _rebuild_handler(size, dupfd):
-        detached = dupfd.detach()
-        return _FileHandler(size, detached)
-
-    reduction.register(_FileHandler, _reduce_handler)
-
-
-def _reduce_memmap(memmap_tensor):
-    return memmap_tensor.__reduce__()
-
-
-ForkingPickler.register(MemoryMappedTensor, _reduce_memmap)
-
-
-def _proc_args_const(*args, **kwargs):
-    if len(args) > 0:
-        # then the first (or the N first) args are the shape
-        if len(args) == 1 and isinstance(args[0], torch.Tensor):
-            shape = args[0]
-        elif len(args) == 1 and not isinstance(args[0], int):
-            shape = torch.Size(args[0])
-        else:
-            shape = torch.Size(args)
-    else:
-        # we should have a "shape" keyword arg
-        shape = kwargs.pop("shape", None)
-        if shape is None:
-            raise TypeError("Could not find the shape argument in the arguments.")
-        if not isinstance(shape, torch.Tensor):
-            shape = torch.Size(shape)
-    return (
-        shape,
-        kwargs.pop("device", None),
-        kwargs.pop("dtype", None),
-        kwargs.pop("fill_value", None),
-        kwargs.pop("filename", None),
-    )
-
-
-# Torch functions
-
-MEMMAP_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
-
-
-def implements_for_memmap(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for MemoryMappedTensor."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        MEMMAP_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-@implements_for_memmap(torch.unbind)
-def _unbind(tensor, dim):
-    return tensor.unbind(dim)
-
-
-@implements_for_memmap(torch.chunk)
-def _chunk(input, chunks, dim=0):
-    return input.chunk(chunks, dim=dim)
-
-
-def _is_writable(file_path):
-    file_path = str(file_path)
-    if os.path.exists(file_path):
-        return os.access(file_path, os.W_OK)
-    # Assume that the file can be written in the directory
-    return True
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+
+import mmap
+import os
+
+import sys
+import tempfile
+from multiprocessing import util
+from multiprocessing.context import reduction
+from pathlib import Path
+from typing import Any, Callable, overload
+
+import numpy as np
+import torch
+
+from tensordict.utils import _shape, implement_for
+
+from torch.multiprocessing.reductions import ForkingPickler
+
+NESTED_TENSOR_ERR = (
+    "The PyTorch version isn't compatible with memmap "
+    "nested tensors. Please upgrade to a more recent "
+    "version."
+)
+
+
+class MemoryMappedTensor(torch.Tensor):
+    """A Memory-mapped Tensor.
+
+    Supports filenames or file handlers.
+
+    The main advantage of MemoryMappedTensor resides in its serialization methods,
+    which ensure that the tensor is passed through queues or RPC remote calls without
+    any copy.
+
+    .. note::
+      When used within RPC settings, the filepath should be accessible to both nodes.
+      If it isn't the behaviour of passing a MemoryMappedTensor from one worker
+      to another is undefined.
+
+    MemoryMappedTensor supports multiple construction methods.
+
+    Examples:
+          >>> # from an existing tensor
+          >>> tensor = torch.randn(3)
+          >>> with tempfile.NamedTemporaryFile() as file:
+          ...     memmap_tensor = MemoryMappedTensor.from_tensor(tensor, filename=file.name)
+          ...     assert memmap_tensor.filename is not None
+          >>> # if no filename is passed, a handler is used
+          >>> tensor = torch.randn(3)
+          >>> memmap_tensor = MemoryMappedTensor.from_tensor(tensor, filename=file.name)
+          >>> assert memmap_tensor.filename is None
+          >>> # one can create an empty tensor too
+          >>> with tempfile.NamedTemporaryFile() as file:
+          ...     memmap_tensor_empty = MemoryMappedTensor.empty_like(tensor, filename=file.name)
+          >>> with tempfile.NamedTemporaryFile() as file:
+          ...     memmap_tensor_zero = MemoryMappedTensor.zeros_like(tensor, filename=file.name)
+          >>> with tempfile.NamedTemporaryFile() as file:
+          ...     memmap_tensor = MemoryMappedTensor.ones_like(tensor, filename=file.name)
+    """
+
+    _filename: str | Path = None
+    _handler: _FileHandler = None
+    _clear: bool
+    index: Any
+    parent_shape: torch.Size
+
+    def __new__(
+        cls,
+        source,
+        *,
+        dtype=None,
+        shape=None,
+        index=None,
+        device=None,
+        handler=None,
+        filename=None,
+    ):
+        if device is not None and torch.device(device).type != "cpu":
+            raise ValueError(f"{cls} device must be cpu!")
+        if isinstance(source, str):
+            if filename is not None:
+                raise TypeError("Duplicated filename argument.")
+            filename = source
+            source = None
+        if filename is not None:
+            if dtype is not None:
+                raise TypeError("Cannot pass new dtype if source is provided.")
+            result = cls.from_tensor(
+                torch.as_tensor(source),
+                filename=filename,
+                # dtype=dtype,
+                shape=shape,
+                # index=index,
+            )
+            if index is not None:
+                return result[index]
+            return result
+        elif isinstance(source, torch.StorageBase):
+            return cls.from_storage(
+                source,
+                dtype=dtype,
+                shape=shape,
+                index=index,
+                device=device,
+                handler=handler,
+                filename=filename,
+            )
+        elif handler is not None:
+            return cls.from_handler(
+                handler,
+                dtype,
+                shape,
+                index,
+            )
+        return super().__new__(cls, source)
+
+    def __init__(
+        self,
+        source,
+        *,
+        handler=None,
+        dtype=None,
+        shape=None,
+        device=None,
+        filename=None,
+    ):
+        ...
+
+    __torch_function__ = torch._C._disabled_torch_function_impl
+
+    @classmethod
+    def from_tensor(
+        cls,
+        input,
+        *,
+        filename=None,
+        existsok=False,
+        copy_existing=False,
+        copy_data=True,
+        shape=None,
+    ):
+        """Creates a MemoryMappedTensor with the same content as another tensor.
+
+        If the tensor is already a MemoryMappedTensor the original tensor is
+        returned if the `filename` argument is `None` or if the two paths match.
+        In all other cases, a new :class:`MemoryMappedTensor` is produced.
+
+        Args:
+            input (torch.Tensor): the tensor which content must be copied onto
+                the MemoryMappedTensor.
+            filename (path to a file): the path to the file where the tensor
+                should be stored. If none is provided, a file handler is used
+                instead.
+            existsok (bool, optional): if ``True``, the file will overwrite
+                an existing file. Defaults to ``False``.
+            copy_existing (bool, optional): if ``True`` and the provided input
+                is a MemoryMappedTensor with an associated filename, copying
+                the content to the new location is permitted. Otherwise, an
+                exception is thrown. This behaviour exists to prevent
+                inadvertently duplicating data on disk.
+            copy_data (bool, optional): if ``True``, the content of the tensor
+                will be copied on the storage. Defaults to ``True``.
+            shape (torch.Size or torch.Tensor): a shape to override the tensor
+                shape. If a tensor is passed, it must represent the nested shapes of a
+                nested tensor.
+        """
+        if isinstance(input, MemoryMappedTensor):
+            if (filename is None and input._filename is None) or (
+                input._filename is not None
+                and filename is not None
+                and Path(filename).absolute() == Path(input.filename).absolute()
+            ):
+                # either location was not specified, or memmap is already in the
+                # correct location, so just return the MemmapTensor unmodified
+                return input
+            elif not copy_existing and (
+                input._filename is not None
+                and filename is not None
+                and Path(filename).absolute() != Path(input.filename).absolute()
+            ):
+                raise RuntimeError(
+                    f"A filename was provided but the tensor already has a file associated "
+                    f"({input.filename}). "
+                    f"To copy the tensor onto the new location, pass copy_existing=True."
+                )
+        elif isinstance(input, np.ndarray):
+            raise TypeError(
+                "Convert input to torch.Tensor before calling MemoryMappedTensor.from_tensor."
+            )
+        if input.requires_grad:
+            raise RuntimeError(
+                "MemoryMappedTensor.from_tensor is incompatible with tensor.requires_grad."
+            )
+        if shape is None:
+            shape = _shape(input, nested_shape=True)
+        if isinstance(shape, torch.Tensor):
+            shape_numel = shape.prod(-1).sum()
+        elif isinstance(shape, torch.Size):
+            shape_numel = shape.numel()
+        else:
+            shape_numel = torch.Size(shape).numel()
+        if filename is None:
+            if input.dtype.is_floating_point:
+                size = torch.finfo(input.dtype).bits // 8 * shape_numel
+            elif input.dtype.is_complex:
+                raise ValueError(
+                    "Complex-valued tensors are not supported by MemoryMappedTensor."
+                )
+            elif input.dtype == torch.bool:
+                size = shape_numel
+            else:
+                # assume integer
+                size = torch.iinfo(input.dtype).bits // 8 * shape_numel
+            handler = _FileHandler(size)
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
+                if copy_data:
+                    result.untyped_storage().copy_(input.untyped_storage())
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=input.dtype)
+                result = result.view(shape)
+            result = cls(result)
+        else:
+            handler = None
+            if not existsok and os.path.exists(str(filename)):
+                raise RuntimeError(f"The file {filename} already exists.")
+            result = torch.from_file(
+                str(filename), shared=True, dtype=input.dtype, size=shape_numel
+            )
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                if copy_data:
+                    result.untyped_storage().copy_(input.untyped_storage())
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                result = result.view(shape)
+            result = cls(result)
+        result._handler = handler
+        result.filename = filename
+        result.index = None
+        result.parent_shape = shape
+        if copy_data:
+            if hasattr(input, "full_tensor"):
+                # for DTensors, cheaper than importing DTensor every time
+                input = input.full_tensor()
+            if not result.is_nested:
+                result.copy_(input)
+        return result
+
+    @classmethod
+    def from_storage(
+        cls,
+        storage,
+        *,
+        shape=None,
+        dtype=None,
+        device=None,
+        index=None,
+        filename=None,
+        handler=None,
+    ):
+        if storage.filename is not None:
+            if filename is None:
+                filename = storage.filename
+            elif str(storage.filename) != str(filename):
+                raise RuntimeError(
+                    "Providing a storage with an associated filename that differs from the filename argument is not permitted unless filename=None. "
+                    f"Got filename={str(filename)}, storage.filename={str(storage.filename)}"
+                )
+        tensor = torch.tensor(storage, dtype=dtype, device=device)
+        if shape is not None:
+            if isinstance(shape, torch.Tensor):
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(
+                        "The PyTorch version isn't compatible with memmap "
+                        "nested tensors. Please upgrade to a more recent "
+                        "version."
+                    )
+                tensor = torch._nested_view_from_buffer(
+                    tensor,
+                    shape,
+                    *offsets_strides,
+                )
+            else:
+                tensor = tensor.view(shape)
+
+        tensor = cls(tensor)
+        if filename is not None:
+            tensor.filename = filename
+        elif handler is not None:
+            tensor._handler = handler
+        if index is not None:
+            return tensor[index]
+        return tensor
+
+    @property
+    def filename(self):
+        """The filename of the tensor, if it has one.
+
+        Raises an exception otherwise.
+        """
+        filename = self._filename
+        if filename is None:
+            raise RuntimeError("The MemoryMappedTensor has no file associated.")
+        return filename
+
+    @filename.setter
+    def filename(self, value):
+        if value is None and self._filename is None:
+            return
+        value = str(Path(value).absolute())
+        if self._filename is not None and value != self._filename:
+            raise RuntimeError(
+                "the MemoryMappedTensor has already a filename associated."
+            )
+        self._filename = value
+
+    @classmethod
+    def empty_like(cls, input, *, filename=None):
+        # noqa: D417
+        """Creates a tensor with no content but the same shape and dtype as the input tensor.
+
+        Args:
+            input (torch.Tensor): the tensor to use as an example.
+
+        Keyword Args:
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+        """
+        return cls.from_tensor(
+            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
+            filename=filename,
+            copy_data=False,
+        )
+
+    @classmethod
+    def full_like(cls, input, fill_value, *, filename=None):
+        # noqa: D417
+        """Creates a tensor with a single content indicated by the `fill_value` argument, but the same shape and dtype as the input tensor.
+
+        Args:
+            input (torch.Tensor): the tensor to use as an example.
+            fill_value (float or equivalent): content of the tensor.
+
+        Keyword Args:
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+        """
+        return cls.from_tensor(
+            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
+            filename=filename,
+            copy_data=False,
+        ).fill_(fill_value)
+
+    @classmethod
+    def zeros_like(cls, input, *, filename=None):
+        # noqa: D417
+        """Creates a tensor with a 0-filled content, but the same shape and dtype as the input tensor.
+
+        Args:
+            input (torch.Tensor): the tensor to use as an example.
+
+        Keyword Args:
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+        """
+        return cls.from_tensor(
+            torch.zeros((), dtype=input.dtype, device=input.device).expand_as(input),
+            filename=filename,
+            copy_data=False,
+        ).fill_(0.0)
+
+    @classmethod
+    def ones_like(cls, input, *, filename=None):
+        # noqa: D417
+        """Creates a tensor with a 1-filled content, but the same shape and dtype as the input tensor.
+
+        Args:
+            input (torch.Tensor): the tensor to use as an example.
+
+        Keyword Args:
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+        """
+        return cls.from_tensor(
+            torch.ones((), dtype=input.dtype, device=input.device).expand_as(input),
+            filename=filename,
+            copy_data=False,
+        ).fill_(1.0)
+
+    @classmethod
+    @overload
+    def ones(cls, *size, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    @overload
+    def ones(cls, shape, *, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    def ones(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with a 1-filled content, specific shape, dtype and filename.
+
+        Args:
+            shape (integers or torch.Size): the shape of the tensor.
+
+        Keyword Args:
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+            existsok (bool, optional): whether it is ok to overwrite an existing file.
+                Defaults to ``False``.
+        """
+        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        result = torch.ones((), dtype=dtype, device=device)
+        if isinstance(shape, torch.Tensor):
+            return cls.empty(
+                shape, device=device, dtype=dtype, filename=filename
+            ).fill_(1)
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        return cls.from_tensor(
+            result,
+            filename=filename,
+            existsok=kwargs.pop("existsok", False),
+        )
+
+    @classmethod
+    @overload
+    def zeros(cls, *size, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    @overload
+    def zeros(cls, shape, *, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    def zeros(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with a 0-filled content, specific shape, dtype and filename.
+
+        Args:
+            shape (integers or torch.Size): the shape of the tensor.
+
+        Keyword Args:
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+            existsok (bool, optional): whether it is ok to overwrite an existing file.
+                Defaults to ``False``.
+        """
+        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        if isinstance(shape, torch.Tensor):
+            return cls.empty(
+                shape, device=device, dtype=dtype, filename=filename
+            ).fill_(0)
+        result = torch.zeros((), dtype=dtype, device=device)
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        result = cls.from_tensor(
+            result,
+            filename=filename,
+            existsok=kwargs.pop("existsok", False),
+        )
+        return result
+
+    @classmethod
+    @overload
+    def empty(cls, *size, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    @overload
+    def empty(cls, shape, *, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    def empty(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with empty content, specific shape, dtype and filename.
+
+        Args:
+            shape (integers or torch.Size): the shape of the tensor.
+
+        Keyword Args:
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+            existsok (bool, optional): whether it is ok to overwrite an existing file.
+                Defaults to ``False``.
+        """
+        shape, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        result = torch.zeros((), dtype=dtype, device=device)
+        if isinstance(shape, torch.Tensor):
+            # nested tensor
+            shape_numel = shape.prod(-1).sum()
+
+            if filename is None:
+                if dtype.is_floating_point:
+                    size = torch.finfo(dtype).bits // 8 * shape_numel
+                elif dtype.is_complex:
+                    raise ValueError(
+                        "Complex-valued tensors are not supported by MemoryMappedTensor."
+                    )
+                elif dtype == torch.bool:
+                    size = shape_numel
+                else:
+                    # assume integer
+                    size = torch.iinfo(dtype).bits // 8 * shape_numel
+                handler = _FileHandler(size)
+
+                # buffer
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+                result = cls(result)
+                result._handler = handler
+                return result
+            else:
+                result = torch.from_file(
+                    str(filename), shared=True, dtype=dtype, size=shape_numel
+                )
+                func_offset_stride = getattr(
+                    torch, "_nested_compute_contiguous_strides_offsets", None
+                )
+                if func_offset_stride is not None:
+                    offsets_strides = func_offset_stride(shape)
+                else:
+                    raise RuntimeError(NESTED_TENSOR_ERR)
+                result = torch._nested_view_from_buffer(
+                    result,
+                    shape,
+                    *offsets_strides,
+                )
+                result = cls(result)
+                result.filename = filename
+                return result
+            return result
+
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        result = cls.from_tensor(
+            result,
+            filename=filename,
+            copy_data=False,
+            existsok=kwargs.pop("existsok", False),
+        )
+        return result
+
+    @classmethod
+    def empty_nested(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with empty content, specific shape, dtype and filename.
+
+        Args:
+            shape (nested_shape): the shapes of the tensors.
+
+        Keyword Args:
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+            existsok (bool, optional): whether it is ok to overwrite an existing file.
+                Defaults to ``False``.
+        """
+        shape = kwargs.pop("shape", args[0])
+        args = (torch.Size([]), *args)
+        _, device, dtype, _, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        result = torch.zeros((), dtype=dtype, device=device)
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        result = cls.from_tensor(
+            result,
+            filename=filename,
+            copy_data=False,
+            existsok=kwargs.pop("existsok", False),
+        )
+        return result
+
+    @classmethod
+    @overload
+    def full(cls, *size, fill_value, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    @overload
+    def full(cls, shape, *, fill_value, dtype=None, device=None, filename=None):
+        ...
+
+    @classmethod
+    def full(cls, *args, **kwargs):
+        # noqa: D417
+        """Creates a tensor with a single content specified by `fill_value`, specific shape, dtype and filename.
+
+        Args:
+            shape (integers or torch.Size): the shape of the tensor.
+
+        Keyword Args:
+            fill_value (float or equivalent): content of the tensor.
+            dtype (torch.dtype): the dtype of the tensor.
+            device (torch.device): the device of the tensor. Only `None` and `"cpu"`
+                are accepted, any other device will raise an exception.
+            filename (path or equivalent): the path to the file, if any. If none
+                is provided, a handler is used.
+            existsok (bool, optional): whether it is ok to overwrite an existing file.
+                Defaults to ``False``.
+        """
+        shape, device, dtype, fill_value, filename = _proc_args_const(*args, **kwargs)
+        if device is not None:
+            device = torch.device(device)
+            if device.type != "cpu":
+                raise RuntimeError("Only CPU tensors are supported.")
+        result = torch.zeros((), dtype=dtype, device=device).fill_(fill_value)
+        if shape:
+            if isinstance(shape[0], (list, tuple)) and len(shape) == 1:
+                shape = torch.Size(shape[0])
+            else:
+                shape = torch.Size(shape)
+            result = result.expand(shape)
+        return cls.from_tensor(
+            result, filename=filename, existsok=kwargs.pop("existsok", False)
+        )
+
+    @classmethod
+    def from_filename(cls, filename, dtype, shape, index=None):
+        # noqa: D417
+        """Loads a MemoryMappedTensor from a given filename.
+
+        Args:
+            filename (path or equivalent): the path to the file.
+            dtype (torch.dtype): the dtype of the tensor.
+            shape (torch.Size or torch.Tensor): the shape of the tensor. If
+                a tensor is provided, it is assumed that the tensor is a nested_tensor
+                instance.
+            index (torch-compatible index type): an index to use to build the
+                tensor.
+
+        """
+        writable = _is_writable(filename)
+
+        if isinstance(shape, torch.Tensor):
+            func_offset_stride = getattr(
+                torch, "_nested_compute_contiguous_strides_offsets", None
+            )
+            if func_offset_stride is not None:
+                offsets_strides = func_offset_stride(shape)
+            else:
+                raise RuntimeError(
+                    "The PyTorch version isn't compatible with memmap "
+                    "nested tensors. Please upgrade to a more recent "
+                    "version."
+                )
+            if writable:
+                tensor = torch.from_file(
+                    str(filename),
+                    shared=True,
+                    dtype=dtype,
+                    size=shape.prod(-1).sum().int(),
+                )
+            else:
+                with open(str(filename), "rb") as f:
+                    mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
+                    tensor = torch.frombuffer(mm, dtype=dtype)
+                    # mm.close()
+            tensor = torch._nested_view_from_buffer(
+                tensor,
+                shape,
+                *offsets_strides,
+            )
+        else:
+            shape = torch.Size(shape)
+            # whether the file already existed
+            if writable:
+                tensor = torch.from_file(
+                    str(filename), shared=True, dtype=dtype, size=shape.numel()
+                )
+            else:
+                with open(str(filename), "rb") as f:
+                    mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
+                    tensor = torch.frombuffer(mm, dtype=dtype)
+            tensor = tensor.view(shape)
+
+        if index is not None:
+            tensor = tensor[index]
+        out = cls(tensor)
+        out.filename = filename
+        out._handler = None
+        out.index = index
+        out.parent_shape = shape
+        return out
+
+    @classmethod
+    def from_handler(cls, handler, dtype, shape, index=None):
+        # noqa: D417
+        """Loads a MemoryMappedTensor from a given handler.
+
+        Args:
+            handler (compatible file handler): the handler for the tensor.
+            dtype (torch.dtype): the dtype of the tensor.
+            shape (torch.Size or torch.Tensor): the shape of the tensor. If
+                a tensor is provided, it is assumed that the tensor is a nested_tensor
+                instance.
+            index (torch-compatible index type, optional): an index to use to build the
+                tensor.
+
+        """
+        out = torch.frombuffer(memoryview(handler.buffer), dtype=dtype)
+        if isinstance(shape, torch.Tensor):
+            func_offset_stride = getattr(
+                torch, "_nested_compute_contiguous_strides_offsets", None
+            )
+            if func_offset_stride is not None:
+                offsets_strides = func_offset_stride(shape)
+            else:
+                raise RuntimeError(
+                    "The PyTorch version isn't compatible with memmap "
+                    "nested tensors. Please upgrade to a more recent "
+                    "version."
+                )
+            out = torch._nested_view_from_buffer(
+                out,
+                shape,
+                *offsets_strides,
+            )
+        else:
+            shape = torch.Size(shape)
+            out = torch.reshape(out, shape)
+
+        if index is not None:
+            out = out[index]
+        out = cls(out)
+        out.filename = None
+        out._handler = handler
+        out.index = index
+        out.parent_shape = shape
+        return out
+
+    @property
+    def _tensor(self):
+        # for bc-compatibility with MemmapTensor, to be deprecated in v0.4
+        return self
+
+    def __setstate__(self, state):
+        if "filename" in state:
+            self.__dict__ = type(self).from_filename(**state).__dict__
+        else:
+            self.__dict__ = type(self).from_handler(**state).__dict__
+
+    def __getstate__(self):
+        if getattr(self, "_handler", None) is not None:
+            return {
+                "handler": self._handler,
+                "dtype": self.dtype,
+                "shape": self.parent_shape,
+                "index": self.index,
+            }
+        elif getattr(self, "_filename", None) is not None:
+            return {
+                "filename": self._filename,
+                "dtype": self.dtype,
+                "shape": self.parent_shape,
+                "index": self.index,
+            }
+        else:
+            raise RuntimeError("Could not find handler or filename.")
+
+    def __reduce_ex__(self, protocol):
+        return self.__reduce__()
+
+    def __reduce__(self):
+        if getattr(self, "_handler", None) is not None:
+            return type(self).from_handler, (
+                self._handler,
+                self.dtype,
+                self.parent_shape,
+                self.index,
+            )
+        elif getattr(self, "_filename", None) is not None:
+            return type(self).from_filename, (
+                self._filename,
+                self.dtype,
+                self.parent_shape,
+                self.index,
+            )
+        else:
+            raise RuntimeError("Could not find handler or filename.")
+
+    @implement_for("torch", "2.0", None)
+    def __getitem__(self, item):
+        try:
+            out = super().__getitem__(item)
+        except ValueError as err:
+            if "is unbound" in str(err):
+                raise ValueError(
+                    "Using first class dimension indices with MemoryMappedTensor "
+                    "isn't supported at the moment."
+                ) from err
+            raise
+        if out.untyped_storage().data_ptr() == self.untyped_storage().data_ptr():
+            out = self._index_wrap(out, item)
+        return out
+
+    @implement_for("torch", None, "2.0")
+    def __getitem__(self, item):  # noqa: F811
+        try:
+            out = super().__getitem__(item)
+        except ValueError as err:
+            if "is unbound" in str(err):
+                raise ValueError(
+                    "Using first class dimension indices with MemoryMappedTensor "
+                    "isn't supported at the moment."
+                ) from err
+            raise
+        if out.storage().data_ptr() == self.storage().data_ptr():
+            out = self._index_wrap(out, item)
+        return out
+
+    def _index_wrap(self, tensor, item, check=False):
+        if check:
+            if tensor.storage().data_ptr() == self.storage().data_ptr():
+                return self._index_wrap(tensor, item)
+            return tensor
+        tensor = MemoryMappedTensor(tensor)
+        tensor._handler = getattr(self, "_handler", None)
+        tensor.filename = getattr(self, "_filename", None)
+        tensor.index = item
+        tensor.parent_shape = getattr(self, "parent_shape", None)
+        return tensor
+
+    def unbind(self, dim):
+        out = super().unbind(dim)
+        if dim < 0:
+            dim = self.ndim + dim
+        index_base = (slice(None),) * dim
+        return tuple(
+            self._index_wrap(_out, index_base + (i,)) for i, _out in enumerate(out)
+        )
+
+    def chunk(self, chunks, dim=0):
+        out = super().chunk(chunks, dim)
+        return tuple(self._index_wrap(chunk, None, check=True) for chunk in out)
+
+
+#####################
+# File handler
+# borrowed from mp.heap
+
+if sys.platform == "win32":
+    import _winapi
+
+    class _FileHandler:
+        _rand = tempfile._RandomNameSequence()
+
+        def __init__(self, size):
+            self.size = size
+            for _ in range(100):
+                name = "pym-%d-%s" % (os.getpid(), next(self._rand))
+                buf = mmap.mmap(-1, size, tagname=name)
+                if _winapi.GetLastError() == 0:
+                    break
+                # We have reopened a preexisting mmap.
+                buf.close()
+            else:
+                raise FileExistsError("Cannot find name for new mmap")
+            self.name = name
+            self.buffer = buf
+            self._state = (self.size, self.name)
+
+        def __getstate__(self):
+            from multiprocessing.context import assert_spawning
+
+            assert_spawning(self)
+            return self._state
+
+        def __setstate__(self, state):
+            self.size, self.name = self._state = state
+            # Reopen existing mmap
+            self.buffer = mmap.mmap(-1, self.size, tagname=self.name)
+            # XXX Temporarily preventing buildbot failures while determining
+            # XXX the correct long-term fix. See issue 23060
+            # assert _winapi.GetLastError() == _winapi.ERROR_ALREADY_EXISTS
+
+else:
+
+    class _FileHandler:
+        if sys.platform == "linux":
+            _dir_candidates = ["/dev/shm"]
+        else:
+            _dir_candidates = []
+
+        def __init__(self, size, fd=-1):
+            self.size = size
+            self.fd = fd
+            if fd == -1:
+                self.fd, name = tempfile.mkstemp(
+                    prefix="pym-%d-" % os.getpid(), dir=self._choose_dir(size)
+                )
+                os.unlink(name)
+                util.Finalize(self, os.close, (self.fd,))
+                os.ftruncate(self.fd, size)
+            self.buffer = mmap.mmap(self.fd, self.size)
+
+        def _choose_dir(self, size):
+            # Choose a non-storage backed directory if possible,
+            # to improve performance
+            for d in self._dir_candidates:
+                st = os.statvfs(d)
+                if st.f_bavail * st.f_frsize >= size:  # enough free space?
+                    return d
+            return util.get_temp_dir()
+
+    def _reduce_handler(handler):
+        if handler.fd == -1:
+            raise ValueError(
+                "Handler is unpicklable because "
+                "forking was enabled when it was created"
+            )
+        return _rebuild_handler, (handler.size, reduction.DupFd(handler.fd))
+
+    def _rebuild_handler(size, dupfd):
+        detached = dupfd.detach()
+        return _FileHandler(size, detached)
+
+    reduction.register(_FileHandler, _reduce_handler)
+
+
+def _reduce_memmap(memmap_tensor):
+    return memmap_tensor.__reduce__()
+
+
+ForkingPickler.register(MemoryMappedTensor, _reduce_memmap)
+
+
+def _proc_args_const(*args, **kwargs):
+    if len(args) > 0:
+        # then the first (or the N first) args are the shape
+        if len(args) == 1 and isinstance(args[0], torch.Tensor):
+            shape = args[0]
+        elif len(args) == 1 and not isinstance(args[0], int):
+            shape = torch.Size(args[0])
+        else:
+            shape = torch.Size(args)
+    else:
+        # we should have a "shape" keyword arg
+        shape = kwargs.pop("shape", None)
+        if shape is None:
+            raise TypeError("Could not find the shape argument in the arguments.")
+        if not isinstance(shape, torch.Tensor):
+            shape = torch.Size(shape)
+    return (
+        shape,
+        kwargs.pop("device", None),
+        kwargs.pop("dtype", None),
+        kwargs.pop("fill_value", None),
+        kwargs.pop("filename", None),
+    )
+
+
+# Torch functions
+
+MEMMAP_HANDLED_FUNCTIONS: dict[Callable, Callable] = {}
+
+
+def implements_for_memmap(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for MemoryMappedTensor."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        MEMMAP_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+@implements_for_memmap(torch.unbind)
+def _unbind(tensor, dim):
+    return tensor.unbind(dim)
+
+
+@implements_for_memmap(torch.chunk)
+def _chunk(input, chunks, dim=0):
+    return input.chunk(chunks, dim=dim)
+
+
+def _is_writable(file_path):
+    file_path = str(file_path)
+    if os.path.exists(file_path):
+        return os.access(file_path, os.W_OK)
+    # Assume that the file can be written in the directory
+    return True
```

## tensordict/persistent.py

 * *Ordering differences only*

```diff
@@ -1,1380 +1,1380 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-"""Persistent tensordicts (H5 and others)."""
-from __future__ import annotations
-
-import importlib
-
-import json
-import os
-
-import tempfile
-import warnings
-from functools import wraps
-from pathlib import Path
-from typing import Any, Callable, Tuple, Type
-
-import numpy as np
-import torch
-
-from tensordict._td import (
-    _TensorDictKeysView,
-    _unravel_key_to_tuple,
-    CompatibleType,
-    NO_DEFAULT,
-    TensorDict,
-)
-from tensordict.base import (
-    _default_is_leaf,
-    _is_leaf_nontensor,
-    is_tensor_collection,
-    T,
-    TensorDictBase,
-)
-from tensordict.memmap import MemoryMappedTensor
-from tensordict.utils import (
-    _CloudpickleWrapper,
-    _KEY_ERROR,
-    _LOCK_ERROR,
-    _parse_to,
-    _proc_init,
-    _split_tensordict,
-    cache,
-    expand_right,
-    IndexType,
-    is_non_tensor,
-    lock_blocked,
-    NestedKey,
-    NUMPY_TO_TORCH_DTYPE_DICT,
-    unravel_key,
-)
-from torch import multiprocessing as mp
-
-_has_h5 = importlib.util.find_spec("h5py", None) is not None
-
-
-class _Visitor:
-    def __init__(self, fun=None):
-        self.elts = []
-        self.fun = fun
-
-    def __call__(self, name):
-        self.elts.append(name)
-
-    def __iter__(self):
-        if self.fun is None:
-            yield from self.elts
-        else:
-            for elt in self.elts:
-                yield self.fun(elt)
-
-
-class _PersistentTDKeysView(_TensorDictKeysView):
-    def __iter__(self):
-        # For consistency with tensordict where currently a non-tensor is stored in a
-        # tensorclass and hence can be seen as a nested tensordict
-        # that situation should be clarified
-        read_non_tensor = self.is_leaf is _is_leaf_nontensor or not self.leaves_only
-        if self.include_nested:
-            visitor = _Visitor(lambda key: unravel_key(tuple(key.split("/"))))
-            self.tensordict.file.visit(visitor)
-        else:
-            visitor = self.tensordict.file.keys()
-        for key in visitor:
-            metadata = self.tensordict._get_metadata(key)
-            if metadata.get("non_tensor"):
-                if read_non_tensor:
-                    yield key
-                else:
-                    continue
-            elif metadata.get("array"):
-                yield key
-            elif not self.leaves_only and (
-                not isinstance(key, tuple) or self.include_nested
-            ):
-                yield key
-
-    def __contains__(self, key):
-        key = unravel_key(key)
-        return key in list(self)
-
-
-class PersistentTensorDict(TensorDictBase):
-    """Persistent TensorDict implementation.
-
-    :class:`PersistentTensorDict` instances provide an interface with data stored
-    on disk such that access to this data is made easy while still taking advantage
-    from the fast access provided by the backend.
-
-    Like other :class:`TensorDictBase` subclasses, :class:`PersistentTensorDict`
-    has a ``device`` attribute. This does *not* mean that the data is being stored
-    on that device, but rather that when loaded, the data will be cast onto
-    the desired device.
-
-    Args:
-        batch_size (torch.Size or compatible): the tensordict batch size.
-        filename (str, optional): the path to the h5 file. Exclusive with ``group``.
-        group (h5py.Group, optional): a file or a group that contains data. Exclusive with ``filename``.
-        mode (str, optional): Reading mode. Defaults to ``"r"``.
-        backend (str, optional): storage backend. Currently only ``"h5"`` is supported.
-        device (torch.device or compatible, optional): device of the tensordict.
-            Defaults to ``None`` (ie. default PyTorch device).
-        **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-    .. note::
-      Currently, PersistentTensorDict instances are not closed when getting out-of-scope.
-      This means that it is the responsibility of the user to close them if necessary.
-
-    Examples:
-        >>> import tempfile
-        >>> with tempfile.NamedTemporaryFile() as f:
-        ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
-        ...     data["a", "b"] = torch.randn(3, 4)
-        ...     print(data)
-
-    """
-
-    _td_dim_names = None
-    LOCKING = None
-
-    def __init__(
-        self,
-        *,
-        batch_size,
-        filename=None,
-        group=None,
-        mode="r",
-        backend="h5",
-        device=None,
-        **kwargs,
-    ):
-        self._locked_tensordicts = []
-        self._lock_id = set()
-        if not _has_h5:
-            raise ModuleNotFoundError("Could not load h5py.")
-        import h5py
-
-        super().__init__()
-        self.filename = filename
-        self.mode = mode
-        if backend != "h5":
-            raise NotImplementedError
-        if filename is not None and group is None:
-            self.file = h5py.File(filename, mode, locking=self.LOCKING)
-        elif group is not None:
-            self.file = group
-        else:
-            raise RuntimeError(
-                f"Either group or filename must be provided, and not both. Got group={group} and filename={filename}."
-            )
-        self._batch_size = torch.Size(batch_size)
-        self._device = torch.device(device) if device is not None else None
-        self._is_shared = False
-        self._is_memmap = False
-        self.kwargs = kwargs
-
-        # we use this to allow nested tensordicts to have a different batch-size
-        self._nested_tensordicts = {}
-        self._pin_mem = False
-
-        # this must be kept last
-        self._check_batch_size(self._batch_size)
-
-    @classmethod
-    def from_h5(cls, filename, mode="r"):
-        """Creates a PersistentTensorDict from a h5 file.
-
-        This function will automatically determine the batch-size for each nested
-        tensordict.
-
-        Args:
-            filename (str): the path to the h5 file.
-            mode (str, optional): reading mode. Defaults to ``"r"``.
-        """
-        out = cls(filename=filename, mode=mode, batch_size=[])
-        # determine batch size
-        _set_max_batch_size(out)
-        return out
-
-    @classmethod
-    def from_dict(cls, input_dict, filename, batch_size=None, device=None, **kwargs):
-        """Converts a dictionary or a TensorDict to a h5 file.
-
-        Args:
-            input_dict (dict, TensorDict or compatible): data to be stored as h5.
-            filename (str or path): path to the h5 file.
-            batch_size (tensordict batch-size, optional): if provided, batch size
-                of the tensordict. If not, the batch size will be gathered from the
-                input structure (if present) or determined automatically.
-            device (torch.device or compatible, optional): the device where to
-                expect the tensor once they are returned. Defaults to ``None``
-                (on cpu by default).
-            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
-
-        Returns:
-            A :class:`PersitentTensorDict` instance linked to the newly created file.
-
-        """
-        import h5py
-
-        file = h5py.File(filename, "w", locking=cls.LOCKING)
-        _has_batch_size = True
-        if batch_size is None:
-            if is_tensor_collection(input_dict):
-                batch_size = input_dict.batch_size
-            else:
-                _has_batch_size = False
-                batch_size = torch.Size([])
-
-        # let's make a tensordict first
-        out = cls(group=file, batch_size=batch_size, device=device, **kwargs)
-        if is_tensor_collection(input_dict):
-            out.update(input_dict)
-        else:
-            out.update(TensorDict(input_dict, batch_size=batch_size))
-        if not _has_batch_size:
-            _set_max_batch_size(out)
-        return out
-
-    def close(self):
-        """Closes the persistent tensordict."""
-        self.file.close()
-
-    def _process_key(self, key):
-        key = _unravel_key_to_tuple(key)
-        return "/".join(key)
-
-    def _check_batch_size(self, batch_size) -> None:
-        for key in self.keys(include_nested=True, leaves_only=True):
-            key = self._process_key(key)
-            array = self.file[key]
-            if _is_non_tensor_h5(array):
-                continue
-            size = array.shape
-            if torch.Size(size[: len(batch_size)]) != batch_size:
-                raise ValueError(
-                    f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
-                )
-
-    def _get_array(self, key, default=NO_DEFAULT):
-        try:
-            key = self._process_key(key)
-            array = self.file[key]
-            return array
-        except KeyError:
-            if default is not NO_DEFAULT:
-                return default
-            raise KeyError(f"key {key} not found in PersistentTensorDict {self}")
-
-    def _process_array(self, key, array):
-        import h5py
-
-        if isinstance(array, (h5py.Dataset,)):
-            if self.device is not None:
-                device = self.device
-            else:
-                device = torch.device("cpu")
-            # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
-            if not _is_non_tensor_h5(array):
-                array = array[()]
-                out = torch.as_tensor(array, device=device)
-                if self._pin_mem:
-                    out = out.pin_memory()
-            else:
-                from tensordict.tensorclass import NonTensorData
-
-                array = array[()]
-                out = NonTensorData(
-                    data=array, device=device, batch_size=self.batch_size
-                )
-            return out
-        else:
-            out = self._nested_tensordicts.get(key, None)
-            if out is None:
-                out = self._nested_tensordicts[key] = PersistentTensorDict(
-                    group=array,
-                    batch_size=self.batch_size,
-                    device=self.device,
-                )
-            return out
-
-    @cache  # noqa: B019
-    def get(self, key, default=NO_DEFAULT):
-        array = self._get_array(key, default)
-        if array is default:
-            return array
-        return self._process_array(key, array)
-
-    _get_str = get
-    _get_tuple = get
-
-    def get_at(
-        self, key: NestedKey, idx: IndexType, default: CompatibleType = NO_DEFAULT
-    ) -> CompatibleType:
-        import h5py
-
-        array = self._get_array(key, default)
-        if isinstance(array, (h5py.Dataset,)):
-            if self.device is not None:
-                device = self.device
-            else:
-                device = torch.device("cpu")
-            # indexing must be done before converting to tensor.
-            idx = self._process_index(idx, array)
-            # `get_at` is there to save us.
-            try:
-                out = torch.as_tensor(array[idx], device=device)
-            except TypeError as err:
-                if "Boolean indexing array has incompatible shape" in str(err):
-                    # Known bug in h5py: cannot broadcast boolean mask on the right as
-                    # done in np and torch. Therefore we put a performance warning
-                    # and convert to torch tensor first.
-                    warnings.warn(
-                        "Indexing an h5py.Dataset object with a boolean mask "
-                        "that needs broadcasting does not work directly. "
-                        "tensordict will cast the entire array in memory and index it using the mask. "
-                        "This is suboptimal and may lead to performance issue."
-                    )
-                    out = torch.as_tensor(np.asarray(array), device=device)[idx]
-                else:
-                    raise err
-            if self._pin_mem:
-                return out.pin_memory()
-            return out
-        elif array is not default:
-            out = self._nested_tensordicts.get(key, None)
-            if out is None:
-                out = self._nested_tensordicts[key] = PersistentTensorDict(
-                    group=array,
-                    batch_size=self.batch_size,
-                    device=self.device,
-                )
-            return out._get_sub_tensordict(idx)
-        else:
-            return default
-
-    def _get_metadata(self, key):
-        """Gets the metadata for an entry.
-
-        This method avoids creating a tensor from scratch, and just reads the metadata of the array.
-        """
-        import h5py
-
-        array = self._get_array(key)
-        if (
-            isinstance(array, (h5py.Dataset,))
-            and array.dtype in NUMPY_TO_TORCH_DTYPE_DICT
-        ):
-            shape = torch.Size(array.shape)
-            return {
-                "dtype": NUMPY_TO_TORCH_DTYPE_DICT[array.dtype],
-                "shape": shape,
-                "dim": len(shape),
-                "array": True,
-            }
-        elif (
-            isinstance(array, (h5py.Dataset,))
-            and array.dtype not in NUMPY_TO_TORCH_DTYPE_DICT
-        ):
-            return {"non_tensor": True}
-        else:
-            val = self.get(key)
-            shape = val.shape
-            return {
-                "dtype": None,
-                "shape": shape,
-                "dim": len(shape),
-                "array": False,
-            }
-
-    @classmethod
-    def _process_index(cls, idx, array=None):
-        if isinstance(idx, tuple):
-            return tuple(cls._process_index(_idx, array) for _idx in idx)
-        if isinstance(idx, torch.Tensor):
-            return idx.cpu().detach().numpy()
-        if isinstance(idx, (range, list)):
-            return np.asarray(idx)
-        return idx
-
-    def __getitem__(self, item):
-        if isinstance(item, str) or (
-            isinstance(item, tuple) and _unravel_key_to_tuple(item)
-        ):
-            result = self.get(item)
-            if is_non_tensor(result):
-                result_data = getattr(result, "data", NO_DEFAULT)
-                if result_data is NO_DEFAULT:
-                    return result.tolist()
-                return result_data
-            return result
-        if isinstance(item, list):
-            # convert to tensor
-            item = torch.tensor(item)
-        return self._get_sub_tensordict(item)
-
-    __getitems__ = __getitem__
-
-    def __setitem__(self, index, value):
-        index_unravel = _unravel_key_to_tuple(index)
-        if index_unravel:
-            return self.set(index_unravel, value, inplace=True)
-
-        if isinstance(index, list):
-            # convert to tensor
-            index = torch.tensor(index)
-        sub_td = self._get_sub_tensordict(index)
-        err_set_batch_size = None
-        if not isinstance(value, TensorDictBase):
-            value = TensorDict.from_dict(value, batch_size=[])
-            # try to assign the current shape. If that does not work, we can
-            # try to expand
-            try:
-                value.batch_size = sub_td.batch_size
-            except RuntimeError as err0:
-                err_set_batch_size = err0
-        if value.shape != sub_td.shape:
-            try:
-                value = value.expand(sub_td.shape)
-            except RuntimeError as err:
-                if err_set_batch_size is not None:
-                    raise err from err_set_batch_size
-                raise RuntimeError(
-                    f"Cannot broadcast the tensordict {value} to the shape of the indexed persistent tensordict {self}[{index}]."
-                ) from err
-        sub_td.update(value, inplace=True)
-
-    @cache  # noqa: B019
-    def _valid_keys(self):
-        keys = []
-        for key in self.file.keys():
-            metadata = self._get_metadata(key)
-            if not metadata.get("non_tensor"):
-                keys.append(key)
-        return keys
-
-    # @cache  # noqa: B019
-    def keys(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> _PersistentTDKeysView:
-        if is_leaf not in (None, _default_is_leaf, _is_leaf_nontensor):
-            raise ValueError(
-                f"is_leaf {is_leaf} is not supported within tensordicts of type {type(self)}."
-            )
-        return _PersistentTDKeysView(
-            tensordict=self,
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-            is_leaf=is_leaf,
-        )
-
-    def _items_metadata(self, include_nested=False, leaves_only=False):
-        """Iterates over the metadata of the PersistentTensorDict."""
-        for key in self.keys(include_nested, leaves_only):
-            yield (key, self._get_metadata(key))
-
-    def _values_metadata(self, include_nested=False, leaves_only=False):
-        """Iterates over the metadata of the PersistentTensorDict."""
-        for key in self.keys(include_nested, leaves_only):
-            yield self._get_metadata(key)
-
-    def _change_batch_size(self, value):
-        raise NotImplementedError
-
-    def _stack_onto_(
-        self, list_item: list[CompatibleType], dim: int
-    ) -> PersistentTensorDict:
-        for key in self.keys():
-            vals = [td._get_str(key, None) for td in list_item]
-            if all(v is None for v in vals):
-                continue
-            stacked = torch.stack(vals, dim=dim)
-            self.set_(key, stacked)
-        return self
-
-    @property
-    def batch_size(self):
-        return self._batch_size
-
-    @batch_size.setter
-    def batch_size(self, value):
-        _batch_size = self._batch_size
-        try:
-            self._batch_size = torch.Size(value)
-            self._check_batch_size(self._batch_size)
-        except ValueError:
-            self._batch_size = _batch_size
-
-    _erase_names = TensorDict._erase_names
-    names = TensorDict.names
-    _has_names = TensorDict._has_names
-
-    def _rename_subtds(self, names):
-        if names is None:
-            names = [None] * self.ndim
-        for item in self._nested_tensordicts.values():
-            if is_tensor_collection(item):
-                td_names = list(names) + [None] * (item.ndim - self.ndim)
-                item.rename_(*td_names)
-
-    def contiguous(self):
-        """Materializes a PersistentTensorDict on a regular TensorDict."""
-        return self.to_tensordict()
-
-    @lock_blocked
-    def del_(self, key):
-        key = self._process_key(key)
-        del self.file[key]
-        return self
-
-    def detach_(self):
-        # PersistentTensorDict do not carry gradients. This is a no-op
-        return self
-
-    @property
-    def device(self):
-        return self._device
-
-    def empty(
-        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
-    ) -> T:
-        if recurse:
-            out = self.empty(
-                recurse=False, batch_size=batch_size, device=device, names=names
-            )
-            for key, val in self.items():
-                if is_tensor_collection(val):
-                    out._set_str(
-                        key,
-                        val.empty(
-                            recurse=True,
-                            batch_size=batch_size,
-                            device=device,
-                            names=names,
-                        ),
-                        inplace=False,
-                        validated=True,
-                        non_blocking=False,
-                    )
-            return out
-        return TensorDict(
-            {},
-            device=self.device if device is NO_DEFAULT else device,
-            batch_size=self.batch_size if batch_size is None else batch_size,
-            names=self.names if names is None and self._has_names() else names,
-        )
-
-    def zero_(self) -> T:
-        for key in self.keys():
-            self.fill_(key, 0)
-        return self
-
-    def entry_class(self, key: NestedKey) -> type:
-        entry_class = self._get_metadata(key)
-        is_array = entry_class.get("array", None)
-        if is_array:
-            return torch.Tensor
-        elif is_array is False:
-            return PersistentTensorDict
-        else:
-            raise RuntimeError(f"Encountered a non-numeric data {key}.")
-
-    def is_contiguous(self):
-        return False
-
-    def masked_fill(self, mask, value):
-        return self.to_tensordict().masked_fill(mask, value)
-
-    def where(self, condition, other, *, out=None, pad=None):
-        return self.to_tensordict().where(
-            condition=condition, other=other, out=out, pad=pad
-        )
-
-    def masked_fill_(self, mask, value):
-        for key in self.keys(include_nested=True, leaves_only=True):
-            array = self._get_array(key)
-            array[expand_right(mask, array.shape).cpu().numpy()] = value
-        return self
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def memmap_(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        num_threads: int = 0,
-    ) -> PersistentTensorDict:
-        raise RuntimeError(
-            "Cannot build a memmap TensorDict in-place from a PersistentTensorDict. Use `td.memmap()` instead."
-        )
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None,
-        copy_existing: bool,
-        executor,
-        futures,
-        inplace,
-        like,
-        share_non_tensor,
-    ) -> T:
-        if inplace:
-            raise RuntimeError("Cannot call memmap inplace in a persistent tensordict.")
-
-        # re-implements this to make it faster using the meta-data
-        def save_metadata(data: TensorDictBase, filepath, metadata=None):
-            if metadata is None:
-                metadata = {}
-            metadata.update(
-                {
-                    "shape": list(data.shape),
-                    "device": str(data.device),
-                    "_type": str(self.__class__),
-                }
-            )
-            with open(filepath, "w") as json_metadata:
-                json.dump(metadata, json_metadata)
-
-        if prefix is not None:
-            prefix = Path(prefix)
-            if not prefix.exists():
-                os.makedirs(prefix, exist_ok=True)
-            metadata = {}
-        if not self.keys():
-            raise Exception(
-                "memmap_like() must be called when the TensorDict is (partially) "
-                "populated. Set a tensor first."
-            )
-        dest = TensorDict(
-            {},
-            batch_size=self.batch_size,
-            names=self.names if self._has_names() else None,
-            device=torch.device("cpu"),
-        )
-        dest._is_memmap = True
-        for key, value in self._items_metadata():
-            if not value["array"]:
-                value = self._get_str(key)
-                dest._set_str(
-                    key,
-                    value._memmap_(
-                        prefix=prefix / key if prefix is not None else None,
-                        executor=executor,
-                        like=like,
-                        copy_existing=copy_existing,
-                        futures=futures,
-                        inplace=inplace,
-                        share_non_tensor=share_non_tensor,
-                    ),
-                    inplace=False,
-                    validated=True,
-                    non_blocking=False,
-                )
-                continue
-            else:
-                value = self._get_str(key)
-                if prefix is not None:
-                    metadata[key] = {
-                        "dtype": str(value.dtype),
-                        "shape": value.shape,
-                        "device": str(value.device),
-                    }
-
-                def _populate(
-                    tensordict=dest, key=key, value=value, prefix=prefix, like=like
-                ):
-                    val = MemoryMappedTensor.from_tensor(
-                        value,
-                        filename=str(prefix / f"{key}.memmap")
-                        if prefix is not None
-                        else None,
-                        copy_data=not like,
-                        copy_existing=copy_existing,
-                        existsok=True,
-                    )
-                    tensordict._set_str(
-                        key,
-                        val,
-                        inplace=False,
-                        validated=True,
-                        non_blocking=False,
-                    )
-
-                if executor is None:
-                    _populate()
-                else:
-                    futures.append(executor.submit(_populate))
-
-        if prefix is not None:
-            if executor is None:
-                save_metadata(dest, prefix / "meta.json", metadata)
-            else:
-                futures.append(
-                    executor.submit(save_metadata, dest, prefix / "meta.json", metadata)
-                )
-        return dest
-
-    _load_memmap = TensorDict._load_memmap
-
-    def pin_memory(self):
-        """Returns a new PersistentTensorDict where any given Tensor key returns a tensor with pin_memory=True.
-
-        This will fail with PersistentTensorDict with a ``cuda`` device attribute.
-
-        """
-        if self.device.type == "cuda":
-            raise RuntimeError("cannot pin memory on a tensordict stored on cuda.")
-        out = self.clone(False)
-        out._pin_mem = True
-        out._nested_tensordicts = {
-            key: val.pin_memory() for key, val in out._nested_tensordicts.items()
-        }
-        return out
-
-    @lock_blocked
-    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
-        raise NotImplementedError(
-            f"popitem not implemented for class {type(self).__name__}."
-        )
-
-    def map(
-        self,
-        fn: Callable,
-        dim: int = 0,
-        num_workers: int = None,
-        *,
-        out: TensorDictBase = None,
-        chunksize: int = None,
-        num_chunks: int = None,
-        pool: mp.Pool = None,
-        generator: torch.Generator | None = None,
-        max_tasks_per_child: int | None = None,
-        worker_threads: int = 1,
-        index_with_generator: bool = False,
-        pbar: bool = False,
-        mp_start_method: str | None = None,
-    ):
-        if pool is None:
-            if num_workers is None:
-                num_workers = mp.cpu_count()  # Get the number of CPU cores
-            if generator is None:
-                generator = torch.Generator()
-            seed = (
-                torch.empty((), dtype=torch.int64).random_(generator=generator).item()
-            )
-            if mp_start_method is not None:
-                ctx = mp.get_context(mp_start_method)
-            else:
-                ctx = mp.get_context()
-
-            queue = ctx.Queue(maxsize=num_workers)
-            for i in range(num_workers):
-                queue.put(i)
-            with ctx.Pool(
-                processes=num_workers,
-                initializer=_proc_init,
-                initargs=(seed, queue, worker_threads),
-                maxtasksperchild=max_tasks_per_child,
-            ) as pool:
-                return self.map(fn, dim=dim, chunksize=chunksize, pool=pool)
-        num_workers = pool._processes
-        dim_orig = dim
-        if dim < 0:
-            dim = self.ndim + dim
-        if dim < 0 or dim >= self.ndim:
-            raise ValueError(f"Got incompatible dimension {dim_orig}")
-
-        self_split = _split_tensordict(
-            self,
-            chunksize,
-            num_chunks,
-            num_workers,
-            dim,
-            use_generator=index_with_generator,
-            to_tensordict=True,
-        )
-        if not index_with_generator:
-            length = len(self_split)
-            self_split = tuple(split.to_tensordict() for split in self_split)
-        else:
-            length = None
-
-        if out is not None and (out.is_shared() or out.is_memmap()):
-
-            def wrap_fn_with_out(fn, out):
-                @wraps(fn)
-                def newfn(item_and_out):
-                    item, out = item_and_out
-                    result = fn(item)
-                    out.update_(result)
-                    return
-
-                out_split = _split_tensordict(
-                    out,
-                    chunksize,
-                    num_chunks,
-                    num_workers,
-                    dim,
-                    use_generator=index_with_generator,
-                )
-                return _CloudpickleWrapper(newfn), zip(self_split, out_split)
-
-            fn, self_split = wrap_fn_with_out(fn, out)
-            out = None
-
-        call_chunksize = 1
-        imap = pool.imap(fn, self_split, call_chunksize)
-
-        if pbar and importlib.util.find_spec("tqdm", None) is not None:
-            import tqdm
-
-            imap = tqdm.tqdm(imap, total=length)
-
-        imaplist = []
-        start = 0
-        for item in imap:
-            if item is not None:
-                if out is not None:
-                    if chunksize:
-                        end = start + item.shape[dim]
-                        chunk = slice(start, end)
-                        out[chunk].update_(item)
-                        start = end
-                    else:
-                        out[start].update_(item)
-                        start += 1
-                else:
-                    imaplist.append(item)
-        del imap
-
-        # support inplace modif
-        if imaplist:
-            if chunksize == 0:
-                out = torch.stack(imaplist, dim)
-            else:
-                out = torch.cat(imaplist, dim)
-        return out
-
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> PersistentTensorDict:
-        old_key = self._process_key(old_key)
-        new_key = self._process_key(new_key)
-        try:
-            self.file.move(old_key, new_key)
-        except ValueError as err:
-            raise KeyError(f"key {new_key} already present in TensorDict.") from err
-        return self
-
-    def fill_(self, key: NestedKey, value: float | bool) -> TensorDictBase:
-        """Fills a tensor pointed by the key with the a given value.
-
-        Args:
-            key (str): key to be remaned
-            value (Number, bool): value to use for the filling
-
-        Returns:
-            self
-
-        """
-        md = self._get_metadata(key)
-        if md.get("array", None):
-            array = self._get_array(key)
-            array[:] = value
-        else:
-            nested = self.get(key)
-            for subkey in nested.keys():
-                nested.fill_(subkey, value)
-        return self
-
-    def _create_nested_str(self, key):
-        self.file.create_group(key)
-        target_td = self._get_str(key)
-        return target_td
-
-    def _select(
-        self, *keys: NestedKey, inplace: bool = False, strict: bool = True
-    ) -> PersistentTensorDict:
-        raise NotImplementedError(
-            "Cannot call select on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> PersistentTensorDict:
-        raise NotImplementedError(
-            "Cannot call exclude on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def flatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
-        if inplace:
-            raise ValueError(
-                "Cannot call flatten_keys in_place with a PersistentTensorDict."
-            )
-        return self.to_tensordict().flatten_keys(separator=separator)
-
-    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
-        if inplace:
-            raise ValueError(
-                "Cannot call unflatten_keys in_place with a PersistentTensorDict."
-            )
-        return self.to_tensordict().unflatten_keys(separator=separator)
-
-    def share_memory_(self):
-        raise NotImplementedError(
-            "Cannot call share_memory_ on a PersistentTensorDict. "
-            "Create a regular tensordict first using the `to_tensordict` method."
-        )
-
-    def to(self, *args, **kwargs: Any) -> PersistentTensorDict:
-        device, dtype, non_blocking, convert_to_format, batch_size = _parse_to(
-            *args, **kwargs
-        )
-        result = self
-        if device is not None and dtype is None and device == self.device:
-            return result
-        if dtype is not None:
-            return self.to_tensordict().to(*args, **kwargs)
-        result = self
-        if device is not None:
-            result = result.clone(False)
-            result._device = device
-            for key, nested in list(result._nested_tensordicts.items()):
-                result._nested_tensordicts[key] = nested.to(device)
-        if batch_size is not None:
-            result.batch_size = batch_size
-        return result
-
-    def _to_numpy(self, value):
-        if hasattr(value, "requires_grad") and value.requires_grad:
-            raise RuntimeError("Cannot set a tensor that has requires_grad=True.")
-        if isinstance(value, torch.Tensor):
-            out = value.cpu().detach().numpy()
-        elif isinstance(value, dict):
-            out = TensorDict(value, [])
-        elif is_non_tensor(value):
-            value = value.data
-            if isinstance(value, str):
-                return value
-            import h5py
-
-            out = np.array(value)
-            out = out.astype(h5py.opaque_dtype(out.dtype))
-        elif is_tensor_collection(value):
-            out = value
-        elif isinstance(value, (np.ndarray,)):
-            out = value
-        else:
-            raise NotImplementedError(
-                f"Cannot set values of type {value} in a PersistentTensorDict."
-            )
-        return out
-
-    def _set(
-        self,
-        key: NestedKey,
-        value: Any,
-        *,
-        inplace: bool = False,
-        idx=None,
-        validated: bool = False,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ) -> PersistentTensorDict:
-        if not validated:
-            value = self._validate_value(value, check_shape=idx is None)
-        value = self._to_numpy(value)
-        if not inplace:
-            if idx is not None:
-                raise RuntimeError("Cannot pass an index to _set when inplace=False.")
-            elif self.is_locked and not ignore_lock:
-                raise RuntimeError(_LOCK_ERROR)
-        # shortcut set if we're placing a tensordict
-        if is_tensor_collection(value):
-            if isinstance(key, tuple):
-                key, subkey = key[0], key[1:]
-            else:
-                key, subkey = key, []
-            target_td = self._get_str(key, default=None)
-            if target_td is None:
-                self.file.create_group(key)
-                target_td = self._get_str(key)
-                target_td.batch_size = value.batch_size
-            elif not is_tensor_collection(target_td):
-                raise RuntimeError(
-                    f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
-                    f"Got self.get({key})={target_td} and value={value}."
-                )
-            if idx is None:
-                if len(subkey):
-                    target_td.set(subkey, value, inplace=inplace)
-                else:
-                    target_td.update(value, inplace=inplace)
-            else:
-                if len(subkey):
-                    target_td.set_at_(subkey, value, idx=idx)
-                else:
-                    target_td.update_at_(value, idx=idx)
-
-            return self
-
-        if inplace:
-            # could be called before but will go under further refactoring of set
-            key = self._process_key(key)
-            array = self.file[key]
-            if idx is None:
-                idx = ()
-            else:
-                idx = self._process_index(idx, array)
-            try:
-                array[idx] = value
-            except TypeError as err:
-                if "Boolean indexing array has incompatible shape" in str(err):
-                    # Known bug in h5py: cannot broadcast boolean mask on the right as
-                    # done in np and torch. Therefore we put a performance warning
-                    # and convert to torch tensor first.
-                    warnings.warn(
-                        "Indexing an h5py.Dataset object with a boolean mask "
-                        "that needs broadcasting does not work directly. "
-                        "tensordict will cast the entire array in memory and index it using the mask. "
-                        "This is suboptimal and may lead to performance issue."
-                    )
-                    idx = tuple(
-                        expand_right(torch.as_tensor(_idx), array.shape).numpy()
-                        if _idx.dtype == np.dtype("bool")
-                        else _idx
-                        for _idx in idx
-                    )
-                    array[idx] = torch.as_tensor(value)
-                else:
-                    raise err
-
-        else:
-            key = self._process_key(key)
-            try:
-                self.file.create_dataset(key, data=value, **self.kwargs)
-            except (ValueError, OSError) as err:
-                if "name already exists" in str(err):
-                    warnings.warn(
-                        "Replacing an array with another one is inefficient. "
-                        "Consider using different names or populating in-place using `inplace=True`."
-                    )
-                    del self.file[key]
-                    self.file.create_dataset(key, data=value, **self.kwargs)
-        return self
-
-    def _convert_inplace(self, inplace, key):
-        key = self._process_key(key)
-        if inplace is not False:
-            has_key = key in self.file
-            if inplace is True and not has_key:  # inplace could be None
-                raise KeyError(
-                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
-                )
-            inplace = has_key
-        return inplace
-
-    def _set_non_tensor(self, key: NestedKey, value: Any):
-        raise NotImplementedError(
-            f"set_non_tensor is not compatible with the tensordict type {type(self)}."
-        )
-
-    def _set_str(
-        self,
-        key: str,
-        value: Any,
-        *,
-        inplace: bool,
-        validated: bool,
-        ignore_lock: bool = False,
-        non_blocking: bool = False,
-    ):
-        inplace = self._convert_inplace(inplace, key)
-        return self._set(
-            key,
-            value,
-            inplace=inplace,
-            validated=validated,
-            ignore_lock=ignore_lock,
-            non_blocking=non_blocking,
-        )
-
-    def _set_tuple(self, key, value, *, inplace, validated, non_blocking):
-        if len(key) == 1:
-            return self._set_str(
-                key[0],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        elif key[0] in self.keys():
-            return self._get_str(key[0])._set_tuple(
-                key[1:],
-                value,
-                inplace=inplace,
-                validated=validated,
-                non_blocking=non_blocking,
-            )
-        inplace = self._convert_inplace(inplace, key)
-        return self._set(
-            key, value, inplace=inplace, validated=validated, non_blocking=non_blocking
-        )
-
-    def _set_at_str(self, key, value, idx, *, validated, non_blocking):
-        return self._set(
-            key,
-            value,
-            inplace=True,
-            idx=idx,
-            validated=validated,
-            non_blocking=non_blocking,
-        )
-
-    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking):
-        return self._set(
-            key,
-            value,
-            inplace=True,
-            idx=idx,
-            validated=validated,
-            non_blocking=non_blocking,
-        )
-
-    def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
-        for key, td in orig_metadata_container._nested_tensordicts.items():
-            array = self._get_array(key)
-            self._nested_tensordicts[key] = PersistentTensorDict(
-                group=array,
-                batch_size=td.batch_size,
-                device=td.device,
-            )
-            self._nested_tensordicts[key].names = td._td_dim_names
-            self._nested_tensordicts[key]._set_metadata(td)
-
-    def _clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
-        import h5py
-
-        if recurse:
-            # this should clone the h5 to a new location indicated by newfile
-            if newfile is None:
-                warnings.warn(
-                    "A destination should be provided when cloning a "
-                    "PersistentTensorDict. A temporary file will be used "
-                    "instead. Use `recurse=False` to keep track of the original data "
-                    "with a new PersistentTensorDict instance."
-                )
-                tmpfile = tempfile.NamedTemporaryFile()
-                newfile = tmpfile.name
-            f_dest = h5py.File(newfile, "w", locking=self.LOCKING)
-            f_src = self.file
-            for key in self.keys(include_nested=True, leaves_only=True):
-                key = self._process_key(key)
-                f_dest.create_dataset(key, data=f_src[key], **self.kwargs)
-                # f_src.copy(f_src[key],  f_dest[key], "DataSet")
-            # create a non-recursive copy and update the file
-            # this way, we can keep the batch-size of every nested tensordict
-            clone = self.clone(False)
-            clone.file = f_dest
-            clone.filename = newfile
-            clone._pin_mem = False
-            clone.names = self._td_dim_names
-            clone._nested_tensordicts = {}
-            clone._set_metadata(self)
-            return clone
-        else:
-            # we need to keep the batch-size of nested tds, which we do manually
-            nested_tds = {
-                key: td.clone(False) for key, td in self._nested_tensordicts.items()
-            }
-            filename = self.filename
-            file = self.file if filename is None else None
-            clone = PersistentTensorDict(
-                filename=filename,
-                group=file,
-                mode=self.mode,
-                backend="h5",
-                device=self.device,
-                batch_size=self.batch_size,
-            )
-            clone._nested_tensordicts = nested_tds
-            clone._pin_mem = False
-            clone.names = self._td_dim_names
-            return clone
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-        filename = state["file"].file.filename
-        group_name = state["file"].name
-        state["file"] = None
-        state["filename"] = filename
-        state["group_name"] = group_name
-        state["__lock_parents_weakrefs"] = None
-        return state
-
-    def __setstate__(self, state):
-        import h5py
-
-        state["file"] = h5py.File(
-            state["filename"], mode=state["mode"], locking=self.LOCKING
-        )
-        if state["group_name"] != "/":
-            state["file"] = state["file"][state["group_name"]]
-        del state["group_name"]
-        self.__dict__.update(state)
-        if self._is_locked:
-            # this can cause avoidable overhead, as we will be locking the leaves
-            # then locking their parent, and the parent of the parent, every
-            # time re-locking tensordicts that have already been locked.
-            # To avoid this, we should lock only at the root, but it isn't easy
-            # to spot what the root is...
-            self._is_locked = False
-            self.lock_()
-
-    def _add_batch_dim(self, *, in_dim, vmap_level):
-        raise RuntimeError("Persistent tensordicts cannot be used with vmap.")
-
-    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
-        # not accessible
-        ...
-
-    def _view(self, *args, **kwargs):
-        raise RuntimeError(
-            "Cannot call `view` on a persistent tensordict. Call `reshape` instead."
-        )
-
-    def _transpose(self, dim0, dim1):
-        raise RuntimeError(
-            "Cannot call `transpose` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _permute(
-        self,
-        *args,
-        **kwargs,
-    ):
-        raise RuntimeError(
-            "Cannot call `permute` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _squeeze(self, dim=None):
-        raise RuntimeError(
-            "Cannot call `squeeze` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    def _unsqueeze(self, dim):
-        raise RuntimeError(
-            "Cannot call `unsqueeze` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
-        )
-
-    __eq__ = TensorDict.__eq__
-    __ne__ = TensorDict.__ne__
-    __xor__ = TensorDict.__xor__
-    __or__ = TensorDict.__or__
-    __ge__ = TensorDict.__ge__
-    __gt__ = TensorDict.__gt__
-    __le__ = TensorDict.__le__
-    __lt__ = TensorDict.__lt__
-
-    _cast_reduction = TensorDict._cast_reduction
-    _apply_nest = TensorDict._apply_nest
-    _check_device = TensorDict._check_device
-    _check_is_shared = TensorDict._check_is_shared
-    _convert_to_tensordict = TensorDict._convert_to_tensordict
-    _index_tensordict = TensorDict._index_tensordict
-    all = TensorDict.all
-    any = TensorDict.any
-    expand = TensorDict.expand
-    masked_select = TensorDict.masked_select
-    reshape = TensorDict.reshape
-    split = TensorDict.split
-    _to_module = TensorDict._to_module
-    _unbind = TensorDict._unbind
-    _get_names_idx = TensorDict._get_names_idx
-    from_dict_instance = TensorDict.from_dict_instance
-
-
-def _set_max_batch_size(source: PersistentTensorDict):
-    """Updates a tensordict with its maximium batch size."""
-    tensor_data = list(source._items_metadata())
-    for key, val in tensor_data:
-        if not val.get("non_tensor") and not val.get("array"):
-            _set_max_batch_size(source.get(key))
-
-    batch_size = []
-    if not tensor_data:  # when source is empty
-        source.batch_size = batch_size
-        return
-
-    curr_dim = 0
-    # We need to reload this list because the value have changed
-    tensor_data = list(source._items_metadata())
-    tensor_keys, tensor_data = zip(*tensor_data)
-    # Filter out the non-tensor data
-    tensor_data = [data for data in tensor_data if not data.get("non_tensor")]
-    while True:
-        if tensor_data[0]["dim"] > curr_dim:
-            curr_dim_size = tensor_data[0]["shape"][curr_dim]
-        else:
-            source.batch_size = batch_size
-            return
-        for tensor in tensor_data[1:]:
-            if tensor["dim"] <= curr_dim or tensor["shape"][curr_dim] != curr_dim_size:
-                source.batch_size = batch_size
-                return
-        batch_size.append(curr_dim_size)
-        curr_dim += 1
-
-
-def _is_non_tensor_h5(val):
-    import h5py
-
-    dt = val.dtype
-    if (
-        h5py.check_string_dtype(dt)
-        or h5py.check_vlen_dtype(dt)
-        or h5py.check_enum_dtype(dt)
-        or h5py.check_opaque_dtype(dt)
-    ):
-        return True
-    return False
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+"""Persistent tensordicts (H5 and others)."""
+from __future__ import annotations
+
+import importlib
+
+import json
+import os
+
+import tempfile
+import warnings
+from functools import wraps
+from pathlib import Path
+from typing import Any, Callable, Tuple, Type
+
+import numpy as np
+import torch
+
+from tensordict._td import (
+    _TensorDictKeysView,
+    _unravel_key_to_tuple,
+    CompatibleType,
+    NO_DEFAULT,
+    TensorDict,
+)
+from tensordict.base import (
+    _default_is_leaf,
+    _is_leaf_nontensor,
+    is_tensor_collection,
+    T,
+    TensorDictBase,
+)
+from tensordict.memmap import MemoryMappedTensor
+from tensordict.utils import (
+    _CloudpickleWrapper,
+    _KEY_ERROR,
+    _LOCK_ERROR,
+    _parse_to,
+    _proc_init,
+    _split_tensordict,
+    cache,
+    expand_right,
+    IndexType,
+    is_non_tensor,
+    lock_blocked,
+    NestedKey,
+    NUMPY_TO_TORCH_DTYPE_DICT,
+    unravel_key,
+)
+from torch import multiprocessing as mp
+
+_has_h5 = importlib.util.find_spec("h5py", None) is not None
+
+
+class _Visitor:
+    def __init__(self, fun=None):
+        self.elts = []
+        self.fun = fun
+
+    def __call__(self, name):
+        self.elts.append(name)
+
+    def __iter__(self):
+        if self.fun is None:
+            yield from self.elts
+        else:
+            for elt in self.elts:
+                yield self.fun(elt)
+
+
+class _PersistentTDKeysView(_TensorDictKeysView):
+    def __iter__(self):
+        # For consistency with tensordict where currently a non-tensor is stored in a
+        # tensorclass and hence can be seen as a nested tensordict
+        # that situation should be clarified
+        read_non_tensor = self.is_leaf is _is_leaf_nontensor or not self.leaves_only
+        if self.include_nested:
+            visitor = _Visitor(lambda key: unravel_key(tuple(key.split("/"))))
+            self.tensordict.file.visit(visitor)
+        else:
+            visitor = self.tensordict.file.keys()
+        for key in visitor:
+            metadata = self.tensordict._get_metadata(key)
+            if metadata.get("non_tensor"):
+                if read_non_tensor:
+                    yield key
+                else:
+                    continue
+            elif metadata.get("array"):
+                yield key
+            elif not self.leaves_only and (
+                not isinstance(key, tuple) or self.include_nested
+            ):
+                yield key
+
+    def __contains__(self, key):
+        key = unravel_key(key)
+        return key in list(self)
+
+
+class PersistentTensorDict(TensorDictBase):
+    """Persistent TensorDict implementation.
+
+    :class:`PersistentTensorDict` instances provide an interface with data stored
+    on disk such that access to this data is made easy while still taking advantage
+    from the fast access provided by the backend.
+
+    Like other :class:`TensorDictBase` subclasses, :class:`PersistentTensorDict`
+    has a ``device`` attribute. This does *not* mean that the data is being stored
+    on that device, but rather that when loaded, the data will be cast onto
+    the desired device.
+
+    Args:
+        batch_size (torch.Size or compatible): the tensordict batch size.
+        filename (str, optional): the path to the h5 file. Exclusive with ``group``.
+        group (h5py.Group, optional): a file or a group that contains data. Exclusive with ``filename``.
+        mode (str, optional): Reading mode. Defaults to ``"r"``.
+        backend (str, optional): storage backend. Currently only ``"h5"`` is supported.
+        device (torch.device or compatible, optional): device of the tensordict.
+            Defaults to ``None`` (ie. default PyTorch device).
+        **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+    .. note::
+      Currently, PersistentTensorDict instances are not closed when getting out-of-scope.
+      This means that it is the responsibility of the user to close them if necessary.
+
+    Examples:
+        >>> import tempfile
+        >>> with tempfile.NamedTemporaryFile() as f:
+        ...     data = PersistentTensorDict(file=f, batch_size=[3], mode="w")
+        ...     data["a", "b"] = torch.randn(3, 4)
+        ...     print(data)
+
+    """
+
+    _td_dim_names = None
+    LOCKING = None
+
+    def __init__(
+        self,
+        *,
+        batch_size,
+        filename=None,
+        group=None,
+        mode="r",
+        backend="h5",
+        device=None,
+        **kwargs,
+    ):
+        self._locked_tensordicts = []
+        self._lock_id = set()
+        if not _has_h5:
+            raise ModuleNotFoundError("Could not load h5py.")
+        import h5py
+
+        super().__init__()
+        self.filename = filename
+        self.mode = mode
+        if backend != "h5":
+            raise NotImplementedError
+        if filename is not None and group is None:
+            self.file = h5py.File(filename, mode, locking=self.LOCKING)
+        elif group is not None:
+            self.file = group
+        else:
+            raise RuntimeError(
+                f"Either group or filename must be provided, and not both. Got group={group} and filename={filename}."
+            )
+        self._batch_size = torch.Size(batch_size)
+        self._device = torch.device(device) if device is not None else None
+        self._is_shared = False
+        self._is_memmap = False
+        self.kwargs = kwargs
+
+        # we use this to allow nested tensordicts to have a different batch-size
+        self._nested_tensordicts = {}
+        self._pin_mem = False
+
+        # this must be kept last
+        self._check_batch_size(self._batch_size)
+
+    @classmethod
+    def from_h5(cls, filename, mode="r"):
+        """Creates a PersistentTensorDict from a h5 file.
+
+        This function will automatically determine the batch-size for each nested
+        tensordict.
+
+        Args:
+            filename (str): the path to the h5 file.
+            mode (str, optional): reading mode. Defaults to ``"r"``.
+        """
+        out = cls(filename=filename, mode=mode, batch_size=[])
+        # determine batch size
+        _set_max_batch_size(out)
+        return out
+
+    @classmethod
+    def from_dict(cls, input_dict, filename, batch_size=None, device=None, **kwargs):
+        """Converts a dictionary or a TensorDict to a h5 file.
+
+        Args:
+            input_dict (dict, TensorDict or compatible): data to be stored as h5.
+            filename (str or path): path to the h5 file.
+            batch_size (tensordict batch-size, optional): if provided, batch size
+                of the tensordict. If not, the batch size will be gathered from the
+                input structure (if present) or determined automatically.
+            device (torch.device or compatible, optional): the device where to
+                expect the tensor once they are returned. Defaults to ``None``
+                (on cpu by default).
+            **kwargs: kwargs to be passed to :meth:`h5py.File.create_dataset`.
+
+        Returns:
+            A :class:`PersitentTensorDict` instance linked to the newly created file.
+
+        """
+        import h5py
+
+        file = h5py.File(filename, "w", locking=cls.LOCKING)
+        _has_batch_size = True
+        if batch_size is None:
+            if is_tensor_collection(input_dict):
+                batch_size = input_dict.batch_size
+            else:
+                _has_batch_size = False
+                batch_size = torch.Size([])
+
+        # let's make a tensordict first
+        out = cls(group=file, batch_size=batch_size, device=device, **kwargs)
+        if is_tensor_collection(input_dict):
+            out.update(input_dict)
+        else:
+            out.update(TensorDict(input_dict, batch_size=batch_size))
+        if not _has_batch_size:
+            _set_max_batch_size(out)
+        return out
+
+    def close(self):
+        """Closes the persistent tensordict."""
+        self.file.close()
+
+    def _process_key(self, key):
+        key = _unravel_key_to_tuple(key)
+        return "/".join(key)
+
+    def _check_batch_size(self, batch_size) -> None:
+        for key in self.keys(include_nested=True, leaves_only=True):
+            key = self._process_key(key)
+            array = self.file[key]
+            if _is_non_tensor_h5(array):
+                continue
+            size = array.shape
+            if torch.Size(size[: len(batch_size)]) != batch_size:
+                raise ValueError(
+                    f"batch size and array size mismatch: array.shape={size}, batch_size={batch_size}."
+                )
+
+    def _get_array(self, key, default=NO_DEFAULT):
+        try:
+            key = self._process_key(key)
+            array = self.file[key]
+            return array
+        except KeyError:
+            if default is not NO_DEFAULT:
+                return default
+            raise KeyError(f"key {key} not found in PersistentTensorDict {self}")
+
+    def _process_array(self, key, array):
+        import h5py
+
+        if isinstance(array, (h5py.Dataset,)):
+            if self.device is not None:
+                device = self.device
+            else:
+                device = torch.device("cpu")
+            # we convert to an array first to avoid "Creating a tensor from a list of numpy.ndarrays is extremely slow."
+            if not _is_non_tensor_h5(array):
+                array = array[()]
+                out = torch.as_tensor(array, device=device)
+                if self._pin_mem:
+                    out = out.pin_memory()
+            else:
+                from tensordict.tensorclass import NonTensorData
+
+                array = array[()]
+                out = NonTensorData(
+                    data=array, device=device, batch_size=self.batch_size
+                )
+            return out
+        else:
+            out = self._nested_tensordicts.get(key, None)
+            if out is None:
+                out = self._nested_tensordicts[key] = PersistentTensorDict(
+                    group=array,
+                    batch_size=self.batch_size,
+                    device=self.device,
+                )
+            return out
+
+    @cache  # noqa: B019
+    def get(self, key, default=NO_DEFAULT):
+        array = self._get_array(key, default)
+        if array is default:
+            return array
+        return self._process_array(key, array)
+
+    _get_str = get
+    _get_tuple = get
+
+    def get_at(
+        self, key: NestedKey, idx: IndexType, default: CompatibleType = NO_DEFAULT
+    ) -> CompatibleType:
+        import h5py
+
+        array = self._get_array(key, default)
+        if isinstance(array, (h5py.Dataset,)):
+            if self.device is not None:
+                device = self.device
+            else:
+                device = torch.device("cpu")
+            # indexing must be done before converting to tensor.
+            idx = self._process_index(idx, array)
+            # `get_at` is there to save us.
+            try:
+                out = torch.as_tensor(array[idx], device=device)
+            except TypeError as err:
+                if "Boolean indexing array has incompatible shape" in str(err):
+                    # Known bug in h5py: cannot broadcast boolean mask on the right as
+                    # done in np and torch. Therefore we put a performance warning
+                    # and convert to torch tensor first.
+                    warnings.warn(
+                        "Indexing an h5py.Dataset object with a boolean mask "
+                        "that needs broadcasting does not work directly. "
+                        "tensordict will cast the entire array in memory and index it using the mask. "
+                        "This is suboptimal and may lead to performance issue."
+                    )
+                    out = torch.as_tensor(np.asarray(array), device=device)[idx]
+                else:
+                    raise err
+            if self._pin_mem:
+                return out.pin_memory()
+            return out
+        elif array is not default:
+            out = self._nested_tensordicts.get(key, None)
+            if out is None:
+                out = self._nested_tensordicts[key] = PersistentTensorDict(
+                    group=array,
+                    batch_size=self.batch_size,
+                    device=self.device,
+                )
+            return out._get_sub_tensordict(idx)
+        else:
+            return default
+
+    def _get_metadata(self, key):
+        """Gets the metadata for an entry.
+
+        This method avoids creating a tensor from scratch, and just reads the metadata of the array.
+        """
+        import h5py
+
+        array = self._get_array(key)
+        if (
+            isinstance(array, (h5py.Dataset,))
+            and array.dtype in NUMPY_TO_TORCH_DTYPE_DICT
+        ):
+            shape = torch.Size(array.shape)
+            return {
+                "dtype": NUMPY_TO_TORCH_DTYPE_DICT[array.dtype],
+                "shape": shape,
+                "dim": len(shape),
+                "array": True,
+            }
+        elif (
+            isinstance(array, (h5py.Dataset,))
+            and array.dtype not in NUMPY_TO_TORCH_DTYPE_DICT
+        ):
+            return {"non_tensor": True}
+        else:
+            val = self.get(key)
+            shape = val.shape
+            return {
+                "dtype": None,
+                "shape": shape,
+                "dim": len(shape),
+                "array": False,
+            }
+
+    @classmethod
+    def _process_index(cls, idx, array=None):
+        if isinstance(idx, tuple):
+            return tuple(cls._process_index(_idx, array) for _idx in idx)
+        if isinstance(idx, torch.Tensor):
+            return idx.cpu().detach().numpy()
+        if isinstance(idx, (range, list)):
+            return np.asarray(idx)
+        return idx
+
+    def __getitem__(self, item):
+        if isinstance(item, str) or (
+            isinstance(item, tuple) and _unravel_key_to_tuple(item)
+        ):
+            result = self.get(item)
+            if is_non_tensor(result):
+                result_data = getattr(result, "data", NO_DEFAULT)
+                if result_data is NO_DEFAULT:
+                    return result.tolist()
+                return result_data
+            return result
+        if isinstance(item, list):
+            # convert to tensor
+            item = torch.tensor(item)
+        return self._get_sub_tensordict(item)
+
+    __getitems__ = __getitem__
+
+    def __setitem__(self, index, value):
+        index_unravel = _unravel_key_to_tuple(index)
+        if index_unravel:
+            return self.set(index_unravel, value, inplace=True)
+
+        if isinstance(index, list):
+            # convert to tensor
+            index = torch.tensor(index)
+        sub_td = self._get_sub_tensordict(index)
+        err_set_batch_size = None
+        if not isinstance(value, TensorDictBase):
+            value = TensorDict.from_dict(value, batch_size=[])
+            # try to assign the current shape. If that does not work, we can
+            # try to expand
+            try:
+                value.batch_size = sub_td.batch_size
+            except RuntimeError as err0:
+                err_set_batch_size = err0
+        if value.shape != sub_td.shape:
+            try:
+                value = value.expand(sub_td.shape)
+            except RuntimeError as err:
+                if err_set_batch_size is not None:
+                    raise err from err_set_batch_size
+                raise RuntimeError(
+                    f"Cannot broadcast the tensordict {value} to the shape of the indexed persistent tensordict {self}[{index}]."
+                ) from err
+        sub_td.update(value, inplace=True)
+
+    @cache  # noqa: B019
+    def _valid_keys(self):
+        keys = []
+        for key in self.file.keys():
+            metadata = self._get_metadata(key)
+            if not metadata.get("non_tensor"):
+                keys.append(key)
+        return keys
+
+    # @cache  # noqa: B019
+    def keys(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> _PersistentTDKeysView:
+        if is_leaf not in (None, _default_is_leaf, _is_leaf_nontensor):
+            raise ValueError(
+                f"is_leaf {is_leaf} is not supported within tensordicts of type {type(self)}."
+            )
+        return _PersistentTDKeysView(
+            tensordict=self,
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+            is_leaf=is_leaf,
+        )
+
+    def _items_metadata(self, include_nested=False, leaves_only=False):
+        """Iterates over the metadata of the PersistentTensorDict."""
+        for key in self.keys(include_nested, leaves_only):
+            yield (key, self._get_metadata(key))
+
+    def _values_metadata(self, include_nested=False, leaves_only=False):
+        """Iterates over the metadata of the PersistentTensorDict."""
+        for key in self.keys(include_nested, leaves_only):
+            yield self._get_metadata(key)
+
+    def _change_batch_size(self, value):
+        raise NotImplementedError
+
+    def _stack_onto_(
+        self, list_item: list[CompatibleType], dim: int
+    ) -> PersistentTensorDict:
+        for key in self.keys():
+            vals = [td._get_str(key, None) for td in list_item]
+            if all(v is None for v in vals):
+                continue
+            stacked = torch.stack(vals, dim=dim)
+            self.set_(key, stacked)
+        return self
+
+    @property
+    def batch_size(self):
+        return self._batch_size
+
+    @batch_size.setter
+    def batch_size(self, value):
+        _batch_size = self._batch_size
+        try:
+            self._batch_size = torch.Size(value)
+            self._check_batch_size(self._batch_size)
+        except ValueError:
+            self._batch_size = _batch_size
+
+    _erase_names = TensorDict._erase_names
+    names = TensorDict.names
+    _has_names = TensorDict._has_names
+
+    def _rename_subtds(self, names):
+        if names is None:
+            names = [None] * self.ndim
+        for item in self._nested_tensordicts.values():
+            if is_tensor_collection(item):
+                td_names = list(names) + [None] * (item.ndim - self.ndim)
+                item.rename_(*td_names)
+
+    def contiguous(self):
+        """Materializes a PersistentTensorDict on a regular TensorDict."""
+        return self.to_tensordict()
+
+    @lock_blocked
+    def del_(self, key):
+        key = self._process_key(key)
+        del self.file[key]
+        return self
+
+    def detach_(self):
+        # PersistentTensorDict do not carry gradients. This is a no-op
+        return self
+
+    @property
+    def device(self):
+        return self._device
+
+    def empty(
+        self, recurse=False, *, batch_size=None, device=NO_DEFAULT, names=None
+    ) -> T:
+        if recurse:
+            out = self.empty(
+                recurse=False, batch_size=batch_size, device=device, names=names
+            )
+            for key, val in self.items():
+                if is_tensor_collection(val):
+                    out._set_str(
+                        key,
+                        val.empty(
+                            recurse=True,
+                            batch_size=batch_size,
+                            device=device,
+                            names=names,
+                        ),
+                        inplace=False,
+                        validated=True,
+                        non_blocking=False,
+                    )
+            return out
+        return TensorDict(
+            {},
+            device=self.device if device is NO_DEFAULT else device,
+            batch_size=self.batch_size if batch_size is None else batch_size,
+            names=self.names if names is None and self._has_names() else names,
+        )
+
+    def zero_(self) -> T:
+        for key in self.keys():
+            self.fill_(key, 0)
+        return self
+
+    def entry_class(self, key: NestedKey) -> type:
+        entry_class = self._get_metadata(key)
+        is_array = entry_class.get("array", None)
+        if is_array:
+            return torch.Tensor
+        elif is_array is False:
+            return PersistentTensorDict
+        else:
+            raise RuntimeError(f"Encountered a non-numeric data {key}.")
+
+    def is_contiguous(self):
+        return False
+
+    def masked_fill(self, mask, value):
+        return self.to_tensordict().masked_fill(mask, value)
+
+    def where(self, condition, other, *, out=None, pad=None):
+        return self.to_tensordict().where(
+            condition=condition, other=other, out=out, pad=pad
+        )
+
+    def masked_fill_(self, mask, value):
+        for key in self.keys(include_nested=True, leaves_only=True):
+            array = self._get_array(key)
+            array[expand_right(mask, array.shape).cpu().numpy()] = value
+        return self
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't allowed for persistent tensordicts."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def memmap_(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        num_threads: int = 0,
+    ) -> PersistentTensorDict:
+        raise RuntimeError(
+            "Cannot build a memmap TensorDict in-place from a PersistentTensorDict. Use `td.memmap()` instead."
+        )
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None,
+        copy_existing: bool,
+        executor,
+        futures,
+        inplace,
+        like,
+        share_non_tensor,
+    ) -> T:
+        if inplace:
+            raise RuntimeError("Cannot call memmap inplace in a persistent tensordict.")
+
+        # re-implements this to make it faster using the meta-data
+        def save_metadata(data: TensorDictBase, filepath, metadata=None):
+            if metadata is None:
+                metadata = {}
+            metadata.update(
+                {
+                    "shape": list(data.shape),
+                    "device": str(data.device),
+                    "_type": str(self.__class__),
+                }
+            )
+            with open(filepath, "w") as json_metadata:
+                json.dump(metadata, json_metadata)
+
+        if prefix is not None:
+            prefix = Path(prefix)
+            if not prefix.exists():
+                os.makedirs(prefix, exist_ok=True)
+            metadata = {}
+        if not self.keys():
+            raise Exception(
+                "memmap_like() must be called when the TensorDict is (partially) "
+                "populated. Set a tensor first."
+            )
+        dest = TensorDict(
+            {},
+            batch_size=self.batch_size,
+            names=self.names if self._has_names() else None,
+            device=torch.device("cpu"),
+        )
+        dest._is_memmap = True
+        for key, value in self._items_metadata():
+            if not value["array"]:
+                value = self._get_str(key)
+                dest._set_str(
+                    key,
+                    value._memmap_(
+                        prefix=prefix / key if prefix is not None else None,
+                        executor=executor,
+                        like=like,
+                        copy_existing=copy_existing,
+                        futures=futures,
+                        inplace=inplace,
+                        share_non_tensor=share_non_tensor,
+                    ),
+                    inplace=False,
+                    validated=True,
+                    non_blocking=False,
+                )
+                continue
+            else:
+                value = self._get_str(key)
+                if prefix is not None:
+                    metadata[key] = {
+                        "dtype": str(value.dtype),
+                        "shape": value.shape,
+                        "device": str(value.device),
+                    }
+
+                def _populate(
+                    tensordict=dest, key=key, value=value, prefix=prefix, like=like
+                ):
+                    val = MemoryMappedTensor.from_tensor(
+                        value,
+                        filename=str(prefix / f"{key}.memmap")
+                        if prefix is not None
+                        else None,
+                        copy_data=not like,
+                        copy_existing=copy_existing,
+                        existsok=True,
+                    )
+                    tensordict._set_str(
+                        key,
+                        val,
+                        inplace=False,
+                        validated=True,
+                        non_blocking=False,
+                    )
+
+                if executor is None:
+                    _populate()
+                else:
+                    futures.append(executor.submit(_populate))
+
+        if prefix is not None:
+            if executor is None:
+                save_metadata(dest, prefix / "meta.json", metadata)
+            else:
+                futures.append(
+                    executor.submit(save_metadata, dest, prefix / "meta.json", metadata)
+                )
+        return dest
+
+    _load_memmap = TensorDict._load_memmap
+
+    def pin_memory(self):
+        """Returns a new PersistentTensorDict where any given Tensor key returns a tensor with pin_memory=True.
+
+        This will fail with PersistentTensorDict with a ``cuda`` device attribute.
+
+        """
+        if self.device.type == "cuda":
+            raise RuntimeError("cannot pin memory on a tensordict stored on cuda.")
+        out = self.clone(False)
+        out._pin_mem = True
+        out._nested_tensordicts = {
+            key: val.pin_memory() for key, val in out._nested_tensordicts.items()
+        }
+        return out
+
+    @lock_blocked
+    def popitem(self) -> Tuple[NestedKey, CompatibleType]:
+        raise NotImplementedError(
+            f"popitem not implemented for class {type(self).__name__}."
+        )
+
+    def map(
+        self,
+        fn: Callable,
+        dim: int = 0,
+        num_workers: int = None,
+        *,
+        out: TensorDictBase = None,
+        chunksize: int = None,
+        num_chunks: int = None,
+        pool: mp.Pool = None,
+        generator: torch.Generator | None = None,
+        max_tasks_per_child: int | None = None,
+        worker_threads: int = 1,
+        index_with_generator: bool = False,
+        pbar: bool = False,
+        mp_start_method: str | None = None,
+    ):
+        if pool is None:
+            if num_workers is None:
+                num_workers = mp.cpu_count()  # Get the number of CPU cores
+            if generator is None:
+                generator = torch.Generator()
+            seed = (
+                torch.empty((), dtype=torch.int64).random_(generator=generator).item()
+            )
+            if mp_start_method is not None:
+                ctx = mp.get_context(mp_start_method)
+            else:
+                ctx = mp.get_context()
+
+            queue = ctx.Queue(maxsize=num_workers)
+            for i in range(num_workers):
+                queue.put(i)
+            with ctx.Pool(
+                processes=num_workers,
+                initializer=_proc_init,
+                initargs=(seed, queue, worker_threads),
+                maxtasksperchild=max_tasks_per_child,
+            ) as pool:
+                return self.map(fn, dim=dim, chunksize=chunksize, pool=pool)
+        num_workers = pool._processes
+        dim_orig = dim
+        if dim < 0:
+            dim = self.ndim + dim
+        if dim < 0 or dim >= self.ndim:
+            raise ValueError(f"Got incompatible dimension {dim_orig}")
+
+        self_split = _split_tensordict(
+            self,
+            chunksize,
+            num_chunks,
+            num_workers,
+            dim,
+            use_generator=index_with_generator,
+            to_tensordict=True,
+        )
+        if not index_with_generator:
+            length = len(self_split)
+            self_split = tuple(split.to_tensordict() for split in self_split)
+        else:
+            length = None
+
+        if out is not None and (out.is_shared() or out.is_memmap()):
+
+            def wrap_fn_with_out(fn, out):
+                @wraps(fn)
+                def newfn(item_and_out):
+                    item, out = item_and_out
+                    result = fn(item)
+                    out.update_(result)
+                    return
+
+                out_split = _split_tensordict(
+                    out,
+                    chunksize,
+                    num_chunks,
+                    num_workers,
+                    dim,
+                    use_generator=index_with_generator,
+                )
+                return _CloudpickleWrapper(newfn), zip(self_split, out_split)
+
+            fn, self_split = wrap_fn_with_out(fn, out)
+            out = None
+
+        call_chunksize = 1
+        imap = pool.imap(fn, self_split, call_chunksize)
+
+        if pbar and importlib.util.find_spec("tqdm", None) is not None:
+            import tqdm
+
+            imap = tqdm.tqdm(imap, total=length)
+
+        imaplist = []
+        start = 0
+        for item in imap:
+            if item is not None:
+                if out is not None:
+                    if chunksize:
+                        end = start + item.shape[dim]
+                        chunk = slice(start, end)
+                        out[chunk].update_(item)
+                        start = end
+                    else:
+                        out[start].update_(item)
+                        start += 1
+                else:
+                    imaplist.append(item)
+        del imap
+
+        # support inplace modif
+        if imaplist:
+            if chunksize == 0:
+                out = torch.stack(imaplist, dim)
+            else:
+                out = torch.cat(imaplist, dim)
+        return out
+
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> PersistentTensorDict:
+        old_key = self._process_key(old_key)
+        new_key = self._process_key(new_key)
+        try:
+            self.file.move(old_key, new_key)
+        except ValueError as err:
+            raise KeyError(f"key {new_key} already present in TensorDict.") from err
+        return self
+
+    def fill_(self, key: NestedKey, value: float | bool) -> TensorDictBase:
+        """Fills a tensor pointed by the key with the a given value.
+
+        Args:
+            key (str): key to be remaned
+            value (Number, bool): value to use for the filling
+
+        Returns:
+            self
+
+        """
+        md = self._get_metadata(key)
+        if md.get("array", None):
+            array = self._get_array(key)
+            array[:] = value
+        else:
+            nested = self.get(key)
+            for subkey in nested.keys():
+                nested.fill_(subkey, value)
+        return self
+
+    def _create_nested_str(self, key):
+        self.file.create_group(key)
+        target_td = self._get_str(key)
+        return target_td
+
+    def _select(
+        self, *keys: NestedKey, inplace: bool = False, strict: bool = True
+    ) -> PersistentTensorDict:
+        raise NotImplementedError(
+            "Cannot call select on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> PersistentTensorDict:
+        raise NotImplementedError(
+            "Cannot call exclude on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def flatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
+        if inplace:
+            raise ValueError(
+                "Cannot call flatten_keys in_place with a PersistentTensorDict."
+            )
+        return self.to_tensordict().flatten_keys(separator=separator)
+
+    def unflatten_keys(self, separator: str = ".", inplace: bool = False) -> T:
+        if inplace:
+            raise ValueError(
+                "Cannot call unflatten_keys in_place with a PersistentTensorDict."
+            )
+        return self.to_tensordict().unflatten_keys(separator=separator)
+
+    def share_memory_(self):
+        raise NotImplementedError(
+            "Cannot call share_memory_ on a PersistentTensorDict. "
+            "Create a regular tensordict first using the `to_tensordict` method."
+        )
+
+    def to(self, *args, **kwargs: Any) -> PersistentTensorDict:
+        device, dtype, non_blocking, convert_to_format, batch_size = _parse_to(
+            *args, **kwargs
+        )
+        result = self
+        if device is not None and dtype is None and device == self.device:
+            return result
+        if dtype is not None:
+            return self.to_tensordict().to(*args, **kwargs)
+        result = self
+        if device is not None:
+            result = result.clone(False)
+            result._device = device
+            for key, nested in list(result._nested_tensordicts.items()):
+                result._nested_tensordicts[key] = nested.to(device)
+        if batch_size is not None:
+            result.batch_size = batch_size
+        return result
+
+    def _to_numpy(self, value):
+        if hasattr(value, "requires_grad") and value.requires_grad:
+            raise RuntimeError("Cannot set a tensor that has requires_grad=True.")
+        if isinstance(value, torch.Tensor):
+            out = value.cpu().detach().numpy()
+        elif isinstance(value, dict):
+            out = TensorDict(value, [])
+        elif is_non_tensor(value):
+            value = value.data
+            if isinstance(value, str):
+                return value
+            import h5py
+
+            out = np.array(value)
+            out = out.astype(h5py.opaque_dtype(out.dtype))
+        elif is_tensor_collection(value):
+            out = value
+        elif isinstance(value, (np.ndarray,)):
+            out = value
+        else:
+            raise NotImplementedError(
+                f"Cannot set values of type {value} in a PersistentTensorDict."
+            )
+        return out
+
+    def _set(
+        self,
+        key: NestedKey,
+        value: Any,
+        *,
+        inplace: bool = False,
+        idx=None,
+        validated: bool = False,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ) -> PersistentTensorDict:
+        if not validated:
+            value = self._validate_value(value, check_shape=idx is None)
+        value = self._to_numpy(value)
+        if not inplace:
+            if idx is not None:
+                raise RuntimeError("Cannot pass an index to _set when inplace=False.")
+            elif self.is_locked and not ignore_lock:
+                raise RuntimeError(_LOCK_ERROR)
+        # shortcut set if we're placing a tensordict
+        if is_tensor_collection(value):
+            if isinstance(key, tuple):
+                key, subkey = key[0], key[1:]
+            else:
+                key, subkey = key, []
+            target_td = self._get_str(key, default=None)
+            if target_td is None:
+                self.file.create_group(key)
+                target_td = self._get_str(key)
+                target_td.batch_size = value.batch_size
+            elif not is_tensor_collection(target_td):
+                raise RuntimeError(
+                    f"cannot set a tensor collection in place of a non-tensor collection in {self.__class__.__name__}. "
+                    f"Got self.get({key})={target_td} and value={value}."
+                )
+            if idx is None:
+                if len(subkey):
+                    target_td.set(subkey, value, inplace=inplace)
+                else:
+                    target_td.update(value, inplace=inplace)
+            else:
+                if len(subkey):
+                    target_td.set_at_(subkey, value, idx=idx)
+                else:
+                    target_td.update_at_(value, idx=idx)
+
+            return self
+
+        if inplace:
+            # could be called before but will go under further refactoring of set
+            key = self._process_key(key)
+            array = self.file[key]
+            if idx is None:
+                idx = ()
+            else:
+                idx = self._process_index(idx, array)
+            try:
+                array[idx] = value
+            except TypeError as err:
+                if "Boolean indexing array has incompatible shape" in str(err):
+                    # Known bug in h5py: cannot broadcast boolean mask on the right as
+                    # done in np and torch. Therefore we put a performance warning
+                    # and convert to torch tensor first.
+                    warnings.warn(
+                        "Indexing an h5py.Dataset object with a boolean mask "
+                        "that needs broadcasting does not work directly. "
+                        "tensordict will cast the entire array in memory and index it using the mask. "
+                        "This is suboptimal and may lead to performance issue."
+                    )
+                    idx = tuple(
+                        expand_right(torch.as_tensor(_idx), array.shape).numpy()
+                        if _idx.dtype == np.dtype("bool")
+                        else _idx
+                        for _idx in idx
+                    )
+                    array[idx] = torch.as_tensor(value)
+                else:
+                    raise err
+
+        else:
+            key = self._process_key(key)
+            try:
+                self.file.create_dataset(key, data=value, **self.kwargs)
+            except (ValueError, OSError) as err:
+                if "name already exists" in str(err):
+                    warnings.warn(
+                        "Replacing an array with another one is inefficient. "
+                        "Consider using different names or populating in-place using `inplace=True`."
+                    )
+                    del self.file[key]
+                    self.file.create_dataset(key, data=value, **self.kwargs)
+        return self
+
+    def _convert_inplace(self, inplace, key):
+        key = self._process_key(key)
+        if inplace is not False:
+            has_key = key in self.file
+            if inplace is True and not has_key:  # inplace could be None
+                raise KeyError(
+                    _KEY_ERROR.format(key, self.__class__.__name__, sorted(self.keys()))
+                )
+            inplace = has_key
+        return inplace
+
+    def _set_non_tensor(self, key: NestedKey, value: Any):
+        raise NotImplementedError(
+            f"set_non_tensor is not compatible with the tensordict type {type(self)}."
+        )
+
+    def _set_str(
+        self,
+        key: str,
+        value: Any,
+        *,
+        inplace: bool,
+        validated: bool,
+        ignore_lock: bool = False,
+        non_blocking: bool = False,
+    ):
+        inplace = self._convert_inplace(inplace, key)
+        return self._set(
+            key,
+            value,
+            inplace=inplace,
+            validated=validated,
+            ignore_lock=ignore_lock,
+            non_blocking=non_blocking,
+        )
+
+    def _set_tuple(self, key, value, *, inplace, validated, non_blocking):
+        if len(key) == 1:
+            return self._set_str(
+                key[0],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        elif key[0] in self.keys():
+            return self._get_str(key[0])._set_tuple(
+                key[1:],
+                value,
+                inplace=inplace,
+                validated=validated,
+                non_blocking=non_blocking,
+            )
+        inplace = self._convert_inplace(inplace, key)
+        return self._set(
+            key, value, inplace=inplace, validated=validated, non_blocking=non_blocking
+        )
+
+    def _set_at_str(self, key, value, idx, *, validated, non_blocking):
+        return self._set(
+            key,
+            value,
+            inplace=True,
+            idx=idx,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+
+    def _set_at_tuple(self, key, value, idx, *, validated, non_blocking):
+        return self._set(
+            key,
+            value,
+            inplace=True,
+            idx=idx,
+            validated=validated,
+            non_blocking=non_blocking,
+        )
+
+    def _set_metadata(self, orig_metadata_container: PersistentTensorDict):
+        for key, td in orig_metadata_container._nested_tensordicts.items():
+            array = self._get_array(key)
+            self._nested_tensordicts[key] = PersistentTensorDict(
+                group=array,
+                batch_size=td.batch_size,
+                device=td.device,
+            )
+            self._nested_tensordicts[key].names = td._td_dim_names
+            self._nested_tensordicts[key]._set_metadata(td)
+
+    def _clone(self, recurse: bool = True, newfile=None) -> PersistentTensorDict:
+        import h5py
+
+        if recurse:
+            # this should clone the h5 to a new location indicated by newfile
+            if newfile is None:
+                warnings.warn(
+                    "A destination should be provided when cloning a "
+                    "PersistentTensorDict. A temporary file will be used "
+                    "instead. Use `recurse=False` to keep track of the original data "
+                    "with a new PersistentTensorDict instance."
+                )
+                tmpfile = tempfile.NamedTemporaryFile()
+                newfile = tmpfile.name
+            f_dest = h5py.File(newfile, "w", locking=self.LOCKING)
+            f_src = self.file
+            for key in self.keys(include_nested=True, leaves_only=True):
+                key = self._process_key(key)
+                f_dest.create_dataset(key, data=f_src[key], **self.kwargs)
+                # f_src.copy(f_src[key],  f_dest[key], "DataSet")
+            # create a non-recursive copy and update the file
+            # this way, we can keep the batch-size of every nested tensordict
+            clone = self.clone(False)
+            clone.file = f_dest
+            clone.filename = newfile
+            clone._pin_mem = False
+            clone.names = self._td_dim_names
+            clone._nested_tensordicts = {}
+            clone._set_metadata(self)
+            return clone
+        else:
+            # we need to keep the batch-size of nested tds, which we do manually
+            nested_tds = {
+                key: td.clone(False) for key, td in self._nested_tensordicts.items()
+            }
+            filename = self.filename
+            file = self.file if filename is None else None
+            clone = PersistentTensorDict(
+                filename=filename,
+                group=file,
+                mode=self.mode,
+                backend="h5",
+                device=self.device,
+                batch_size=self.batch_size,
+            )
+            clone._nested_tensordicts = nested_tds
+            clone._pin_mem = False
+            clone.names = self._td_dim_names
+            return clone
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        filename = state["file"].file.filename
+        group_name = state["file"].name
+        state["file"] = None
+        state["filename"] = filename
+        state["group_name"] = group_name
+        state["__lock_parents_weakrefs"] = None
+        return state
+
+    def __setstate__(self, state):
+        import h5py
+
+        state["file"] = h5py.File(
+            state["filename"], mode=state["mode"], locking=self.LOCKING
+        )
+        if state["group_name"] != "/":
+            state["file"] = state["file"][state["group_name"]]
+        del state["group_name"]
+        self.__dict__.update(state)
+        if self._is_locked:
+            # this can cause avoidable overhead, as we will be locking the leaves
+            # then locking their parent, and the parent of the parent, every
+            # time re-locking tensordicts that have already been locked.
+            # To avoid this, we should lock only at the root, but it isn't easy
+            # to spot what the root is...
+            self._is_locked = False
+            self.lock_()
+
+    def _add_batch_dim(self, *, in_dim, vmap_level):
+        raise RuntimeError("Persistent tensordicts cannot be used with vmap.")
+
+    def _remove_batch_dim(self, vmap_level, batch_size, out_dim):
+        # not accessible
+        ...
+
+    def _view(self, *args, **kwargs):
+        raise RuntimeError(
+            "Cannot call `view` on a persistent tensordict. Call `reshape` instead."
+        )
+
+    def _transpose(self, dim0, dim1):
+        raise RuntimeError(
+            "Cannot call `transpose` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _permute(
+        self,
+        *args,
+        **kwargs,
+    ):
+        raise RuntimeError(
+            "Cannot call `permute` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _squeeze(self, dim=None):
+        raise RuntimeError(
+            "Cannot call `squeeze` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    def _unsqueeze(self, dim):
+        raise RuntimeError(
+            "Cannot call `unsqueeze` on a persistent tensordict. Make it dense before calling this method by calling `to_tensordict`."
+        )
+
+    __eq__ = TensorDict.__eq__
+    __ne__ = TensorDict.__ne__
+    __xor__ = TensorDict.__xor__
+    __or__ = TensorDict.__or__
+    __ge__ = TensorDict.__ge__
+    __gt__ = TensorDict.__gt__
+    __le__ = TensorDict.__le__
+    __lt__ = TensorDict.__lt__
+
+    _cast_reduction = TensorDict._cast_reduction
+    _apply_nest = TensorDict._apply_nest
+    _check_device = TensorDict._check_device
+    _check_is_shared = TensorDict._check_is_shared
+    _convert_to_tensordict = TensorDict._convert_to_tensordict
+    _index_tensordict = TensorDict._index_tensordict
+    all = TensorDict.all
+    any = TensorDict.any
+    expand = TensorDict.expand
+    masked_select = TensorDict.masked_select
+    reshape = TensorDict.reshape
+    split = TensorDict.split
+    _to_module = TensorDict._to_module
+    _unbind = TensorDict._unbind
+    _get_names_idx = TensorDict._get_names_idx
+    from_dict_instance = TensorDict.from_dict_instance
+
+
+def _set_max_batch_size(source: PersistentTensorDict):
+    """Updates a tensordict with its maximium batch size."""
+    tensor_data = list(source._items_metadata())
+    for key, val in tensor_data:
+        if not val.get("non_tensor") and not val.get("array"):
+            _set_max_batch_size(source.get(key))
+
+    batch_size = []
+    if not tensor_data:  # when source is empty
+        source.batch_size = batch_size
+        return
+
+    curr_dim = 0
+    # We need to reload this list because the value have changed
+    tensor_data = list(source._items_metadata())
+    tensor_keys, tensor_data = zip(*tensor_data)
+    # Filter out the non-tensor data
+    tensor_data = [data for data in tensor_data if not data.get("non_tensor")]
+    while True:
+        if tensor_data[0]["dim"] > curr_dim:
+            curr_dim_size = tensor_data[0]["shape"][curr_dim]
+        else:
+            source.batch_size = batch_size
+            return
+        for tensor in tensor_data[1:]:
+            if tensor["dim"] <= curr_dim or tensor["shape"][curr_dim] != curr_dim_size:
+                source.batch_size = batch_size
+                return
+        batch_size.append(curr_dim_size)
+        curr_dim += 1
+
+
+def _is_non_tensor_h5(val):
+    import h5py
+
+    dt = val.dtype
+    if (
+        h5py.check_string_dtype(dt)
+        or h5py.check_vlen_dtype(dt)
+        or h5py.check_enum_dtype(dt)
+        or h5py.check_opaque_dtype(dt)
+    ):
+        return True
+    return False
```

## tensordict/tensorclass.py

 * *Ordering differences only*

```diff
@@ -1,2855 +1,2855 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import ctypes
-
-import dataclasses
-import functools
-import inspect
-import json
-import multiprocessing.managers
-import multiprocessing.sharedctypes
-import numbers
-import os
-import pickle
-import shutil
-
-import sys
-import warnings
-from copy import copy, deepcopy
-from dataclasses import dataclass
-from pathlib import Path
-from textwrap import indent
-from typing import Any, Callable, get_type_hints, List, Sequence, TypeVar
-
-import numpy as np
-import tensordict as tensordict_lib
-
-import torch
-from tensordict import LazyStackedTensorDict
-from tensordict._pytree import _register_td_node
-from tensordict._td import is_tensor_collection, NO_DEFAULT, TensorDict, TensorDictBase
-from tensordict._tensordict import _unravel_key_to_tuple
-from tensordict._torch_func import TD_HANDLED_FUNCTIONS
-from tensordict.base import (
-    _ACCEPTED_CLASSES,
-    _is_tensor_collection,
-    _register_tensor_class,
-    CompatibleType,
-)
-from tensordict.utils import (
-    _get_repr,
-    _is_json_serializable,
-    _LOCK_ERROR,
-    DeviceType,
-    IndexType,
-    is_non_tensor,
-    is_tensorclass,
-    KeyDependentDefaultDict,
-    NestedKey,
-)
-from torch import multiprocessing as mp, Tensor
-from torch.multiprocessing import Manager
-from torch.utils._pytree import tree_map
-
-T = TypeVar("T", bound=TensorDictBase)
-# We use an abstract AnyType instead of Any because Any isn't recognised as a type for python < 3.10
-major, minor = sys.version_info[:2]
-if (major, minor) < (3, 11):
-
-    class _AnyType:
-        def __subclasscheck__(self, subclass):
-            return False
-
-else:
-    _AnyType = Any
-
-# methods where non_tensordict data should be cleared in the return value
-_CLEAR_METADATA = {"all", "any"}
-# torch functions where we can wrap the corresponding TensorDict version
-_TD_PASS_THROUGH = {
-    torch.unbind,
-    torch.full_like,
-    torch.zeros_like,
-    torch.ones_like,
-    torch.rand_like,
-    torch.empty_like,
-    torch.randn_like,
-    torch.clone,
-    torch.squeeze,
-    torch.unsqueeze,
-    torch.split,
-    torch.permute,
-    torch.split,
-    torch.stack,
-    torch.cat,
-    torch.gather,
-}
-# Methods to be executed from tensordict, any ref to self means 'tensorclass'
-_METHOD_FROM_TD = [
-    "gather",
-    "replace",
-]
-# Methods to be executed from tensordict, any ref to self means 'self._tensordict'
-_FALLBACK_METHOD_FROM_TD = [
-    "__abs__",
-    "__add__",
-    "__iadd__",
-    "__imul__",
-    "__ipow__",
-    "__isub__",
-    "__itruediv__",
-    "__mul__",
-    "__pow__",
-    "__sub__",
-    "__truediv__",
-    "_add_batch_dim",
-    "apply",
-    "_apply_nest",
-    "_fast_apply",
-    "apply_",
-    "named_apply",
-    "_check_unlock",
-    "unsqueeze",
-    "squeeze",
-    "_erase_names",  # TODO: must be specialized
-    "_exclude",  # TODO: must be specialized
-    "_get_str",
-    "_get_tuple",
-    "_set_at_tuple",
-    "_has_names",
-    "_propagate_lock",
-    "_propagate_unlock",
-    "_remove_batch_dim",
-    "is_memmap",
-    "is_shared",
-    "_select",  # TODO: must be specialized
-    "_set_str",
-    "_set_tuple",
-    "all",
-    "any",
-    "empty",
-    "exclude",
-    "expand",
-    "expand_as",
-    "is_empty",
-    "is_shared",
-    "items",
-    "keys",
-    "lock_",
-    "masked_fill",
-    "masked_fill_",
-    "permute",
-    "flatten",
-    "unflatten",
-    "ndimension",
-    "rename_",  # TODO: must be specialized
-    "reshape",
-    "select",
-    "to",
-    "transpose",
-    "unlock_",
-    "values",
-    "view",
-    "zero_",
-    "add",
-    "add_",
-    "mul",
-    "mul_",
-    "abs",
-    "abs_",
-    "acos",
-    "acos_",
-    "exp",
-    "exp_",
-    "neg",
-    "neg_",
-    "reciprocal",
-    "reciprocal_",
-    "sigmoid",
-    "sigmoid_",
-    "sign",
-    "sign_",
-    "sin",
-    "sin_",
-    "sinh",
-    "sinh_",
-    "tan",
-    "tan_",
-    "tanh",
-    "tanh_",
-    "trunc",
-    "trunc_",
-    "norm",
-    "lgamma",
-    "lgamma_",
-    "frac",
-    "frac_",
-    "expm1",
-    "expm1_",
-    "log",
-    "log_",
-    "log10",
-    "log10_",
-    "log1p",
-    "log1p_",
-    "log2",
-    "log2_",
-    "ceil",
-    "ceil_",
-    "floor",
-    "floor_",
-    "round",
-    "round_",
-    "erf",
-    "erf_",
-    "erfc",
-    "erfc_",
-    "asin",
-    "asin_",
-    "atan",
-    "atan_",
-    "cos",
-    "cos_",
-    "cosh",
-    "cosh_",
-    "lerp",
-    "lerp_",
-    "addcdiv",
-    "addcdiv_",
-    "addcmul",
-    "addcmul_",
-    "sub",
-    "sub_",
-    "maximum_",
-    "maximum",
-    "minimum_",
-    "minimum",
-    "clamp_max_",
-    "clamp_max",
-    "clamp_min_",
-    "clamp_min",
-    "pow",
-    "pow_",
-    "div",
-    "div_",
-    "sqrt",
-    "sqrt_",
-]
-_FALLBACK_METHOD_FROM_TD_COPY = [
-    "_clone",  # TODO: must be specialized
-    "clone",  # TODO: must be specialized
-    "copy",  # TODO: must be specialized
-]
-
-
-class tensorclass:
-    """A decorator to create :obj:`tensorclass` classes.
-
-    :obj:`tensorclass` classes are specialized :obj:`dataclass` instances that
-    can execute some pre-defined tensor operations out of the box, such as
-    indexing, item assignment, reshaping, casting to device or storage and many
-    others.
-
-    Examples:
-        >>> from tensordict import tensorclass
-        >>> import torch
-        >>> from typing import Optional
-        >>>
-        >>> @tensorclass
-        ... class MyData:
-        ...     X: torch.Tensor
-        ...     y: torch.Tensor
-        ...     z: str
-        ...     def expand_and_mask(self):
-        ...         X = self.X.unsqueeze(-1).expand_as(self.y)
-        ...         X = X[self.y]
-        ...         return X
-        ...
-        >>> data = MyData(
-        ...     X=torch.ones(3, 4, 1),
-        ...     y=torch.zeros(3, 4, 2, 2, dtype=torch.bool),
-        ...     z="test"
-        ...     batch_size=[3, 4])
-        >>> print(data)
-        MyData(
-            X=Tensor(torch.Size([3, 4, 1]), dtype=torch.float32),
-            y=Tensor(torch.Size([3, 4, 2, 2]), dtype=torch.bool),
-            z="test"
-            batch_size=[3, 4],
-            device=None,
-            is_shared=False)
-        >>> print(data.expand_and_mask())
-        tensor([])
-
-    It is also possible to nest tensorclasses instances within each other:
-        Examples:
-        >>> from tensordict import tensorclass
-        >>> import torch
-        >>> from typing import Optional
-        >>>
-        >>> @tensorclass
-        ... class NestingMyData:
-        ...     nested: MyData
-        ...
-        >>> nesting_data = NestingMyData(nested=data, batch_size=[3, 4])
-        >>> # although the data is stored as a TensorDict, the type hint helps us
-        >>> # to appropriately cast the data to the right type
-        >>> assert isinstance(nesting_data.nested, type(data))
-
-
-    """
-
-    def __new__(cls, autocast: bool = False):
-        if not isinstance(autocast, bool):
-            clz = autocast
-            self = super().__new__(cls)
-            self.__init__(autocast=False)
-            return self.__call__(clz)
-        return super().__new__(cls)
-
-    def __init__(self, autocast: bool):
-        self.autocast = autocast
-
-    def __call__(self, cls):
-        clz = _tensorclass(cls)
-        clz.autocast = self.autocast
-        return clz
-
-
-def _tensorclass(cls: T) -> T:
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if func not in _TD_PASS_THROUGH or not all(
-            issubclass(t, (Tensor, cls, TensorDictBase)) for t in types
-        ):
-            return NotImplemented
-
-        if kwargs is None:
-            kwargs = {}
-
-        # get the output type from the arguments / keyword arguments
-        if len(args) > 0:
-            tensorclass_instance = args[0]
-        else:
-            tensorclass_instance = kwargs.get("input", kwargs["tensors"])
-        if isinstance(tensorclass_instance, (tuple, list)):
-            tensorclass_instance = tensorclass_instance[0]
-        args = tuple(_arg_to_tensordict(arg) for arg in args)
-        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
-
-        result = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-        if isinstance(result, (list, tuple)):
-            return result.__class__(
-                _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
-                for tensordict_result in result
-            )
-        return _from_tensordict_with_copy(tensorclass_instance, result)
-
-    _is_non_tensor = getattr(cls, "_is_non_tensor", False)
-
-    cls = dataclass(cls)
-    expected_keys = set(cls.__dataclass_fields__)
-
-    for attr in cls.__dataclass_fields__:
-        if attr in dir(TensorDict) and attr not in ("_is_non_tensor", "data"):
-            raise AttributeError(
-                f"Attribute name {attr} can't be used with @tensorclass"
-            )
-
-    cls.fields = classmethod(lambda cls: dataclasses.fields(cls))
-    for field in cls.fields():
-        if hasattr(cls, field.name):
-            delattr(cls, field.name)
-
-    _get_type_hints(cls)
-    cls.__init__ = _init_wrapper(cls.__init__)
-    cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
-    cls.from_tensordict = cls._from_tensordict
-    if not hasattr(cls, "__torch_function__"):
-        cls.__torch_function__ = classmethod(__torch_function__)
-    cls.__getstate__ = _getstate
-    cls.__setstate__ = _setstate
-    # cls.__getattribute__ = object.__getattribute__
-    cls.__getattr__ = _getattr
-    cls.__setattr__ = _setattr_wrapper(cls.__setattr__, expected_keys)
-    # cls.__getattr__ = _getattr
-    cls.__getitem__ = _getitem
-    cls.__getitems__ = _getitem
-    cls.__setitem__ = _setitem
-    cls.__repr__ = _repr
-    cls.__len__ = _len
-    cls.__eq__ = _eq
-    cls.__ne__ = _ne
-    cls.__or__ = _or
-    cls.__xor__ = _xor
-    cls.__bool__ = _bool
-    cls.non_tensor_items = _non_tensor_items
-    # if not hasattr(cls, "keys"):
-    #     cls.keys = _keys
-    # if not hasattr(cls, "values"):
-    #     cls.values = _values
-    # if not hasattr(cls, "items"):
-    #     cls.items = _items
-    if not hasattr(cls, "set"):
-        cls.set = _set
-    if not hasattr(cls, "set_at_"):
-        cls.set_at_ = _set_at_
-    if not hasattr(cls, "del_"):
-        cls.del_ = _del_
-    if not hasattr(cls, "get"):
-        cls.get = _get
-    if not hasattr(cls, "get_at"):
-        cls.get_at = _get_at
-    if not hasattr(cls, "unbind"):
-        cls.unbind = _unbind
-    cls._unbind = _unbind
-    if not hasattr(cls, "state_dict"):
-        cls.state_dict = _state_dict
-    if not hasattr(cls, "load_state_dict"):
-        cls.load_state_dict = _load_state_dict
-    if not hasattr(cls, "_memmap_"):
-        cls._memmap_ = _memmap_
-    if not hasattr(cls, "share_memory_"):
-        cls.share_memory_ = _share_memory_
-    if not hasattr(cls, "update"):
-        cls.update = _update
-    if not hasattr(cls, "update_"):
-        cls.update_ = _update_
-    if not hasattr(cls, "update_at_"):
-        cls.update_at_ = _update_at_
-    for method_name in _METHOD_FROM_TD:
-        if not hasattr(cls, method_name):
-            setattr(cls, method_name, getattr(TensorDict, method_name))
-    for method_name in _FALLBACK_METHOD_FROM_TD:
-        if not hasattr(cls, method_name):
-            setattr(cls, method_name, _wrap_td_method(method_name))
-    for method_name in _FALLBACK_METHOD_FROM_TD_COPY:
-        if not hasattr(cls, method_name):
-            setattr(
-                cls,
-                method_name,
-                _wrap_td_method(method_name, copy_non_tensor=True),
-            )
-
-    if not hasattr(cls, "_apply_nest"):
-        cls._apply_nest = TensorDict._apply_nest
-
-    if not hasattr(cls, "filter_non_tensor_data"):
-        cls.filter_non_tensor_data = _filter_non_tensor_data
-    cls.__enter__ = __enter__
-    cls.__exit__ = __exit__
-
-    # Memmap
-    if not hasattr(cls, "memmap_like"):
-        cls.memmap_like = TensorDictBase.memmap_like
-    if not hasattr(cls, "memmap_"):
-        cls.memmap_ = TensorDictBase.memmap_
-    if not hasattr(cls, "memmap"):
-        cls.memmap = TensorDictBase.memmap
-    if not hasattr(cls, "load_memmap"):
-        cls.load_memmap = TensorDictBase.load_memmap
-    if not hasattr(cls, "_load_memmap"):
-        cls._load_memmap = classmethod(_load_memmap)
-    if not hasattr(cls, "from_dict"):
-        cls.from_dict = classmethod(_from_dict)
-    if not hasattr(cls, "from_dict_instance"):
-        cls.from_dict_instance = _from_dict_instance
-
-    for attr in TensorDict.__dict__.keys():
-        func = getattr(TensorDict, attr)
-        if inspect.ismethod(func) and attr not in cls.__dict__:
-            tdcls = func.__self__
-            if issubclass(tdcls, TensorDictBase):  # detects classmethods
-                setattr(cls, attr, _wrap_classmethod(tdcls, cls, func))
-
-    if not hasattr(cls, "to_tensordict"):
-        cls.to_tensordict = _to_tensordict
-    if not hasattr(cls, "device"):
-        cls.device = property(_device, _device_setter)
-    if not hasattr(cls, "batch_size"):
-        cls.batch_size = property(_batch_size, _batch_size_setter)
-    if not hasattr(cls, "names"):
-        cls.names = property(_names, _names_setter)
-    if not hasattr(cls, "to_dict"):
-        cls.to_dict = _to_dict
-
-    cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
-
-    _register_tensor_class(cls)
-    _register_td_node(cls)
-
-    # faster than doing instance checks
-    cls._is_non_tensor = _is_non_tensor
-    cls._is_tensorclass = True
-
-    from tensordict import _pytree
-
-    _pytree._CONSTRUCTORS[cls] = _pytree._tensorclass_constructor
-    return cls
-
-
-def _filter_non_tensor_data(self):
-    return super(type(self), self).__getattribute__("_tensordict")
-
-
-def _arg_to_tensordict(arg):
-    # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
-    # tensordicts and return those instead
-    if is_tensorclass(arg):
-        return arg._tensordict
-    elif isinstance(arg, (tuple, list)) and all(is_tensorclass(item) for item in arg):
-        return arg.__class__(item._tensordict for item in arg)
-    return arg
-
-
-def _from_tensordict_with_copy(tc, tensordict):
-    # creates a new tensorclass with the same type as tc, and a copy of the
-    # non_tensordict data
-    return tc._from_tensordict(
-        tensordict=tensordict, non_tensordict=copy(tc._non_tensordict)
-    )
-
-
-def _from_tensordict_with_none(tc, tensordict):
-    # creates a new tensorclass with the same type as tc, and all non_tensordict entries
-    # set to None
-    return tc._from_tensordict(
-        tensordict=tensordict,
-        non_tensordict={key: None for key in tc._non_tensordict},
-    )
-
-
-def _init_wrapper(init: Callable) -> Callable:
-    init_sig = inspect.signature(init)
-    params = list(init_sig.parameters.values())
-    # drop first entry of params which corresponds to self and isn't passed by the user
-    required_params = [p.name for p in params[1:] if p.default is inspect._empty]
-
-    @functools.wraps(init)
-    def wrapper(
-        self,
-        *args: Any,
-        batch_size: Sequence[int] | torch.Size | int = None,
-        device: DeviceType | None = None,
-        names: List[str] | None = None,
-        **kwargs,
-    ):
-
-        for value, key in zip(args, self.__dataclass_fields__):
-            if key in kwargs:
-                raise ValueError(f"The key {key} is already set in kwargs")
-            kwargs[key] = value
-        if batch_size is None:
-            batch_size = torch.Size([])
-        for key, field in self.__dataclass_fields__.items():
-            if field.default_factory is not dataclasses.MISSING:
-                default = field.default_factory()
-            else:
-                default = field.default
-            if default not in (None, dataclasses.MISSING):
-                kwargs.setdefault(key, default)
-
-        missing_params = [p for p in required_params if p not in kwargs]
-        if missing_params:
-            n_missing = len(missing_params)
-            raise TypeError(
-                f"{self.__class__.__name__}.__init__() missing {n_missing} "
-                f"required positional argument{'' if n_missing == 1 else 's'}: "
-                f"""{", ".join(f"'{name}'" for name in missing_params)}"""
-            )
-
-        self._tensordict = TensorDict(
-            {},
-            batch_size=torch.Size(batch_size),
-            device=device,
-            names=names,
-            _run_checks=False,
-        )
-        self._non_tensordict = {}
-
-        init(self, **kwargs)
-
-    new_params = [
-        inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
-        inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
-        inspect.Parameter("names", inspect.Parameter.KEYWORD_ONLY, default=None),
-    ]
-    wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
-
-    return wrapper
-
-
-_cast_funcs = KeyDependentDefaultDict(lambda cls: cls)
-_cast_funcs[torch.Tensor] = torch.as_tensor
-_cast_funcs[np.ndarray] = np.asarray
-
-
-def _get_type_hints(cls, with_locals=False):
-    #######
-    # Set proper type annotations for autocasting to tensordict/tensorclass
-    #
-    # by updating locals, we can allow this to be used within a function
-    # local-cross referencing will not work though
-    # def foo():
-    #     @tensorclass
-    #     class MyOtherClass:
-    #         x: torch.Tensor
-    #     @tensorclass
-    #     class MyClass:
-    #         x: MyClass # works
-    #         y: MyOtherClass # fails
-    #
-    # In this case, we will use the get_parent_local function to get the locals
-    # from the parent frame and so recursively until we can find the class.
-
-    if with_locals:
-        # This function gets the parent frame recursively until we can find the current class.
-        # Any exception leads to this to be None and auto-casting will be disabled
-        localns = locals()
-        localns = copy(localns)
-
-        def get_parent_locals(cls, localns=localns):
-            # Get the current frame
-            frame = inspect.currentframe()
-            try:
-                parent_locs = localns
-                while cls.__name__ not in parent_locs:
-                    # Get the parent frame
-                    parent_frame = frame.f_back
-                    # Get the locals dictionary of the parent frame
-                    parent_locs = parent_frame.f_locals
-                    frame = parent_frame
-            except Exception:
-                localns.setdefault(cls.__name__, cls)
-                return localns
-            finally:
-                # Clean up the frame reference
-                del frame
-            return copy(parent_locs)
-
-        localns = get_parent_locals(cls)
-    else:
-        localns = None
-
-    globalns = None
-
-    try:
-        cls._type_hints = get_type_hints(
-            cls,
-            localns=localns,
-            # globalns=globals(),
-        )
-        cls._type_hints = {
-            key: val if isinstance(val, type) else _AnyType
-            for key, val in cls._type_hints.items()
-        }
-    except NameError:
-        if not with_locals:
-            return _get_type_hints(cls, with_locals=True)
-        cls._set_dict_warn_msg = (
-            "A NameError occurred while trying to retrieve a type annotation. "
-            "This can occur when a tensorclass references another locally defined "
-            "tensorclass. "
-            f"As a result type hints cannot be read and {cls}.from_dict(...) "
-            f"or `{cls}.set` will not attempt to map dictionaries to "
-            "the relevant tensorclass. To resolve this issue, consider defining "
-            "your tensorclass globally."
-        )
-        cls._type_hints = None
-    except TypeError:
-        # This is a rather common case where type annotation is like
-        # class MyClass:
-        #     x: int | str
-        # in which case get_type_hints doesn't work (it does work
-        # however with old-school Optional or Union...)
-        # We simply differ the warning till _set() is called
-        cls._set_dict_warn_msg = (
-            "A TypeError occurred when trying to retrieve a type annotation. "
-            "This may be caused by annotations that use plain `|` instead of typing.Union "
-            "or typing.Optional which are supported. If you wish to use the feature "
-            "of setting dict as attributes with automapping to tensordict/tensorclass "
-            "(`my_obj.attr = dict(...)`), consider re-writing the tensorclass with "
-            "traditional type annotations."
-        )
-        cls._type_hints = None
-
-
-def _from_tensordict_wrapper(expected_keys):
-    def wrapper(cls, tensordict, non_tensordict=None):  # noqa: D417
-        """Tensor class wrapper to instantiate a new tensor class object.
-
-        Args:
-            tensordict (TensorDict): Dictionary of tensor types
-            non_tensordict (dict): Dictionary with non-tensor and nested tensor class objects
-
-        """
-        if not isinstance(tensordict, TensorDictBase):
-            raise RuntimeError(
-                f"Expected a TensorDictBase instance but got {type(tensordict)}"
-            )
-        # Validating keys of tensordict
-        for key in tensordict.keys():
-            if key not in expected_keys:
-                raise ValueError(
-                    f"Keys from the tensordict ({set(tensordict.keys())}) must "
-                    f"correspond to the class attributes ({expected_keys})."
-                )
-
-        # Validating non-tensor keys and for key clash
-        tensor_keys = set(tensordict.keys())
-        if non_tensordict is not None:
-            for key in list(non_tensordict.keys()):
-                if key not in expected_keys:
-                    raise ValueError(
-                        f"Keys from the non-tensor data ({set(non_tensordict.keys())}) must "
-                        f"correspond to the class attributes ({expected_keys})."
-                    )
-                if key in tensor_keys:
-                    if non_tensordict[key] is None:
-                        del non_tensordict[key]
-                        continue
-                    raise KeyError(
-                        f"{key} is present in both tensor and non-tensor dicts."
-                    )
-        # bypass initialisation. this means we don't incur any overhead creating an
-        # empty tensordict and writing values to it. we can skip this because we already
-        # have a tensordict to use as the underlying tensordict
-        tc = cls.__new__(cls)
-        tc.__dict__["_tensordict"] = tensordict
-
-        tc.__dict__["_non_tensordict"] = (
-            non_tensordict if non_tensordict is not None else {}
-        )
-        # since we aren't calling the dataclass init method, we need to manually check
-        # whether a __post_init__ method has been defined and invoke it if so
-        if hasattr(tc, "__post_init__"):
-            tc.__post_init__()
-        return tc
-
-    return wrapper
-
-
-def _memmap_(
-    self,
-    *,
-    prefix: str | None = None,
-    copy_existing: bool = False,
-    executor=None,
-    futures=None,
-    inplace=True,
-    like=False,
-    memmaped: bool = False,
-    share_non_tensor: bool = False,
-):
-    _non_tensordict = copy(self._non_tensordict)
-    cls = self.__class__
-
-    if not memmaped and prefix is not None:
-        prefix = Path(prefix)
-        if not prefix.exists():
-            os.makedirs(prefix, exist_ok=True)
-
-        def save_metadata(cls=cls, _non_tensordict=_non_tensordict, prefix=prefix):
-            with open(prefix / "meta.json", "w") as f:
-                metadata = {"_type": str(cls)}
-                to_pickle = {}
-                for key, value in _non_tensordict.items():
-                    value = _from_shared_nontensor(value)
-                    if _is_json_serializable(value):
-                        metadata[key] = value
-                    else:
-                        to_pickle[key] = value
-                json.dump(metadata, f)
-                if to_pickle:
-                    with open(prefix / "other.pickle", "wb") as pickle_file:
-                        pickle.dump(to_pickle, pickle_file)
-
-        if executor is None:
-            save_metadata()
-        else:
-            futures.append(executor.submit(save_metadata))
-
-        prefix = prefix / "_tensordict"
-
-    td = self._tensordict._memmap_(
-        prefix=prefix,
-        executor=executor,
-        futures=futures,
-        inplace=inplace,
-        like=like,
-        copy_existing=copy_existing,
-        share_non_tensor=share_non_tensor,
-    )
-    td._device = torch.device("cpu")
-    if not inplace:
-        result = cls._from_tensordict(td, _non_tensordict)
-    else:
-        result = self
-    return result
-
-
-def _share_memory_(self):
-    self._tensordict.share_memory_()
-    return self
-
-
-def _load_memmap(cls, prefix: Path, metadata: dict, **kwargs):
-    non_tensordict = copy(metadata)
-    del non_tensordict["_type"]
-    if os.path.exists(prefix / "other.pickle"):
-        with open(prefix / "other.pickle", "rb") as pickle_file:
-            non_tensordict.update(pickle.load(pickle_file))
-    td = TensorDict.load_memmap(prefix / "_tensordict", **kwargs, non_blocking=False)
-    return cls._from_tensordict(td, non_tensordict)
-
-
-def __enter__(self, *args, **kwargs):
-    return self._tensordict.__enter__(*args, **kwargs)
-
-
-def __exit__(self, *args, **kwargs):
-    return self._tensordict.__exit__(*args, **kwargs)
-
-
-def _getstate(self) -> dict[str, Any]:
-    """Returns a state dict which consists of tensor and non_tensor dicts for serialization.
-
-    Returns:
-        dictionary of state of tensor class
-
-    """
-    return {"tensordict": self._tensordict, "non_tensordict": self._non_tensordict}
-
-
-def _setstate(self, state: dict[str, Any]) -> None:  # noqa: D417
-    """Used to set the state of an object using state parameter.
-
-    Args:
-        state (dict): State parameter to set the object
-    """
-    self._tensordict = state.get("tensordict", None)
-    self._non_tensordict = state.get("non_tensordict", None)
-
-
-def _getattr(self, item: str) -> Any:
-    # if not item.startswith("__"):
-    __dict__ = self.__dict__
-    _non_tensordict = __dict__.get("_non_tensordict")
-    if _non_tensordict is not None:
-        out = _non_tensordict.get(item, NO_DEFAULT)
-        if out is not NO_DEFAULT:
-            if (
-                isinstance(self, NonTensorData)
-                and item == "data"
-                and (self._is_shared or self._is_memmap)
-            ):
-                return _from_shared_nontensor(out)
-            return out
-    _tensordict = __dict__.get("_tensordict")
-    if _tensordict is not None:
-        out = _tensordict._get_str(item, default=None)
-        if out is not None:
-            return out
-        out = getattr(_tensordict, item, NO_DEFAULT)
-        if out is not NO_DEFAULT:
-            if not callable(out):
-                return out
-            return _wrap_method(self, item, out)
-    raise AttributeError
-
-
-SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts", "names")
-
-
-def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
-    @functools.wraps(setattr_)
-    def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
-        """Set the value of an attribute for the tensor class object.
-
-        Args:
-            key (str): the name of the attribute to set
-            value (any): the value to set for the attribute
-
-        """
-        __dict__ = self.__dict__
-        if (
-            "_tensordict" not in __dict__
-            or "_non_tensordict" not in __dict__
-            or key in SET_ATTRIBUTES
-        ):
-            return setattr_(self, key, value)
-
-        out = self.set(key, value)
-        if out is not self:
-            raise RuntimeError(
-                "Cannot set attribute on a locked tensorclass, even if "
-                "clone_on_set is set to True. Use my_obj.set(...) instead."
-            )
-
-    return wrapper
-
-
-def _wrap_td_method(funcname, *, copy_non_tensor=False):
-    def wrapped_func(self, *args, **kwargs):
-        td = super(type(self), self).__getattribute__("_tensordict")
-        result = getattr(td, funcname)(*args, **kwargs)
-
-        def check_out(kwargs, result):
-            out = kwargs.get("out")
-            if out is result:
-                # No need to transform output
-                return True
-            return False
-
-        if isinstance(result, TensorDictBase) and not check_out(kwargs, result):
-            if result is td:
-                return self
-            nontd = super(type(self), self).__getattribute__("_non_tensordict")
-            if copy_non_tensor:
-                # use tree_map to copy
-                nontd = tree_map(lambda x: x, nontd)
-            return super(type(self), self).__getattribute__("_from_tensordict")(
-                result, nontd
-            )
-        return result
-
-    return wrapped_func
-
-
-def _wrap_method(self, attr, func):
-    warnings.warn(
-        f"The method {func} wasn't explicitly implemented for tensorclass. "
-        f"This fallback will be deprecated in future releases because it is inefficient "
-        f"and non-compilable. Please raise an issue in tensordict repo to support this method!"
-    )
-
-    @functools.wraps(func)
-    def wrapped_func(*args, **kwargs):
-        args = tuple(_arg_to_tensordict(arg) for arg in args)
-        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
-        res = func(*args, **kwargs)
-        if isinstance(res, TensorDictBase):
-            if attr.endswith("_"):
-                # in-place operation, return the current object
-                return self
-            elif attr in _CLEAR_METADATA:
-                # this is an attribute where copying the metadata makes no sense, e.g.
-                # .all or .any, so we replace all values with None
-                return self._from_tensordict(
-                    res, {k: None for k in self._non_tensordict}
-                )
-            # create a new tensorclass from res and copy the metadata from self
-            return self._from_tensordict(res, copy(self._non_tensordict))
-        return res
-
-    return wrapped_func
-
-
-def _update(
-    self,
-    input_dict_or_td: dict[str, CompatibleType] | T,
-    clone: bool = False,
-    inplace: bool = False,
-    *,
-    keys_to_update: Sequence[NestedKey] | None = None,
-    non_blocking: bool = False,
-):
-    if isinstance(input_dict_or_td, dict):
-        input_dict_or_td = self.from_dict(input_dict_or_td)
-
-    if is_tensorclass(input_dict_or_td):
-        self._tensordict.update(input_dict_or_td.__dict__["_tensordict"])
-        self._non_tensordict.update(input_dict_or_td.__dict__["_non_tensordict"])
-        return self
-
-    non_tensordict = {}
-    for key, value in input_dict_or_td.items():
-        if is_non_tensor(value):
-            non_tensordict[key] = value.data
-
-    self._tensordict.update(
-        input_dict_or_td.exclude(*non_tensordict.keys()),
-        clone=clone,
-        inplace=inplace,
-        keys_to_update=keys_to_update,
-        non_blocking=non_blocking,
-    )
-    self._non_tensordict.update(non_tensordict)
-    return self
-
-
-def _update_(
-    self,
-    input_dict_or_td: dict[str, CompatibleType] | T,
-    clone: bool = False,
-    inplace: bool = False,
-    *,
-    keys_to_update: Sequence[NestedKey] | None = None,
-    non_blocking: bool = False,
-):
-    if isinstance(input_dict_or_td, dict):
-        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
-
-    if is_tensorclass(input_dict_or_td):
-        self._tensordict.update_(input_dict_or_td._tensordict)
-        self._non_tensordict.update(input_dict_or_td._non_tensordict)
-        return self
-
-    non_tensordict = {}
-    for key, value in input_dict_or_td.items():
-        if is_non_tensor(value):
-            non_tensordict[key] = value.data
-
-    self._tensordict.update_(
-        input_dict_or_td.exclude(*non_tensordict.keys()),
-        clone=clone,
-        inplace=inplace,
-        keys_to_update=keys_to_update,
-        non_blocking=non_blocking,
-    )
-    self._non_tensordict.update(non_tensordict)
-    return self
-
-
-def _update_at_(
-    self,
-    input_dict_or_td: dict[str, CompatibleType] | T,
-    index: IndexType,
-    clone: bool = False,
-    *,
-    keys_to_update: Sequence[NestedKey] | None = None,
-    non_blocking: bool = False,
-):
-    if isinstance(input_dict_or_td, dict):
-        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
-
-    if is_tensorclass(input_dict_or_td):
-        self._tensordict.update(input_dict_or_td._tensordict)
-        self._non_tensordict.update(input_dict_or_td._non_tensordict)
-        return self
-
-    non_tensordict = {}
-    for key, value in input_dict_or_td.items():
-        if is_non_tensor(value):
-            non_tensordict[key] = value.data
-
-    self._tensordict.update_at_(
-        input_dict_or_td.exclude(*non_tensordict.keys()),
-        index=index,
-        clone=clone,
-        keys_to_update=keys_to_update,
-        non_blocking=non_blocking,
-    )
-    self._non_tensordict.update(non_tensordict)
-    return self
-
-
-def _wrap_classmethod(td_cls, cls, func):
-    @functools.wraps(func)
-    def wrapped_func(*args, **kwargs):
-        res = func.__get__(td_cls)(*args, **kwargs)
-        # res = func(*args, **kwargs)
-        if isinstance(res, TensorDictBase):
-            # create a new tensorclass from res and copy the metadata from self
-            return cls._from_tensordict(res)
-        return res
-
-    return wrapped_func
-
-
-def _getitem(self, item: NestedKey) -> Any:
-    """Retrieve the class object at the given index. Indexing will happen for nested tensors as well.
-
-    Args:
-       item (int or any other valid index type): index of the object to retrieve
-
-    Returns:
-        Tensor class object at the given index
-
-    """
-    if isinstance(item, str) or (
-        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
-    ):
-        raise ValueError(f"Invalid indexing arguments: {item}.")
-    tensor_res = self._tensordict[item]
-    return _from_tensordict_with_copy(self, tensor_res)  # device=res.device)
-
-
-def _setitem(self, item: NestedKey, value: Any) -> None:  # noqa: D417
-    """Set the value of the Tensor class object at the given index. Note that there is no strict validation on non-tensor values.
-
-    Args:
-        item (int or any other valid index type): index of the object to set
-        value (any): value to set for the item
-
-    """
-    if isinstance(item, str) or (
-        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
-    ):
-        raise ValueError(f"Invalid indexing arguments: {item}.")
-
-    if not is_tensorclass(value) and not isinstance(
-        value, (TensorDictBase, numbers.Number, Tensor)
-    ):
-        raise ValueError(
-            f"__setitem__ only supports tensorclasses, tensordicts,"
-            f" numeric scalars and tensors. Got {type(value)}"
-        )
-
-    if is_tensorclass(value):
-        if not isinstance(value, self.__class__):
-            self_keys = set().union(self._non_tensordict, self._tensordict.keys())
-            value_keys = set().union(value._non_tensordict, value._tensordict.keys())
-            if self_keys != value_keys:
-                # if tensorclass but different class ensure that all keys are equal
-                raise ValueError(
-                    "__setitem__ is only allowed for same-class or "
-                    "compatible class (i.e. same members) assignment"
-                )
-
-        # Validating the non-tensor data before setting the item
-        for key, val in value._non_tensordict.items():
-            # Raise a warning if non_tensor data doesn't match
-            if (
-                key in self._non_tensordict.keys()
-                and val is not self._non_tensordict[key]
-            ):
-                warnings.warn(
-                    f"Meta data at {repr(key)} may or may not be equal, "
-                    f"this may result in undefined behaviours",
-                    category=UserWarning,
-                    stacklevel=2,
-                )
-
-        for key in value._tensordict.keys():
-            # Making sure that the key-clashes won't happen, if the key is present
-            # in tensor data in value we will honor that and remove the key-value
-            # pair from non-tensor data
-            if key in self._non_tensordict.keys():
-                del self._non_tensordict[key]
-
-        self._tensordict[item] = value._tensordict
-    elif isinstance(value, TensorDictBase):  # it is one of accepted "broadcast" types
-        # attempt broadcast on all tensordata and nested tensorclasses
-        self._tensordict[item] = value.filter_non_tensor_data()
-        self._non_tensordict.update(
-            {
-                key: val.data
-                for key, val in value.items(is_leaf=is_non_tensor, leaves_only=True)
-            }
-        )
-    else:
-        # int, float etc.
-        self._tensordict[item] = value
-
-
-def _repr(self) -> str:
-    """Return a string representation of Tensor class object."""
-    fields = _all_td_fields_as_str(self._tensordict)
-    field_str = [fields] if fields else []
-    non_tensor_fields = _all_non_td_fields_as_str(self._non_tensordict)
-    batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
-    device_str = indent(f"device={self.device}", 4 * " ")
-    is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
-    if len(non_tensor_fields) > 0:
-        non_tensor_field_str = indent(
-            ",\n".join(non_tensor_fields),
-            4 * " ",
-        )
-        string = ",\n".join(
-            field_str
-            + [non_tensor_field_str, batch_size_str, device_str, is_shared_str]
-        )
-    else:
-        string = ",\n".join(field_str + [batch_size_str, device_str, is_shared_str])
-    return f"{self.__class__.__name__}(\n{string})"
-
-
-def _len(self) -> int:
-    """Returns the length of first dimension, if there is, otherwise 0."""
-    return len(self._tensordict)
-
-
-def _to_dict(self) -> dict:
-    td_dict = self._tensordict.to_dict()
-    td_dict.update(self._non_tensordict)
-    return td_dict
-
-
-def _from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
-    # we pass through a tensordict because keys could be passed as NestedKeys
-    # We can't assume all keys are strings, otherwise calling cls(**kwargs)
-    # would work ok
-
-    td = TensorDict.from_dict(
-        input_dict, batch_size=batch_size, device=device, batch_dims=batch_dims
-    )
-    non_tensor = {}
-
-    # Note: this won't deal with sub-tensordicts which may or may not be tensorclasses.
-    # We don't want to enforce them to be tensorclasses so we can't do much about it...
-    for key, value in list(td.items()):
-        if is_non_tensor(value):
-            non_tensor[key] = value.data
-            del td[key]
-
-    return cls.from_tensordict(tensordict=td, non_tensordict=non_tensor)
-
-
-def _from_dict_instance(
-    self, input_dict, batch_size=None, device=None, batch_dims=None
-):
-    if batch_dims is not None and batch_size is not None:
-        raise ValueError("Cannot pass both batch_size and batch_dims to `from_dict`.")
-    from tensordict import TensorDict
-
-    batch_size_set = torch.Size(()) if batch_size is None else batch_size
-    # TODO: this is a bit slow and will be a bottleneck every time td[idx] = dict(subtd)
-    # is called when there are non tensor data in it
-    if not _is_tensor_collection(type(input_dict)):
-        input_tdict = TensorDict.from_dict(input_dict)
-    else:
-        input_tdict = input_dict
-    trsf_dict = {}
-    for key, value in list(input_tdict.items()):
-        # cur_value = getattr(self, key, None)
-        cur_value = self.get(key, None)
-        if _is_tensor_collection(type(cur_value)):
-            trsf_dict[key] = cur_value.from_dict_instance(
-                value, batch_size=[], device=device, batch_dims=None
-            )
-        elif not isinstance(cur_value, torch.Tensor) and is_non_tensor(value):
-            trsf_dict[key] = value.data
-        elif cur_value is not None and not isinstance(cur_value, torch.Tensor):
-            # This is slightly unsafe but will work with bool, float and int
-            try:
-                trsf_dict[key] = type(cur_value)(value)
-            except Exception:
-                trsf_dict[key] = input_dict[key]
-        else:
-            trsf_dict[key] = value
-    out = type(self)(
-        **trsf_dict,
-        batch_size=batch_size_set,
-        device=device,
-    )
-    # check that
-    if batch_size is None:
-        out._tensordict.auto_batch_size_()
-    return out
-
-
-def _to_tensordict(self) -> TensorDict:
-    """Convert the tensorclass into a regular TensorDict.
-
-    Makes a copy of all entries. Memmap and shared memory tensors are converted to
-    regular tensors.
-
-    Returns:
-        A new TensorDict object containing the same values as the tensorclass.
-
-    """
-    td = self._tensordict.to_tensordict()
-    for key, val in self._non_tensordict.items():
-        td.set_non_tensor(key, val)
-    return td
-
-
-def _device(self) -> torch.device:
-    """Retrieves the device type of tensor class."""
-    return self._tensordict.device
-
-
-def _device_setter(self, value: DeviceType) -> None:
-    raise RuntimeError(
-        "device cannot be set using tensorclass.device = device, "
-        "because device cannot be updated in-place. To update device, use "
-        "tensorclass.to(new_device), which will return a new tensorclass "
-        "on the new device."
-    )
-
-
-def _set(
-    self, key: NestedKey, value: Any, inplace: bool = False, non_blocking: bool = False
-):
-    """Sets a new key-value pair.
-
-    Args:
-        key (str, tuple of str): name of the key to be set.
-           If tuple of str it is equivalent to chained calls of getattr
-           followed by a final setattr.
-        value (Any): value to be stored in the tensorclass
-        inplace (bool, optional): if ``True``, set will tentatively try to
-            update the value in-place. If ``False`` or if the key isn't present,
-            the value will be simply written at its destination.
-
-    Returns:
-        self
-
-    """
-    if isinstance(key, str):
-        cls = type(self)
-        __dict__ = self.__dict__
-        if __dict__["_tensordict"].is_locked:
-            raise RuntimeError(_LOCK_ERROR)
-        if key in ("batch_size", "names", "device"):
-            # handled by setattr
-            return
-        expected_keys = cls.__dataclass_fields__
-        if key not in expected_keys:
-            raise AttributeError(
-                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
-            )
-
-        def set_tensor(
-            key=key, value=value, inplace=inplace, non_blocking=non_blocking
-        ):
-            # Avoiding key clash, honoring the user input to assign tensor type data to the key
-            if key in self._non_tensordict.keys():
-                if inplace:
-                    raise RuntimeError(
-                        f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
-                    )
-                del self._non_tensordict[key]
-            self._tensordict.set(key, value, inplace=inplace, non_blocking=non_blocking)
-            return self
-
-        def _is_castable(datatype):
-            return issubclass(datatype, (int, float, np.ndarray))
-
-        if cls.autocast:
-            type_hints = cls._type_hints
-            if type_hints is not None:
-                target_cls = type_hints.get(key, _AnyType)
-            else:
-                warnings.warn("type_hints are none, cannot perform auto-casting")
-                target_cls = _AnyType
-
-            if isinstance(value, dict):
-                if _is_tensor_collection(target_cls):
-                    value = target_cls.from_dict(value)
-                    self._tensordict.set(
-                        key, value, inplace=inplace, non_blocking=non_blocking
-                    )
-                    return self
-                elif type_hints is None:
-                    warnings.warn(type(self)._set_dict_warn_msg)
-            elif value is not None and issubclass(
-                target_cls, tuple(tensordict_lib.base._ACCEPTED_CLASSES)
-            ):
-                try:
-                    if not issubclass(type(value), target_cls):
-                        if issubclass(target_cls, torch.Tensor):
-                            # first convert to tensor to make sure that the dtype is preserved
-                            value = torch.as_tensor(value)
-                        cast_val = _cast_funcs[target_cls](value)
-                    else:
-                        cast_val = value
-                except TypeError:
-                    raise TypeError(
-                        f"Failed to cast the value {key} to the type annotation {target_cls}."
-                    )
-                return set_tensor(value=cast_val)
-            elif value is not None and target_cls is not _AnyType:
-                value = _cast_funcs[target_cls](value)
-            elif target_cls is _AnyType and _is_castable(type(value)):
-                return set_tensor()
-        else:
-            if isinstance(value, tuple(tensordict_lib.base._ACCEPTED_CLASSES)):
-                return set_tensor()
-
-        # Avoiding key clash, honoring the user input to assign non-tensor data to the key
-        if key in self._tensordict.keys():
-            if inplace:
-                raise RuntimeError(
-                    f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
-                )
-            self._tensordict.del_(key)
-        # Saving all non-tensor attributes
-        self._non_tensordict[key] = value
-        return self
-
-    if isinstance(key, tuple) and len(key):
-        key = _unravel_key_to_tuple(key)
-        if len(key) > 1:
-            return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
-        out = self.set(key[0], value)
-        return out
-    raise ValueError(
-        f"Supported type for key are str and tuple, got {key} of type {type(key)}"
-    )
-
-
-def _del_(self, key):
-    key = _unravel_key_to_tuple(key)
-    if len(key) > 1:
-        td = self.get(key[0])
-        td.del_(key[1:])
-        return
-    if key[0] in self._tensordict.keys():
-        self._tensordict.del_(key[0])
-        # self.set(key[0], None)
-    elif key[0] in self._non_tensordict.keys():
-        self._non_tensordict[key[0]] = None
-    else:
-        raise KeyError(f"Key {key} could not be found in tensorclass {self}.")
-    return
-
-
-def _set_at_(
-    self, key: NestedKey, value: Any, idx: IndexType, non_blocking: bool = False
-):
-    if key in self._non_tensordict:
-        del self._non_tensordict[key]
-    return self._tensordict.set_at_(key, value, idx, non_blocking=non_blocking)
-
-
-def _get(self, key: NestedKey, default: Any = NO_DEFAULT):
-    """Gets the value stored with the input key.
-
-    Args:
-        key (str, tuple of str): key to be queried. If tuple of str it is
-            equivalent to chained calls of getattr.
-        default: default value if the key is not found in the tensorclass.
-
-    Returns:
-        value stored with the input key
-
-    """
-    if isinstance(key, str):
-        key = (key,)
-
-    if isinstance(key, tuple):
-        try:
-            if len(key) > 1:
-                return getattr(self, key[0]).get(key[1:])
-            return getattr(self, key[0])
-        except AttributeError:
-            if default is NO_DEFAULT:
-                raise
-            return default
-    raise ValueError(f"Supported type for key are str and tuple, got {type(key)}")
-
-
-def _get_at(self, key: NestedKey, idx, default: Any = NO_DEFAULT):
-    try:
-        return self.get(key, NO_DEFAULT)[idx]
-    except AttributeError:
-        if default is NO_DEFAULT:
-            raise
-        return default
-
-
-def _batch_size(self) -> torch.Size:
-    """Retrieves the batch size for the tensor class.
-
-    Returns:
-        batch size (torch.Size)
-
-    """
-    return self._tensordict.batch_size
-
-
-def _batch_size_setter(self, new_size: torch.Size) -> None:  # noqa: D417
-    """Set the value of batch_size.
-
-    Args:
-        new_size (torch.Size): new_batch size to be set
-
-    """
-    self._tensordict._batch_size_setter(new_size)
-
-
-def _names(self) -> torch.Size:
-    """Retrieves the dim names for the tensor class.
-
-    Returns:
-        names (list of str)
-
-    """
-    return self._tensordict.names
-
-
-def _names_setter(self, names: str) -> None:  # noqa: D417
-    """Set the value of ``tensorclass.names``.
-
-    Args:
-        names (sequence of str)
-
-    """
-    self._tensordict.names = names
-
-
-def _state_dict(
-    self, destination=None, prefix="", keep_vars=False, flatten=False
-) -> dict[str, Any]:
-    """Returns a state_dict dictionary that can be used to save and load data from a tensorclass."""
-    state_dict = {
-        "_tensordict": super(type(self), self)
-        .__getattribute__("_tensordict")
-        .state_dict(
-            destination=destination, prefix=prefix, keep_vars=keep_vars, flatten=flatten
-        )
-    }
-    state_dict["_non_tensordict"] = copy(self._non_tensordict)
-    return state_dict
-
-
-def _load_state_dict(
-    self, state_dict: dict[str, Any], strict=True, assign=False, from_flatten=False
-):
-    """Loads a state_dict attemptedly in-place on the destination tensorclass."""
-    for key, item in state_dict.items():
-        # keys will never be nested which facilitates everything, but let's
-        # double check in case someone does something nasty
-        if not isinstance(key, str):
-            raise TypeError("Only str keys are allowed when calling load_state_dict.")
-        if key == "_non_tensordict":
-            for sub_key, sub_item in item.items():
-                # sub_item is the state dict of a tensorclass
-                if isinstance(sub_item, dict) and "_non_tensordict" in sub_item:
-                    raise RuntimeError(
-                        "Loading a saved tensorclass on a uninitialized tensorclass is not allowed"
-                    )
-                else:
-                    # check that sub_key is part of the tensorclass
-                    if sub_key not in self.__class__.__dataclass_fields__:
-                        raise KeyError(
-                            f"Key '{sub_key}' wasn't expected in the state-dict."
-                        )
-                    super(type(self), self).__getattribute__("_non_tensordict")[
-                        sub_key
-                    ] = sub_item
-        elif key == "_tensordict":
-            for sub_key in item.keys():
-                if (
-                    sub_key not in self.__class__.__dataclass_fields__
-                    and sub_key not in ("__batch_size", "__device")
-                ):
-                    raise KeyError(
-                        f"Key '{sub_key}' wasn't expected in the state-dict."
-                    )
-            super(type(self), self).__getattribute__("_tensordict").load_state_dict(
-                item, strict=strict, assign=assign, from_flatten=from_flatten
-            )
-        else:
-            raise KeyError(f"Key '{key}' wasn't expected in the state-dict.")
-
-    return self
-
-
-def _eq(self, other: object) -> bool:
-    """Compares the Tensor class object to another object for equality. However, the equality check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object. Can be a tensorclass, a
-            tensordict or any compatible type (int, float or tensor), in
-            which case the equality check will be propagated to the leaves.
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean
-        values for tensor attributes and None for non-tensor attributes
-
-    Examples:
-        >>> @tensorclass
-        ... class MyClass:
-        ...     x: Tensor
-        ...     y: "MyClass"
-        ...     z: str
-        ...
-        >>> c1 = MyClass(
-        ...     x=torch.randn(3, 4),
-        ...     y=MyClass(
-        ...         x=torch.randn(3, 4, 1),
-        ...         y=None,
-        ...         z="bar",
-        ...         batch_size=[3, 4, 1],
-        ...     ),
-        ...     z="foo",
-        ...     batch_size=[3, 4],
-        ... )
-        >>> c2 = c1.clone()
-        >>> print(c1 == c2)
-        MyClass(
-            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
-            y=MyClass(
-                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
-                y=None,
-                z=None,
-                batch_size=torch.Size([3, 4, 1]),
-                device=None,
-                is_shared=False),
-            z=None,
-            batch_size=torch.Size([3, 4]),
-            device=None,
-            is_shared=False)
-        >>> assert (c1 == c2).all()
-        >>> assert (c1[:2] == c2[:2]).all()
-        >>> assert not (c1 == c2.apply(lambda x: x+1)).all()
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor)
-    ):
-        return False
-    if is_tensorclass(other):
-        tensor = self._tensordict == other._tensordict
-    elif _is_tensor_collection(type(other)):
-        # other can be a tensordict reconstruction of self, in which case we discard
-        # the non-tensor data
-        tensor = self.filter_non_tensor_data() == other.filter_non_tensor_data()
-    else:
-        tensor = self._tensordict == other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def _ne(self, other: object) -> bool:
-    """Compare the Tensor class object to another object for inequality. However, the equality check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean values for tensor attributes and None for non-tensor attributes
-
-    Examples:
-        >>> @tensorclass
-        ... class MyClass:
-        ...     x: Tensor
-        ...     y: "MyClass"
-        ...     z: str
-        ...
-        >>> c1 = MyClass(
-        ...     x=torch.randn(3, 4),
-        ...     y=MyClass(
-        ...         x=torch.randn(3, 4, 1),
-        ...         y=None,
-        ...         z="bar",
-        ...         batch_size=[3, 4, 1],
-        ...     ),
-        ...     z="foo",
-        ...     batch_size=[3, 4],
-        ... )
-        >>> c2 = c1.clone()
-        >>> print(c1 != c2)
-        MyClass(
-            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
-            y=MyClass(
-                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
-                y=None,
-                z=None,
-                batch_size=torch.Size([3, 4, 1]),
-                device=None,
-                is_shared=False),
-            z=None,
-            batch_size=torch.Size([3, 4]),
-            device=None,
-            is_shared=False)
-        >>> c2 = c2.apply(lambda x: x+1)
-        >>> assert (c1 != c2).all()
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor)
-    ):
-        return True
-    if is_tensorclass(other):
-        tensor = self._tensordict != other._tensordict
-    elif _is_tensor_collection(type(other)):
-        # other can be a tensordict reconstruction of self, in which case we discard
-        # the non-tensor data
-        tensor = self._tensordict != other.exclude(*self._non_tensordict.keys())
-    else:
-        tensor = self._tensordict != other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def _or(self, other: object) -> bool:
-    """Compares the Tensor class object to another object for logical OR. However, the logical OR check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object. Can be a tensorclass, a
-            tensordict or any compatible type (int, float or tensor), in
-            which case the equality check will be propagated to the leaves.
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean
-        values for tensor attributes and None for non-tensor attributes
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor)
-    ):
-        return False
-    if is_tensorclass(other):
-        tensor = self._tensordict | other._tensordict
-    elif _is_tensor_collection(type(other)):
-        # other can be a tensordict reconstruction of self, in which case we discard
-        # the non-tensor data
-        tensor = self._tensordict | other.exclude(*self._non_tensordict.keys())
-    else:
-        tensor = self._tensordict | other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def _xor(self, other: object) -> bool:
-    """Compares the Tensor class object to another object for exclusive OR. However, the exclusive OR check for non-tensor data is not performed.
-
-    Args:
-        other: object to compare to this object. Can be a tensorclass, a
-            tensordict or any compatible type (int, float or tensor), in
-            which case the equality check will be propagated to the leaves.
-
-    Returns:
-        False if the objects are of different class types, Tensorclass of boolean
-        values for tensor attributes and None for non-tensor attributes
-
-    """
-    if not is_tensor_collection(other) and not isinstance(
-        other, (dict, numbers.Number, Tensor)
-    ):
-        return False
-    if is_tensorclass(other):
-        tensor = self._tensordict ^ other._tensordict
-    elif _is_tensor_collection(type(other)):
-        # other can be a tensordict reconstruction of self, in which case we discard
-        # the non-tensor data
-        tensor = self._tensordict ^ other.exclude(*self._non_tensordict.keys())
-    else:
-        tensor = self._tensordict ^ other
-    return _from_tensordict_with_none(self, tensor)
-
-
-def _non_tensor_items(self, include_nested=False):
-    if include_nested:
-        return self.non_tensor_items() + self._tensordict.non_tensor_items(
-            include_nested=True
-        )
-    else:
-        return list(self._non_tensordict.items())
-
-
-def _bool(self):
-    raise RuntimeError("Converting a tensorclass to boolean value is not permitted")
-
-
-def _single_td_field_as_str(key, item, tensordict):
-    """Returns a string as a  key-value pair of tensordict.
-
-    Args:
-        key (str): key of tensor dict item
-        item (tensor type): value to be returned for key
-        tensordict (Tensordict): Tensordict object
-
-    Returns:
-        String representation of a key-value pair
-
-    """
-    if is_tensor_collection(type(item)):
-        return f"{key}={repr(tensordict[key])}"
-    return f"{key}={_get_repr(item)}"
-
-
-def _all_td_fields_as_str(td: TensorDictBase) -> str:
-    """Returns indented representation of tensor dict values as a key-value pairs.
-
-    Args:
-        td (TensorDict) : Tensordict object
-
-    Returns:
-        String representation of all tensor data
-
-    """
-    return indent(
-        ",\n".join(
-            sorted([_single_td_field_as_str(key, item, td) for key, item in td.items()])
-        ),
-        4 * " ",
-    )
-
-
-def _all_non_td_fields_as_str(src_dict) -> list:
-    """Returns a list of string representation of non-tensor key-value pairs.
-
-    Args:
-        src_dict (dict): non_tensor_dict
-
-    Returns:
-        result (list): list of strings with key-value representation
-
-    """
-    result = []
-    for key, val in src_dict.items():
-        if not is_tensor_collection(val):
-            result.append(f"{key}={repr(val)}")
-
-    return result
-
-
-def _unbind(self, dim: int):
-    """Returns a tuple of indexed tensorclass instances unbound along the indicated dimension.
-
-    Resulting tensorclass instances will share the storage of the initial tensorclass instance.
-
-    """
-    return tuple(
-        self._from_tensordict(td, non_tensordict=copy(self._non_tensordict))
-        for td in self._tensordict.unbind(dim)
-    )
-
-
-################
-# Custom classes
-# --------------
-
-NONTENSOR_HANDLED_FUNCTIONS = []
-
-_MP_MANAGER = None
-
-
-def _mp_manager():
-    global _MP_MANAGER
-    if _MP_MANAGER is None:
-        _MP_MANAGER = Manager()
-    return _MP_MANAGER
-
-
-@tensorclass
-class NonTensorData:
-    """A carrier for non-tensordict data.
-
-    This class can be used whenever non-tensor data needs to be carrier at
-    any level of a tensordict instance.
-
-    :class:`~tensordict.tensorclass.NonTensorData` instances can be created
-    explicitely or using :meth:`~tensordict.TensorDictBase.set_non_tensor`.
-
-    This class is serializable using :meth:`tensordict.TensorDictBase.memmap`
-    and related methods, and can be loaded through :meth:`~tensordict.TensorDictBase.load_memmap`.
-    If the content of the object is JSON-serializable, it will be serializsed in
-    the `meta.json` file in the directory pointed by the parent key of the `NoneTensorData`
-    object. If it isn't, serialization will fall back on pickle. This implies
-    that we assume that the content of this class is either json-serializable or
-    pickable, and it is the user responsibility to make sure that one of these
-    holds. We try to avoid pickling/unpickling objects for performance and security
-    reasons (as pickle can execute arbitrary code during loading).
-
-    .. note:: if the data passed to :class:`NonTensorData` is a :class:`NonTensorData`
-        itself, the data from the nested object will be gathered.
-
-        >>> non_tensor = NonTensorData("a string!")
-        >>> non_tensor = NonTensorData(non_tensor)
-        >>> assert non_tensor.data == "a string!"
-
-    .. note:: To faciliate ``NonTensorData`` integration in tensordict, the
-        :meth:`~tensordict.TensorDictBase.__getitem__` and :meth:`~tensordict.TensorDictBase.__setitem__`
-        are overloaded to set non-tensor data appropriately (unlike :meth:`~tensordict.TensorDictBase.set`
-        and :meth:`~tensordict.TensorDictBase.get` which are reserved for tensor-like
-        objects):
-
-        >>> td = TensorDict({"a": torch.zeros(3)}, batch_size=[3])
-        >>> td["a"]  # gets a tensor
-        >>> td["b"] = "a string!"
-        >>> assert td["b"] == "a string!"
-        >>> # indexing preserves the meta-data
-        >>> assert td[0]["b"] == "a string!"
-        >>> td.get("b")  # returns the NonTensorData
-
-    .. note:: Unlike other tensorclass classes, :class:`NonTensorData` supports
-        comparisons of two non-tensor data through :meth:`~.__eq__`, :meth:`~.__ne__`,
-        :meth:`~.__xor__` or :meth:`~.__or__`. These operations return a tensor
-        of shape `batch_size`. For compatibility with `<a tensordict> == <float_number>`,
-        comparison with non-:class:`NonTensorData` will always return an empty
-        :class:`NonTensorData`.
-
-        >>> a = NonTensorData(True, batch_size=[])
-        >>> b = NonTensorData(True, batch_size=[])
-        >>> assert a == b
-        >>> assert not (a != b)
-        >>> assert not (a ^ b)
-        >>> assert a | b
-        >>> # The output is a tensor of shape batch-size
-        >>> a = NonTensorData(True, batch_size=[3])
-        >>> b = NonTensorData(True, batch_size=[3])
-        >>> print(a == b)
-        tensor([True, True, True])
-
-    .. note:: Stacking :class:`NonTensorData` instances results in either
-        a single :class:`NonTensorData` instance if all shapes match, or a
-        :class:`~tensordict.LazyStackedTensorDict` object if the content
-        mismatch. To get to this result, the content of the :class:`NonTensorData`
-        instances must be compared, which can be computationally intensive
-        depending on what this content is.
-
-        >>> data = torch.stack([NonTensorData(1, batch_size=[]) for _ in range(10)])
-        >>> data
-        NonTensorData(
-            data=1,
-            batch_size=torch.Size([10]),
-            device=None,
-            is_shared=False)
-        >>> data = torch.stack([NonTensorData(i, batch_size=[3,]) for i in range(10)], 1)
-        >>> data[:, 0]
-        NonTensorData(
-            data=0,
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    .. note:: Non-tensor data can be filtered out from a tensordict using
-        :meth:`~tensordict.TensorDictBase.filter_non_tensor`.
-
-    Examples:
-        >>> # create an instance explicitly
-        >>> non_tensor = NonTensorData("a string!", batch_size=[]) # batch-size can be anything
-        >>> data = TensorDict({}, batch_size=[3])
-        >>> data.set_non_tensor(("nested", "key"), "a string!")
-        >>> assert isinstance(data.get(("nested", "key")), NonTensorData)
-        >>> assert data.get_non_tensor(("nested", "key")) == "a string!"
-        >>> # serialization
-        >>> class MyPickableClass:
-        ...     value = 10
-        >>> data.set_non_tensor("pickable", MyPickableClass())
-        >>> import tempfile
-        >>> with tempfile.TemporaryDirectory() as tmpdir:
-        ...     data.memmap(tmpdir)
-        ...     loaded = TensorDict.load_memmap(tmpdir)
-        ...     # print directory path
-        ...     print_directory_tree(tmpdir)
-        Directory size: 511.00 B
-        tmp2cso9og_/
-            pickable/
-                _tensordict/
-                    meta.json
-                other.pickle
-                meta.json
-            nested/
-                key/
-                    _tensordict/
-                        meta.json
-                    meta.json
-                meta.json
-            meta.json
-        >>> assert loaded.get_non_tensor("pickable").value == 10
-
-    .. note:: __Preallocation__ is also possible with ``NonTensorData``.
-      This class can handle conversion from ``NonTensorData`` to
-      ``NonTensorStack`` where appropriate, as the following example
-      demonstrates:
-
-        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
-        >>> print(td)
-        TensorDict(
-            fields={
-                val: NonTensorData(
-                    data=0,
-                    _metadata=None,
-                    _is_non_tensor=True,
-                    batch_size=torch.Size([10]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([10]),
-            device=None,
-            is_shared=False)
-        >>> print(td["val"])
-        0
-        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
-        >>> td[1::2] = newdata
-        >>> print(td)
-        TensorDict(
-            fields={
-                val: NonTensorStack(
-                    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
-                    batch_size=torch.Size([10]),
-                    device=None)},
-            batch_size=torch.Size([10]),
-            device=None,
-            is_shared=False)
-        >>> print(td["val"])  # the stack is automatically converted to a list
-        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
-
-      If the value is unique, the ``NonTensorData`` container is kept and
-      retrieving the value only returns this value. If a ``NonTensorStack``
-      is used, ``__getitem__`` will return the list of values instead.
-      This makes the two operations not exactly interchangeable. The reason
-      for this inconsistency is that a single ``NonTensorData`` with a non-empty
-      batch-size is intended to be used as a metadata carrier for bigger
-      tensordicts, whereas ``NonTensorStack`` usage is aimed at allocating
-      one metadata atom to each corresponding batch element.
-
-    .. note::
-      ``NonTensorData`` can be shared between processes. In fact, both
-      :meth:`~tensordict.TensorDict.memmap_` (and the likes) and
-      :meth:`~tensordict.TensorDict.share_memory_` will produce sharable
-      instances.
-
-      Valid methods to write data are :meth:`~tensordict.TensorDictBase.update`
-      with the `inplace=True` flag and :meth:`~tensordict.TensorDictBase.update_`
-      or :meth:`~tensordict.TensorDictBase.update_at_`.
-
-        >>> if __name__ == "__main__":
-        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
-        ...     td.share_memory_()
-        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))  # works
-        ...     td.update(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []), inplace=True)  # works
-        ...     td["val"] = 1  # breaks
-
-      A shared ``NonTensorData`` is writable whenever its content is a ``str``,
-      ``int``, ``float``, ``bool``, ``dict`` or ``list`` instance. Other types
-      (e.g., dataclasses) will not raise an exception during the call to
-      ``memmap_`` or ``share_memory_`` but they will cause the code to break
-      when the data is overwritten.
-
-        >>> @dataclass
-        ... class MyClass:
-        ...     string: str
-        ...
-        >>> if __name__ == "__main__":
-        ...     td = TensorDict({"val": MyClass("a string!")}, [])
-        ...     td.share_memory_()  # works and can be shared between processes
-        ...     td.update_(TensorDict({"val": MyClass("another string!")}, []))  # breaks!
-
-      :class:`~tensordict.tensorclass.TensorStack` instances are also sharable
-      in a similar way. Crucially, preallocation must be properly handled for
-      this to work.
-
-        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
-        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
-        >>> td[1::2] = newdata
-        >>> # If TD is properly preallocated, we can share it and change its content
-        >>> td.share_memory_()
-        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
-        >>> td[1::2] = newdata  # Works!
-        >>> # In contrast, not preallocating the tensordict properly will break when assigning values
-        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
-        >>> td.share_memory_()
-        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
-        >>> td[1::2] = newdata  # breaks!
-
-      Writable memmapped-``NonTensorData`` instances will update the underlying
-      metadata if required. This involves writing in a JSON file, which can
-      introduce some overhead. We advise against this usage whenever one seeks
-      performance and long-lasting data sharing isn't required (``share_memory_``
-      should be preferred in these cases).
-
-        >>> if __name__ == "__main__":
-        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
-        ...     td.memmap_(dest_folder)
-        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))
-        ...     # The underlying metadata on disk is updated during calls to update_
-        ...     td_load = TensorDict.load_memmap(dest_folder)
-        ...     assert (td == td_load).all()
-
-    """
-
-    # Used to carry non-tensor data in a tensordict.
-    # The advantage of storing this in a tensorclass is that we don't need
-    # to patch tensordict with additional checks that will encur unwanted overhead
-    # and all the overhead falls back on this class.
-    data: Any
-    _metadata: dict | None = None
-
-    _is_non_tensor: bool = True
-
-    def __post_init__(self):
-        if is_non_tensor(self.data):
-            data = getattr(self.data, "data", None)
-            if data is None:
-                data = self.data.tolist()
-            del self._tensordict["data"]
-            self._non_tensordict["data"] = data
-        assert self._tensordict.is_empty(), self._tensordict
-
-        def __repr__(self):
-            data_str = str(self.data)
-            if len(data_str) > 200:
-                data_str = data_str[:20] + "  ...  " + data_str[-20:]
-            return f"{type(self).__name__}(data={data_str}, batch_size={self.batch_size}, device={self.device})"
-
-        self.__class__.__repr__ = __repr__
-
-        old_eq = self.__class__.__eq__
-        if old_eq is _eq:
-            global NONTENSOR_HANDLED_FUNCTIONS
-            NONTENSOR_HANDLED_FUNCTIONS.extend(TD_HANDLED_FUNCTIONS)
-
-            # Patch only the first time a class is created
-
-            @functools.wraps(_eq)
-            def __eq__(self, other):
-                if isinstance(other, NonTensorData):
-                    return torch.full(
-                        self.batch_size, self.data == other.data, device=self.device
-                    )
-                return old_eq(self, other)
-
-            self.__class__.__eq__ = __eq__
-
-            _ne = self.__class__.__ne__
-
-            @functools.wraps(_ne)
-            def __ne__(self, other):
-                if isinstance(other, NonTensorData):
-                    return torch.full(
-                        self.batch_size, self.data != other.data, device=self.device
-                    )
-                return _ne(self, other)
-
-            self.__class__.__ne__ = __ne__
-
-            _xor = self.__class__.__xor__
-
-            @functools.wraps(_xor)
-            def __xor__(self, other):
-                if isinstance(other, NonTensorData):
-                    return torch.full(
-                        self.batch_size, self.data ^ other.data, device=self.device
-                    )
-                return _xor(self, other)
-
-            self.__class__.__xor__ = __xor__
-
-            _or = self.__class__.__or__
-
-            @functools.wraps(_or)
-            def __or__(self, other):
-                if isinstance(other, NonTensorData):
-                    return torch.full(
-                        self.batch_size, self.data | other.data, device=self.device
-                    )
-                return _or(self, other)
-
-            self.__class__.__or__ = __or__
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        return self._update(
-            input_dict_or_td=input_dict_or_td,
-            clone=clone,
-            inplace=inplace,
-            keys_to_update=keys_to_update,
-        )
-
-    def _update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        keys_to_update: Sequence[NestedKey] | None = None,
-        break_on_memmap: bool = None,
-    ) -> T:
-        if isinstance(input_dict_or_td, NonTensorData):
-            data = input_dict_or_td.data
-            if inplace and self._tensordict._is_shared:
-                _update_shared_nontensor(self._non_tensordict["data"], data)
-                return self
-            elif inplace and self._is_memmap:
-                _is_memmaped_from_above = self._is_memmaped_from_above()
-                if break_on_memmap is None:
-                    global _BREAK_ON_MEMMAP
-                    break_on_memmap = _BREAK_ON_MEMMAP
-                if _is_memmaped_from_above and break_on_memmap:
-                    raise RuntimeError(
-                        "Cannot update a leaf NonTensorData from a memmaped parent NonTensorStack. "
-                        "To update this leaf node, please update the NonTensorStack with the proper index."
-                    )
-                share_non_tensor = self._metadata["_share_non_tensor"]
-                if share_non_tensor:
-                    _update_shared_nontensor(self._non_tensordict["data"], data)
-                else:
-                    self._non_tensordict["data"] = data
-                # Force json update by setting is memmap to False
-                if not _is_memmaped_from_above and "memmap_prefix" in self._metadata:
-                    self._tensordict._is_memmap = False
-                    self._memmap_(
-                        prefix=self._metadata["memmap_prefix"],
-                        copy_existing=False,
-                        executor=None,
-                        futures=None,
-                        inplace=True,
-                        like=False,
-                        share_non_tensor=share_non_tensor,
-                    )
-                return self
-            elif not inplace and self.is_locked:
-                raise RuntimeError(_LOCK_ERROR)
-            if clone:
-                data = deepcopy(data)
-            self.data = data
-        elif isinstance(input_dict_or_td, NonTensorStack):
-            raise ValueError(
-                "Cannot update a NonTensorData object with a NonTensorStack. Call `non_tensor_data.maybe_to_stack()` "
-                "before calling update()."
-            )
-        elif not input_dict_or_td.is_empty():
-            raise RuntimeError(f"Unexpected type {type(input_dict_or_td)}")
-        return self
-
-    def maybe_to_stack(self):
-        """Converts the NonTensorData object to a NonTensorStack object if it has a non-empty batch-size."""
-        datalist = self.data
-        if not self.batch_size:
-            return self
-        for i in reversed(self.batch_size):
-            datalist = [datalist] * i
-        return NonTensorStack._from_list(datalist, device=self.device)
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        return self._update_(
-            input_dict_or_td=input_dict_or_td,
-            clone=clone,
-            keys_to_update=keys_to_update,
-        )
-
-    def _update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        *,
-        keys_to_update: Sequence[NestedKey] | None = None,
-        break_on_memmap: bool = None,
-    ) -> T:
-
-        if isinstance(input_dict_or_td, NonTensorStack):
-            raise RuntimeError(
-                "Cannot update a NonTensorData with a NonTensorStack object."
-            )
-        if not isinstance(input_dict_or_td, NonTensorData):
-            raise RuntimeError(
-                "NonTensorData.copy_ / update_ requires the source to be a NonTensorData object."
-            )
-        return self._update(
-            input_dict_or_td,
-            inplace=True,
-            clone=clone,
-            keys_to_update=keys_to_update,
-            break_on_memmap=break_on_memmap,
-        )
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        index: IndexType,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-    ) -> NonTensorData:
-        if index != () and index != slice(None):
-            raise RuntimeError("Cannot update a part of a NonTensorData.")
-        return self.update_(
-            input_dict_or_td=input_dict_or_td, clone=clone, non_blocking=non_blocking
-        )
-
-    def empty(self, recurse=False, *, device=NO_DEFAULT, batch_size=None, names=None):
-        if batch_size is not None and names is None:
-            names = None
-        elif names is None and self._has_names():
-            names = self.names
-        else:
-            names = None
-        return NonTensorData(
-            data=self.data,
-            batch_size=self.batch_size if batch_size is None else batch_size,
-            names=names,
-            device=self.device if device is NO_DEFAULT else device,
-        )
-
-    def _apply_nest(self, *args, out=None, **kwargs):
-        # kwargs["filter_empty"] = False
-        if out is not None:
-            return out
-        return self.empty(
-            batch_size=kwargs.get("batch_size"),
-            device=kwargs.get("device", NO_DEFAULT),
-            names=kwargs.get("names"),
-        )
-
-    def to_dict(self):
-        # override to_dict to return just the data
-        return self.data
-
-    def to_tensordict(self):
-        return self
-
-    @classmethod
-    def _stack_non_tensor(cls, list_of_non_tensor, dim=0):
-        # checks have been performed previously, so we're sure the list is non-empty
-        first = list_of_non_tensor[0]
-
-        def _check_equal(a, b):
-            if isinstance(a, _ACCEPTED_CLASSES) or isinstance(b, _ACCEPTED_CLASSES):
-                return (a == b).all()
-            if isinstance(a, np.ndarray) or isinstance(b, np.ndarray):
-                return (a == b).all()
-            try:
-                iseq = a == b
-            except Exception:
-                iseq = False
-            return iseq
-
-        if all(isinstance(data, NonTensorData) for data in list_of_non_tensor) and all(
-            _check_equal(data.data, first.data) for data in list_of_non_tensor[1:]
-        ):
-            batch_size = list(first.batch_size)
-            batch_size.insert(dim, len(list_of_non_tensor))
-            return NonTensorData(
-                data=first.data,
-                batch_size=batch_size,
-                names=first.names if first._has_names() else None,
-                device=first.device,
-            )
-
-        return NonTensorStack(*list_of_non_tensor, stack_dim=dim)
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        # A modified version of __torch_function__ to account for the different behaviour
-        # of stack, which should return lazy stacks of data of data does not match.
-        if func not in _TD_PASS_THROUGH or not all(
-            issubclass(t, (Tensor, cls)) for t in types
-        ):
-            return NotImplemented
-
-        escape_conversion = func in (torch.stack,)
-
-        if kwargs is None:
-            kwargs = {}
-
-        # get the output type from the arguments / keyword arguments
-        if len(args) > 0:
-            tensorclass_instance = args[0]
-        else:
-            tensorclass_instance = kwargs.get("input", kwargs["tensors"])
-        if isinstance(tensorclass_instance, (tuple, list)):
-            tensorclass_instance = tensorclass_instance[0]
-        if not escape_conversion:
-            args = tuple(_arg_to_tensordict(arg) for arg in args)
-            kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
-
-        result = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
-        if isinstance(result, (list, tuple)):
-            return result.__class__(
-                _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
-                for tensordict_result in result
-            )
-        if not escape_conversion:
-            return _from_tensordict_with_copy(tensorclass_instance, result)
-        return result
-
-    def _fast_apply(self, *args, **kwargs):
-        kwargs["filter_empty"] = False
-        return _wrap_method(self, "_fast_apply", self._tensordict._fast_apply)(
-            *args, **kwargs
-        )
-
-    def tolist(self):
-        """Converts the data in a list if the batch-size is non-empty.
-
-        If the batch-size is empty, returns the data.
-
-        """
-        if not self.batch_size:
-            return self.data
-        return [ntd.tolist() for ntd in self.unbind(0)]
-
-    def copy_(self, src: NonTensorData | NonTensorStack, non_blocking: bool = False):
-        return self.update_(src, non_blocking=non_blocking)
-
-    def clone(self, recurse: bool = True):
-        if recurse:
-            return type(self)(
-                data=deepcopy(self.data),
-                batch_size=self.batch_size,
-                device=self.device,
-                names=self.names,
-            )
-        return type(self)(
-            data=self.data,
-            batch_size=self.batch_size,
-            device=self.device,
-            names=self.names,
-        )
-
-    def share_memory_(self):
-        if self._tensordict._is_shared:
-            return self
-        with self.unlock_():
-            self._non_tensordict["data"] = _share_memory_nontensor(
-                self.data, manager=_mp_manager()
-            )
-        self._tensordict.share_memory_()
-        return self
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        executor=None,
-        futures=None,
-        inplace=True,
-        like=False,
-        memmaped: bool = False,
-        share_non_tensor: bool = False,
-    ):
-        if self._tensordict._is_memmap:
-            return self
-
-        _metadata = {}
-        if prefix is not None:
-            _metadata = copy(self._metadata)
-            if _metadata is None:
-                _metadata = {}
-            _metadata["memmap_prefix"] = prefix
-            _metadata["memmaped"] = memmaped
-
-        out = _memmap_(
-            self,
-            prefix=prefix,
-            copy_existing=copy_existing,
-            executor=executor,
-            futures=futures,
-            inplace=inplace,
-            like=like,
-            memmaped=memmaped,
-            share_non_tensor=share_non_tensor,
-        )
-        _metadata["_share_non_tensor"] = share_non_tensor
-        out._non_tensordict["_metadata"] = _metadata
-        if share_non_tensor:
-            out._non_tensordict["data"] = _share_memory_nontensor(
-                out.data, manager=_mp_manager()
-            )
-        return out
-
-    def _is_memmaped_from_above(self):
-        _metadata = self._metadata
-        if _metadata is None:
-            return False
-        return _metadata.get("memmaped", False)
-
-
-# For __setitem__ and _update_at_ we don't pass a kwarg but use a global variable instead
-_BREAK_ON_MEMMAP = True
-
-
-class NonTensorStack(LazyStackedTensorDict):
-    """A thin wrapper around LazyStackedTensorDict to make stack on non-tensor data easily recognizable.
-
-    A ``NonTensorStack`` is returned whenever :func:`~torch.stack` is called on
-    a list of :class:`~tensordict.NonTensorData` or ``NonTensorStack``.
-
-    Examples:
-        >>> from tensordict import NonTensorData
-        >>> import torch
-        >>> data = torch.stack([
-        ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
-        ...    for j in range(3)])
-        >>> print(data)
-        NonTensorStack(
-            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, ...,
-            batch_size=torch.Size([3, 2]),
-            device=None)
-
-    To obtain the values stored in a ``NonTensorStack``, call :class:`~.tolist`.
-
-    """
-
-    _is_non_tensor: bool = True
-
-    def __init__(self, *args, **kwargs):
-        super().__init__(*args, **kwargs)
-        if not all(is_non_tensor(item) for item in self.tensordicts):
-            raise RuntimeError("All tensordicts must be non-tensors.")
-
-    def tolist(self):
-        """Extracts the content of a :class:`tensordict.tensorclass.NonTensorStack` in a nested list.
-
-        Examples:
-            >>> from tensordict import NonTensorData
-            >>> import torch
-            >>> data = torch.stack([
-            ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
-            ...    for j in range(3)])
-            >>> data.tolist()
-            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, 2)]]
-
-        """
-        iterator = self.tensordicts if self.stack_dim == 0 else self.unbind(0)
-        return [td.tolist() for td in iterator]
-
-    @classmethod
-    def from_nontensordata(cls, non_tensor: NonTensorData):
-        data = non_tensor.data
-        prev = NonTensorData(data, batch_size=[], device=non_tensor.device)
-        for dim in reversed(non_tensor.shape):
-            prev = cls(*[prev.clone(False) for _ in range(dim)], stack_dim=0)
-        return prev
-
-    def __repr__(self):
-        selfrepr = str(self.tolist())
-        if len(selfrepr) > 50:
-            selfrepr = f"{selfrepr[:50]}..."
-        selfrepr = indent(selfrepr, prefix=4 * " ")
-        batch_size = indent(f"batch_size={self.batch_size}", prefix=4 * " ")
-        device = indent(f"device={self.device}", prefix=4 * " ")
-        return f"NonTensorStack(\n{selfrepr}," f"\n{batch_size}," f"\n{device})"
-
-    @classmethod
-    def lazy_stack(
-        cls,
-        items: Sequence[TensorDictBase],
-        dim: int = 0,
-        *,
-        device: DeviceType | None = None,
-        out: T | None = None,
-        stack_dim_name: str | None = None,
-    ) -> T:
-        result = super().lazy_stack(
-            items=items, dim=dim, out=out, stack_dim_name=stack_dim_name, device=device
-        )
-        if not isinstance(result, cls):
-            raise RuntimeError(
-                f"Unexpected result type: {type(result)} - expected one of {cls}."
-            )
-        return result
-
-    def to_dict(self) -> dict[str, Any]:
-        return self.tolist()
-
-    def to_tensordict(self):
-        return self
-
-    def _memmap_(
-        self,
-        *,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        executor=None,
-        futures=None,
-        inplace=True,
-        like=False,
-        memmaped: bool = False,
-        share_non_tensor: bool = False,
-    ) -> T:
-
-        memmaped_leaves = memmaped
-        if not memmaped and prefix is not None:
-            memmaped_leaves = True
-
-            def save_metadata(prefix=prefix, self=self):
-                data = self.tolist()
-                device = str(self.device) if self.device is not None else None
-                if not prefix.exists():
-                    os.makedirs(prefix, exist_ok=True)
-                jsondict = {
-                    "_type": str(self.__class__),
-                    "stack_dim": self.stack_dim,
-                    "device": device,
-                }
-                if _is_json_serializable(data):
-                    jsondict["data"] = data
-                else:
-                    jsondict["data"] = "pickle.pkl"
-                    with open(prefix / "pickle.pkl", "wb") as f:
-                        pickle.dump(data, f)
-                with open(prefix / "meta.json", "w") as f:
-                    json.dump(jsondict, f)
-
-            if executor is None:
-                save_metadata()
-            else:
-                futures.append(executor.submit(save_metadata))
-        # The leaves are all non-tensor or non-tensor stacks, and we already saved this on disk
-        # The only thing remaining to do is share the data between processes
-        results = []
-        for i, td in enumerate(self.tensordicts):
-            results.append(
-                td._memmap_(
-                    prefix=(prefix / str(i)) if prefix is not None else None,
-                    copy_existing=copy_existing,
-                    executor=executor,
-                    futures=futures,
-                    inplace=inplace,
-                    like=like,
-                    # tell the nested stack / nontensor that
-                    # no memmapping should be executed
-                    memmaped=memmaped_leaves,
-                    share_non_tensor=share_non_tensor,
-                )
-            )
-        if not inplace:
-            results = self.lazy_stack(results, dim=self.stack_dim)
-        else:
-            results = self
-        if not memmaped and prefix is not None:
-            results.__dict__["_path_to_memmap"] = prefix
-        return results
-
-    @classmethod
-    def _load_memmap(
-        cls, prefix: str, metadata: dict, *, out=None, **kwargs
-    ) -> LazyStackedTensorDict:
-        data = metadata.get("data", None)
-        if data is not None:
-            if isinstance(data, str):
-                with open(prefix / data, "rb") as file:
-                    data = pickle.load(file)
-            device = metadata["device"]
-            if device is not None:
-                device = torch.device(device)
-            return cls._from_list(data, device=device)
-        return super()._load_memmap(prefix=prefix, metadata=metadata, **kwargs)
-
-    @classmethod
-    def _from_list(cls, datalist: List, device: torch.device):
-        if all(isinstance(item, list) for item in datalist) and all(
-            len(item) == len(datalist[0]) for item in datalist
-        ):
-            return NonTensorStack(
-                *(cls._from_list(item, device=device) for item in datalist), stack_dim=0
-            )
-        return NonTensorStack(
-            *(
-                NonTensorData(data=item, device=device, batch_size=torch.Size([]))
-                for item in datalist
-            ),
-            stack_dim=0,
-        )
-
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        return self._update(
-            input_dict_or_td=input_dict_or_td,
-            clone=clone,
-            inplace=inplace,
-            keys_to_update=keys_to_update,
-        )
-
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        return self._update(
-            input_dict_or_td=input_dict_or_td,
-            clone=clone,
-            inplace=True,
-            keys_to_update=keys_to_update,
-        )
-
-    def _update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        keys_to_update: Sequence[NestedKey] | None = None,
-        break_on_memmap: bool = None,
-        non_blocking: bool = False,
-    ) -> T:
-        if inplace and self.is_locked and not (self._is_shared or self._is_memmap):
-            raise RuntimeError(_LOCK_ERROR)
-
-        if isinstance(input_dict_or_td, NonTensorData):
-            datalist = input_dict_or_td.data
-            for d in reversed(self.batch_size):
-                datalist = [datalist] * d
-            reconstructed = self._from_list(datalist, device=self.device)
-            return self.update(
-                reconstructed,
-                clone=clone,
-                inplace=inplace,
-                keys_to_update=keys_to_update,
-            )
-
-        memmap = False
-        if self._is_memmap and hasattr(self, "_path_to_memmap"):
-            if break_on_memmap is None:
-                global _BREAK_ON_MEMMAP
-                break_on_memmap = _BREAK_ON_MEMMAP
-            if not break_on_memmap:
-                raise RuntimeError(
-                    "Calling _update with break_on_memmap=False is not permitted if the stack has a path."
-                )
-            # this is the only way break_on_memmap is False
-            break_on_memmap = False
-            # remove memmap
-            if self._path_to_memmap.exists():
-                shutil.rmtree(self._path_to_memmap)
-            memmap = True
-
-        # update content
-        if isinstance(input_dict_or_td, NonTensorStack):
-            for leaf_dest, leaf_src in zip(
-                self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
-            ):
-                leaf_dest._update(
-                    leaf_src,
-                    clone=clone,
-                    inplace=inplace,
-                    keys_to_update=keys_to_update,
-                    break_on_memmap=break_on_memmap,
-                )
-            if memmap:
-                self._memmap_(prefix=self._path_to_memmap, inplace=True)
-        else:
-            raise NotImplementedError(
-                f"The data type {type(input_dict_or_td)} is not supported within {type(self).__name__}.update"
-            )
-        return self
-
-    def __setitem__(self, index, value):
-        memmap = False
-        if self._is_memmap and hasattr(self, "_path_to_memmap"):
-            global _BREAK_ON_MEMMAP
-            _BREAK_ON_MEMMAP = False
-            memmap = True
-        try:
-            super().__setitem__(index, value)
-            if memmap:
-                self._memmap_(prefix=self._path_to_memmap, inplace=True)
-        finally:
-            _BREAK_ON_MEMMAP = True
-
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        index: IndexType,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-    ) -> T:
-        memmap = False
-        if self._is_memmap and hasattr(self, "_path_to_memmap"):
-            global _BREAK_ON_MEMMAP
-            _BREAK_ON_MEMMAP = False
-            memmap = True
-        try:
-            super().update_at_(
-                input_dict_or_td, index, clone=clone, non_blocking=non_blocking
-            )
-            if memmap:
-                self._memmap_(prefix=self._path_to_memmap, inplace=True)
-        finally:
-            _BREAK_ON_MEMMAP = True
-        return self
-
-    @property
-    def data(self):
-        raise AttributeError
-
-
-_register_tensor_class(NonTensorStack)
-
-
-def _share_memory_nontensor(data, manager: Manager):
-    if isinstance(data, int):
-        return mp.Value(ctypes.c_int, data)
-    if isinstance(data, float):
-        return mp.Value(ctypes.c_double, data)
-    if isinstance(data, bool):
-        return mp.Value(ctypes.c_bool, data)
-    if isinstance(data, bytes):
-        return mp.Value(ctypes.c_byte, data)
-    if isinstance(data, dict):
-        result = manager.dict()
-        result.update(data)
-        return result
-    if isinstance(data, str):
-        result = mp.Array(ctypes.c_char, 100)
-        data = data.encode("utf-8")
-        result[: len(data)] = data
-        return result
-    if isinstance(data, list):
-        result = manager.list()
-        result.extend(data)
-        return result
-    # In all other cases, we just return the tensor. It's ok because the content
-    # will be passed to the remote process using regular serialization. We will
-    # lock the update in _update_shared_nontensor though.
-    return data
-
-
-def _from_shared_nontensor(nontensor):
-    if isinstance(nontensor, multiprocessing.managers.ListProxy):
-        return list(nontensor)
-    if isinstance(nontensor, multiprocessing.managers.DictProxy):
-        return dict(nontensor)
-    if isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
-        return nontensor.value
-    if isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
-        byte_list = []
-        for byte in nontensor:
-            if byte == b"\x00":
-                break
-            byte_list.append(byte)
-        return b"".join(byte_list).decode("utf-8")
-    return nontensor
-
-
-def _update_shared_nontensor(nontensor, val):
-    if isinstance(nontensor, multiprocessing.managers.ListProxy):
-        nontensor[:] = []
-        nontensor.extend(val)
-    elif isinstance(nontensor, multiprocessing.managers.DictProxy):
-        nontensor.clear()
-        nontensor.update(val)
-    elif isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
-        nontensor.value = val
-    elif isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
-        val = val.encode("utf-8")
-        for i, byte in enumerate(nontensor):
-            if i < len(val):
-                v = val[i]
-                nontensor[i] = v
-            elif byte == b"\x00":
-                break
-            else:
-                nontensor[i] = b"\x00"
-        # nontensor[0] = val.encode("utf-8")
-    else:
-        raise NotImplementedError(
-            f"Updating {type(nontensor).__name__} within a shared/memmaped structure is not supported."
-        )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import ctypes
+
+import dataclasses
+import functools
+import inspect
+import json
+import multiprocessing.managers
+import multiprocessing.sharedctypes
+import numbers
+import os
+import pickle
+import shutil
+
+import sys
+import warnings
+from copy import copy, deepcopy
+from dataclasses import dataclass
+from pathlib import Path
+from textwrap import indent
+from typing import Any, Callable, get_type_hints, List, Sequence, TypeVar
+
+import numpy as np
+import tensordict as tensordict_lib
+
+import torch
+from tensordict import LazyStackedTensorDict
+from tensordict._pytree import _register_td_node
+from tensordict._td import is_tensor_collection, NO_DEFAULT, TensorDict, TensorDictBase
+from tensordict._tensordict import _unravel_key_to_tuple
+from tensordict._torch_func import TD_HANDLED_FUNCTIONS
+from tensordict.base import (
+    _ACCEPTED_CLASSES,
+    _is_tensor_collection,
+    _register_tensor_class,
+    CompatibleType,
+)
+from tensordict.utils import (
+    _get_repr,
+    _is_json_serializable,
+    _LOCK_ERROR,
+    DeviceType,
+    IndexType,
+    is_non_tensor,
+    is_tensorclass,
+    KeyDependentDefaultDict,
+    NestedKey,
+)
+from torch import multiprocessing as mp, Tensor
+from torch.multiprocessing import Manager
+from torch.utils._pytree import tree_map
+
+T = TypeVar("T", bound=TensorDictBase)
+# We use an abstract AnyType instead of Any because Any isn't recognised as a type for python < 3.10
+major, minor = sys.version_info[:2]
+if (major, minor) < (3, 11):
+
+    class _AnyType:
+        def __subclasscheck__(self, subclass):
+            return False
+
+else:
+    _AnyType = Any
+
+# methods where non_tensordict data should be cleared in the return value
+_CLEAR_METADATA = {"all", "any"}
+# torch functions where we can wrap the corresponding TensorDict version
+_TD_PASS_THROUGH = {
+    torch.unbind,
+    torch.full_like,
+    torch.zeros_like,
+    torch.ones_like,
+    torch.rand_like,
+    torch.empty_like,
+    torch.randn_like,
+    torch.clone,
+    torch.squeeze,
+    torch.unsqueeze,
+    torch.split,
+    torch.permute,
+    torch.split,
+    torch.stack,
+    torch.cat,
+    torch.gather,
+}
+# Methods to be executed from tensordict, any ref to self means 'tensorclass'
+_METHOD_FROM_TD = [
+    "gather",
+    "replace",
+]
+# Methods to be executed from tensordict, any ref to self means 'self._tensordict'
+_FALLBACK_METHOD_FROM_TD = [
+    "__abs__",
+    "__add__",
+    "__iadd__",
+    "__imul__",
+    "__ipow__",
+    "__isub__",
+    "__itruediv__",
+    "__mul__",
+    "__pow__",
+    "__sub__",
+    "__truediv__",
+    "_add_batch_dim",
+    "apply",
+    "_apply_nest",
+    "_fast_apply",
+    "apply_",
+    "named_apply",
+    "_check_unlock",
+    "unsqueeze",
+    "squeeze",
+    "_erase_names",  # TODO: must be specialized
+    "_exclude",  # TODO: must be specialized
+    "_get_str",
+    "_get_tuple",
+    "_set_at_tuple",
+    "_has_names",
+    "_propagate_lock",
+    "_propagate_unlock",
+    "_remove_batch_dim",
+    "is_memmap",
+    "is_shared",
+    "_select",  # TODO: must be specialized
+    "_set_str",
+    "_set_tuple",
+    "all",
+    "any",
+    "empty",
+    "exclude",
+    "expand",
+    "expand_as",
+    "is_empty",
+    "is_shared",
+    "items",
+    "keys",
+    "lock_",
+    "masked_fill",
+    "masked_fill_",
+    "permute",
+    "flatten",
+    "unflatten",
+    "ndimension",
+    "rename_",  # TODO: must be specialized
+    "reshape",
+    "select",
+    "to",
+    "transpose",
+    "unlock_",
+    "values",
+    "view",
+    "zero_",
+    "add",
+    "add_",
+    "mul",
+    "mul_",
+    "abs",
+    "abs_",
+    "acos",
+    "acos_",
+    "exp",
+    "exp_",
+    "neg",
+    "neg_",
+    "reciprocal",
+    "reciprocal_",
+    "sigmoid",
+    "sigmoid_",
+    "sign",
+    "sign_",
+    "sin",
+    "sin_",
+    "sinh",
+    "sinh_",
+    "tan",
+    "tan_",
+    "tanh",
+    "tanh_",
+    "trunc",
+    "trunc_",
+    "norm",
+    "lgamma",
+    "lgamma_",
+    "frac",
+    "frac_",
+    "expm1",
+    "expm1_",
+    "log",
+    "log_",
+    "log10",
+    "log10_",
+    "log1p",
+    "log1p_",
+    "log2",
+    "log2_",
+    "ceil",
+    "ceil_",
+    "floor",
+    "floor_",
+    "round",
+    "round_",
+    "erf",
+    "erf_",
+    "erfc",
+    "erfc_",
+    "asin",
+    "asin_",
+    "atan",
+    "atan_",
+    "cos",
+    "cos_",
+    "cosh",
+    "cosh_",
+    "lerp",
+    "lerp_",
+    "addcdiv",
+    "addcdiv_",
+    "addcmul",
+    "addcmul_",
+    "sub",
+    "sub_",
+    "maximum_",
+    "maximum",
+    "minimum_",
+    "minimum",
+    "clamp_max_",
+    "clamp_max",
+    "clamp_min_",
+    "clamp_min",
+    "pow",
+    "pow_",
+    "div",
+    "div_",
+    "sqrt",
+    "sqrt_",
+]
+_FALLBACK_METHOD_FROM_TD_COPY = [
+    "_clone",  # TODO: must be specialized
+    "clone",  # TODO: must be specialized
+    "copy",  # TODO: must be specialized
+]
+
+
+class tensorclass:
+    """A decorator to create :obj:`tensorclass` classes.
+
+    :obj:`tensorclass` classes are specialized :obj:`dataclass` instances that
+    can execute some pre-defined tensor operations out of the box, such as
+    indexing, item assignment, reshaping, casting to device or storage and many
+    others.
+
+    Examples:
+        >>> from tensordict import tensorclass
+        >>> import torch
+        >>> from typing import Optional
+        >>>
+        >>> @tensorclass
+        ... class MyData:
+        ...     X: torch.Tensor
+        ...     y: torch.Tensor
+        ...     z: str
+        ...     def expand_and_mask(self):
+        ...         X = self.X.unsqueeze(-1).expand_as(self.y)
+        ...         X = X[self.y]
+        ...         return X
+        ...
+        >>> data = MyData(
+        ...     X=torch.ones(3, 4, 1),
+        ...     y=torch.zeros(3, 4, 2, 2, dtype=torch.bool),
+        ...     z="test"
+        ...     batch_size=[3, 4])
+        >>> print(data)
+        MyData(
+            X=Tensor(torch.Size([3, 4, 1]), dtype=torch.float32),
+            y=Tensor(torch.Size([3, 4, 2, 2]), dtype=torch.bool),
+            z="test"
+            batch_size=[3, 4],
+            device=None,
+            is_shared=False)
+        >>> print(data.expand_and_mask())
+        tensor([])
+
+    It is also possible to nest tensorclasses instances within each other:
+        Examples:
+        >>> from tensordict import tensorclass
+        >>> import torch
+        >>> from typing import Optional
+        >>>
+        >>> @tensorclass
+        ... class NestingMyData:
+        ...     nested: MyData
+        ...
+        >>> nesting_data = NestingMyData(nested=data, batch_size=[3, 4])
+        >>> # although the data is stored as a TensorDict, the type hint helps us
+        >>> # to appropriately cast the data to the right type
+        >>> assert isinstance(nesting_data.nested, type(data))
+
+
+    """
+
+    def __new__(cls, autocast: bool = False):
+        if not isinstance(autocast, bool):
+            clz = autocast
+            self = super().__new__(cls)
+            self.__init__(autocast=False)
+            return self.__call__(clz)
+        return super().__new__(cls)
+
+    def __init__(self, autocast: bool):
+        self.autocast = autocast
+
+    def __call__(self, cls):
+        clz = _tensorclass(cls)
+        clz.autocast = self.autocast
+        return clz
+
+
+def _tensorclass(cls: T) -> T:
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if func not in _TD_PASS_THROUGH or not all(
+            issubclass(t, (Tensor, cls, TensorDictBase)) for t in types
+        ):
+            return NotImplemented
+
+        if kwargs is None:
+            kwargs = {}
+
+        # get the output type from the arguments / keyword arguments
+        if len(args) > 0:
+            tensorclass_instance = args[0]
+        else:
+            tensorclass_instance = kwargs.get("input", kwargs["tensors"])
+        if isinstance(tensorclass_instance, (tuple, list)):
+            tensorclass_instance = tensorclass_instance[0]
+        args = tuple(_arg_to_tensordict(arg) for arg in args)
+        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
+
+        result = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+        if isinstance(result, (list, tuple)):
+            return result.__class__(
+                _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
+                for tensordict_result in result
+            )
+        return _from_tensordict_with_copy(tensorclass_instance, result)
+
+    _is_non_tensor = getattr(cls, "_is_non_tensor", False)
+
+    cls = dataclass(cls)
+    expected_keys = set(cls.__dataclass_fields__)
+
+    for attr in cls.__dataclass_fields__:
+        if attr in dir(TensorDict) and attr not in ("_is_non_tensor", "data"):
+            raise AttributeError(
+                f"Attribute name {attr} can't be used with @tensorclass"
+            )
+
+    cls.fields = classmethod(lambda cls: dataclasses.fields(cls))
+    for field in cls.fields():
+        if hasattr(cls, field.name):
+            delattr(cls, field.name)
+
+    _get_type_hints(cls)
+    cls.__init__ = _init_wrapper(cls.__init__)
+    cls._from_tensordict = classmethod(_from_tensordict_wrapper(expected_keys))
+    cls.from_tensordict = cls._from_tensordict
+    if not hasattr(cls, "__torch_function__"):
+        cls.__torch_function__ = classmethod(__torch_function__)
+    cls.__getstate__ = _getstate
+    cls.__setstate__ = _setstate
+    # cls.__getattribute__ = object.__getattribute__
+    cls.__getattr__ = _getattr
+    cls.__setattr__ = _setattr_wrapper(cls.__setattr__, expected_keys)
+    # cls.__getattr__ = _getattr
+    cls.__getitem__ = _getitem
+    cls.__getitems__ = _getitem
+    cls.__setitem__ = _setitem
+    cls.__repr__ = _repr
+    cls.__len__ = _len
+    cls.__eq__ = _eq
+    cls.__ne__ = _ne
+    cls.__or__ = _or
+    cls.__xor__ = _xor
+    cls.__bool__ = _bool
+    cls.non_tensor_items = _non_tensor_items
+    # if not hasattr(cls, "keys"):
+    #     cls.keys = _keys
+    # if not hasattr(cls, "values"):
+    #     cls.values = _values
+    # if not hasattr(cls, "items"):
+    #     cls.items = _items
+    if not hasattr(cls, "set"):
+        cls.set = _set
+    if not hasattr(cls, "set_at_"):
+        cls.set_at_ = _set_at_
+    if not hasattr(cls, "del_"):
+        cls.del_ = _del_
+    if not hasattr(cls, "get"):
+        cls.get = _get
+    if not hasattr(cls, "get_at"):
+        cls.get_at = _get_at
+    if not hasattr(cls, "unbind"):
+        cls.unbind = _unbind
+    cls._unbind = _unbind
+    if not hasattr(cls, "state_dict"):
+        cls.state_dict = _state_dict
+    if not hasattr(cls, "load_state_dict"):
+        cls.load_state_dict = _load_state_dict
+    if not hasattr(cls, "_memmap_"):
+        cls._memmap_ = _memmap_
+    if not hasattr(cls, "share_memory_"):
+        cls.share_memory_ = _share_memory_
+    if not hasattr(cls, "update"):
+        cls.update = _update
+    if not hasattr(cls, "update_"):
+        cls.update_ = _update_
+    if not hasattr(cls, "update_at_"):
+        cls.update_at_ = _update_at_
+    for method_name in _METHOD_FROM_TD:
+        if not hasattr(cls, method_name):
+            setattr(cls, method_name, getattr(TensorDict, method_name))
+    for method_name in _FALLBACK_METHOD_FROM_TD:
+        if not hasattr(cls, method_name):
+            setattr(cls, method_name, _wrap_td_method(method_name))
+    for method_name in _FALLBACK_METHOD_FROM_TD_COPY:
+        if not hasattr(cls, method_name):
+            setattr(
+                cls,
+                method_name,
+                _wrap_td_method(method_name, copy_non_tensor=True),
+            )
+
+    if not hasattr(cls, "_apply_nest"):
+        cls._apply_nest = TensorDict._apply_nest
+
+    if not hasattr(cls, "filter_non_tensor_data"):
+        cls.filter_non_tensor_data = _filter_non_tensor_data
+    cls.__enter__ = __enter__
+    cls.__exit__ = __exit__
+
+    # Memmap
+    if not hasattr(cls, "memmap_like"):
+        cls.memmap_like = TensorDictBase.memmap_like
+    if not hasattr(cls, "memmap_"):
+        cls.memmap_ = TensorDictBase.memmap_
+    if not hasattr(cls, "memmap"):
+        cls.memmap = TensorDictBase.memmap
+    if not hasattr(cls, "load_memmap"):
+        cls.load_memmap = TensorDictBase.load_memmap
+    if not hasattr(cls, "_load_memmap"):
+        cls._load_memmap = classmethod(_load_memmap)
+    if not hasattr(cls, "from_dict"):
+        cls.from_dict = classmethod(_from_dict)
+    if not hasattr(cls, "from_dict_instance"):
+        cls.from_dict_instance = _from_dict_instance
+
+    for attr in TensorDict.__dict__.keys():
+        func = getattr(TensorDict, attr)
+        if inspect.ismethod(func) and attr not in cls.__dict__:
+            tdcls = func.__self__
+            if issubclass(tdcls, TensorDictBase):  # detects classmethods
+                setattr(cls, attr, _wrap_classmethod(tdcls, cls, func))
+
+    if not hasattr(cls, "to_tensordict"):
+        cls.to_tensordict = _to_tensordict
+    if not hasattr(cls, "device"):
+        cls.device = property(_device, _device_setter)
+    if not hasattr(cls, "batch_size"):
+        cls.batch_size = property(_batch_size, _batch_size_setter)
+    if not hasattr(cls, "names"):
+        cls.names = property(_names, _names_setter)
+    if not hasattr(cls, "to_dict"):
+        cls.to_dict = _to_dict
+
+    cls.__doc__ = f"{cls.__name__}{inspect.signature(cls)}"
+
+    _register_tensor_class(cls)
+    _register_td_node(cls)
+
+    # faster than doing instance checks
+    cls._is_non_tensor = _is_non_tensor
+    cls._is_tensorclass = True
+
+    from tensordict import _pytree
+
+    _pytree._CONSTRUCTORS[cls] = _pytree._tensorclass_constructor
+    return cls
+
+
+def _filter_non_tensor_data(self):
+    return super(type(self), self).__getattribute__("_tensordict")
+
+
+def _arg_to_tensordict(arg):
+    # if arg is a tensorclass or sequence of tensorclasses, extract the underlying
+    # tensordicts and return those instead
+    if is_tensorclass(arg):
+        return arg._tensordict
+    elif isinstance(arg, (tuple, list)) and all(is_tensorclass(item) for item in arg):
+        return arg.__class__(item._tensordict for item in arg)
+    return arg
+
+
+def _from_tensordict_with_copy(tc, tensordict):
+    # creates a new tensorclass with the same type as tc, and a copy of the
+    # non_tensordict data
+    return tc._from_tensordict(
+        tensordict=tensordict, non_tensordict=copy(tc._non_tensordict)
+    )
+
+
+def _from_tensordict_with_none(tc, tensordict):
+    # creates a new tensorclass with the same type as tc, and all non_tensordict entries
+    # set to None
+    return tc._from_tensordict(
+        tensordict=tensordict,
+        non_tensordict={key: None for key in tc._non_tensordict},
+    )
+
+
+def _init_wrapper(init: Callable) -> Callable:
+    init_sig = inspect.signature(init)
+    params = list(init_sig.parameters.values())
+    # drop first entry of params which corresponds to self and isn't passed by the user
+    required_params = [p.name for p in params[1:] if p.default is inspect._empty]
+
+    @functools.wraps(init)
+    def wrapper(
+        self,
+        *args: Any,
+        batch_size: Sequence[int] | torch.Size | int = None,
+        device: DeviceType | None = None,
+        names: List[str] | None = None,
+        **kwargs,
+    ):
+
+        for value, key in zip(args, self.__dataclass_fields__):
+            if key in kwargs:
+                raise ValueError(f"The key {key} is already set in kwargs")
+            kwargs[key] = value
+        if batch_size is None:
+            batch_size = torch.Size([])
+        for key, field in self.__dataclass_fields__.items():
+            if field.default_factory is not dataclasses.MISSING:
+                default = field.default_factory()
+            else:
+                default = field.default
+            if default not in (None, dataclasses.MISSING):
+                kwargs.setdefault(key, default)
+
+        missing_params = [p for p in required_params if p not in kwargs]
+        if missing_params:
+            n_missing = len(missing_params)
+            raise TypeError(
+                f"{self.__class__.__name__}.__init__() missing {n_missing} "
+                f"required positional argument{'' if n_missing == 1 else 's'}: "
+                f"""{", ".join(f"'{name}'" for name in missing_params)}"""
+            )
+
+        self._tensordict = TensorDict(
+            {},
+            batch_size=torch.Size(batch_size),
+            device=device,
+            names=names,
+            _run_checks=False,
+        )
+        self._non_tensordict = {}
+
+        init(self, **kwargs)
+
+    new_params = [
+        inspect.Parameter("batch_size", inspect.Parameter.KEYWORD_ONLY),
+        inspect.Parameter("device", inspect.Parameter.KEYWORD_ONLY, default=None),
+        inspect.Parameter("names", inspect.Parameter.KEYWORD_ONLY, default=None),
+    ]
+    wrapper.__signature__ = init_sig.replace(parameters=params + new_params)
+
+    return wrapper
+
+
+_cast_funcs = KeyDependentDefaultDict(lambda cls: cls)
+_cast_funcs[torch.Tensor] = torch.as_tensor
+_cast_funcs[np.ndarray] = np.asarray
+
+
+def _get_type_hints(cls, with_locals=False):
+    #######
+    # Set proper type annotations for autocasting to tensordict/tensorclass
+    #
+    # by updating locals, we can allow this to be used within a function
+    # local-cross referencing will not work though
+    # def foo():
+    #     @tensorclass
+    #     class MyOtherClass:
+    #         x: torch.Tensor
+    #     @tensorclass
+    #     class MyClass:
+    #         x: MyClass # works
+    #         y: MyOtherClass # fails
+    #
+    # In this case, we will use the get_parent_local function to get the locals
+    # from the parent frame and so recursively until we can find the class.
+
+    if with_locals:
+        # This function gets the parent frame recursively until we can find the current class.
+        # Any exception leads to this to be None and auto-casting will be disabled
+        localns = locals()
+        localns = copy(localns)
+
+        def get_parent_locals(cls, localns=localns):
+            # Get the current frame
+            frame = inspect.currentframe()
+            try:
+                parent_locs = localns
+                while cls.__name__ not in parent_locs:
+                    # Get the parent frame
+                    parent_frame = frame.f_back
+                    # Get the locals dictionary of the parent frame
+                    parent_locs = parent_frame.f_locals
+                    frame = parent_frame
+            except Exception:
+                localns.setdefault(cls.__name__, cls)
+                return localns
+            finally:
+                # Clean up the frame reference
+                del frame
+            return copy(parent_locs)
+
+        localns = get_parent_locals(cls)
+    else:
+        localns = None
+
+    globalns = None
+
+    try:
+        cls._type_hints = get_type_hints(
+            cls,
+            localns=localns,
+            # globalns=globals(),
+        )
+        cls._type_hints = {
+            key: val if isinstance(val, type) else _AnyType
+            for key, val in cls._type_hints.items()
+        }
+    except NameError:
+        if not with_locals:
+            return _get_type_hints(cls, with_locals=True)
+        cls._set_dict_warn_msg = (
+            "A NameError occurred while trying to retrieve a type annotation. "
+            "This can occur when a tensorclass references another locally defined "
+            "tensorclass. "
+            f"As a result type hints cannot be read and {cls}.from_dict(...) "
+            f"or `{cls}.set` will not attempt to map dictionaries to "
+            "the relevant tensorclass. To resolve this issue, consider defining "
+            "your tensorclass globally."
+        )
+        cls._type_hints = None
+    except TypeError:
+        # This is a rather common case where type annotation is like
+        # class MyClass:
+        #     x: int | str
+        # in which case get_type_hints doesn't work (it does work
+        # however with old-school Optional or Union...)
+        # We simply differ the warning till _set() is called
+        cls._set_dict_warn_msg = (
+            "A TypeError occurred when trying to retrieve a type annotation. "
+            "This may be caused by annotations that use plain `|` instead of typing.Union "
+            "or typing.Optional which are supported. If you wish to use the feature "
+            "of setting dict as attributes with automapping to tensordict/tensorclass "
+            "(`my_obj.attr = dict(...)`), consider re-writing the tensorclass with "
+            "traditional type annotations."
+        )
+        cls._type_hints = None
+
+
+def _from_tensordict_wrapper(expected_keys):
+    def wrapper(cls, tensordict, non_tensordict=None):  # noqa: D417
+        """Tensor class wrapper to instantiate a new tensor class object.
+
+        Args:
+            tensordict (TensorDict): Dictionary of tensor types
+            non_tensordict (dict): Dictionary with non-tensor and nested tensor class objects
+
+        """
+        if not isinstance(tensordict, TensorDictBase):
+            raise RuntimeError(
+                f"Expected a TensorDictBase instance but got {type(tensordict)}"
+            )
+        # Validating keys of tensordict
+        for key in tensordict.keys():
+            if key not in expected_keys:
+                raise ValueError(
+                    f"Keys from the tensordict ({set(tensordict.keys())}) must "
+                    f"correspond to the class attributes ({expected_keys})."
+                )
+
+        # Validating non-tensor keys and for key clash
+        tensor_keys = set(tensordict.keys())
+        if non_tensordict is not None:
+            for key in list(non_tensordict.keys()):
+                if key not in expected_keys:
+                    raise ValueError(
+                        f"Keys from the non-tensor data ({set(non_tensordict.keys())}) must "
+                        f"correspond to the class attributes ({expected_keys})."
+                    )
+                if key in tensor_keys:
+                    if non_tensordict[key] is None:
+                        del non_tensordict[key]
+                        continue
+                    raise KeyError(
+                        f"{key} is present in both tensor and non-tensor dicts."
+                    )
+        # bypass initialisation. this means we don't incur any overhead creating an
+        # empty tensordict and writing values to it. we can skip this because we already
+        # have a tensordict to use as the underlying tensordict
+        tc = cls.__new__(cls)
+        tc.__dict__["_tensordict"] = tensordict
+
+        tc.__dict__["_non_tensordict"] = (
+            non_tensordict if non_tensordict is not None else {}
+        )
+        # since we aren't calling the dataclass init method, we need to manually check
+        # whether a __post_init__ method has been defined and invoke it if so
+        if hasattr(tc, "__post_init__"):
+            tc.__post_init__()
+        return tc
+
+    return wrapper
+
+
+def _memmap_(
+    self,
+    *,
+    prefix: str | None = None,
+    copy_existing: bool = False,
+    executor=None,
+    futures=None,
+    inplace=True,
+    like=False,
+    memmaped: bool = False,
+    share_non_tensor: bool = False,
+):
+    _non_tensordict = copy(self._non_tensordict)
+    cls = self.__class__
+
+    if not memmaped and prefix is not None:
+        prefix = Path(prefix)
+        if not prefix.exists():
+            os.makedirs(prefix, exist_ok=True)
+
+        def save_metadata(cls=cls, _non_tensordict=_non_tensordict, prefix=prefix):
+            with open(prefix / "meta.json", "w") as f:
+                metadata = {"_type": str(cls)}
+                to_pickle = {}
+                for key, value in _non_tensordict.items():
+                    value = _from_shared_nontensor(value)
+                    if _is_json_serializable(value):
+                        metadata[key] = value
+                    else:
+                        to_pickle[key] = value
+                json.dump(metadata, f)
+                if to_pickle:
+                    with open(prefix / "other.pickle", "wb") as pickle_file:
+                        pickle.dump(to_pickle, pickle_file)
+
+        if executor is None:
+            save_metadata()
+        else:
+            futures.append(executor.submit(save_metadata))
+
+        prefix = prefix / "_tensordict"
+
+    td = self._tensordict._memmap_(
+        prefix=prefix,
+        executor=executor,
+        futures=futures,
+        inplace=inplace,
+        like=like,
+        copy_existing=copy_existing,
+        share_non_tensor=share_non_tensor,
+    )
+    td._device = torch.device("cpu")
+    if not inplace:
+        result = cls._from_tensordict(td, _non_tensordict)
+    else:
+        result = self
+    return result
+
+
+def _share_memory_(self):
+    self._tensordict.share_memory_()
+    return self
+
+
+def _load_memmap(cls, prefix: Path, metadata: dict, **kwargs):
+    non_tensordict = copy(metadata)
+    del non_tensordict["_type"]
+    if os.path.exists(prefix / "other.pickle"):
+        with open(prefix / "other.pickle", "rb") as pickle_file:
+            non_tensordict.update(pickle.load(pickle_file))
+    td = TensorDict.load_memmap(prefix / "_tensordict", **kwargs, non_blocking=False)
+    return cls._from_tensordict(td, non_tensordict)
+
+
+def __enter__(self, *args, **kwargs):
+    return self._tensordict.__enter__(*args, **kwargs)
+
+
+def __exit__(self, *args, **kwargs):
+    return self._tensordict.__exit__(*args, **kwargs)
+
+
+def _getstate(self) -> dict[str, Any]:
+    """Returns a state dict which consists of tensor and non_tensor dicts for serialization.
+
+    Returns:
+        dictionary of state of tensor class
+
+    """
+    return {"tensordict": self._tensordict, "non_tensordict": self._non_tensordict}
+
+
+def _setstate(self, state: dict[str, Any]) -> None:  # noqa: D417
+    """Used to set the state of an object using state parameter.
+
+    Args:
+        state (dict): State parameter to set the object
+    """
+    self._tensordict = state.get("tensordict", None)
+    self._non_tensordict = state.get("non_tensordict", None)
+
+
+def _getattr(self, item: str) -> Any:
+    # if not item.startswith("__"):
+    __dict__ = self.__dict__
+    _non_tensordict = __dict__.get("_non_tensordict")
+    if _non_tensordict is not None:
+        out = _non_tensordict.get(item, NO_DEFAULT)
+        if out is not NO_DEFAULT:
+            if (
+                isinstance(self, NonTensorData)
+                and item == "data"
+                and (self._is_shared or self._is_memmap)
+            ):
+                return _from_shared_nontensor(out)
+            return out
+    _tensordict = __dict__.get("_tensordict")
+    if _tensordict is not None:
+        out = _tensordict._get_str(item, default=None)
+        if out is not None:
+            return out
+        out = getattr(_tensordict, item, NO_DEFAULT)
+        if out is not NO_DEFAULT:
+            if not callable(out):
+                return out
+            return _wrap_method(self, item, out)
+    raise AttributeError
+
+
+SET_ATTRIBUTES = ("batch_size", "device", "_locked_tensordicts", "names")
+
+
+def _setattr_wrapper(setattr_: Callable, expected_keys: set[str]) -> Callable:
+    @functools.wraps(setattr_)
+    def wrapper(self, key: str, value: Any) -> None:  # noqa: D417
+        """Set the value of an attribute for the tensor class object.
+
+        Args:
+            key (str): the name of the attribute to set
+            value (any): the value to set for the attribute
+
+        """
+        __dict__ = self.__dict__
+        if (
+            "_tensordict" not in __dict__
+            or "_non_tensordict" not in __dict__
+            or key in SET_ATTRIBUTES
+        ):
+            return setattr_(self, key, value)
+
+        out = self.set(key, value)
+        if out is not self:
+            raise RuntimeError(
+                "Cannot set attribute on a locked tensorclass, even if "
+                "clone_on_set is set to True. Use my_obj.set(...) instead."
+            )
+
+    return wrapper
+
+
+def _wrap_td_method(funcname, *, copy_non_tensor=False):
+    def wrapped_func(self, *args, **kwargs):
+        td = super(type(self), self).__getattribute__("_tensordict")
+        result = getattr(td, funcname)(*args, **kwargs)
+
+        def check_out(kwargs, result):
+            out = kwargs.get("out")
+            if out is result:
+                # No need to transform output
+                return True
+            return False
+
+        if isinstance(result, TensorDictBase) and not check_out(kwargs, result):
+            if result is td:
+                return self
+            nontd = super(type(self), self).__getattribute__("_non_tensordict")
+            if copy_non_tensor:
+                # use tree_map to copy
+                nontd = tree_map(lambda x: x, nontd)
+            return super(type(self), self).__getattribute__("_from_tensordict")(
+                result, nontd
+            )
+        return result
+
+    return wrapped_func
+
+
+def _wrap_method(self, attr, func):
+    warnings.warn(
+        f"The method {func} wasn't explicitly implemented for tensorclass. "
+        f"This fallback will be deprecated in future releases because it is inefficient "
+        f"and non-compilable. Please raise an issue in tensordict repo to support this method!"
+    )
+
+    @functools.wraps(func)
+    def wrapped_func(*args, **kwargs):
+        args = tuple(_arg_to_tensordict(arg) for arg in args)
+        kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
+        res = func(*args, **kwargs)
+        if isinstance(res, TensorDictBase):
+            if attr.endswith("_"):
+                # in-place operation, return the current object
+                return self
+            elif attr in _CLEAR_METADATA:
+                # this is an attribute where copying the metadata makes no sense, e.g.
+                # .all or .any, so we replace all values with None
+                return self._from_tensordict(
+                    res, {k: None for k in self._non_tensordict}
+                )
+            # create a new tensorclass from res and copy the metadata from self
+            return self._from_tensordict(res, copy(self._non_tensordict))
+        return res
+
+    return wrapped_func
+
+
+def _update(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    clone: bool = False,
+    inplace: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update(input_dict_or_td.__dict__["_tensordict"])
+        self._non_tensordict.update(input_dict_or_td.__dict__["_non_tensordict"])
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        clone=clone,
+        inplace=inplace,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
+def _update_(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    clone: bool = False,
+    inplace: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update_(input_dict_or_td._tensordict)
+        self._non_tensordict.update(input_dict_or_td._non_tensordict)
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update_(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        clone=clone,
+        inplace=inplace,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
+def _update_at_(
+    self,
+    input_dict_or_td: dict[str, CompatibleType] | T,
+    index: IndexType,
+    clone: bool = False,
+    *,
+    keys_to_update: Sequence[NestedKey] | None = None,
+    non_blocking: bool = False,
+):
+    if isinstance(input_dict_or_td, dict):
+        input_dict_or_td = self.from_dict(input_dict_or_td, batch_size=self.batch_size)
+
+    if is_tensorclass(input_dict_or_td):
+        self._tensordict.update(input_dict_or_td._tensordict)
+        self._non_tensordict.update(input_dict_or_td._non_tensordict)
+        return self
+
+    non_tensordict = {}
+    for key, value in input_dict_or_td.items():
+        if is_non_tensor(value):
+            non_tensordict[key] = value.data
+
+    self._tensordict.update_at_(
+        input_dict_or_td.exclude(*non_tensordict.keys()),
+        index=index,
+        clone=clone,
+        keys_to_update=keys_to_update,
+        non_blocking=non_blocking,
+    )
+    self._non_tensordict.update(non_tensordict)
+    return self
+
+
+def _wrap_classmethod(td_cls, cls, func):
+    @functools.wraps(func)
+    def wrapped_func(*args, **kwargs):
+        res = func.__get__(td_cls)(*args, **kwargs)
+        # res = func(*args, **kwargs)
+        if isinstance(res, TensorDictBase):
+            # create a new tensorclass from res and copy the metadata from self
+            return cls._from_tensordict(res)
+        return res
+
+    return wrapped_func
+
+
+def _getitem(self, item: NestedKey) -> Any:
+    """Retrieve the class object at the given index. Indexing will happen for nested tensors as well.
+
+    Args:
+       item (int or any other valid index type): index of the object to retrieve
+
+    Returns:
+        Tensor class object at the given index
+
+    """
+    if isinstance(item, str) or (
+        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
+    ):
+        raise ValueError(f"Invalid indexing arguments: {item}.")
+    tensor_res = self._tensordict[item]
+    return _from_tensordict_with_copy(self, tensor_res)  # device=res.device)
+
+
+def _setitem(self, item: NestedKey, value: Any) -> None:  # noqa: D417
+    """Set the value of the Tensor class object at the given index. Note that there is no strict validation on non-tensor values.
+
+    Args:
+        item (int or any other valid index type): index of the object to set
+        value (any): value to set for the item
+
+    """
+    if isinstance(item, str) or (
+        isinstance(item, tuple) and all(isinstance(_item, str) for _item in item)
+    ):
+        raise ValueError(f"Invalid indexing arguments: {item}.")
+
+    if not is_tensorclass(value) and not isinstance(
+        value, (TensorDictBase, numbers.Number, Tensor)
+    ):
+        raise ValueError(
+            f"__setitem__ only supports tensorclasses, tensordicts,"
+            f" numeric scalars and tensors. Got {type(value)}"
+        )
+
+    if is_tensorclass(value):
+        if not isinstance(value, self.__class__):
+            self_keys = set().union(self._non_tensordict, self._tensordict.keys())
+            value_keys = set().union(value._non_tensordict, value._tensordict.keys())
+            if self_keys != value_keys:
+                # if tensorclass but different class ensure that all keys are equal
+                raise ValueError(
+                    "__setitem__ is only allowed for same-class or "
+                    "compatible class (i.e. same members) assignment"
+                )
+
+        # Validating the non-tensor data before setting the item
+        for key, val in value._non_tensordict.items():
+            # Raise a warning if non_tensor data doesn't match
+            if (
+                key in self._non_tensordict.keys()
+                and val is not self._non_tensordict[key]
+            ):
+                warnings.warn(
+                    f"Meta data at {repr(key)} may or may not be equal, "
+                    f"this may result in undefined behaviours",
+                    category=UserWarning,
+                    stacklevel=2,
+                )
+
+        for key in value._tensordict.keys():
+            # Making sure that the key-clashes won't happen, if the key is present
+            # in tensor data in value we will honor that and remove the key-value
+            # pair from non-tensor data
+            if key in self._non_tensordict.keys():
+                del self._non_tensordict[key]
+
+        self._tensordict[item] = value._tensordict
+    elif isinstance(value, TensorDictBase):  # it is one of accepted "broadcast" types
+        # attempt broadcast on all tensordata and nested tensorclasses
+        self._tensordict[item] = value.filter_non_tensor_data()
+        self._non_tensordict.update(
+            {
+                key: val.data
+                for key, val in value.items(is_leaf=is_non_tensor, leaves_only=True)
+            }
+        )
+    else:
+        # int, float etc.
+        self._tensordict[item] = value
+
+
+def _repr(self) -> str:
+    """Return a string representation of Tensor class object."""
+    fields = _all_td_fields_as_str(self._tensordict)
+    field_str = [fields] if fields else []
+    non_tensor_fields = _all_non_td_fields_as_str(self._non_tensordict)
+    batch_size_str = indent(f"batch_size={self.batch_size}", 4 * " ")
+    device_str = indent(f"device={self.device}", 4 * " ")
+    is_shared_str = indent(f"is_shared={self.is_shared()}", 4 * " ")
+    if len(non_tensor_fields) > 0:
+        non_tensor_field_str = indent(
+            ",\n".join(non_tensor_fields),
+            4 * " ",
+        )
+        string = ",\n".join(
+            field_str
+            + [non_tensor_field_str, batch_size_str, device_str, is_shared_str]
+        )
+    else:
+        string = ",\n".join(field_str + [batch_size_str, device_str, is_shared_str])
+    return f"{self.__class__.__name__}(\n{string})"
+
+
+def _len(self) -> int:
+    """Returns the length of first dimension, if there is, otherwise 0."""
+    return len(self._tensordict)
+
+
+def _to_dict(self) -> dict:
+    td_dict = self._tensordict.to_dict()
+    td_dict.update(self._non_tensordict)
+    return td_dict
+
+
+def _from_dict(cls, input_dict, batch_size=None, device=None, batch_dims=None):
+    # we pass through a tensordict because keys could be passed as NestedKeys
+    # We can't assume all keys are strings, otherwise calling cls(**kwargs)
+    # would work ok
+
+    td = TensorDict.from_dict(
+        input_dict, batch_size=batch_size, device=device, batch_dims=batch_dims
+    )
+    non_tensor = {}
+
+    # Note: this won't deal with sub-tensordicts which may or may not be tensorclasses.
+    # We don't want to enforce them to be tensorclasses so we can't do much about it...
+    for key, value in list(td.items()):
+        if is_non_tensor(value):
+            non_tensor[key] = value.data
+            del td[key]
+
+    return cls.from_tensordict(tensordict=td, non_tensordict=non_tensor)
+
+
+def _from_dict_instance(
+    self, input_dict, batch_size=None, device=None, batch_dims=None
+):
+    if batch_dims is not None and batch_size is not None:
+        raise ValueError("Cannot pass both batch_size and batch_dims to `from_dict`.")
+    from tensordict import TensorDict
+
+    batch_size_set = torch.Size(()) if batch_size is None else batch_size
+    # TODO: this is a bit slow and will be a bottleneck every time td[idx] = dict(subtd)
+    # is called when there are non tensor data in it
+    if not _is_tensor_collection(type(input_dict)):
+        input_tdict = TensorDict.from_dict(input_dict)
+    else:
+        input_tdict = input_dict
+    trsf_dict = {}
+    for key, value in list(input_tdict.items()):
+        # cur_value = getattr(self, key, None)
+        cur_value = self.get(key, None)
+        if _is_tensor_collection(type(cur_value)):
+            trsf_dict[key] = cur_value.from_dict_instance(
+                value, batch_size=[], device=device, batch_dims=None
+            )
+        elif not isinstance(cur_value, torch.Tensor) and is_non_tensor(value):
+            trsf_dict[key] = value.data
+        elif cur_value is not None and not isinstance(cur_value, torch.Tensor):
+            # This is slightly unsafe but will work with bool, float and int
+            try:
+                trsf_dict[key] = type(cur_value)(value)
+            except Exception:
+                trsf_dict[key] = input_dict[key]
+        else:
+            trsf_dict[key] = value
+    out = type(self)(
+        **trsf_dict,
+        batch_size=batch_size_set,
+        device=device,
+    )
+    # check that
+    if batch_size is None:
+        out._tensordict.auto_batch_size_()
+    return out
+
+
+def _to_tensordict(self) -> TensorDict:
+    """Convert the tensorclass into a regular TensorDict.
+
+    Makes a copy of all entries. Memmap and shared memory tensors are converted to
+    regular tensors.
+
+    Returns:
+        A new TensorDict object containing the same values as the tensorclass.
+
+    """
+    td = self._tensordict.to_tensordict()
+    for key, val in self._non_tensordict.items():
+        td.set_non_tensor(key, val)
+    return td
+
+
+def _device(self) -> torch.device:
+    """Retrieves the device type of tensor class."""
+    return self._tensordict.device
+
+
+def _device_setter(self, value: DeviceType) -> None:
+    raise RuntimeError(
+        "device cannot be set using tensorclass.device = device, "
+        "because device cannot be updated in-place. To update device, use "
+        "tensorclass.to(new_device), which will return a new tensorclass "
+        "on the new device."
+    )
+
+
+def _set(
+    self, key: NestedKey, value: Any, inplace: bool = False, non_blocking: bool = False
+):
+    """Sets a new key-value pair.
+
+    Args:
+        key (str, tuple of str): name of the key to be set.
+           If tuple of str it is equivalent to chained calls of getattr
+           followed by a final setattr.
+        value (Any): value to be stored in the tensorclass
+        inplace (bool, optional): if ``True``, set will tentatively try to
+            update the value in-place. If ``False`` or if the key isn't present,
+            the value will be simply written at its destination.
+
+    Returns:
+        self
+
+    """
+    if isinstance(key, str):
+        cls = type(self)
+        __dict__ = self.__dict__
+        if __dict__["_tensordict"].is_locked:
+            raise RuntimeError(_LOCK_ERROR)
+        if key in ("batch_size", "names", "device"):
+            # handled by setattr
+            return
+        expected_keys = cls.__dataclass_fields__
+        if key not in expected_keys:
+            raise AttributeError(
+                f"Cannot set the attribute '{key}', expected attributes are {expected_keys}."
+            )
+
+        def set_tensor(
+            key=key, value=value, inplace=inplace, non_blocking=non_blocking
+        ):
+            # Avoiding key clash, honoring the user input to assign tensor type data to the key
+            if key in self._non_tensordict.keys():
+                if inplace:
+                    raise RuntimeError(
+                        f"Cannot update an existing entry of type {type(self._non_tensordict.get(key))} with a value of type {type(value)}."
+                    )
+                del self._non_tensordict[key]
+            self._tensordict.set(key, value, inplace=inplace, non_blocking=non_blocking)
+            return self
+
+        def _is_castable(datatype):
+            return issubclass(datatype, (int, float, np.ndarray))
+
+        if cls.autocast:
+            type_hints = cls._type_hints
+            if type_hints is not None:
+                target_cls = type_hints.get(key, _AnyType)
+            else:
+                warnings.warn("type_hints are none, cannot perform auto-casting")
+                target_cls = _AnyType
+
+            if isinstance(value, dict):
+                if _is_tensor_collection(target_cls):
+                    value = target_cls.from_dict(value)
+                    self._tensordict.set(
+                        key, value, inplace=inplace, non_blocking=non_blocking
+                    )
+                    return self
+                elif type_hints is None:
+                    warnings.warn(type(self)._set_dict_warn_msg)
+            elif value is not None and issubclass(
+                target_cls, tuple(tensordict_lib.base._ACCEPTED_CLASSES)
+            ):
+                try:
+                    if not issubclass(type(value), target_cls):
+                        if issubclass(target_cls, torch.Tensor):
+                            # first convert to tensor to make sure that the dtype is preserved
+                            value = torch.as_tensor(value)
+                        cast_val = _cast_funcs[target_cls](value)
+                    else:
+                        cast_val = value
+                except TypeError:
+                    raise TypeError(
+                        f"Failed to cast the value {key} to the type annotation {target_cls}."
+                    )
+                return set_tensor(value=cast_val)
+            elif value is not None and target_cls is not _AnyType:
+                value = _cast_funcs[target_cls](value)
+            elif target_cls is _AnyType and _is_castable(type(value)):
+                return set_tensor()
+        else:
+            if isinstance(value, tuple(tensordict_lib.base._ACCEPTED_CLASSES)):
+                return set_tensor()
+
+        # Avoiding key clash, honoring the user input to assign non-tensor data to the key
+        if key in self._tensordict.keys():
+            if inplace:
+                raise RuntimeError(
+                    f"Cannot update an existing entry of type {type(self._tensordict.get(key))} with a value of type {type(value)}."
+                )
+            self._tensordict.del_(key)
+        # Saving all non-tensor attributes
+        self._non_tensordict[key] = value
+        return self
+
+    if isinstance(key, tuple) and len(key):
+        key = _unravel_key_to_tuple(key)
+        if len(key) > 1:
+            return self.set(key[0], getattr(self, key[0]).set(key[1:], value))
+        out = self.set(key[0], value)
+        return out
+    raise ValueError(
+        f"Supported type for key are str and tuple, got {key} of type {type(key)}"
+    )
+
+
+def _del_(self, key):
+    key = _unravel_key_to_tuple(key)
+    if len(key) > 1:
+        td = self.get(key[0])
+        td.del_(key[1:])
+        return
+    if key[0] in self._tensordict.keys():
+        self._tensordict.del_(key[0])
+        # self.set(key[0], None)
+    elif key[0] in self._non_tensordict.keys():
+        self._non_tensordict[key[0]] = None
+    else:
+        raise KeyError(f"Key {key} could not be found in tensorclass {self}.")
+    return
+
+
+def _set_at_(
+    self, key: NestedKey, value: Any, idx: IndexType, non_blocking: bool = False
+):
+    if key in self._non_tensordict:
+        del self._non_tensordict[key]
+    return self._tensordict.set_at_(key, value, idx, non_blocking=non_blocking)
+
+
+def _get(self, key: NestedKey, default: Any = NO_DEFAULT):
+    """Gets the value stored with the input key.
+
+    Args:
+        key (str, tuple of str): key to be queried. If tuple of str it is
+            equivalent to chained calls of getattr.
+        default: default value if the key is not found in the tensorclass.
+
+    Returns:
+        value stored with the input key
+
+    """
+    if isinstance(key, str):
+        key = (key,)
+
+    if isinstance(key, tuple):
+        try:
+            if len(key) > 1:
+                return getattr(self, key[0]).get(key[1:])
+            return getattr(self, key[0])
+        except AttributeError:
+            if default is NO_DEFAULT:
+                raise
+            return default
+    raise ValueError(f"Supported type for key are str and tuple, got {type(key)}")
+
+
+def _get_at(self, key: NestedKey, idx, default: Any = NO_DEFAULT):
+    try:
+        return self.get(key, NO_DEFAULT)[idx]
+    except AttributeError:
+        if default is NO_DEFAULT:
+            raise
+        return default
+
+
+def _batch_size(self) -> torch.Size:
+    """Retrieves the batch size for the tensor class.
+
+    Returns:
+        batch size (torch.Size)
+
+    """
+    return self._tensordict.batch_size
+
+
+def _batch_size_setter(self, new_size: torch.Size) -> None:  # noqa: D417
+    """Set the value of batch_size.
+
+    Args:
+        new_size (torch.Size): new_batch size to be set
+
+    """
+    self._tensordict._batch_size_setter(new_size)
+
+
+def _names(self) -> torch.Size:
+    """Retrieves the dim names for the tensor class.
+
+    Returns:
+        names (list of str)
+
+    """
+    return self._tensordict.names
+
+
+def _names_setter(self, names: str) -> None:  # noqa: D417
+    """Set the value of ``tensorclass.names``.
+
+    Args:
+        names (sequence of str)
+
+    """
+    self._tensordict.names = names
+
+
+def _state_dict(
+    self, destination=None, prefix="", keep_vars=False, flatten=False
+) -> dict[str, Any]:
+    """Returns a state_dict dictionary that can be used to save and load data from a tensorclass."""
+    state_dict = {
+        "_tensordict": super(type(self), self)
+        .__getattribute__("_tensordict")
+        .state_dict(
+            destination=destination, prefix=prefix, keep_vars=keep_vars, flatten=flatten
+        )
+    }
+    state_dict["_non_tensordict"] = copy(self._non_tensordict)
+    return state_dict
+
+
+def _load_state_dict(
+    self, state_dict: dict[str, Any], strict=True, assign=False, from_flatten=False
+):
+    """Loads a state_dict attemptedly in-place on the destination tensorclass."""
+    for key, item in state_dict.items():
+        # keys will never be nested which facilitates everything, but let's
+        # double check in case someone does something nasty
+        if not isinstance(key, str):
+            raise TypeError("Only str keys are allowed when calling load_state_dict.")
+        if key == "_non_tensordict":
+            for sub_key, sub_item in item.items():
+                # sub_item is the state dict of a tensorclass
+                if isinstance(sub_item, dict) and "_non_tensordict" in sub_item:
+                    raise RuntimeError(
+                        "Loading a saved tensorclass on a uninitialized tensorclass is not allowed"
+                    )
+                else:
+                    # check that sub_key is part of the tensorclass
+                    if sub_key not in self.__class__.__dataclass_fields__:
+                        raise KeyError(
+                            f"Key '{sub_key}' wasn't expected in the state-dict."
+                        )
+                    super(type(self), self).__getattribute__("_non_tensordict")[
+                        sub_key
+                    ] = sub_item
+        elif key == "_tensordict":
+            for sub_key in item.keys():
+                if (
+                    sub_key not in self.__class__.__dataclass_fields__
+                    and sub_key not in ("__batch_size", "__device")
+                ):
+                    raise KeyError(
+                        f"Key '{sub_key}' wasn't expected in the state-dict."
+                    )
+            super(type(self), self).__getattribute__("_tensordict").load_state_dict(
+                item, strict=strict, assign=assign, from_flatten=from_flatten
+            )
+        else:
+            raise KeyError(f"Key '{key}' wasn't expected in the state-dict.")
+
+    return self
+
+
+def _eq(self, other: object) -> bool:
+    """Compares the Tensor class object to another object for equality. However, the equality check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object. Can be a tensorclass, a
+            tensordict or any compatible type (int, float or tensor), in
+            which case the equality check will be propagated to the leaves.
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean
+        values for tensor attributes and None for non-tensor attributes
+
+    Examples:
+        >>> @tensorclass
+        ... class MyClass:
+        ...     x: Tensor
+        ...     y: "MyClass"
+        ...     z: str
+        ...
+        >>> c1 = MyClass(
+        ...     x=torch.randn(3, 4),
+        ...     y=MyClass(
+        ...         x=torch.randn(3, 4, 1),
+        ...         y=None,
+        ...         z="bar",
+        ...         batch_size=[3, 4, 1],
+        ...     ),
+        ...     z="foo",
+        ...     batch_size=[3, 4],
+        ... )
+        >>> c2 = c1.clone()
+        >>> print(c1 == c2)
+        MyClass(
+            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
+            y=MyClass(
+                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
+                y=None,
+                z=None,
+                batch_size=torch.Size([3, 4, 1]),
+                device=None,
+                is_shared=False),
+            z=None,
+            batch_size=torch.Size([3, 4]),
+            device=None,
+            is_shared=False)
+        >>> assert (c1 == c2).all()
+        >>> assert (c1[:2] == c2[:2]).all()
+        >>> assert not (c1 == c2.apply(lambda x: x+1)).all()
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor)
+    ):
+        return False
+    if is_tensorclass(other):
+        tensor = self._tensordict == other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self.filter_non_tensor_data() == other.filter_non_tensor_data()
+    else:
+        tensor = self._tensordict == other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def _ne(self, other: object) -> bool:
+    """Compare the Tensor class object to another object for inequality. However, the equality check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean values for tensor attributes and None for non-tensor attributes
+
+    Examples:
+        >>> @tensorclass
+        ... class MyClass:
+        ...     x: Tensor
+        ...     y: "MyClass"
+        ...     z: str
+        ...
+        >>> c1 = MyClass(
+        ...     x=torch.randn(3, 4),
+        ...     y=MyClass(
+        ...         x=torch.randn(3, 4, 1),
+        ...         y=None,
+        ...         z="bar",
+        ...         batch_size=[3, 4, 1],
+        ...     ),
+        ...     z="foo",
+        ...     batch_size=[3, 4],
+        ... )
+        >>> c2 = c1.clone()
+        >>> print(c1 != c2)
+        MyClass(
+            x=Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.bool, is_shared=False),
+            y=MyClass(
+                x=Tensor(shape=torch.Size([3, 4, 1]), device=cpu, dtype=torch.bool, is_shared=False),
+                y=None,
+                z=None,
+                batch_size=torch.Size([3, 4, 1]),
+                device=None,
+                is_shared=False),
+            z=None,
+            batch_size=torch.Size([3, 4]),
+            device=None,
+            is_shared=False)
+        >>> c2 = c2.apply(lambda x: x+1)
+        >>> assert (c1 != c2).all()
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor)
+    ):
+        return True
+    if is_tensorclass(other):
+        tensor = self._tensordict != other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict != other.exclude(*self._non_tensordict.keys())
+    else:
+        tensor = self._tensordict != other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def _or(self, other: object) -> bool:
+    """Compares the Tensor class object to another object for logical OR. However, the logical OR check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object. Can be a tensorclass, a
+            tensordict or any compatible type (int, float or tensor), in
+            which case the equality check will be propagated to the leaves.
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean
+        values for tensor attributes and None for non-tensor attributes
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor)
+    ):
+        return False
+    if is_tensorclass(other):
+        tensor = self._tensordict | other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict | other.exclude(*self._non_tensordict.keys())
+    else:
+        tensor = self._tensordict | other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def _xor(self, other: object) -> bool:
+    """Compares the Tensor class object to another object for exclusive OR. However, the exclusive OR check for non-tensor data is not performed.
+
+    Args:
+        other: object to compare to this object. Can be a tensorclass, a
+            tensordict or any compatible type (int, float or tensor), in
+            which case the equality check will be propagated to the leaves.
+
+    Returns:
+        False if the objects are of different class types, Tensorclass of boolean
+        values for tensor attributes and None for non-tensor attributes
+
+    """
+    if not is_tensor_collection(other) and not isinstance(
+        other, (dict, numbers.Number, Tensor)
+    ):
+        return False
+    if is_tensorclass(other):
+        tensor = self._tensordict ^ other._tensordict
+    elif _is_tensor_collection(type(other)):
+        # other can be a tensordict reconstruction of self, in which case we discard
+        # the non-tensor data
+        tensor = self._tensordict ^ other.exclude(*self._non_tensordict.keys())
+    else:
+        tensor = self._tensordict ^ other
+    return _from_tensordict_with_none(self, tensor)
+
+
+def _non_tensor_items(self, include_nested=False):
+    if include_nested:
+        return self.non_tensor_items() + self._tensordict.non_tensor_items(
+            include_nested=True
+        )
+    else:
+        return list(self._non_tensordict.items())
+
+
+def _bool(self):
+    raise RuntimeError("Converting a tensorclass to boolean value is not permitted")
+
+
+def _single_td_field_as_str(key, item, tensordict):
+    """Returns a string as a  key-value pair of tensordict.
+
+    Args:
+        key (str): key of tensor dict item
+        item (tensor type): value to be returned for key
+        tensordict (Tensordict): Tensordict object
+
+    Returns:
+        String representation of a key-value pair
+
+    """
+    if is_tensor_collection(type(item)):
+        return f"{key}={repr(tensordict[key])}"
+    return f"{key}={_get_repr(item)}"
+
+
+def _all_td_fields_as_str(td: TensorDictBase) -> str:
+    """Returns indented representation of tensor dict values as a key-value pairs.
+
+    Args:
+        td (TensorDict) : Tensordict object
+
+    Returns:
+        String representation of all tensor data
+
+    """
+    return indent(
+        ",\n".join(
+            sorted([_single_td_field_as_str(key, item, td) for key, item in td.items()])
+        ),
+        4 * " ",
+    )
+
+
+def _all_non_td_fields_as_str(src_dict) -> list:
+    """Returns a list of string representation of non-tensor key-value pairs.
+
+    Args:
+        src_dict (dict): non_tensor_dict
+
+    Returns:
+        result (list): list of strings with key-value representation
+
+    """
+    result = []
+    for key, val in src_dict.items():
+        if not is_tensor_collection(val):
+            result.append(f"{key}={repr(val)}")
+
+    return result
+
+
+def _unbind(self, dim: int):
+    """Returns a tuple of indexed tensorclass instances unbound along the indicated dimension.
+
+    Resulting tensorclass instances will share the storage of the initial tensorclass instance.
+
+    """
+    return tuple(
+        self._from_tensordict(td, non_tensordict=copy(self._non_tensordict))
+        for td in self._tensordict.unbind(dim)
+    )
+
+
+################
+# Custom classes
+# --------------
+
+NONTENSOR_HANDLED_FUNCTIONS = []
+
+_MP_MANAGER = None
+
+
+def _mp_manager():
+    global _MP_MANAGER
+    if _MP_MANAGER is None:
+        _MP_MANAGER = Manager()
+    return _MP_MANAGER
+
+
+@tensorclass
+class NonTensorData:
+    """A carrier for non-tensordict data.
+
+    This class can be used whenever non-tensor data needs to be carrier at
+    any level of a tensordict instance.
+
+    :class:`~tensordict.tensorclass.NonTensorData` instances can be created
+    explicitely or using :meth:`~tensordict.TensorDictBase.set_non_tensor`.
+
+    This class is serializable using :meth:`tensordict.TensorDictBase.memmap`
+    and related methods, and can be loaded through :meth:`~tensordict.TensorDictBase.load_memmap`.
+    If the content of the object is JSON-serializable, it will be serializsed in
+    the `meta.json` file in the directory pointed by the parent key of the `NoneTensorData`
+    object. If it isn't, serialization will fall back on pickle. This implies
+    that we assume that the content of this class is either json-serializable or
+    pickable, and it is the user responsibility to make sure that one of these
+    holds. We try to avoid pickling/unpickling objects for performance and security
+    reasons (as pickle can execute arbitrary code during loading).
+
+    .. note:: if the data passed to :class:`NonTensorData` is a :class:`NonTensorData`
+        itself, the data from the nested object will be gathered.
+
+        >>> non_tensor = NonTensorData("a string!")
+        >>> non_tensor = NonTensorData(non_tensor)
+        >>> assert non_tensor.data == "a string!"
+
+    .. note:: To faciliate ``NonTensorData`` integration in tensordict, the
+        :meth:`~tensordict.TensorDictBase.__getitem__` and :meth:`~tensordict.TensorDictBase.__setitem__`
+        are overloaded to set non-tensor data appropriately (unlike :meth:`~tensordict.TensorDictBase.set`
+        and :meth:`~tensordict.TensorDictBase.get` which are reserved for tensor-like
+        objects):
+
+        >>> td = TensorDict({"a": torch.zeros(3)}, batch_size=[3])
+        >>> td["a"]  # gets a tensor
+        >>> td["b"] = "a string!"
+        >>> assert td["b"] == "a string!"
+        >>> # indexing preserves the meta-data
+        >>> assert td[0]["b"] == "a string!"
+        >>> td.get("b")  # returns the NonTensorData
+
+    .. note:: Unlike other tensorclass classes, :class:`NonTensorData` supports
+        comparisons of two non-tensor data through :meth:`~.__eq__`, :meth:`~.__ne__`,
+        :meth:`~.__xor__` or :meth:`~.__or__`. These operations return a tensor
+        of shape `batch_size`. For compatibility with `<a tensordict> == <float_number>`,
+        comparison with non-:class:`NonTensorData` will always return an empty
+        :class:`NonTensorData`.
+
+        >>> a = NonTensorData(True, batch_size=[])
+        >>> b = NonTensorData(True, batch_size=[])
+        >>> assert a == b
+        >>> assert not (a != b)
+        >>> assert not (a ^ b)
+        >>> assert a | b
+        >>> # The output is a tensor of shape batch-size
+        >>> a = NonTensorData(True, batch_size=[3])
+        >>> b = NonTensorData(True, batch_size=[3])
+        >>> print(a == b)
+        tensor([True, True, True])
+
+    .. note:: Stacking :class:`NonTensorData` instances results in either
+        a single :class:`NonTensorData` instance if all shapes match, or a
+        :class:`~tensordict.LazyStackedTensorDict` object if the content
+        mismatch. To get to this result, the content of the :class:`NonTensorData`
+        instances must be compared, which can be computationally intensive
+        depending on what this content is.
+
+        >>> data = torch.stack([NonTensorData(1, batch_size=[]) for _ in range(10)])
+        >>> data
+        NonTensorData(
+            data=1,
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)
+        >>> data = torch.stack([NonTensorData(i, batch_size=[3,]) for i in range(10)], 1)
+        >>> data[:, 0]
+        NonTensorData(
+            data=0,
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    .. note:: Non-tensor data can be filtered out from a tensordict using
+        :meth:`~tensordict.TensorDictBase.filter_non_tensor`.
+
+    Examples:
+        >>> # create an instance explicitly
+        >>> non_tensor = NonTensorData("a string!", batch_size=[]) # batch-size can be anything
+        >>> data = TensorDict({}, batch_size=[3])
+        >>> data.set_non_tensor(("nested", "key"), "a string!")
+        >>> assert isinstance(data.get(("nested", "key")), NonTensorData)
+        >>> assert data.get_non_tensor(("nested", "key")) == "a string!"
+        >>> # serialization
+        >>> class MyPickableClass:
+        ...     value = 10
+        >>> data.set_non_tensor("pickable", MyPickableClass())
+        >>> import tempfile
+        >>> with tempfile.TemporaryDirectory() as tmpdir:
+        ...     data.memmap(tmpdir)
+        ...     loaded = TensorDict.load_memmap(tmpdir)
+        ...     # print directory path
+        ...     print_directory_tree(tmpdir)
+        Directory size: 511.00 B
+        tmp2cso9og_/
+            pickable/
+                _tensordict/
+                    meta.json
+                other.pickle
+                meta.json
+            nested/
+                key/
+                    _tensordict/
+                        meta.json
+                    meta.json
+                meta.json
+            meta.json
+        >>> assert loaded.get_non_tensor("pickable").value == 10
+
+    .. note:: __Preallocation__ is also possible with ``NonTensorData``.
+      This class can handle conversion from ``NonTensorData`` to
+      ``NonTensorStack`` where appropriate, as the following example
+      demonstrates:
+
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> print(td)
+        TensorDict(
+            fields={
+                val: NonTensorData(
+                    data=0,
+                    _metadata=None,
+                    _is_non_tensor=True,
+                    batch_size=torch.Size([10]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)
+        >>> print(td["val"])
+        0
+        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata
+        >>> print(td)
+        TensorDict(
+            fields={
+                val: NonTensorStack(
+                    [0, 1, 0, 1, 0, 1, 0, 1, 0, 1],
+                    batch_size=torch.Size([10]),
+                    device=None)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)
+        >>> print(td["val"])  # the stack is automatically converted to a list
+        [0, 1, 0, 1, 0, 1, 0, 1, 0, 1]
+
+      If the value is unique, the ``NonTensorData`` container is kept and
+      retrieving the value only returns this value. If a ``NonTensorStack``
+      is used, ``__getitem__`` will return the list of values instead.
+      This makes the two operations not exactly interchangeable. The reason
+      for this inconsistency is that a single ``NonTensorData`` with a non-empty
+      batch-size is intended to be used as a metadata carrier for bigger
+      tensordicts, whereas ``NonTensorStack`` usage is aimed at allocating
+      one metadata atom to each corresponding batch element.
+
+    .. note::
+      ``NonTensorData`` can be shared between processes. In fact, both
+      :meth:`~tensordict.TensorDict.memmap_` (and the likes) and
+      :meth:`~tensordict.TensorDict.share_memory_` will produce sharable
+      instances.
+
+      Valid methods to write data are :meth:`~tensordict.TensorDictBase.update`
+      with the `inplace=True` flag and :meth:`~tensordict.TensorDictBase.update_`
+      or :meth:`~tensordict.TensorDictBase.update_at_`.
+
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
+        ...     td.share_memory_()
+        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))  # works
+        ...     td.update(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []), inplace=True)  # works
+        ...     td["val"] = 1  # breaks
+
+      A shared ``NonTensorData`` is writable whenever its content is a ``str``,
+      ``int``, ``float``, ``bool``, ``dict`` or ``list`` instance. Other types
+      (e.g., dataclasses) will not raise an exception during the call to
+      ``memmap_`` or ``share_memory_`` but they will cause the code to break
+      when the data is overwritten.
+
+        >>> @dataclass
+        ... class MyClass:
+        ...     string: str
+        ...
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": MyClass("a string!")}, [])
+        ...     td.share_memory_()  # works and can be shared between processes
+        ...     td.update_(TensorDict({"val": MyClass("another string!")}, []))  # breaks!
+
+      :class:`~tensordict.tensorclass.TensorStack` instances are also sharable
+      in a similar way. Crucially, preallocation must be properly handled for
+      this to work.
+
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> newdata = TensorDict({"val": NonTensorData(data=1, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata
+        >>> # If TD is properly preallocated, we can share it and change its content
+        >>> td.share_memory_()
+        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata  # Works!
+        >>> # In contrast, not preallocating the tensordict properly will break when assigning values
+        >>> td = TensorDict({"val": NonTensorData(data=0, batch_size=[10])}, [10])
+        >>> td.share_memory_()
+        >>> newdata = TensorDict({"val": NonTensorData(data=2, batch_size=[5])}, [5])
+        >>> td[1::2] = newdata  # breaks!
+
+      Writable memmapped-``NonTensorData`` instances will update the underlying
+      metadata if required. This involves writing in a JSON file, which can
+      introduce some overhead. We advise against this usage whenever one seeks
+      performance and long-lasting data sharing isn't required (``share_memory_``
+      should be preferred in these cases).
+
+        >>> if __name__ == "__main__":
+        ...     td = TensorDict({"val": NonTensorData(data=0, batch_size=[])}, [])
+        ...     td.memmap_(dest_folder)
+        ...     td.update_(TensorDict({"val": NonTensorData(data=1, batch_size=[])}, []))
+        ...     # The underlying metadata on disk is updated during calls to update_
+        ...     td_load = TensorDict.load_memmap(dest_folder)
+        ...     assert (td == td_load).all()
+
+    """
+
+    # Used to carry non-tensor data in a tensordict.
+    # The advantage of storing this in a tensorclass is that we don't need
+    # to patch tensordict with additional checks that will encur unwanted overhead
+    # and all the overhead falls back on this class.
+    data: Any
+    _metadata: dict | None = None
+
+    _is_non_tensor: bool = True
+
+    def __post_init__(self):
+        if is_non_tensor(self.data):
+            data = getattr(self.data, "data", None)
+            if data is None:
+                data = self.data.tolist()
+            del self._tensordict["data"]
+            self._non_tensordict["data"] = data
+        assert self._tensordict.is_empty(), self._tensordict
+
+        def __repr__(self):
+            data_str = str(self.data)
+            if len(data_str) > 200:
+                data_str = data_str[:20] + "  ...  " + data_str[-20:]
+            return f"{type(self).__name__}(data={data_str}, batch_size={self.batch_size}, device={self.device})"
+
+        self.__class__.__repr__ = __repr__
+
+        old_eq = self.__class__.__eq__
+        if old_eq is _eq:
+            global NONTENSOR_HANDLED_FUNCTIONS
+            NONTENSOR_HANDLED_FUNCTIONS.extend(TD_HANDLED_FUNCTIONS)
+
+            # Patch only the first time a class is created
+
+            @functools.wraps(_eq)
+            def __eq__(self, other):
+                if isinstance(other, NonTensorData):
+                    return torch.full(
+                        self.batch_size, self.data == other.data, device=self.device
+                    )
+                return old_eq(self, other)
+
+            self.__class__.__eq__ = __eq__
+
+            _ne = self.__class__.__ne__
+
+            @functools.wraps(_ne)
+            def __ne__(self, other):
+                if isinstance(other, NonTensorData):
+                    return torch.full(
+                        self.batch_size, self.data != other.data, device=self.device
+                    )
+                return _ne(self, other)
+
+            self.__class__.__ne__ = __ne__
+
+            _xor = self.__class__.__xor__
+
+            @functools.wraps(_xor)
+            def __xor__(self, other):
+                if isinstance(other, NonTensorData):
+                    return torch.full(
+                        self.batch_size, self.data ^ other.data, device=self.device
+                    )
+                return _xor(self, other)
+
+            self.__class__.__xor__ = __xor__
+
+            _or = self.__class__.__or__
+
+            @functools.wraps(_or)
+            def __or__(self, other):
+                if isinstance(other, NonTensorData):
+                    return torch.full(
+                        self.batch_size, self.data | other.data, device=self.device
+                    )
+                return _or(self, other)
+
+            self.__class__.__or__ = __or__
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=inplace,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+    ) -> T:
+        if isinstance(input_dict_or_td, NonTensorData):
+            data = input_dict_or_td.data
+            if inplace and self._tensordict._is_shared:
+                _update_shared_nontensor(self._non_tensordict["data"], data)
+                return self
+            elif inplace and self._is_memmap:
+                _is_memmaped_from_above = self._is_memmaped_from_above()
+                if break_on_memmap is None:
+                    global _BREAK_ON_MEMMAP
+                    break_on_memmap = _BREAK_ON_MEMMAP
+                if _is_memmaped_from_above and break_on_memmap:
+                    raise RuntimeError(
+                        "Cannot update a leaf NonTensorData from a memmaped parent NonTensorStack. "
+                        "To update this leaf node, please update the NonTensorStack with the proper index."
+                    )
+                share_non_tensor = self._metadata["_share_non_tensor"]
+                if share_non_tensor:
+                    _update_shared_nontensor(self._non_tensordict["data"], data)
+                else:
+                    self._non_tensordict["data"] = data
+                # Force json update by setting is memmap to False
+                if not _is_memmaped_from_above and "memmap_prefix" in self._metadata:
+                    self._tensordict._is_memmap = False
+                    self._memmap_(
+                        prefix=self._metadata["memmap_prefix"],
+                        copy_existing=False,
+                        executor=None,
+                        futures=None,
+                        inplace=True,
+                        like=False,
+                        share_non_tensor=share_non_tensor,
+                    )
+                return self
+            elif not inplace and self.is_locked:
+                raise RuntimeError(_LOCK_ERROR)
+            if clone:
+                data = deepcopy(data)
+            self.data = data
+        elif isinstance(input_dict_or_td, NonTensorStack):
+            raise ValueError(
+                "Cannot update a NonTensorData object with a NonTensorStack. Call `non_tensor_data.maybe_to_stack()` "
+                "before calling update()."
+            )
+        elif not input_dict_or_td.is_empty():
+            raise RuntimeError(f"Unexpected type {type(input_dict_or_td)}")
+        return self
+
+    def maybe_to_stack(self):
+        """Converts the NonTensorData object to a NonTensorStack object if it has a non-empty batch-size."""
+        datalist = self.data
+        if not self.batch_size:
+            return self
+        for i in reversed(self.batch_size):
+            datalist = [datalist] * i
+        return NonTensorStack._from_list(datalist, device=self.device)
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update_(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+    ) -> T:
+
+        if isinstance(input_dict_or_td, NonTensorStack):
+            raise RuntimeError(
+                "Cannot update a NonTensorData with a NonTensorStack object."
+            )
+        if not isinstance(input_dict_or_td, NonTensorData):
+            raise RuntimeError(
+                "NonTensorData.copy_ / update_ requires the source to be a NonTensorData object."
+            )
+        return self._update(
+            input_dict_or_td,
+            inplace=True,
+            clone=clone,
+            keys_to_update=keys_to_update,
+            break_on_memmap=break_on_memmap,
+        )
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+    ) -> NonTensorData:
+        if index != () and index != slice(None):
+            raise RuntimeError("Cannot update a part of a NonTensorData.")
+        return self.update_(
+            input_dict_or_td=input_dict_or_td, clone=clone, non_blocking=non_blocking
+        )
+
+    def empty(self, recurse=False, *, device=NO_DEFAULT, batch_size=None, names=None):
+        if batch_size is not None and names is None:
+            names = None
+        elif names is None and self._has_names():
+            names = self.names
+        else:
+            names = None
+        return NonTensorData(
+            data=self.data,
+            batch_size=self.batch_size if batch_size is None else batch_size,
+            names=names,
+            device=self.device if device is NO_DEFAULT else device,
+        )
+
+    def _apply_nest(self, *args, out=None, **kwargs):
+        # kwargs["filter_empty"] = False
+        if out is not None:
+            return out
+        return self.empty(
+            batch_size=kwargs.get("batch_size"),
+            device=kwargs.get("device", NO_DEFAULT),
+            names=kwargs.get("names"),
+        )
+
+    def to_dict(self):
+        # override to_dict to return just the data
+        return self.data
+
+    def to_tensordict(self):
+        return self
+
+    @classmethod
+    def _stack_non_tensor(cls, list_of_non_tensor, dim=0):
+        # checks have been performed previously, so we're sure the list is non-empty
+        first = list_of_non_tensor[0]
+
+        def _check_equal(a, b):
+            if isinstance(a, _ACCEPTED_CLASSES) or isinstance(b, _ACCEPTED_CLASSES):
+                return (a == b).all()
+            if isinstance(a, np.ndarray) or isinstance(b, np.ndarray):
+                return (a == b).all()
+            try:
+                iseq = a == b
+            except Exception:
+                iseq = False
+            return iseq
+
+        if all(isinstance(data, NonTensorData) for data in list_of_non_tensor) and all(
+            _check_equal(data.data, first.data) for data in list_of_non_tensor[1:]
+        ):
+            batch_size = list(first.batch_size)
+            batch_size.insert(dim, len(list_of_non_tensor))
+            return NonTensorData(
+                data=first.data,
+                batch_size=batch_size,
+                names=first.names if first._has_names() else None,
+                device=first.device,
+            )
+
+        return NonTensorStack(*list_of_non_tensor, stack_dim=dim)
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        # A modified version of __torch_function__ to account for the different behaviour
+        # of stack, which should return lazy stacks of data of data does not match.
+        if func not in _TD_PASS_THROUGH or not all(
+            issubclass(t, (Tensor, cls)) for t in types
+        ):
+            return NotImplemented
+
+        escape_conversion = func in (torch.stack,)
+
+        if kwargs is None:
+            kwargs = {}
+
+        # get the output type from the arguments / keyword arguments
+        if len(args) > 0:
+            tensorclass_instance = args[0]
+        else:
+            tensorclass_instance = kwargs.get("input", kwargs["tensors"])
+        if isinstance(tensorclass_instance, (tuple, list)):
+            tensorclass_instance = tensorclass_instance[0]
+        if not escape_conversion:
+            args = tuple(_arg_to_tensordict(arg) for arg in args)
+            kwargs = {key: _arg_to_tensordict(value) for key, value in kwargs.items()}
+
+        result = TD_HANDLED_FUNCTIONS[func](*args, **kwargs)
+        if isinstance(result, (list, tuple)):
+            return result.__class__(
+                _from_tensordict_with_copy(tensorclass_instance, tensordict_result)
+                for tensordict_result in result
+            )
+        if not escape_conversion:
+            return _from_tensordict_with_copy(tensorclass_instance, result)
+        return result
+
+    def _fast_apply(self, *args, **kwargs):
+        kwargs["filter_empty"] = False
+        return _wrap_method(self, "_fast_apply", self._tensordict._fast_apply)(
+            *args, **kwargs
+        )
+
+    def tolist(self):
+        """Converts the data in a list if the batch-size is non-empty.
+
+        If the batch-size is empty, returns the data.
+
+        """
+        if not self.batch_size:
+            return self.data
+        return [ntd.tolist() for ntd in self.unbind(0)]
+
+    def copy_(self, src: NonTensorData | NonTensorStack, non_blocking: bool = False):
+        return self.update_(src, non_blocking=non_blocking)
+
+    def clone(self, recurse: bool = True):
+        if recurse:
+            return type(self)(
+                data=deepcopy(self.data),
+                batch_size=self.batch_size,
+                device=self.device,
+                names=self.names,
+            )
+        return type(self)(
+            data=self.data,
+            batch_size=self.batch_size,
+            device=self.device,
+            names=self.names,
+        )
+
+    def share_memory_(self):
+        if self._tensordict._is_shared:
+            return self
+        with self.unlock_():
+            self._non_tensordict["data"] = _share_memory_nontensor(
+                self.data, manager=_mp_manager()
+            )
+        self._tensordict.share_memory_()
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        executor=None,
+        futures=None,
+        inplace=True,
+        like=False,
+        memmaped: bool = False,
+        share_non_tensor: bool = False,
+    ):
+        if self._tensordict._is_memmap:
+            return self
+
+        _metadata = {}
+        if prefix is not None:
+            _metadata = copy(self._metadata)
+            if _metadata is None:
+                _metadata = {}
+            _metadata["memmap_prefix"] = prefix
+            _metadata["memmaped"] = memmaped
+
+        out = _memmap_(
+            self,
+            prefix=prefix,
+            copy_existing=copy_existing,
+            executor=executor,
+            futures=futures,
+            inplace=inplace,
+            like=like,
+            memmaped=memmaped,
+            share_non_tensor=share_non_tensor,
+        )
+        _metadata["_share_non_tensor"] = share_non_tensor
+        out._non_tensordict["_metadata"] = _metadata
+        if share_non_tensor:
+            out._non_tensordict["data"] = _share_memory_nontensor(
+                out.data, manager=_mp_manager()
+            )
+        return out
+
+    def _is_memmaped_from_above(self):
+        _metadata = self._metadata
+        if _metadata is None:
+            return False
+        return _metadata.get("memmaped", False)
+
+
+# For __setitem__ and _update_at_ we don't pass a kwarg but use a global variable instead
+_BREAK_ON_MEMMAP = True
+
+
+class NonTensorStack(LazyStackedTensorDict):
+    """A thin wrapper around LazyStackedTensorDict to make stack on non-tensor data easily recognizable.
+
+    A ``NonTensorStack`` is returned whenever :func:`~torch.stack` is called on
+    a list of :class:`~tensordict.NonTensorData` or ``NonTensorStack``.
+
+    Examples:
+        >>> from tensordict import NonTensorData
+        >>> import torch
+        >>> data = torch.stack([
+        ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
+        ...    for j in range(3)])
+        >>> print(data)
+        NonTensorStack(
+            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, ...,
+            batch_size=torch.Size([3, 2]),
+            device=None)
+
+    To obtain the values stored in a ``NonTensorStack``, call :class:`~.tolist`.
+
+    """
+
+    _is_non_tensor: bool = True
+
+    def __init__(self, *args, **kwargs):
+        super().__init__(*args, **kwargs)
+        if not all(is_non_tensor(item) for item in self.tensordicts):
+            raise RuntimeError("All tensordicts must be non-tensors.")
+
+    def tolist(self):
+        """Extracts the content of a :class:`tensordict.tensorclass.NonTensorStack` in a nested list.
+
+        Examples:
+            >>> from tensordict import NonTensorData
+            >>> import torch
+            >>> data = torch.stack([
+            ...     torch.stack([NonTensorData(data=(i, j), batch_size=[]) for i in range(2)])
+            ...    for j in range(3)])
+            >>> data.tolist()
+            [[(0, 0), (1, 0)], [(0, 1), (1, 1)], [(0, 2), (1, 2)]]
+
+        """
+        iterator = self.tensordicts if self.stack_dim == 0 else self.unbind(0)
+        return [td.tolist() for td in iterator]
+
+    @classmethod
+    def from_nontensordata(cls, non_tensor: NonTensorData):
+        data = non_tensor.data
+        prev = NonTensorData(data, batch_size=[], device=non_tensor.device)
+        for dim in reversed(non_tensor.shape):
+            prev = cls(*[prev.clone(False) for _ in range(dim)], stack_dim=0)
+        return prev
+
+    def __repr__(self):
+        selfrepr = str(self.tolist())
+        if len(selfrepr) > 50:
+            selfrepr = f"{selfrepr[:50]}..."
+        selfrepr = indent(selfrepr, prefix=4 * " ")
+        batch_size = indent(f"batch_size={self.batch_size}", prefix=4 * " ")
+        device = indent(f"device={self.device}", prefix=4 * " ")
+        return f"NonTensorStack(\n{selfrepr}," f"\n{batch_size}," f"\n{device})"
+
+    @classmethod
+    def lazy_stack(
+        cls,
+        items: Sequence[TensorDictBase],
+        dim: int = 0,
+        *,
+        device: DeviceType | None = None,
+        out: T | None = None,
+        stack_dim_name: str | None = None,
+    ) -> T:
+        result = super().lazy_stack(
+            items=items, dim=dim, out=out, stack_dim_name=stack_dim_name, device=device
+        )
+        if not isinstance(result, cls):
+            raise RuntimeError(
+                f"Unexpected result type: {type(result)} - expected one of {cls}."
+            )
+        return result
+
+    def to_dict(self) -> dict[str, Any]:
+        return self.tolist()
+
+    def to_tensordict(self):
+        return self
+
+    def _memmap_(
+        self,
+        *,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        executor=None,
+        futures=None,
+        inplace=True,
+        like=False,
+        memmaped: bool = False,
+        share_non_tensor: bool = False,
+    ) -> T:
+
+        memmaped_leaves = memmaped
+        if not memmaped and prefix is not None:
+            memmaped_leaves = True
+
+            def save_metadata(prefix=prefix, self=self):
+                data = self.tolist()
+                device = str(self.device) if self.device is not None else None
+                if not prefix.exists():
+                    os.makedirs(prefix, exist_ok=True)
+                jsondict = {
+                    "_type": str(self.__class__),
+                    "stack_dim": self.stack_dim,
+                    "device": device,
+                }
+                if _is_json_serializable(data):
+                    jsondict["data"] = data
+                else:
+                    jsondict["data"] = "pickle.pkl"
+                    with open(prefix / "pickle.pkl", "wb") as f:
+                        pickle.dump(data, f)
+                with open(prefix / "meta.json", "w") as f:
+                    json.dump(jsondict, f)
+
+            if executor is None:
+                save_metadata()
+            else:
+                futures.append(executor.submit(save_metadata))
+        # The leaves are all non-tensor or non-tensor stacks, and we already saved this on disk
+        # The only thing remaining to do is share the data between processes
+        results = []
+        for i, td in enumerate(self.tensordicts):
+            results.append(
+                td._memmap_(
+                    prefix=(prefix / str(i)) if prefix is not None else None,
+                    copy_existing=copy_existing,
+                    executor=executor,
+                    futures=futures,
+                    inplace=inplace,
+                    like=like,
+                    # tell the nested stack / nontensor that
+                    # no memmapping should be executed
+                    memmaped=memmaped_leaves,
+                    share_non_tensor=share_non_tensor,
+                )
+            )
+        if not inplace:
+            results = self.lazy_stack(results, dim=self.stack_dim)
+        else:
+            results = self
+        if not memmaped and prefix is not None:
+            results.__dict__["_path_to_memmap"] = prefix
+        return results
+
+    @classmethod
+    def _load_memmap(
+        cls, prefix: str, metadata: dict, *, out=None, **kwargs
+    ) -> LazyStackedTensorDict:
+        data = metadata.get("data", None)
+        if data is not None:
+            if isinstance(data, str):
+                with open(prefix / data, "rb") as file:
+                    data = pickle.load(file)
+            device = metadata["device"]
+            if device is not None:
+                device = torch.device(device)
+            return cls._from_list(data, device=device)
+        return super()._load_memmap(prefix=prefix, metadata=metadata, **kwargs)
+
+    @classmethod
+    def _from_list(cls, datalist: List, device: torch.device):
+        if all(isinstance(item, list) for item in datalist) and all(
+            len(item) == len(datalist[0]) for item in datalist
+        ):
+            return NonTensorStack(
+                *(cls._from_list(item, device=device) for item in datalist), stack_dim=0
+            )
+        return NonTensorStack(
+            *(
+                NonTensorData(data=item, device=device, batch_size=torch.Size([]))
+                for item in datalist
+            ),
+            stack_dim=0,
+        )
+
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=inplace,
+            keys_to_update=keys_to_update,
+        )
+
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        return self._update(
+            input_dict_or_td=input_dict_or_td,
+            clone=clone,
+            inplace=True,
+            keys_to_update=keys_to_update,
+        )
+
+    def _update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        keys_to_update: Sequence[NestedKey] | None = None,
+        break_on_memmap: bool = None,
+        non_blocking: bool = False,
+    ) -> T:
+        if inplace and self.is_locked and not (self._is_shared or self._is_memmap):
+            raise RuntimeError(_LOCK_ERROR)
+
+        if isinstance(input_dict_or_td, NonTensorData):
+            datalist = input_dict_or_td.data
+            for d in reversed(self.batch_size):
+                datalist = [datalist] * d
+            reconstructed = self._from_list(datalist, device=self.device)
+            return self.update(
+                reconstructed,
+                clone=clone,
+                inplace=inplace,
+                keys_to_update=keys_to_update,
+            )
+
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            if break_on_memmap is None:
+                global _BREAK_ON_MEMMAP
+                break_on_memmap = _BREAK_ON_MEMMAP
+            if not break_on_memmap:
+                raise RuntimeError(
+                    "Calling _update with break_on_memmap=False is not permitted if the stack has a path."
+                )
+            # this is the only way break_on_memmap is False
+            break_on_memmap = False
+            # remove memmap
+            if self._path_to_memmap.exists():
+                shutil.rmtree(self._path_to_memmap)
+            memmap = True
+
+        # update content
+        if isinstance(input_dict_or_td, NonTensorStack):
+            for leaf_dest, leaf_src in zip(
+                self.tensordicts, input_dict_or_td.unbind(self.stack_dim)
+            ):
+                leaf_dest._update(
+                    leaf_src,
+                    clone=clone,
+                    inplace=inplace,
+                    keys_to_update=keys_to_update,
+                    break_on_memmap=break_on_memmap,
+                )
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        else:
+            raise NotImplementedError(
+                f"The data type {type(input_dict_or_td)} is not supported within {type(self).__name__}.update"
+            )
+        return self
+
+    def __setitem__(self, index, value):
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            global _BREAK_ON_MEMMAP
+            _BREAK_ON_MEMMAP = False
+            memmap = True
+        try:
+            super().__setitem__(index, value)
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        finally:
+            _BREAK_ON_MEMMAP = True
+
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        index: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+    ) -> T:
+        memmap = False
+        if self._is_memmap and hasattr(self, "_path_to_memmap"):
+            global _BREAK_ON_MEMMAP
+            _BREAK_ON_MEMMAP = False
+            memmap = True
+        try:
+            super().update_at_(
+                input_dict_or_td, index, clone=clone, non_blocking=non_blocking
+            )
+            if memmap:
+                self._memmap_(prefix=self._path_to_memmap, inplace=True)
+        finally:
+            _BREAK_ON_MEMMAP = True
+        return self
+
+    @property
+    def data(self):
+        raise AttributeError
+
+
+_register_tensor_class(NonTensorStack)
+
+
+def _share_memory_nontensor(data, manager: Manager):
+    if isinstance(data, int):
+        return mp.Value(ctypes.c_int, data)
+    if isinstance(data, float):
+        return mp.Value(ctypes.c_double, data)
+    if isinstance(data, bool):
+        return mp.Value(ctypes.c_bool, data)
+    if isinstance(data, bytes):
+        return mp.Value(ctypes.c_byte, data)
+    if isinstance(data, dict):
+        result = manager.dict()
+        result.update(data)
+        return result
+    if isinstance(data, str):
+        result = mp.Array(ctypes.c_char, 100)
+        data = data.encode("utf-8")
+        result[: len(data)] = data
+        return result
+    if isinstance(data, list):
+        result = manager.list()
+        result.extend(data)
+        return result
+    # In all other cases, we just return the tensor. It's ok because the content
+    # will be passed to the remote process using regular serialization. We will
+    # lock the update in _update_shared_nontensor though.
+    return data
+
+
+def _from_shared_nontensor(nontensor):
+    if isinstance(nontensor, multiprocessing.managers.ListProxy):
+        return list(nontensor)
+    if isinstance(nontensor, multiprocessing.managers.DictProxy):
+        return dict(nontensor)
+    if isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
+        return nontensor.value
+    if isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
+        byte_list = []
+        for byte in nontensor:
+            if byte == b"\x00":
+                break
+            byte_list.append(byte)
+        return b"".join(byte_list).decode("utf-8")
+    return nontensor
+
+
+def _update_shared_nontensor(nontensor, val):
+    if isinstance(nontensor, multiprocessing.managers.ListProxy):
+        nontensor[:] = []
+        nontensor.extend(val)
+    elif isinstance(nontensor, multiprocessing.managers.DictProxy):
+        nontensor.clear()
+        nontensor.update(val)
+    elif isinstance(nontensor, multiprocessing.sharedctypes.Synchronized):
+        nontensor.value = val
+    elif isinstance(nontensor, multiprocessing.sharedctypes.SynchronizedArray):
+        val = val.encode("utf-8")
+        for i, byte in enumerate(nontensor):
+            if i < len(val):
+                v = val[i]
+                nontensor[i] = v
+            elif byte == b"\x00":
+                break
+            else:
+                nontensor[i] = b"\x00"
+        # nontensor[0] = val.encode("utf-8")
+    else:
+        raise NotImplementedError(
+            f"Updating {type(nontensor).__name__} within a shared/memmaped structure is not supported."
+        )
```

## tensordict/tensordict.py

 * *Ordering differences only*

```diff
@@ -1,37 +1,37 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict._lazy import LazyStackedTensorDict  # noqa: F401
-from tensordict._td import TensorDict  # noqa: F401
-from tensordict.base import (  # noqa: F401
-    is_tensor_collection,
-    NO_DEFAULT,
-    TensorDictBase,
-)
-from tensordict.functional import (  # noqa: F401
-    dense_stack_tds,
-    make_tensordict,
-    merge_tensordicts,
-    pad,
-    pad_sequence,
-)
-from tensordict.memmap import MemoryMappedTensor  # noqa: F401
-from tensordict.utils import (  # noqa: F401
-    assert_allclose_td,
-    cache,
-    convert_ellipsis_to_idx,
-    erase_cache,
-    expand_as_right,
-    expand_right,
-    implement_for,
-    index_keyedjaggedtensor,
-    infer_size_impl,
-    int_generator,
-    is_nested_key,
-    is_seq_of_nested_key,
-    is_tensorclass,
-    lock_blocked,
-    NestedKey,
-)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict._lazy import LazyStackedTensorDict  # noqa: F401
+from tensordict._td import TensorDict  # noqa: F401
+from tensordict.base import (  # noqa: F401
+    is_tensor_collection,
+    NO_DEFAULT,
+    TensorDictBase,
+)
+from tensordict.functional import (  # noqa: F401
+    dense_stack_tds,
+    make_tensordict,
+    merge_tensordicts,
+    pad,
+    pad_sequence,
+)
+from tensordict.memmap import MemoryMappedTensor  # noqa: F401
+from tensordict.utils import (  # noqa: F401
+    assert_allclose_td,
+    cache,
+    convert_ellipsis_to_idx,
+    erase_cache,
+    expand_as_right,
+    expand_right,
+    implement_for,
+    index_keyedjaggedtensor,
+    infer_size_impl,
+    int_generator,
+    is_nested_key,
+    is_seq_of_nested_key,
+    is_tensorclass,
+    lock_blocked,
+    NestedKey,
+)
```

## tensordict/utils.py

 * *Ordering differences only*

```diff
@@ -1,2261 +1,2261 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from __future__ import annotations
-
-import collections
-import concurrent.futures
-import inspect
-import logging
-
-import math
-import os
-
-import sys
-import time
-
-import warnings
-from collections import defaultdict, OrderedDict
-from collections.abc import KeysView
-from copy import copy
-from distutils.util import strtobool
-from functools import wraps
-from importlib import import_module
-from numbers import Number
-from textwrap import indent
-from typing import (
-    Any,
-    Callable,
-    Dict,
-    Iterator,
-    List,
-    Sequence,
-    Tuple,
-    TYPE_CHECKING,
-    TypeVar,
-    Union,
-)
-
-import numpy as np
-import torch
-
-try:
-    from functorch import dim as ftdim
-
-    _has_funcdim = True
-except ImportError:
-    _has_funcdim = False
-from packaging.version import parse
-from tensordict._contextlib import _DecoratorContextManager
-from tensordict._tensordict import (  # noqa: F401
-    _unravel_key_to_tuple,
-    unravel_key,
-    unravel_key_list,
-    unravel_keys,
-)
-
-from torch import Tensor
-from torch._C import _disabled_torch_function_impl
-from torch.nn.parameter import (
-    _ParameterMeta,
-    UninitializedBuffer,
-    UninitializedParameter,
-    UninitializedTensorMixin,
-)
-from torch.utils.data._utils.worker import _generate_state
-
-if TYPE_CHECKING:
-    from tensordict.tensordict import TensorDictBase
-
-try:
-    try:
-        from functorch._C import get_unwrapped, is_batchedtensor
-    except ImportError:
-        from torch._C._functorch import get_unwrapped, is_batchedtensor
-except ImportError:
-    pass
-
-TORCHREC_ERR = None
-try:
-    from torchrec import KeyedJaggedTensor
-
-    _has_torchrec = True
-except ImportError as err:
-    _has_torchrec = False
-
-    class KeyedJaggedTensor:  # noqa: D103, D101
-        pass
-
-    TORCHREC_ERR = err
-
-if not _has_funcdim:
-
-    class _ftdim_mock:
-        class Dim:
-            pass
-
-        class Tensor:
-            pass
-
-        def dims(self, *args, **kwargs):
-            raise ImportError("functorch.dim not found")
-
-    ftdim = _ftdim_mock  # noqa: F811
-
-T = TypeVar("T", bound="TensorDictBase")
-
-_STRDTYPE2DTYPE = {
-    str(dtype): dtype
-    for dtype in (
-        torch.float32,
-        torch.float64,
-        torch.float16,
-        torch.bfloat16,
-        torch.complex32,
-        torch.complex64,
-        torch.complex128,
-        torch.uint8,
-        torch.int8,
-        torch.int16,
-        torch.int32,
-        torch.int64,
-        torch.bool,
-        torch.quint8,
-        torch.qint8,
-        torch.qint32,
-        torch.quint4x2,
-    )
-}
-
-IndexType = Union[None, int, slice, str, Tensor, List[Any], Tuple[Any, ...]]
-DeviceType = Union[torch.device, str, int]
-NestedKey = Union[str, Tuple[str, ...]]
-
-_KEY_ERROR = 'key "{}" not found in {} with ' "keys {}"
-_LOCK_ERROR = (
-    "Cannot modify locked TensorDict. For in-place modification, consider "
-    "using the `set_()` method and make sure the key is present."
-)
-
-
-LOGGING_LEVEL = os.environ.get("TD_LOGGING_LEVEL", "DEBUG")
-logger = logging.getLogger("tensordict")
-logger.setLevel(getattr(logging, LOGGING_LEVEL))
-# Disable propagation to the root logger
-logger.propagate = False
-# Remove all attached handlers
-while logger.hasHandlers():
-    logger.removeHandler(logger.handlers[0])
-console_handler = logging.StreamHandler()
-console_handler.setLevel(logging.INFO)
-formatter = logging.Formatter("%(asctime)s [%(name)s][%(levelname)s] %(message)s")
-console_handler.setFormatter(formatter)
-logger.addHandler(console_handler)
-
-
-def _sub_index(tensor: Tensor, idx: IndexType) -> Tensor:
-    """Allows indexing of tensors with nested tuples.
-
-     >>> sub_tensor1 = tensor[tuple1][tuple2]
-     >>> sub_tensor2 = _sub_index(tensor, (tuple1, tuple2))
-     >>> assert torch.allclose(sub_tensor1, sub_tensor2)
-
-    Args:
-        tensor (Tensor): tensor to be indexed.
-        idx (tuple of indices): indices sequence to be used.
-
-    """
-    if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
-        idx0 = idx[0]
-        idx1 = idx[1:]
-        return _sub_index(_sub_index(tensor, idx0), idx1)
-    return tensor[idx]
-
-
-def convert_ellipsis_to_idx(
-    idx: tuple[int | Ellipsis] | Ellipsis, batch_size: list[int]
-) -> tuple[int, ...]:
-    """Given an index containing an ellipsis or just an ellipsis, converts any ellipsis to slice(None).
-
-    Example:
-        >>> idx = (..., 0)
-        >>> batch_size = [1,2,3]
-        >>> new_index = convert_ellipsis_to_idx(idx, batch_size)
-        >>> print(new_index)
-        (slice(None, None, None), slice(None, None, None), 0)
-
-    Args:
-        idx (tuple, Ellipsis): Input index
-        batch_size (list): Shape of tensor to be indexed
-
-    Returns:
-        new_index (tuple): Output index
-    """
-    istuple = isinstance(idx, tuple)
-    if (not istuple and idx is not Ellipsis) or (
-        istuple and all(_idx is not Ellipsis for _idx in idx)
-    ):
-        return idx
-    new_index = ()
-    num_dims = len(batch_size)
-
-    if idx is Ellipsis:
-        idx = (...,)
-
-    num_ellipsis = sum(_idx is Ellipsis for _idx in idx)
-    if num_dims < (len(idx) - num_ellipsis - sum(item is None for item in idx)):
-        raise RuntimeError("Not enough dimensions in TensorDict for index provided.")
-
-    start_pos, after_ellipsis_length = None, 0
-    for i, item in enumerate(idx):
-        if item is Ellipsis:
-            if start_pos is not None:
-                raise RuntimeError("An index can only have one ellipsis at most.")
-            else:
-                start_pos = i
-        if item is not Ellipsis and start_pos is not None:
-            after_ellipsis_length += 1
-        if item is None:
-            # unsqueeze
-            num_dims += 1
-
-    before_ellipsis_length = start_pos
-    if start_pos is None:
-        return idx
-    else:
-        ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
-
-    new_index += idx[:start_pos]
-
-    ellipsis_start = start_pos
-    ellipsis_end = start_pos + ellipsis_length
-    new_index += (slice(None),) * (ellipsis_end - ellipsis_start)
-
-    new_index += idx[start_pos + 1 : start_pos + 1 + after_ellipsis_length]
-
-    if len(new_index) != num_dims:
-        raise RuntimeError(
-            f"The new index {new_index} is incompatible with the dimensions of the batch size {num_dims}."
-        )
-
-    return new_index
-
-
-def _copy(self: list[int]) -> list[int]:
-    return list(self)
-
-
-def infer_size_impl(shape: list[int], numel: int) -> list[int]:
-    """Infers the shape of an expanded tensor whose number of elements is indicated by :obj:`numel`.
-
-    Copied from pytorch for compatibility issues (See #386).
-    See https://github.com/pytorch/pytorch/blob/35d4fa444b67cbcbe34a862782ddf2d92f5b1ce7/torch/jit/_shape_functions.py
-    for the original copy.
-
-    """
-    newsize = 1
-    infer_dim: int | None = None
-    for dim in range(len(shape)):
-        if shape[dim] == -1:
-            if infer_dim is not None:
-                raise AssertionError("only one dimension can be inferred")
-            infer_dim = dim
-        elif shape[dim] >= 0:
-            newsize *= shape[dim]
-        else:
-            raise AssertionError("invalid shape dimensions")
-    if not (
-        numel == newsize
-        or (infer_dim is not None and newsize > 0 and numel % newsize == 0)
-    ):
-        raise AssertionError("invalid shape")
-    out = _copy(shape)
-    if infer_dim is not None:
-        out[infer_dim] = numel // newsize
-    return out
-
-
-def _unwrap_value(value: Tensor) -> Tensor:
-    # batch_dims = value.ndimension()
-    if not isinstance(value, Tensor):
-        out = value
-    elif is_batchedtensor(value):
-        out = get_unwrapped(value)
-    else:
-        out = value
-    return out
-    # batch_dims = out.ndimension() - batch_dims
-    # batch_size = out.shape[:batch_dims]
-    # return out, batch_size
-
-
-if hasattr(math, "prod"):  # Python 3.8+
-
-    def prod(sequence):
-        """General prod function, that generalised usage across math and np.
-
-        Created for multiple python versions compatibility.
-
-        """
-        return math.prod(sequence)
-
-else:
-
-    def prod(sequence):
-        """General prod function, that generalised usage across math and np.
-
-        Created for multiple python versions compatibility.
-
-        """
-        return int(np.prod(sequence))
-
-
-def expand_as_right(
-    tensor: torch.Tensor | TensorDictBase,
-    dest: torch.Tensor | TensorDictBase,
-) -> torch.Tensor | TensorDictBase:
-    """Expand a tensor on the right to match another tensor shape.
-
-    Args:
-        tensor: tensor to be expanded
-        dest: tensor providing the target shape
-
-    Returns:
-         a tensor with shape matching the dest input tensor shape.
-
-    Examples:
-        >>> tensor = torch.zeros(3,4)
-        >>> dest = torch.zeros(3,4,5)
-        >>> print(expand_as_right(tensor, dest).shape)
-        torch.Size([3,4,5])
-
-    """
-    if dest.ndimension() < tensor.ndimension():
-        raise RuntimeError(
-            "expand_as_right requires the destination tensor to have less "
-            f"dimensions than the input tensor, got"
-            f" tensor.ndimension()={tensor.ndimension()} and "
-            f"dest.ndimension()={dest.ndimension()}"
-        )
-    if any(
-        tensor.shape[i] != dest.shape[i] and tensor.shape[i] != 1
-        for i in range(tensor.ndimension())
-    ):
-        raise RuntimeError(
-            f"tensor shape is incompatible with dest shape, "
-            f"got: tensor.shape={tensor.shape}, dest={dest.shape}"
-        )
-    for _ in range(dest.ndimension() - tensor.ndimension()):
-        tensor = tensor.unsqueeze(-1)
-    return tensor.expand(dest.shape)
-
-
-def expand_right(tensor: Tensor, shape: Sequence[int]) -> Tensor:
-    """Expand a tensor on the right to match a desired shape.
-
-    Args:
-        tensor: tensor to be expanded
-        shape: target shape
-
-    Returns:
-         a tensor with shape matching the target shape.
-
-    Examples:
-        >>> tensor = torch.zeros(3,4)
-        >>> shape = (3,4,5)
-        >>> print(expand_right(tensor, shape).shape)
-        torch.Size([3,4,5])
-
-    """
-    tensor_expand = tensor
-    while tensor_expand.ndimension() < len(shape):
-        tensor_expand = tensor_expand.unsqueeze(-1)
-    tensor_expand = tensor_expand.expand(shape)
-    return tensor_expand
-
-
-NUMPY_TO_TORCH_DTYPE_DICT = {
-    np.dtype("bool"): torch.bool,
-    np.dtype("uint8"): torch.uint8,
-    np.dtype("int8"): torch.int8,
-    np.dtype("int16"): torch.int16,
-    np.dtype("int32"): torch.int32,
-    np.dtype("int64"): torch.int64,
-    np.dtype("float16"): torch.float16,
-    np.dtype("float32"): torch.float32,
-    np.dtype("float64"): torch.float64,
-    np.dtype("complex64"): torch.complex64,
-    np.dtype("complex128"): torch.complex128,
-}
-TORCH_TO_NUMPY_DTYPE_DICT = {
-    value: key for key, value in NUMPY_TO_TORCH_DTYPE_DICT.items()
-}
-
-
-def is_nested_key(key: NestedKey) -> bool:
-    """Returns True if key is a NestedKey."""
-    if isinstance(key, str):
-        return True
-    if key and isinstance(key, (list, tuple)):
-        return all(isinstance(subkey, str) for subkey in key)
-    return False
-
-
-def is_seq_of_nested_key(seq: Sequence[NestedKey]) -> bool:
-    """Returns True if seq is a Sequence[NestedKey]."""
-    if seq and isinstance(seq, Sequence):
-        return all(is_nested_key(k) for k in seq)
-    elif isinstance(seq, Sequence):
-        # we allow empty inputs
-        return True
-    return False
-
-
-def index_keyedjaggedtensor(
-    kjt: KeyedJaggedTensor, index: slice | range | list | torch.Tensor | np.ndarray
-) -> KeyedJaggedTensor:
-    """Indexes a KeyedJaggedTensor along the batch dimension.
-
-    Args:
-        kjt (KeyedJaggedTensor): a KeyedJaggedTensor to index
-        index (torch.Tensor or other indexing type): batch index to use.
-            Indexing with an integer will result in an error.
-
-    Examples:
-        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
-        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
-        >>>
-        >>> jag_tensor = KeyedJaggedTensor(
-        ...     values=values,
-        ...     keys=keys,
-        ...     offsets=offsets,
-        ...     weights=weights,
-        ... )
-        >>> ikjt = index_keyedjaggedtensor(jag_tensor, [0, 2])
-        >>> print(ikjt["index_0"].to_padded_dense(), j0.to_padded_dense())
-
-    """
-    if not _has_torchrec:
-        raise TORCHREC_ERR
-    if isinstance(index, (int,)):
-        raise ValueError(
-            "Indexing KeyedJaggedTensor instances with an integer is prohibited, "
-            "as this would result in a KeyedJaggedTensor without batch size. "
-            "If you want to get a single element from a KeyedJaggedTensor, "
-            "call `index_keyedjaggedtensor(kjt, torch.tensor([index]))` instead."
-        )
-    lengths = kjt.lengths()
-    keys = kjt.keys()
-    numel = len(lengths) // len(keys)
-    offsets = kjt.offsets()
-
-    _offsets1 = offsets[:-1].view(len(keys), numel)[:, index]
-    _offsets2 = offsets[1:].view(len(keys), numel)[:, index]
-    lengths = lengths.view(len(keys), numel)[:, index].reshape(-1)
-
-    full_index = torch.arange(offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.unsqueeze(-1)) & (
-        full_index < _offsets2.unsqueeze(-1)
-    )
-    sel = sel.any(0).any(0)
-    full_index = full_index.squeeze()[sel]
-    values = kjt._values[full_index]
-    weights = kjt._weights[full_index]
-    return KeyedJaggedTensor(
-        values=values, keys=kjt.keys(), weights=weights, lengths=lengths
-    )
-
-
-def setitem_keyedjaggedtensor(
-    orig_tensor: KeyedJaggedTensor,
-    index: slice | range | list | torch.Tensor | np.ndarray,
-    other: KeyedJaggedTensor,
-) -> KeyedJaggedTensor:
-    """Equivalent of `tensor[index] = other` for KeyedJaggedTensors indexed along the batch dimension.
-
-    Args:
-        orig_tensor (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be updated.
-        index (list or equivalent index): batch index to be written.
-        other (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be written at
-            the batch locations.
-
-    Examples:
-        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
-        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
-        >>> jag_tensor = KeyedJaggedTensor(
-        ...    values=values,
-        ...    keys=keys,
-        ...    offsets=offsets,
-        ...    weights=weights,
-        ... )
-        >>> keys = ["index_0", "index_1", "index_2"]
-        >>> lengths2 = torch.IntTensor([2, 4, 6, 4, 2, 1])
-        >>> values2 = torch.zeros(
-        ...     lengths2.sum(),
-        ... )
-        >>> weights2 = -torch.ones(
-        ...     lengths2.sum(),
-        ... )
-        >>> sub_jag_tensor = KeyedJaggedTensor(
-        ...     values=values2,
-        ...     keys=keys,
-        ...     lengths=lengths2,
-        ...     weights=weights2,
-        ... )
-        >>> setitem_keyedjaggedtensor(jag_tensor, [0, 2], sub_jag_tensor)
-    """
-    orig_tensor_lengths = orig_tensor.lengths()
-    orig_tensor_keys = orig_tensor.keys()
-    orig_tensor_numel = len(orig_tensor_lengths) // len(orig_tensor_keys)
-    orig_tensor_offsets = orig_tensor.offsets()
-
-    other_lengths = other.lengths()
-    other_keys = other.keys()
-    other_numel = len(other_lengths) // len(other_keys)
-    # other_offsets = other.offsets()
-
-    if not other_keys == orig_tensor_keys:
-        raise KeyError("Mismatch in orig_tensor and other keys.")
-    #     if other_numel - len(index) != orig_tensor_numel:
-    #         raise RuntimeError("orig_tensor and otherination batch differ.")
-
-    _offsets1 = orig_tensor_offsets[:-1]
-    _offsets2 = orig_tensor_offsets[1:]
-    _orig_tensor_shape = len(orig_tensor_keys), orig_tensor_numel
-
-    _lengths_out = orig_tensor_lengths.view(_orig_tensor_shape).clone()
-    _lengths_out[:, index] = other_lengths.view(len(orig_tensor_keys), other_numel)
-    _lengths_out = _lengths_out.view(-1)
-
-    # get the values of orig_tensor that we'll be keeping
-    full_index = torch.arange(orig_tensor_offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = (~sel).all(0).all(0)
-    index_to_keep = full_index.squeeze()[sel]
-    values_to_keep = orig_tensor._values[index_to_keep]
-    new_values = other._values
-    weights_to_keep = orig_tensor._weights[index_to_keep]
-    new_weights = other._weights
-
-    # compute new offsets
-    _offsets = torch.cat([_lengths_out[:1] * 0, _lengths_out], 0)
-    _offsets = _offsets.cumsum(0)
-
-    # get indices of offsets for new elts
-    _offsets1 = _offsets[:-1]
-    _offsets2 = _offsets[1:]
-    full_index = torch.arange(_offsets[-1]).view(1, 1, -1)
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = sel.any(0).any(0)
-    new_index_new_elts = full_index.squeeze()[sel]
-    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
-        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
-    )
-    sel = (~sel).all(0).all(0)
-    new_index_to_keep = full_index.squeeze()[sel]
-
-    # create an empty values tensor
-    values_numel = values_to_keep.shape[0] + other._values.shape[0]
-    tensor = torch.empty(
-        [values_numel, *values_to_keep.shape[1:]],
-        dtype=values_to_keep.dtype,
-        device=values_to_keep.device,
-    )
-    tensor_weights = torch.empty(
-        [values_numel, *values_to_keep.shape[1:]],
-        dtype=weights_to_keep.dtype,
-        device=weights_to_keep.device,
-    )
-    tensor[new_index_to_keep] = values_to_keep
-    tensor[new_index_new_elts] = new_values
-    tensor_weights[new_index_to_keep] = weights_to_keep
-    tensor_weights[new_index_new_elts] = new_weights
-
-    kjt = KeyedJaggedTensor(
-        values=tensor,
-        keys=orig_tensor_keys,
-        weights=tensor_weights,
-        lengths=_lengths_out,
-    )
-    for k, item in kjt.__dict__.items():
-        orig_tensor.__dict__[k] = item
-    return orig_tensor
-
-
-def _ndimension(tensor: Tensor) -> int:
-    if isinstance(tensor, Tensor):
-        return tensor.ndimension()
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return 1
-    else:
-        return tensor.ndimension()
-
-
-def _shape(tensor: Tensor, nested_shape=False) -> torch.Size:
-    if isinstance(tensor, UninitializedTensorMixin):
-        return torch.Size([*getattr(tensor, "batch_size", ()), -1])
-    elif not isinstance(tensor, Tensor):
-        if type(tensor) is KeyedJaggedTensor:
-            return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
-        return tensor.shape
-    if tensor.is_nested:
-        if nested_shape:
-            return tensor._nested_tensor_size()
-        shape = []
-        for i in range(tensor.ndim):
-            try:
-                shape.append(tensor.size(i))
-            except RuntimeError:
-                shape.append(-1)
-        return torch.Size(shape)
-    return tensor.shape
-
-
-def _device(tensor: Tensor) -> torch.device:
-    if isinstance(tensor, Tensor):
-        return tensor.device
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor.device()
-    else:
-        return tensor.device
-
-
-def _is_shared(tensor: Tensor) -> bool:
-    if isinstance(tensor, Tensor):
-        if torch._C._functorch.is_batchedtensor(tensor):
-            return None
-        return tensor.is_shared()
-    if isinstance(tensor, ftdim.Tensor):
-        return None
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return False
-    else:
-        return tensor.is_shared()
-
-
-def _is_meta(tensor: Tensor) -> bool:
-    if isinstance(tensor, Tensor):
-        return tensor.is_meta
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return False
-    else:
-        return tensor.is_meta
-
-
-def _dtype(tensor: Tensor) -> torch.dtype:
-    if isinstance(tensor, Tensor):
-        return tensor.dtype
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor._values.dtype
-    else:
-        return tensor.dtype
-
-
-def _get_item(tensor: Tensor, index: IndexType) -> Tensor:
-    if isinstance(tensor, Tensor):
-        try:
-            return tensor[index]
-        except IndexError as err:
-            # try to map list index to tensor, and assess type. If bool, we
-            # likely have a nested list of booleans which is not supported by pytorch
-            if _is_lis_of_list_of_bools(index):
-                index = torch.tensor(index, device=tensor.device)
-                if index.dtype is torch.bool:
-                    warnings.warn(
-                        "Indexing a tensor with a nested list of boolean values is "
-                        "going to be deprecated as this functionality is not supported "
-                        f"by PyTorch. (follows error: {err})",
-                        category=DeprecationWarning,
-                    )
-                return tensor[index]
-            raise err
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return index_keyedjaggedtensor(tensor, index)
-    else:
-        return tensor[index]
-
-
-def _set_item(
-    tensor: Tensor, index: IndexType, value: Tensor, *, validated, non_blocking
-) -> Tensor:
-    # the tensor must be validated
-    if not validated:
-        raise RuntimeError
-    if isinstance(tensor, Tensor):
-        tensor[index] = value
-        return tensor
-    elif isinstance(tensor, KeyedJaggedTensor):
-        tensor = setitem_keyedjaggedtensor(tensor, index, value)
-        return tensor
-    from tensordict.tensorclass import NonTensorData, NonTensorStack
-
-    if is_non_tensor(tensor):
-        if (
-            isinstance(value, NonTensorData)
-            and isinstance(tensor, NonTensorData)
-            and tensor.data == value.data
-        ):
-            return tensor
-        elif isinstance(tensor, NonTensorData):
-            tensor = NonTensorStack.from_nontensordata(tensor)
-        if tensor.stack_dim != 0:
-            tensor = NonTensorStack(*tensor.unbind(0), stack_dim=0)
-        tensor[index] = value
-        return tensor
-    else:
-        tensor[index] = value
-        return tensor
-
-
-def _requires_grad(tensor: Tensor) -> bool:
-    if isinstance(tensor, Tensor):
-        return tensor.requires_grad
-    elif isinstance(tensor, KeyedJaggedTensor):
-        return tensor._values.requires_grad
-    else:
-        return tensor.requires_grad
-
-
-class timeit:
-    """A dirty but easy to use decorator for profiling code."""
-
-    _REG = {}
-
-    def __init__(self, name) -> None:
-        self.name = name
-
-    def __call__(self, fn):
-        @wraps(fn)
-        def decorated_fn(*args, **kwargs):
-            with self:
-                out = fn(*args, **kwargs)
-                return out
-
-        return decorated_fn
-
-    def __enter__(self):
-        self.t0 = time.time()
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        t = time.time() - self.t0
-        val = self._REG.setdefault(self.name, [0.0, 0.0, 0])
-
-        count = val[2]
-        N = count + 1
-        val[0] = val[0] * (count / N) + t / N
-        val[1] += t
-        val[2] = N
-
-    @staticmethod
-    def print(prefix=None):  # noqa: T202
-        keys = list(timeit._REG)
-        keys.sort()
-        for name in keys:
-            strings = []
-            if prefix:
-                strings.append(prefix)
-            strings.append(
-                f"{name} took {timeit._REG[name][0] * 1000:4.4} msec (total = {timeit._REG[name][1]} sec)"
-            )
-            logger.info(" -- ".join(strings))
-
-    @staticmethod
-    def erase():
-        for k in timeit._REG:
-            timeit._REG[k] = [0.0, 0.0, 0]
-
-
-def int_generator(seed):
-    """A pseudo-random chaing generator.
-
-    To be used to produce deterministic integer sequences
-
-    Examples:
-        >>> for _ in range(2):
-        ...     init_int = 10
-        ...     for _ in range(10):
-        ...        init_int = int_generator(init_int)
-        ...        print(init_int, end=", ")
-        ...     print("")
-        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
-        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
-    """
-    max_seed_val = 10_000
-    rng = np.random.default_rng(seed)
-    seed = int.from_bytes(rng.bytes(8), "big")
-    return seed % max_seed_val
-
-
-def _is_lis_of_list_of_bools(index, first_level=True):
-    # determines if an index is a list of list of bools.
-    # this is aimed at catching a deprecation feature where list of list
-    # of bools are valid indices
-    if first_level:
-        if not isinstance(index, list):
-            return False
-        if not len(index):
-            return False
-        if isinstance(index[0], list):
-            return _is_lis_of_list_of_bools(index[0], False)
-        return False
-    # then we know it is a list of lists
-    if isinstance(index[0], bool):
-        return True
-    if isinstance(index[0], list):
-        return _is_lis_of_list_of_bools(index[0], False)
-    return False
-
-
-def is_tensorclass(obj: type | Any) -> bool:
-    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
-    cls = obj if isinstance(obj, type) else type(obj)
-    return _is_tensorclass(cls)
-
-
-def _is_tensorclass(cls: type) -> bool:
-    return getattr(cls, "_is_tensorclass", False)
-
-
-class implement_for:
-    """A version decorator that checks the version in the environment and implements a function with the fitting one.
-
-    If specified module is missing or there is no fitting implementation, call of the decorated function
-    will lead to the explicit error.
-    In case of intersected ranges, last fitting implementation is used.
-
-    Args:
-        module_name (str or callable): version is checked for the module with this
-            name (e.g. "gym"). If a callable is provided, it should return the
-            module.
-        from_version: version from which implementation is compatible. Can be open (None).
-        to_version: version from which implementation is no longer compatible. Can be open (None).
-
-    Examples:
-        >>> @implement_for("torch", None, "1.13")
-        >>> def fun(self, x):
-        ...     # Older torch versions will return x + 1
-        ...     return x + 1
-        ...
-        >>> @implement_for("torch", "0.13", "2.0")
-        >>> def fun(self, x):
-        ...     # More recent torch versions will return x + 2
-        ...     return x + 2
-        ...
-        >>> @implement_for(lambda: import_module("torch"), "0.", None)
-        >>> def fun(self, x):
-        ...     # More recent gym versions will return x + 2
-        ...     return x + 2
-        ...
-        >>> @implement_for("gymnasium", "0.27", None)
-        >>> def fun(self, x):
-        ...     # If gymnasium is to be used instead of gym, x+3 will be returned
-        ...     return x + 3
-        ...
-
-        This indicates that the function is compatible with gym 0.13+, but doesn't with gym 0.14+.
-    """
-
-    # Stores pointers to fitting implementations: dict[func_name] = func_pointer
-    _implementations = {}
-    _setters = []
-    _cache_modules = {}
-
-    def __init__(
-        self,
-        module_name: Union[str, Callable],
-        from_version: str = None,
-        to_version: str = None,
-    ):
-        self.module_name = module_name
-        self.from_version = from_version
-        self.to_version = to_version
-        implement_for._setters.append(self)
-
-    @staticmethod
-    def check_version(version: str, from_version: str | None, to_version: str | None):
-        version = parse(".".join([str(v) for v in parse(version).release]))
-        return (from_version is None or version >= parse(from_version)) and (
-            to_version is None or version < parse(to_version)
-        )
-
-    @staticmethod
-    def get_class_that_defined_method(f):
-        """Returns the class of a method, if it is defined, and None otherwise."""
-        return f.__globals__.get(f.__qualname__.split(".")[0], None)
-
-    @classmethod
-    def get_func_name(cls, fn):
-        # produces a name like torchrl.module.Class.method or torchrl.module.function
-        first = str(fn).split(".")[0][len("<function ") :]
-        last = str(fn).split(".")[1:]
-        if last:
-            first = [first]
-            last[-1] = last[-1].split(" ")[0]
-        else:
-            last = [first.split(" ")[0]]
-            first = []
-        return ".".join([fn.__module__] + first + last)
-
-    def _get_cls(self, fn):
-        cls = self.get_class_that_defined_method(fn)
-        if cls is None:
-            # class not yet defined
-            return
-        if cls.__class__.__name__ == "function":
-            cls = inspect.getmodule(fn)
-        return cls
-
-    def module_set(self):
-        """Sets the function in its module, if it exists already."""
-        prev_setter = type(self)._implementations.get(self.get_func_name(self.fn), None)
-        if prev_setter is not None:
-            prev_setter.do_set = False
-        type(self)._implementations[self.get_func_name(self.fn)] = self
-        cls = self.get_class_that_defined_method(self.fn)
-        if cls is not None:
-            if cls.__class__.__name__ == "function":
-                cls = inspect.getmodule(self.fn)
-        else:
-            # class not yet defined
-            return
-        setattr(cls, self.fn.__name__, self.fn)
-
-    @classmethod
-    def import_module(cls, module_name: Union[Callable, str]) -> str:
-        """Imports module and returns its version."""
-        if not callable(module_name):
-            module = cls._cache_modules.get(module_name, None)
-            if module is None:
-                if module_name in sys.modules:
-                    sys.modules[module_name] = module = import_module(module_name)
-                else:
-                    cls._cache_modules[module_name] = module = import_module(
-                        module_name
-                    )
-        else:
-            module = module_name()
-        return module.__version__
-
-    _lazy_impl = collections.defaultdict(list)
-
-    def _delazify(self, func_name):
-        for local_call in implement_for._lazy_impl[func_name]:
-            out = local_call()
-        return out
-
-    def __call__(self, fn):
-        # function names are unique
-        self.func_name = self.get_func_name(fn)
-        self.fn = fn
-        implement_for._lazy_impl[self.func_name].append(self._call)
-
-        @wraps(fn)
-        def _lazy_call_fn(*args, **kwargs):
-            # first time we call the function, we also do the replacement.
-            # This will cause the imports to occur only during the first call to fn
-            return self._delazify(self.func_name)(*args, **kwargs)
-
-        return _lazy_call_fn
-
-    def _call(self):
-
-        # If the module is missing replace the function with the mock.
-        fn = self.fn
-        func_name = self.func_name
-        implementations = implement_for._implementations
-
-        @wraps(fn)
-        def unsupported(*args, **kwargs):
-            raise ModuleNotFoundError(
-                f"Supported version of '{func_name}' has not been found."
-            )
-
-        self.do_set = False
-        # Return fitting implementation if it was encountered before.
-        if func_name in implementations:
-            try:
-                # check that backends don't conflict
-                version = self.import_module(self.module_name)
-                if self.check_version(version, self.from_version, self.to_version):
-                    self.do_set = True
-                if not self.do_set:
-                    return implementations[func_name].fn
-            except ModuleNotFoundError:
-                # then it's ok, there is no conflict
-                return implementations[func_name].fn
-        else:
-            try:
-                version = self.import_module(self.module_name)
-                if self.check_version(version, self.from_version, self.to_version):
-                    self.do_set = True
-            except ModuleNotFoundError:
-                return unsupported
-        if self.do_set:
-            self.module_set()
-            return fn
-        return unsupported
-
-    @classmethod
-    def reset(cls, setters_dict: Dict[str, implement_for] = None):
-        """Resets the setters in setter_dict.
-
-        ``setter_dict`` is a copy of implementations. We just need to iterate through its
-        values and call :meth:`~.module_set` for each.
-
-        """
-        if setters_dict is None:
-            setters_dict = copy(cls._implementations)
-        for setter in setters_dict.values():
-            setter.module_set()
-
-    def __repr__(self):
-        return (
-            f"{self.__class__.__name__}("
-            f"module_name={self.module_name}({self.from_version, self.to_version}), "
-            f"fn_name={self.fn.__name__}, cls={self._get_cls(self.fn)}, is_set={self.do_set})"
-        )
-
-
-def _unfold_sequence(seq):
-    for item in seq:
-        if isinstance(item, (list, tuple)):
-            yield tuple(_unfold_sequence(item))
-        else:
-            if isinstance(item, (str, int, slice)) or item is Ellipsis:
-                yield item
-            else:
-                yield id(item)
-
-
-def _make_cache_key(args, kwargs):
-    """Creates a key for the cache such that memory footprint is minimized."""
-    # Fast path for the common args
-    if not args and not kwargs:
-        return ((), ())
-    elif not kwargs and len(args) == 1 and type(args[0]) is str:
-        return (args, ())
-    else:
-        return (
-            tuple(_unfold_sequence(args)),
-            tuple(_unfold_sequence(sorted(kwargs.items()))),
-        )
-
-
-def cache(fun):
-    """A cache for TensorDictBase subclasses.
-
-    This decorator will cache the values returned by a method as long as the
-    input arguments match.
-    Leaves (tensors and such) are not cached.
-    The cache is stored within the tensordict such that it can be erased at any
-    point in time.
-
-    Examples:
-        >>> import timeit
-        >>> from tensordict import TensorDict
-        >>> class SomeOtherTd(TensorDict):
-        ...     @cache
-        ...     def all_keys(self):
-        ...         return set(self.keys(include_nested=True))
-        >>> td = SomeOtherTd({("a", "b", "c", "d", "e", "f", "g"): 1.0}, [])
-        >>> td.lock_()
-        >>> print(timeit.timeit("set(td.keys(True))", globals={'td': td}))
-        11.057
-        >>> print(timeit.timeit("set(td.all_keys())", globals={'td': td}))
-        0.88
-    """
-
-    @wraps(fun)
-    def newfun(_self: "TensorDictBase", *args, **kwargs):
-        if not _self.is_locked:
-            return fun(_self, *args, **kwargs)
-        cache = _self._cache
-        if cache is None:
-            cache = _self._cache = defaultdict(dict)
-        cache = cache[fun.__name__]
-        key = _make_cache_key(args, kwargs)
-        if key not in cache:
-            out = fun(_self, *args, **kwargs)
-            if not isinstance(out, (Tensor, KeyedJaggedTensor)):
-                # we don't cache tensors to avoid filling the mem and / or
-                # stacking them from their origin
-                cache[key] = out
-        else:
-            out = cache[key]
-        return out
-
-    return newfun
-
-
-def erase_cache(fun):
-    """A decorator to erase the cache at each call."""
-
-    @wraps(fun)
-    def new_fun(self, *args, **kwargs):
-        self._erase_cache()
-        return fun(self, *args, **kwargs)
-
-    return new_fun
-
-
-_NON_STR_KEY_TUPLE_ERR = "Nested membership checks with tuples of strings is only supported when setting `include_nested=True`."
-_NON_STR_KEY_ERR = "TensorDict keys are always strings. Membership checks are only supported for strings or non-empty tuples of strings (for nested TensorDicts)"
-_GENERIC_NESTED_ERR = "Only NestedKeys are supported. Got key {}."
-
-
-class _StringKeys(KeysView):
-    """A key view where contains is restricted to strings."""
-
-    def __contains__(self, item):
-        if not isinstance(item, str):
-            try:
-                unravel_item = _unravel_key_to_tuple(item)
-                if not unravel_item:  # catch errors during unravel
-                    raise TypeError
-            except Exception:
-                raise TypeError(_NON_STR_KEY_ERR)
-            if len(unravel_item) > 1:
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-            else:
-                item = unravel_item[0]
-        return super().__contains__(item)
-
-
-class _StringOnlyDict(dict):
-    """A dict class where contains is restricted to strings."""
-
-    # kept here for debugging
-    # def __setitem__(self, key, value):
-    #     if not isinstance(key, str):
-    #         raise RuntimeError
-    #     return super().__setitem__(key, value)
-
-    def __contains__(self, item):
-        if not isinstance(item, str):
-            try:
-                unravel_item = _unravel_key_to_tuple(item)
-                if not unravel_item:  # catch errors during unravel
-                    raise TypeError
-            except Exception:
-                raise TypeError(_NON_STR_KEY_ERR)
-            if len(unravel_item) > 1:
-                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
-            else:
-                item = unravel_item[0]
-        return super().__contains__(item)
-
-    def keys(self):
-        return _StringKeys(self)
-
-
-def lock_blocked(func):
-    """Checks that the tensordict is unlocked before executing a function."""
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        if self.is_locked:
-            raise RuntimeError(_LOCK_ERROR)
-        return func(self, *args, **kwargs)
-
-    return new_func
-
-
-class as_decorator:
-    """Converts a method to a decorator.
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> data = TensorDict({}, [])
-        >>> with data.lock_(): # lock_ is decorated
-        ...     assert data.is_locked
-        >>> assert not data.is_locked
-    """
-
-    def __init__(self, attr=None):
-        self.attr = attr
-
-    def __call__(self, func):
-        if self.attr is not None:
-
-            @wraps(func)
-            def new_func(_self, *args, **kwargs):
-                _attr_pre = getattr(_self, self.attr)
-                out = func(_self, *args, **kwargs)
-                _attr_post = getattr(_self, self.attr)
-                if out is not None:
-                    if _attr_post is not _attr_pre:
-                        out._last_op = (new_func.__name__, (args, kwargs, _self))
-                    else:
-                        out._last_op = None
-                return out
-
-        else:
-
-            @wraps(func)
-            def new_func(_self, *args, **kwargs):
-                out = func(_self, *args, **kwargs)
-                if out is not None:
-                    out._last_op = (new_func.__name__, (args, kwargs, _self))
-                return out
-
-        return new_func
-
-
-def _split_tensordict(
-    td,
-    chunksize,
-    num_chunks,
-    num_workers,
-    dim,
-    use_generator=False,
-    to_tensordict=False,
-):
-    if chunksize is None and num_chunks is None:
-        num_chunks = num_workers
-    if chunksize is not None and num_chunks is not None:
-        raise ValueError(
-            "Either chunksize or num_chunks must be provided, but not both."
-        )
-    if num_chunks is not None:
-        num_chunks = min(td.shape[dim], num_chunks)
-        if use_generator:
-
-            def _chunk_generator():
-                chunksize = -(td.shape[dim] // -num_chunks)
-                idx_start = 0
-                base = (slice(None),) * dim
-                for _ in range(num_chunks):
-                    idx_end = idx_start + chunksize
-                    out = td[base + (slice(idx_start, idx_end),)]
-                    if to_tensordict:
-                        out = out.to_tensordict()
-                    yield out
-                    idx_start = idx_end
-
-            return _chunk_generator()
-        return td.chunk(num_chunks, dim=dim)
-    else:
-        if chunksize == 0:
-            if use_generator:
-
-                def _unbind_generator():
-                    base = (slice(None),) * dim
-                    for i in range(td.shape[dim]):
-                        out = td[base + (i,)]
-                        if to_tensordict:
-                            out = out.to_tensordict()
-                        yield out
-
-                return _unbind_generator()
-            return td.unbind(dim=dim)
-        if use_generator:
-
-            def _split_generator():
-                idx_start = 0
-                base = (slice(None),) * dim
-                for _ in range(num_chunks):
-                    idx_end = idx_start + chunksize
-                    out = td[base + (slice(idx_start, idx_end),)]
-                    if to_tensordict:
-                        out = out.to_tensordict()
-                    yield out
-                    idx_start = idx_end
-
-            return _split_generator()
-        chunksize = min(td.shape[dim], chunksize)
-        return td.split(chunksize, dim=dim)
-
-
-def _parse_to(*args, **kwargs):
-    batch_size = kwargs.pop("batch_size", None)
-    other = kwargs.pop("other", None)
-    device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
-        *args, **kwargs
-    )
-    if other is not None:
-        if device is not None and device != other.device:
-            raise ValueError("other and device cannot be both passed")
-        device = other.device
-        dtypes = {val.dtype for val in other.values(True, True)}
-        if len(dtypes) > 1 or len(dtypes) == 0:
-            dtype = None
-        elif len(dtypes) == 1:
-            dtype = list(dtypes)[0]
-    return device, dtype, non_blocking, convert_to_format, batch_size
-
-
-class _ErrorInteceptor:
-    """Context manager for catching errors and modifying message.
-
-    Intended for use with stacking / concatenation operations applied to TensorDicts.
-
-    """
-
-    DEFAULT_EXC_MSG = "Expected all tensors to be on the same device"
-
-    def __init__(
-        self,
-        key: NestedKey,
-        prefix: str,
-        exc_msg: str | None = None,
-        exc_type: type[Exception] | None = None,
-    ) -> None:
-        self.exc_type = exc_type if exc_type is not None else RuntimeError
-        self.exc_msg = exc_msg if exc_msg is not None else self.DEFAULT_EXC_MSG
-        self.prefix = prefix
-        self.key = key
-
-    def _add_key_to_error_msg(self, msg: str) -> str:
-        if msg.startswith(self.prefix):
-            return f'{self.prefix} "{self.key}" /{msg[len(self.prefix):]}'
-        return f'{self.prefix} "{self.key}". {msg}'
-
-    def __enter__(self):
-        pass
-
-    def __exit__(self, exc_type, exc_value, _):
-        if exc_type is self.exc_type and (
-            self.exc_msg is None or self.exc_msg in str(exc_value)
-        ):
-            exc_value.args = (self._add_key_to_error_msg(str(exc_value)),)
-
-
-def _nested_keys_to_dict(keys: Iterator[NestedKey]) -> dict[str, Any]:
-    nested_keys = {}
-    for key in keys:
-        if isinstance(key, str):
-            nested_keys.setdefault(key, {})
-        else:
-            d = nested_keys
-            for subkey in key:
-                d = d.setdefault(subkey, {})
-    return nested_keys
-
-
-def _dict_to_nested_keys(
-    nested_keys: dict[NestedKey, NestedKey], prefix: tuple[str, ...] = ()
-) -> tuple[str, ...]:
-    for key, subkeys in nested_keys.items():
-        if subkeys:
-            yield from _dict_to_nested_keys(subkeys, prefix=(*prefix, key))
-        elif prefix:
-            yield (*prefix, key)
-        else:
-            yield key
-
-
-def _default_hook(td: T, key: tuple[str, ...]) -> None:
-    """Used to populate a tensordict.
-
-    For example, ``td.set(("a", "b"))`` may require to create ``"a"``.
-
-    """
-    out = td.get(key[0], None)
-    if out is None:
-        td._create_nested_str(key[0])
-        out = td._get_str(key[0], None)
-    return out
-
-
-def _get_leaf_tensordict(
-    tensordict: T, key: tuple[str, ...], hook: Callable = None
-) -> tuple[TensorDictBase, str]:
-    # utility function for traversing nested tensordicts
-    # hook should return the default value for tensordit.get(key)
-    while len(key) > 1:
-        if hook is not None:
-            tensordict = hook(tensordict, key)
-        else:
-            tensordict = tensordict.get(key[0])
-        key = key[1:]
-    return tensordict, key[0]
-
-
-def assert_allclose_td(
-    actual: T,
-    expected: T,
-    rtol: float | None = None,
-    atol: float | None = None,
-    equal_nan: bool = True,
-    msg: str = "",
-) -> bool:
-    """Compares two tensordicts and raise an exception if their content does not match exactly."""
-    from tensordict.base import _is_tensor_collection
-
-    if not _is_tensor_collection(actual.__class__) or not _is_tensor_collection(
-        expected.__class__
-    ):
-        raise TypeError("assert_allclose inputs must be of TensorDict type")
-
-    from tensordict._lazy import LazyStackedTensorDict
-
-    if isinstance(actual, LazyStackedTensorDict) and isinstance(
-        expected, LazyStackedTensorDict
-    ):
-        for sub_actual, sub_expected in zip(actual.tensordicts, expected.tensordicts):
-            assert_allclose_td(sub_actual, sub_expected, rtol=rtol, atol=atol)
-        return True
-
-    try:
-        set1 = set(
-            actual.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
-        )
-        set2 = set(
-            expected.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
-        )
-    except ValueError:
-        # Persistent tensordicts do not work with is_leaf
-        set1 = set(actual.keys())
-        set2 = set(expected.keys())
-    if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
-        raise KeyError(
-            "actual and expected tensordict keys mismatch, "
-            f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
-            f"the other."
-        )
-    keys = sorted(actual.keys(), key=str)
-    for key in keys:
-        input1 = actual.get(key)
-        input2 = expected.get(key)
-        if _is_tensor_collection(input1.__class__):
-            if is_non_tensor(input1):
-                # We skip non-tensor data
-                continue
-            assert_allclose_td(input1, input2, rtol=rtol, atol=atol)
-            continue
-
-        mse = (input1.to(torch.float) - input2.to(torch.float)).pow(2).sum()
-        mse = mse.div(input1.numel()).sqrt().item()
-
-        default_msg = f"key {key} does not match, got mse = {mse:4.4f}"
-        msg = "\t".join([default_msg, msg]) if len(msg) else default_msg
-        torch.testing.assert_close(
-            input1, input2, rtol=rtol, atol=atol, equal_nan=equal_nan, msg=msg
-        )
-    return True
-
-
-def _get_repr(tensor: Tensor) -> str:
-    s = ", ".join(
-        [
-            f"shape={_shape(tensor)}",
-            f"device={_device(tensor)}",
-            f"dtype={_dtype(tensor)}",
-            f"is_shared={_is_shared(tensor)}",
-        ]
-    )
-    return f"{tensor.__class__.__name__}({s})"
-
-
-def _get_repr_custom(cls, shape, device, dtype, is_shared) -> str:
-    s = ", ".join(
-        [
-            f"shape={shape}",
-            f"device={device}",
-            f"dtype={dtype}",
-            f"is_shared={is_shared}",
-        ]
-    )
-    return f"{cls.__name__}({s})"
-
-
-def _make_repr(key: NestedKey, item, tensordict: T) -> str:
-    from tensordict.base import _is_tensor_collection
-
-    if _is_tensor_collection(type(item)):
-        return f"{key}: {repr(tensordict.get(key))}"
-    return f"{key}: {_get_repr(item)}"
-
-
-def _td_fields(td: T, keys=None) -> str:
-    strs = []
-    if keys is None:
-        keys = td.keys()
-    for key in keys:
-        shape = td.get_item_shape(key)
-        if -1 not in shape:
-            item = td.get(key)
-            strs.append(_make_repr(key, item, td))
-        else:
-            # we know td is lazy stacked and the key is a leaf
-            # so we can get the shape and escape the error
-            temp_td = td
-            from tensordict import LazyStackedTensorDict, TensorDictBase
-
-            while isinstance(
-                temp_td, LazyStackedTensorDict
-            ):  # we need to grab the het tensor from the inner nesting level
-                temp_td = temp_td.tensordicts[0]
-            tensor = temp_td.get(key)
-
-            if isinstance(tensor, TensorDictBase):
-                substr = _td_fields(tensor)
-            else:
-                is_shared = (
-                    tensor.is_shared()
-                    if not isinstance(tensor, UninitializedTensorMixin)
-                    else None
-                )
-                substr = _get_repr_custom(
-                    tensor.__class__,
-                    shape=shape,
-                    device=tensor.device,
-                    dtype=tensor.dtype,
-                    is_shared=is_shared,
-                )
-            strs.append(f"{key}: {substr}")
-
-    return indent(
-        "\n" + ",\n".join(sorted(strs)),
-        4 * " ",
-    )
-
-
-def _check_keys(
-    list_of_tensordicts: Sequence[TensorDictBase],
-    strict: bool = False,
-    include_nested: bool = False,
-    leaves_only: bool = False,
-) -> set[str]:
-    from tensordict.base import _is_leaf_nontensor
-
-    if not len(list_of_tensordicts):
-        return set()
-    keys: set[str] = set(
-        list_of_tensordicts[0].keys(
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-            is_leaf=_is_leaf_nontensor,
-        )
-    )
-    for td in list_of_tensordicts[1:]:
-        k = td.keys(
-            include_nested=include_nested,
-            leaves_only=leaves_only,
-            is_leaf=_is_leaf_nontensor,
-        )
-        if not strict:
-            keys = keys.intersection(k)
-        else:
-            if set(k) != keys:
-                raise KeyError(
-                    f"got keys {keys} and {set(td.keys())} which are incompatible"
-                )
-    return keys
-
-
-def _expand_to_match_shape(
-    parent_batch_size: torch.Size,
-    tensor: Tensor,
-    self_batch_dims: int,
-    self_device: DeviceType,
-) -> Tensor | TensorDictBase:
-    from tensordict.base import _is_tensor_collection
-
-    if not _is_tensor_collection(type(tensor)):
-        return torch.zeros(
-            (
-                *parent_batch_size,
-                *_shape(tensor)[self_batch_dims:],
-            ),
-            dtype=tensor.dtype,
-            device=self_device,
-        )
-    else:
-        # tensordict
-        out = tensor.empty()
-        out.batch_size = torch.Size(
-            [*parent_batch_size, *_shape(tensor)[self_batch_dims:]]
-        )
-        return out
-
-
-def _set_max_batch_size(source: T, batch_dims=None):
-    """Updates a tensordict with its maximum batch size."""
-    from tensordict.base import _is_tensor_collection
-
-    tensor_data = [val for val in source.values() if not is_non_tensor(val)]
-
-    for val in tensor_data:
-        if _is_tensor_collection(val.__class__):
-            _set_max_batch_size(val, batch_dims=batch_dims)
-
-    batch_size = []
-    if not tensor_data:  # when source is empty
-        if batch_dims:
-            source.batch_size = source.batch_size[:batch_dims]
-            return source
-        else:
-            return source
-
-    curr_dim = 0
-    tensor_shapes = [_shape(_tensor_data) for _tensor_data in tensor_data]
-
-    while True:
-        if len(tensor_shapes[0]) > curr_dim:
-            curr_dim_size = tensor_shapes[0][curr_dim]
-        else:
-            source.batch_size = batch_size
-            return
-        for leaf, shape in zip(tensor_data[1:], tensor_shapes[1:]):
-            # if we have a nested empty tensordict we can modify its batch size at will
-            if _is_tensor_collection(type(leaf)) and leaf.is_empty():
-                continue
-            if (len(shape) <= curr_dim) or (shape[curr_dim] != curr_dim_size):
-                source.batch_size = batch_size
-                return
-        if batch_dims is None or len(batch_size) < batch_dims:
-            batch_size.append(curr_dim_size)
-        curr_dim += 1
-
-
-def _clone_value(value, recurse: bool):
-    from tensordict.base import _is_tensor_collection
-
-    if recurse:
-        # this is not a problem for locked tds as we will not lock it
-        return value.clone()
-    elif _is_tensor_collection(value.__class__):
-        return value._clone(recurse=False)
-    else:
-        return value
-
-
-def _is_number(item):
-    if isinstance(item, (Number, ftdim.Dim)):
-        return True
-    if isinstance(item, Tensor) and item.ndim == 0:
-        return True
-    if isinstance(item, np.ndarray) and item.ndim == 0:
-        return True
-    return False
-
-
-def _expand_index(index, batch_size):
-    len_index = sum(True for idx in index if idx is not None)
-    if len_index > len(batch_size):
-        raise ValueError
-    if len_index < len(batch_size):
-        index = index + (slice(None),) * (len(batch_size) - len_index)
-    return index
-
-
-def _renamed_inplace_method(fn):
-    def wrapper(*args, **kwargs):
-        warnings.warn(
-            f"{fn.__name__.rstrip('_')} has been deprecated, use {fn.__name__} instead"
-        )
-        return fn(*args, **kwargs)
-
-    return wrapper
-
-
-def _broadcast_tensors(index):
-    # tensors and range need to be broadcast
-    tensors = {
-        i: torch.as_tensor(tensor)
-        for i, tensor in enumerate(index)
-        if isinstance(tensor, (range, list, np.ndarray, Tensor))
-    }
-    if tensors:
-        shape = torch.broadcast_shapes(*[tensor.shape for tensor in tensors.values()])
-        tensors = {i: tensor.expand(shape) for i, tensor in tensors.items()}
-        index = tuple(
-            idx if i not in tensors else tensors[i] for i, idx in enumerate(index)
-        )
-    return index
-
-
-def _reduce_index(index):
-    if all(
-        idx is Ellipsis or (isinstance(idx, slice) and idx == slice(None))
-        for idx in index
-    ):
-        index = ()
-    return index
-
-
-def _get_shape_from_args(*args, kwarg_name="size", **kwargs):
-    if not args and not kwargs:
-        return ()
-    if args:
-        if len(args) > 1 or isinstance(args[0], Number):
-            size = args
-        else:
-            size = args[0]
-        if len(kwargs):
-            raise TypeError(
-                f"Either the kwarg `{kwarg_name}`, a single shape argument or a sequence of integers can be passed. Got args={args} and kwargs={kwargs}."
-            )
-    else:
-        size = kwargs.pop(kwarg_name, None)
-        if size is None:
-            raise TypeError(
-                f"Either the kwarg `{kwarg_name}`, a single shape argument or a sequence of integers can be passed. Got args={args} and kwargs={kwargs}."
-            )
-    return size
-
-
-class Buffer(Tensor, metaclass=_ParameterMeta):
-    r"""A kind of Tensor that is to be considered a module buffer.
-
-    Args:
-        data (Tensor): buffer tensor.
-        requires_grad (bool, optional): if the buffer requires gradient. See
-            :ref:`locally-disable-grad-doc` for more details. Default: `False`
-    """
-
-    def __new__(cls, data=None, requires_grad=False):
-        if data is None:
-            data = torch.empty(0)
-        if type(data) is Tensor or type(data) is Buffer:
-            # For ease of BC maintenance, keep this path for standard Tensor.
-            # Eventually (tm), we should change the behavior for standard Tensor to match.
-            return Tensor._make_subclass(cls, data, requires_grad)
-
-        # Path for custom tensors: set a flag on the instance to indicate parameter-ness.
-        t = data.detach().requires_grad_(requires_grad)
-        t._is_buffer = True
-        return t
-
-    def __deepcopy__(self, memo):
-        if id(self) in memo:
-            return memo[id(self)]
-        else:
-            result = type(self)(
-                self.data.clone(memory_format=torch.preserve_format), self.requires_grad
-            )
-            memo[id(self)] = result
-            return result
-
-    def __repr__(self):
-        return "Buffer containing:\n" + super(Buffer, self).__repr__()
-
-    def __reduce_ex__(self, proto):
-        # See Note [Don't serialize hooks]
-        return (
-            torch._utils._rebuild_parameter,
-            (self.data, self.requires_grad, OrderedDict()),
-        )
-
-    __torch_function__ = _disabled_torch_function_impl
-
-
-def _getitem_batch_size(batch_size, index):
-    """Given an input shape and an index, returns the size of the resulting indexed tensor.
-
-    This function is aimed to be used when indexing is an
-    expensive operation.
-    Args:
-        shape (torch.Size): Input shape
-        items (index): Index of the hypothetical tensor
-
-    Returns:
-        Size of the resulting object (tensor or tensordict)
-
-    Examples:
-        >>> idx = (None, ..., None)
-        >>> torch.zeros(4, 3, 2, 1)[idx].shape
-        torch.Size([1, 4, 3, 2, 1, 1])
-        >>> _getitem_batch_size([4, 3, 2, 1], idx)
-        torch.Size([1, 4, 3, 2, 1, 1])
-    """
-    if not isinstance(index, tuple):
-        if isinstance(index, int):
-            return batch_size[1:]
-        if isinstance(index, slice) and index == slice(None):
-            return batch_size
-        index = (index,)
-    # index = convert_ellipsis_to_idx(index, batch_size)
-    # broadcast shapes
-    shapes_dict = {}
-    look_for_disjoint = False
-    disjoint = False
-    bools = []
-    for i, idx in enumerate(index):
-        boolean = False
-        if isinstance(idx, (range, list)):
-            shape = len(idx)
-        elif isinstance(idx, (torch.Tensor, np.ndarray)):
-            if idx.dtype == torch.bool or idx.dtype == np.dtype("bool"):
-                shape = torch.Size([idx.sum()])
-                boolean = True
-            else:
-                shape = idx.shape
-        elif isinstance(idx, slice):
-            look_for_disjoint = not disjoint and (len(shapes_dict) > 0)
-            shape = None
-        else:
-            shape = None
-        if shape is not None:
-            if look_for_disjoint:
-                disjoint = True
-            shapes_dict[i] = shape
-        bools.append(boolean)
-    bs_shape = None
-    if shapes_dict:
-        bs_shape = torch.broadcast_shapes(*shapes_dict.values())
-    out = []
-    count = -1
-    for i, idx in enumerate(index):
-        if idx is None:
-            out.append(1)
-            continue
-        count += 1 if not bools[i] else idx.ndim
-        if i in shapes_dict:
-            if bs_shape is not None:
-                if disjoint:
-                    # the indices will be put at the beginning
-                    out = list(bs_shape) + out
-                else:
-                    # if there is a single tensor or similar, we just extend
-                    out.extend(bs_shape)
-                bs_shape = None
-            continue
-        elif isinstance(idx, (int, ftdim.Dim)):
-            # could be spared for efficiency
-            continue
-        elif isinstance(idx, slice):
-            batch = batch_size[count]
-            out.append(len(range(*idx.indices(batch))))
-    count += 1
-    if batch_size[count:]:
-        out.extend(batch_size[count:])
-    return torch.Size(out)
-
-
-# Lazy classes control (legacy feature)
-_DEFAULT_LAZY_OP = False
-_LAZY_OP = os.environ.get("LAZY_LEGACY_OP", None)
-
-
-class set_lazy_legacy(_DecoratorContextManager):
-    """Sets the behaviour of some methods to a lazy transform.
-
-    These methods include :meth:`~tensordict.TensorDict.view`, :meth:`~tensordict.TensorDict.permute`,
-    :meth:`~tensordict.TensorDict.transpose`, :meth:`~tensordict.TensorDict.squeeze`
-    and :meth:`~tensordict.TensorDict.unsqueeze`.
-
-    This property is dynamic, ie. it can be changed during the code execution, but
-    it won't propagate to sub-processes unless it has been called before the process
-    has been created.
-
-    """
-
-    def __init__(self, mode: bool) -> None:
-        super().__init__()
-        self.mode = mode
-
-    def clone(self) -> set_lazy_legacy:
-        # override this method if your children class takes __init__ parameters
-        return self.__class__(self.mode)
-
-    def __enter__(self) -> None:
-        self.set()
-
-    def set(self) -> None:
-        global _LAZY_OP
-        self._old_mode = _LAZY_OP
-        _LAZY_OP = bool(self.mode)
-        # we do this such that sub-processes see the same lazy op than the main one
-        os.environ["LAZY_LEGACY_OP"] = str(_LAZY_OP)
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _LAZY_OP
-        _LAZY_OP = bool(self._old_mode)
-        os.environ["LAZY_LEGACY_OP"] = str(_LAZY_OP)
-
-
-def lazy_legacy(allow_none=False):
-    """Returns `True` if lazy representations will be used for selected methods."""
-    global _LAZY_OP
-    if _LAZY_OP is None and allow_none:
-        return None
-    elif _LAZY_OP is None:
-        return _DEFAULT_LAZY_OP
-    return strtobool(_LAZY_OP) if isinstance(_LAZY_OP, str) else _LAZY_OP
-
-
-def _legacy_lazy(func):
-    if not func.__name__.startswith("_legacy_"):
-        raise NameError(
-            f"The function name {func.__name__} must start with _legacy_ if it's decorated with _legacy_lazy."
-        )
-    func.LEGACY = True
-    return func
-
-
-# Process initializer for map
-def _proc_init(base_seed, queue, num_threads):
-    worker_id = queue.get(timeout=120)
-    seed = base_seed + worker_id
-    torch.manual_seed(seed)
-    np_seed = _generate_state(base_seed, worker_id)
-    np.random.seed(np_seed)
-    torch.set_num_threads(num_threads)
-
-
-def _prune_selected_keys(keys_to_update, prefix):
-    if keys_to_update is None:
-        return None
-    return tuple(
-        key[1:] for key in keys_to_update if isinstance(key, tuple) and key[0] == prefix
-    )
-
-
-class TensorDictFuture:
-    """A custom future class for TensorDict multithreaded operations.
-
-    Args:
-        futures (list of futures): a list of concurrent.futures.Future objects to wait for.
-        resulting_td (TensorDictBase): instance that will result from the futures
-            completing.
-
-    """
-
-    def __init__(self, futures, resulting_td):
-        self.futures = futures
-        self.resulting_td = resulting_td
-
-    def result(self):
-        """Wait and returns the resulting tensordict."""
-        concurrent.futures.wait(self.futures)
-        return self.resulting_td
-
-
-def _is_json_serializable(item):
-    if isinstance(item, dict):
-        for key, val in item.items():
-            # Per se, int, float and bool are serializable but not recoverable
-            # as such
-            if not isinstance(key, (str,)) or not _is_json_serializable(val):
-                return False
-        else:
-            return True
-    if isinstance(item, (list, tuple, set)):
-        for val in item:
-            if not _is_json_serializable(val):
-                return False
-        else:
-            return True
-    return isinstance(item, (str, int, float, bool)) or item is None
-
-
-def print_directory_tree(path, indent="", display_metadata=True):
-    """Prints the directory tree starting from the specified path.
-
-    Args:
-        path (str): The path of the directory to print.
-        indent (str): The current indentation level for formatting.
-        display_metadata (bool): if ``True``, metadata of the dir will be
-            displayed too.
-
-    """
-    if display_metadata:
-
-        def get_directory_size(path="."):
-            total_size = 0
-
-            for dirpath, _, filenames in os.walk(path):
-                for filename in filenames:
-                    file_path = os.path.join(dirpath, filename)
-                    total_size += os.path.getsize(file_path)
-
-            return total_size
-
-        def format_size(size):
-            # Convert size to a human-readable format
-            for unit in ["B", "KB", "MB", "GB", "TB"]:
-                if size < 1024.0:
-                    return f"{size:.2f} {unit}"
-                size /= 1024.0
-
-        total_size_bytes = get_directory_size(path)
-        formatted_size = format_size(total_size_bytes)
-        logger.info(f"Directory size: {formatted_size}")
-
-    if os.path.isdir(path):
-        logger.info(indent + os.path.basename(path) + "/")
-        indent += "    "
-        for item in os.listdir(path):
-            print_directory_tree(
-                os.path.join(path, item), indent=indent, display_metadata=False
-            )
-    else:
-        logger.info(indent + os.path.basename(path))
-
-
-def isin(
-    input: TensorDictBase,
-    reference: TensorDictBase,
-    key: NestedKey,
-    dim: int = 0,
-) -> Tensor:
-    """Tests if each element of ``key`` in input ``dim`` is also present in the reference.
-
-    This function returns a boolean tensor of length  ``input.batch_size[dim]`` that is ``True`` for elements in
-    the entry ``key`` that are also present in the ``reference``. This function assumes that both ``input`` and
-    ``reference`` have the same batch size and contain the specified entry, otherwise an error will be raised.
-
-    Args:
-        input (TensorDictBase): Input TensorDict.
-        reference (TensorDictBase): Target TensorDict against which to test.
-        key (Nestedkey): The key to test.
-        dim (int, optional): The dimension along which to test. Defaults to ``0``.
-
-    Returns:
-        out (Tensor): A boolean tensor of length ``input.batch_size[dim]`` that is ``True`` for elements in
-            the ``input`` ``key`` tensor that are also present in the ``reference``.
-
-    Examples:
-        >>> td = TensorDict(
-        ...     {
-        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]]),
-        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [40, 50], [50, 60]]),
-        ...     },
-        ...     batch_size=[4],
-        ... )
-        >>> td_ref = TensorDict(
-        ...     {
-        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [10, 11, 12]]),
-        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [50, 60]]),
-        ...     },
-        ...     batch_size=[3],
-        ... )
-        >>> in_reference = isin(td, td_ref, key="tensor1")
-        >>> expected_in_reference = torch.tensor([True, True, True, False])
-        >>> torch.testing.assert_close(in_reference, expected_in_reference)
-    """
-    # Get the data
-    reference_tensor = reference.get(key, default=None)
-    target_tensor = input.get(key, default=None)
-
-    # Check key is present in both tensordict and reference_tensordict
-    if not isinstance(target_tensor, torch.Tensor):
-        raise KeyError(f"Key '{key}' not found in input or not a tensor.")
-    if not isinstance(reference_tensor, torch.Tensor):
-        raise KeyError(f"Key '{key}' not found in reference or not a tensor.")
-
-    # Check that both TensorDicts have the same number of dimensions
-    if len(input.batch_size) != len(reference.batch_size):
-        raise ValueError(
-            "The number of dimensions in the batch size of the input and reference must be the same."
-        )
-
-    # Check dim is valid
-    batch_dims = input.ndim
-    if dim >= batch_dims or dim < -batch_dims or batch_dims == 0:
-        raise ValueError(
-            f"The specified dimension '{dim}' is invalid for an input TensorDict with batch size '{input.batch_size}'."
-        )
-
-    # Convert negative dimension to its positive equivalent
-    if dim < 0:
-        dim = batch_dims + dim
-
-    # Find the common indices
-    N = reference_tensor.shape[dim]
-    cat_data = torch.cat([reference_tensor, target_tensor], dim=dim)
-    _, unique_indices = torch.unique(
-        cat_data, dim=dim, sorted=True, return_inverse=True
-    )
-    out = torch.isin(unique_indices[N:], unique_indices[:N], assume_unique=True)
-
-    return out
-
-
-def _index_preserve_data_ptr(index):
-    if isinstance(index, tuple):
-        return all(_index_preserve_data_ptr(idx) for idx in index)
-    # we can't use a list comprehension here because it fails with tensor indices
-    if index is None or index is Ellipsis:
-        return True
-    if isinstance(index, int):
-        return True
-    if isinstance(index, slice) and (index.start == 0 or index.start is None):
-        return True
-    return False
-
-
-def remove_duplicates(
-    input: TensorDictBase,
-    key: NestedKey,
-    dim: int = 0,
-    *,
-    return_indices: bool = False,
-) -> TensorDictBase:
-    """Removes indices duplicated in `key` along the specified dimension.
-
-    This method detects duplicate elements in the tensor associated with the specified `key` along the specified
-    `dim` and removes elements in the same indices in all other tensors within the TensorDict. It is expected for
-    `dim` to be one of the dimensions within the batch size of the input TensorDict to ensure consistency in all
-    tensors. Otherwise, an error will be raised.
-
-    Args:
-        input (TensorDictBase): The TensorDict containing potentially duplicate elements.
-        key (NestedKey): The key of the tensor along which duplicate elements should be identified and removed. It
-            must be one of the leaf keys within the TensorDict, pointing to a tensor and not to another TensorDict.
-        dim (int, optional): The dimension along which duplicate elements should be identified and removed. It must be one of
-            the dimensions within the batch size of the input TensorDict. Defaults to ``0``.
-        return_indices (bool, optional): If ``True``, the indices of the unique elements in the input tensor will be
-            returned as well. Defaults to ``False``.
-
-    Returns:
-        output (TensorDictBase): input tensordict with the indices corrsponding to duplicated elements
-            in tensor `key` along dimension `dim` removed.
-        unique_indices (torch.Tensor, optional): The indices of the first occurrences of the unique elements in the
-            input tensordict for the specified `key` along the specified `dim`. Only provided if return_index is True.
-
-    Example:
-        >>> td = TensorDict(
-        ...     {
-        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]]),
-        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [40, 50], [50, 60]]),
-        ...     }
-        ...     batch_size=[4],
-        ... )
-        >>> output_tensordict = remove_duplicate_elements(td, key="tensor1", dim=0)
-        >>> expected_output = TensorDict(
-        ...     {
-        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
-        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [50, 60]]),
-        ...     },
-        ...     batch_size=[3],
-        ... )
-        >>> assert (td == expected_output).all()
-    """
-    tensor = input.get(key, default=None)
-
-    # Check if the key is a TensorDict
-    if tensor is None:
-        raise KeyError(f"The key '{key}' does not exist in the TensorDict.")
-
-    # Check that the key points to a tensor
-    if not isinstance(tensor, torch.Tensor):
-        raise KeyError(f"The key '{key}' does not point to a tensor in the TensorDict.")
-
-    # Check dim is valid
-    batch_dims = input.ndim
-    if dim >= batch_dims or dim < -batch_dims or batch_dims == 0:
-        raise ValueError(
-            f"The specified dimension '{dim}' is invalid for a TensorDict with batch size '{input.batch_size}'."
-        )
-
-    # Convert negative dimension to its positive equivalent
-    if dim < 0:
-        dim = batch_dims + dim
-
-    # Get indices of unique elements (e.g. [0, 1, 0, 2])
-    _, unique_indices, counts = torch.unique(
-        tensor, dim=dim, sorted=True, return_inverse=True, return_counts=True
-    )
-
-    # Find first occurrence of each index  (e.g. [0, 1, 3])
-    _, unique_indices_sorted = torch.sort(unique_indices, stable=True)
-    cum_sum = counts.cumsum(0, dtype=torch.long)
-    cum_sum = torch.cat(
-        (torch.zeros(1, device=input.device, dtype=torch.long), cum_sum[:-1])
-    )
-    first_indices = unique_indices_sorted[cum_sum]
-
-    # Remove duplicate elements in the TensorDict
-    output = input[(slice(None),) * dim + (first_indices,)]
-
-    if return_indices:
-        return output, unique_indices
-
-    return output
-
-
-class _CloudpickleWrapper(object):
-    def __init__(self, fn):
-        self.fn = fn
-
-    def __getstate__(self):
-        import cloudpickle
-
-        return cloudpickle.dumps(self.fn)
-
-    def __setstate__(self, ob: bytes):
-        import pickle
-
-        self.fn = pickle.loads(ob)
-
-    def __call__(self, *args, **kwargs):
-        return self.fn(*args, **kwargs)
-
-
-class _BatchedUninitializedParameter(UninitializedParameter):
-    batch_size: torch.Size
-    in_dim: int | None = None
-    vmap_level: int | None = None
-
-    def materialize(self, shape, device=None, dtype=None):
-        UninitializedParameter.materialize(
-            self, (*self.batch_size, *shape), device=device, dtype=dtype
-        )
-
-
-class _BatchedUninitializedBuffer(UninitializedBuffer):
-    batch_size: torch.Size
-    in_dim: int | None = None
-    vmap_level: int | None = None
-
-    def materialize(self, shape, device=None, dtype=None):
-        UninitializedBuffer.materialize(
-            self, (*self.batch_size, *shape), device=device, dtype=dtype
-        )
-
-
-class _add_batch_dim_pre_hook:
-    def __call__(self, mod: torch.nn.Module, args, kwargs):
-        for name, param in list(mod.named_parameters(recurse=False)):
-            if hasattr(param, "in_dim") and hasattr(param, "vmap_level"):
-                from torch._C._functorch import _add_batch_dim
-
-                param = _add_batch_dim(param, param.in_dim, param.vmap_level)
-                delattr(mod, name)
-                setattr(mod, name, param)
-        for key, val in list(mod._forward_pre_hooks.items()):
-            if val is self:
-                del mod._forward_pre_hooks[key]
-                return
-        else:
-            raise RuntimeError("did not find pre-hook")
-
-
-def is_non_tensor(data):
-    """Checks if an item is a non-tensor."""
-    return getattr(type(data), "_is_non_tensor", False)
-
-
-def _is_non_tensor(cls: type):
-    return getattr(cls, "_is_non_tensor", False)
-
-
-class KeyDependentDefaultDict(collections.defaultdict):
-    """A key-dependent default dict.
-
-    Examples:
-        >>> my_dict = KeyDependentDefaultDict(lambda key: "foo_" + key)
-        >>> print(my_dict["bar"])
-        foo_bar
-    """
-
-    def __init__(self, fun):
-        self.fun = fun
-        super().__init__()
-
-    def __missing__(self, key):
-        value = self.fun(key)
-        self[key] = value
-        return value
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from __future__ import annotations
+
+import collections
+import concurrent.futures
+import inspect
+import logging
+
+import math
+import os
+
+import sys
+import time
+
+import warnings
+from collections import defaultdict, OrderedDict
+from collections.abc import KeysView
+from copy import copy
+from distutils.util import strtobool
+from functools import wraps
+from importlib import import_module
+from numbers import Number
+from textwrap import indent
+from typing import (
+    Any,
+    Callable,
+    Dict,
+    Iterator,
+    List,
+    Sequence,
+    Tuple,
+    TYPE_CHECKING,
+    TypeVar,
+    Union,
+)
+
+import numpy as np
+import torch
+
+try:
+    from functorch import dim as ftdim
+
+    _has_funcdim = True
+except ImportError:
+    _has_funcdim = False
+from packaging.version import parse
+from tensordict._contextlib import _DecoratorContextManager
+from tensordict._tensordict import (  # noqa: F401
+    _unravel_key_to_tuple,
+    unravel_key,
+    unravel_key_list,
+    unravel_keys,
+)
+
+from torch import Tensor
+from torch._C import _disabled_torch_function_impl
+from torch.nn.parameter import (
+    _ParameterMeta,
+    UninitializedBuffer,
+    UninitializedParameter,
+    UninitializedTensorMixin,
+)
+from torch.utils.data._utils.worker import _generate_state
+
+if TYPE_CHECKING:
+    from tensordict.tensordict import TensorDictBase
+
+try:
+    try:
+        from functorch._C import get_unwrapped, is_batchedtensor
+    except ImportError:
+        from torch._C._functorch import get_unwrapped, is_batchedtensor
+except ImportError:
+    pass
+
+TORCHREC_ERR = None
+try:
+    from torchrec import KeyedJaggedTensor
+
+    _has_torchrec = True
+except ImportError as err:
+    _has_torchrec = False
+
+    class KeyedJaggedTensor:  # noqa: D103, D101
+        pass
+
+    TORCHREC_ERR = err
+
+if not _has_funcdim:
+
+    class _ftdim_mock:
+        class Dim:
+            pass
+
+        class Tensor:
+            pass
+
+        def dims(self, *args, **kwargs):
+            raise ImportError("functorch.dim not found")
+
+    ftdim = _ftdim_mock  # noqa: F811
+
+T = TypeVar("T", bound="TensorDictBase")
+
+_STRDTYPE2DTYPE = {
+    str(dtype): dtype
+    for dtype in (
+        torch.float32,
+        torch.float64,
+        torch.float16,
+        torch.bfloat16,
+        torch.complex32,
+        torch.complex64,
+        torch.complex128,
+        torch.uint8,
+        torch.int8,
+        torch.int16,
+        torch.int32,
+        torch.int64,
+        torch.bool,
+        torch.quint8,
+        torch.qint8,
+        torch.qint32,
+        torch.quint4x2,
+    )
+}
+
+IndexType = Union[None, int, slice, str, Tensor, List[Any], Tuple[Any, ...]]
+DeviceType = Union[torch.device, str, int]
+NestedKey = Union[str, Tuple[str, ...]]
+
+_KEY_ERROR = 'key "{}" not found in {} with ' "keys {}"
+_LOCK_ERROR = (
+    "Cannot modify locked TensorDict. For in-place modification, consider "
+    "using the `set_()` method and make sure the key is present."
+)
+
+
+LOGGING_LEVEL = os.environ.get("TD_LOGGING_LEVEL", "DEBUG")
+logger = logging.getLogger("tensordict")
+logger.setLevel(getattr(logging, LOGGING_LEVEL))
+# Disable propagation to the root logger
+logger.propagate = False
+# Remove all attached handlers
+while logger.hasHandlers():
+    logger.removeHandler(logger.handlers[0])
+console_handler = logging.StreamHandler()
+console_handler.setLevel(logging.INFO)
+formatter = logging.Formatter("%(asctime)s [%(name)s][%(levelname)s] %(message)s")
+console_handler.setFormatter(formatter)
+logger.addHandler(console_handler)
+
+
+def _sub_index(tensor: Tensor, idx: IndexType) -> Tensor:
+    """Allows indexing of tensors with nested tuples.
+
+     >>> sub_tensor1 = tensor[tuple1][tuple2]
+     >>> sub_tensor2 = _sub_index(tensor, (tuple1, tuple2))
+     >>> assert torch.allclose(sub_tensor1, sub_tensor2)
+
+    Args:
+        tensor (Tensor): tensor to be indexed.
+        idx (tuple of indices): indices sequence to be used.
+
+    """
+    if isinstance(idx, tuple) and len(idx) and isinstance(idx[0], tuple):
+        idx0 = idx[0]
+        idx1 = idx[1:]
+        return _sub_index(_sub_index(tensor, idx0), idx1)
+    return tensor[idx]
+
+
+def convert_ellipsis_to_idx(
+    idx: tuple[int | Ellipsis] | Ellipsis, batch_size: list[int]
+) -> tuple[int, ...]:
+    """Given an index containing an ellipsis or just an ellipsis, converts any ellipsis to slice(None).
+
+    Example:
+        >>> idx = (..., 0)
+        >>> batch_size = [1,2,3]
+        >>> new_index = convert_ellipsis_to_idx(idx, batch_size)
+        >>> print(new_index)
+        (slice(None, None, None), slice(None, None, None), 0)
+
+    Args:
+        idx (tuple, Ellipsis): Input index
+        batch_size (list): Shape of tensor to be indexed
+
+    Returns:
+        new_index (tuple): Output index
+    """
+    istuple = isinstance(idx, tuple)
+    if (not istuple and idx is not Ellipsis) or (
+        istuple and all(_idx is not Ellipsis for _idx in idx)
+    ):
+        return idx
+    new_index = ()
+    num_dims = len(batch_size)
+
+    if idx is Ellipsis:
+        idx = (...,)
+
+    num_ellipsis = sum(_idx is Ellipsis for _idx in idx)
+    if num_dims < (len(idx) - num_ellipsis - sum(item is None for item in idx)):
+        raise RuntimeError("Not enough dimensions in TensorDict for index provided.")
+
+    start_pos, after_ellipsis_length = None, 0
+    for i, item in enumerate(idx):
+        if item is Ellipsis:
+            if start_pos is not None:
+                raise RuntimeError("An index can only have one ellipsis at most.")
+            else:
+                start_pos = i
+        if item is not Ellipsis and start_pos is not None:
+            after_ellipsis_length += 1
+        if item is None:
+            # unsqueeze
+            num_dims += 1
+
+    before_ellipsis_length = start_pos
+    if start_pos is None:
+        return idx
+    else:
+        ellipsis_length = num_dims - after_ellipsis_length - before_ellipsis_length
+
+    new_index += idx[:start_pos]
+
+    ellipsis_start = start_pos
+    ellipsis_end = start_pos + ellipsis_length
+    new_index += (slice(None),) * (ellipsis_end - ellipsis_start)
+
+    new_index += idx[start_pos + 1 : start_pos + 1 + after_ellipsis_length]
+
+    if len(new_index) != num_dims:
+        raise RuntimeError(
+            f"The new index {new_index} is incompatible with the dimensions of the batch size {num_dims}."
+        )
+
+    return new_index
+
+
+def _copy(self: list[int]) -> list[int]:
+    return list(self)
+
+
+def infer_size_impl(shape: list[int], numel: int) -> list[int]:
+    """Infers the shape of an expanded tensor whose number of elements is indicated by :obj:`numel`.
+
+    Copied from pytorch for compatibility issues (See #386).
+    See https://github.com/pytorch/pytorch/blob/35d4fa444b67cbcbe34a862782ddf2d92f5b1ce7/torch/jit/_shape_functions.py
+    for the original copy.
+
+    """
+    newsize = 1
+    infer_dim: int | None = None
+    for dim in range(len(shape)):
+        if shape[dim] == -1:
+            if infer_dim is not None:
+                raise AssertionError("only one dimension can be inferred")
+            infer_dim = dim
+        elif shape[dim] >= 0:
+            newsize *= shape[dim]
+        else:
+            raise AssertionError("invalid shape dimensions")
+    if not (
+        numel == newsize
+        or (infer_dim is not None and newsize > 0 and numel % newsize == 0)
+    ):
+        raise AssertionError("invalid shape")
+    out = _copy(shape)
+    if infer_dim is not None:
+        out[infer_dim] = numel // newsize
+    return out
+
+
+def _unwrap_value(value: Tensor) -> Tensor:
+    # batch_dims = value.ndimension()
+    if not isinstance(value, Tensor):
+        out = value
+    elif is_batchedtensor(value):
+        out = get_unwrapped(value)
+    else:
+        out = value
+    return out
+    # batch_dims = out.ndimension() - batch_dims
+    # batch_size = out.shape[:batch_dims]
+    # return out, batch_size
+
+
+if hasattr(math, "prod"):  # Python 3.8+
+
+    def prod(sequence):
+        """General prod function, that generalised usage across math and np.
+
+        Created for multiple python versions compatibility.
+
+        """
+        return math.prod(sequence)
+
+else:
+
+    def prod(sequence):
+        """General prod function, that generalised usage across math and np.
+
+        Created for multiple python versions compatibility.
+
+        """
+        return int(np.prod(sequence))
+
+
+def expand_as_right(
+    tensor: torch.Tensor | TensorDictBase,
+    dest: torch.Tensor | TensorDictBase,
+) -> torch.Tensor | TensorDictBase:
+    """Expand a tensor on the right to match another tensor shape.
+
+    Args:
+        tensor: tensor to be expanded
+        dest: tensor providing the target shape
+
+    Returns:
+         a tensor with shape matching the dest input tensor shape.
+
+    Examples:
+        >>> tensor = torch.zeros(3,4)
+        >>> dest = torch.zeros(3,4,5)
+        >>> print(expand_as_right(tensor, dest).shape)
+        torch.Size([3,4,5])
+
+    """
+    if dest.ndimension() < tensor.ndimension():
+        raise RuntimeError(
+            "expand_as_right requires the destination tensor to have less "
+            f"dimensions than the input tensor, got"
+            f" tensor.ndimension()={tensor.ndimension()} and "
+            f"dest.ndimension()={dest.ndimension()}"
+        )
+    if any(
+        tensor.shape[i] != dest.shape[i] and tensor.shape[i] != 1
+        for i in range(tensor.ndimension())
+    ):
+        raise RuntimeError(
+            f"tensor shape is incompatible with dest shape, "
+            f"got: tensor.shape={tensor.shape}, dest={dest.shape}"
+        )
+    for _ in range(dest.ndimension() - tensor.ndimension()):
+        tensor = tensor.unsqueeze(-1)
+    return tensor.expand(dest.shape)
+
+
+def expand_right(tensor: Tensor, shape: Sequence[int]) -> Tensor:
+    """Expand a tensor on the right to match a desired shape.
+
+    Args:
+        tensor: tensor to be expanded
+        shape: target shape
+
+    Returns:
+         a tensor with shape matching the target shape.
+
+    Examples:
+        >>> tensor = torch.zeros(3,4)
+        >>> shape = (3,4,5)
+        >>> print(expand_right(tensor, shape).shape)
+        torch.Size([3,4,5])
+
+    """
+    tensor_expand = tensor
+    while tensor_expand.ndimension() < len(shape):
+        tensor_expand = tensor_expand.unsqueeze(-1)
+    tensor_expand = tensor_expand.expand(shape)
+    return tensor_expand
+
+
+NUMPY_TO_TORCH_DTYPE_DICT = {
+    np.dtype("bool"): torch.bool,
+    np.dtype("uint8"): torch.uint8,
+    np.dtype("int8"): torch.int8,
+    np.dtype("int16"): torch.int16,
+    np.dtype("int32"): torch.int32,
+    np.dtype("int64"): torch.int64,
+    np.dtype("float16"): torch.float16,
+    np.dtype("float32"): torch.float32,
+    np.dtype("float64"): torch.float64,
+    np.dtype("complex64"): torch.complex64,
+    np.dtype("complex128"): torch.complex128,
+}
+TORCH_TO_NUMPY_DTYPE_DICT = {
+    value: key for key, value in NUMPY_TO_TORCH_DTYPE_DICT.items()
+}
+
+
+def is_nested_key(key: NestedKey) -> bool:
+    """Returns True if key is a NestedKey."""
+    if isinstance(key, str):
+        return True
+    if key and isinstance(key, (list, tuple)):
+        return all(isinstance(subkey, str) for subkey in key)
+    return False
+
+
+def is_seq_of_nested_key(seq: Sequence[NestedKey]) -> bool:
+    """Returns True if seq is a Sequence[NestedKey]."""
+    if seq and isinstance(seq, Sequence):
+        return all(is_nested_key(k) for k in seq)
+    elif isinstance(seq, Sequence):
+        # we allow empty inputs
+        return True
+    return False
+
+
+def index_keyedjaggedtensor(
+    kjt: KeyedJaggedTensor, index: slice | range | list | torch.Tensor | np.ndarray
+) -> KeyedJaggedTensor:
+    """Indexes a KeyedJaggedTensor along the batch dimension.
+
+    Args:
+        kjt (KeyedJaggedTensor): a KeyedJaggedTensor to index
+        index (torch.Tensor or other indexing type): batch index to use.
+            Indexing with an integer will result in an error.
+
+    Examples:
+        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
+        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
+        >>>
+        >>> jag_tensor = KeyedJaggedTensor(
+        ...     values=values,
+        ...     keys=keys,
+        ...     offsets=offsets,
+        ...     weights=weights,
+        ... )
+        >>> ikjt = index_keyedjaggedtensor(jag_tensor, [0, 2])
+        >>> print(ikjt["index_0"].to_padded_dense(), j0.to_padded_dense())
+
+    """
+    if not _has_torchrec:
+        raise TORCHREC_ERR
+    if isinstance(index, (int,)):
+        raise ValueError(
+            "Indexing KeyedJaggedTensor instances with an integer is prohibited, "
+            "as this would result in a KeyedJaggedTensor without batch size. "
+            "If you want to get a single element from a KeyedJaggedTensor, "
+            "call `index_keyedjaggedtensor(kjt, torch.tensor([index]))` instead."
+        )
+    lengths = kjt.lengths()
+    keys = kjt.keys()
+    numel = len(lengths) // len(keys)
+    offsets = kjt.offsets()
+
+    _offsets1 = offsets[:-1].view(len(keys), numel)[:, index]
+    _offsets2 = offsets[1:].view(len(keys), numel)[:, index]
+    lengths = lengths.view(len(keys), numel)[:, index].reshape(-1)
+
+    full_index = torch.arange(offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.unsqueeze(-1)) & (
+        full_index < _offsets2.unsqueeze(-1)
+    )
+    sel = sel.any(0).any(0)
+    full_index = full_index.squeeze()[sel]
+    values = kjt._values[full_index]
+    weights = kjt._weights[full_index]
+    return KeyedJaggedTensor(
+        values=values, keys=kjt.keys(), weights=weights, lengths=lengths
+    )
+
+
+def setitem_keyedjaggedtensor(
+    orig_tensor: KeyedJaggedTensor,
+    index: slice | range | list | torch.Tensor | np.ndarray,
+    other: KeyedJaggedTensor,
+) -> KeyedJaggedTensor:
+    """Equivalent of `tensor[index] = other` for KeyedJaggedTensors indexed along the batch dimension.
+
+    Args:
+        orig_tensor (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be updated.
+        index (list or equivalent index): batch index to be written.
+        other (torchrec.KeyedJaggedTensor): KeyedJaggedTensor to be written at
+            the batch locations.
+
+    Examples:
+        >>> values = torch.Tensor([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0])
+        >>> weights = torch.Tensor([1.0, 0.5, 1.5, 1.0, 0.5, 1.0, 1.0, 1.5, 1.0, 1.0, 1.0])
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> offsets = torch.IntTensor([0, 2, 2, 3, 4, 5, 8, 9, 10, 11])
+        >>> jag_tensor = KeyedJaggedTensor(
+        ...    values=values,
+        ...    keys=keys,
+        ...    offsets=offsets,
+        ...    weights=weights,
+        ... )
+        >>> keys = ["index_0", "index_1", "index_2"]
+        >>> lengths2 = torch.IntTensor([2, 4, 6, 4, 2, 1])
+        >>> values2 = torch.zeros(
+        ...     lengths2.sum(),
+        ... )
+        >>> weights2 = -torch.ones(
+        ...     lengths2.sum(),
+        ... )
+        >>> sub_jag_tensor = KeyedJaggedTensor(
+        ...     values=values2,
+        ...     keys=keys,
+        ...     lengths=lengths2,
+        ...     weights=weights2,
+        ... )
+        >>> setitem_keyedjaggedtensor(jag_tensor, [0, 2], sub_jag_tensor)
+    """
+    orig_tensor_lengths = orig_tensor.lengths()
+    orig_tensor_keys = orig_tensor.keys()
+    orig_tensor_numel = len(orig_tensor_lengths) // len(orig_tensor_keys)
+    orig_tensor_offsets = orig_tensor.offsets()
+
+    other_lengths = other.lengths()
+    other_keys = other.keys()
+    other_numel = len(other_lengths) // len(other_keys)
+    # other_offsets = other.offsets()
+
+    if not other_keys == orig_tensor_keys:
+        raise KeyError("Mismatch in orig_tensor and other keys.")
+    #     if other_numel - len(index) != orig_tensor_numel:
+    #         raise RuntimeError("orig_tensor and otherination batch differ.")
+
+    _offsets1 = orig_tensor_offsets[:-1]
+    _offsets2 = orig_tensor_offsets[1:]
+    _orig_tensor_shape = len(orig_tensor_keys), orig_tensor_numel
+
+    _lengths_out = orig_tensor_lengths.view(_orig_tensor_shape).clone()
+    _lengths_out[:, index] = other_lengths.view(len(orig_tensor_keys), other_numel)
+    _lengths_out = _lengths_out.view(-1)
+
+    # get the values of orig_tensor that we'll be keeping
+    full_index = torch.arange(orig_tensor_offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = (~sel).all(0).all(0)
+    index_to_keep = full_index.squeeze()[sel]
+    values_to_keep = orig_tensor._values[index_to_keep]
+    new_values = other._values
+    weights_to_keep = orig_tensor._weights[index_to_keep]
+    new_weights = other._weights
+
+    # compute new offsets
+    _offsets = torch.cat([_lengths_out[:1] * 0, _lengths_out], 0)
+    _offsets = _offsets.cumsum(0)
+
+    # get indices of offsets for new elts
+    _offsets1 = _offsets[:-1]
+    _offsets2 = _offsets[1:]
+    full_index = torch.arange(_offsets[-1]).view(1, 1, -1)
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = sel.any(0).any(0)
+    new_index_new_elts = full_index.squeeze()[sel]
+    sel = (full_index >= _offsets1.view(_orig_tensor_shape)[:, index].unsqueeze(-1)) & (
+        full_index < _offsets2.view(_orig_tensor_shape)[:, index].unsqueeze(-1)
+    )
+    sel = (~sel).all(0).all(0)
+    new_index_to_keep = full_index.squeeze()[sel]
+
+    # create an empty values tensor
+    values_numel = values_to_keep.shape[0] + other._values.shape[0]
+    tensor = torch.empty(
+        [values_numel, *values_to_keep.shape[1:]],
+        dtype=values_to_keep.dtype,
+        device=values_to_keep.device,
+    )
+    tensor_weights = torch.empty(
+        [values_numel, *values_to_keep.shape[1:]],
+        dtype=weights_to_keep.dtype,
+        device=weights_to_keep.device,
+    )
+    tensor[new_index_to_keep] = values_to_keep
+    tensor[new_index_new_elts] = new_values
+    tensor_weights[new_index_to_keep] = weights_to_keep
+    tensor_weights[new_index_new_elts] = new_weights
+
+    kjt = KeyedJaggedTensor(
+        values=tensor,
+        keys=orig_tensor_keys,
+        weights=tensor_weights,
+        lengths=_lengths_out,
+    )
+    for k, item in kjt.__dict__.items():
+        orig_tensor.__dict__[k] = item
+    return orig_tensor
+
+
+def _ndimension(tensor: Tensor) -> int:
+    if isinstance(tensor, Tensor):
+        return tensor.ndimension()
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return 1
+    else:
+        return tensor.ndimension()
+
+
+def _shape(tensor: Tensor, nested_shape=False) -> torch.Size:
+    if isinstance(tensor, UninitializedTensorMixin):
+        return torch.Size([*getattr(tensor, "batch_size", ()), -1])
+    elif not isinstance(tensor, Tensor):
+        if type(tensor) is KeyedJaggedTensor:
+            return torch.Size([len(tensor.lengths()) // len(tensor.keys())])
+        return tensor.shape
+    if tensor.is_nested:
+        if nested_shape:
+            return tensor._nested_tensor_size()
+        shape = []
+        for i in range(tensor.ndim):
+            try:
+                shape.append(tensor.size(i))
+            except RuntimeError:
+                shape.append(-1)
+        return torch.Size(shape)
+    return tensor.shape
+
+
+def _device(tensor: Tensor) -> torch.device:
+    if isinstance(tensor, Tensor):
+        return tensor.device
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor.device()
+    else:
+        return tensor.device
+
+
+def _is_shared(tensor: Tensor) -> bool:
+    if isinstance(tensor, Tensor):
+        if torch._C._functorch.is_batchedtensor(tensor):
+            return None
+        return tensor.is_shared()
+    if isinstance(tensor, ftdim.Tensor):
+        return None
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return False
+    else:
+        return tensor.is_shared()
+
+
+def _is_meta(tensor: Tensor) -> bool:
+    if isinstance(tensor, Tensor):
+        return tensor.is_meta
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return False
+    else:
+        return tensor.is_meta
+
+
+def _dtype(tensor: Tensor) -> torch.dtype:
+    if isinstance(tensor, Tensor):
+        return tensor.dtype
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor._values.dtype
+    else:
+        return tensor.dtype
+
+
+def _get_item(tensor: Tensor, index: IndexType) -> Tensor:
+    if isinstance(tensor, Tensor):
+        try:
+            return tensor[index]
+        except IndexError as err:
+            # try to map list index to tensor, and assess type. If bool, we
+            # likely have a nested list of booleans which is not supported by pytorch
+            if _is_lis_of_list_of_bools(index):
+                index = torch.tensor(index, device=tensor.device)
+                if index.dtype is torch.bool:
+                    warnings.warn(
+                        "Indexing a tensor with a nested list of boolean values is "
+                        "going to be deprecated as this functionality is not supported "
+                        f"by PyTorch. (follows error: {err})",
+                        category=DeprecationWarning,
+                    )
+                return tensor[index]
+            raise err
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return index_keyedjaggedtensor(tensor, index)
+    else:
+        return tensor[index]
+
+
+def _set_item(
+    tensor: Tensor, index: IndexType, value: Tensor, *, validated, non_blocking
+) -> Tensor:
+    # the tensor must be validated
+    if not validated:
+        raise RuntimeError
+    if isinstance(tensor, Tensor):
+        tensor[index] = value
+        return tensor
+    elif isinstance(tensor, KeyedJaggedTensor):
+        tensor = setitem_keyedjaggedtensor(tensor, index, value)
+        return tensor
+    from tensordict.tensorclass import NonTensorData, NonTensorStack
+
+    if is_non_tensor(tensor):
+        if (
+            isinstance(value, NonTensorData)
+            and isinstance(tensor, NonTensorData)
+            and tensor.data == value.data
+        ):
+            return tensor
+        elif isinstance(tensor, NonTensorData):
+            tensor = NonTensorStack.from_nontensordata(tensor)
+        if tensor.stack_dim != 0:
+            tensor = NonTensorStack(*tensor.unbind(0), stack_dim=0)
+        tensor[index] = value
+        return tensor
+    else:
+        tensor[index] = value
+        return tensor
+
+
+def _requires_grad(tensor: Tensor) -> bool:
+    if isinstance(tensor, Tensor):
+        return tensor.requires_grad
+    elif isinstance(tensor, KeyedJaggedTensor):
+        return tensor._values.requires_grad
+    else:
+        return tensor.requires_grad
+
+
+class timeit:
+    """A dirty but easy to use decorator for profiling code."""
+
+    _REG = {}
+
+    def __init__(self, name) -> None:
+        self.name = name
+
+    def __call__(self, fn):
+        @wraps(fn)
+        def decorated_fn(*args, **kwargs):
+            with self:
+                out = fn(*args, **kwargs)
+                return out
+
+        return decorated_fn
+
+    def __enter__(self):
+        self.t0 = time.time()
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        t = time.time() - self.t0
+        val = self._REG.setdefault(self.name, [0.0, 0.0, 0])
+
+        count = val[2]
+        N = count + 1
+        val[0] = val[0] * (count / N) + t / N
+        val[1] += t
+        val[2] = N
+
+    @staticmethod
+    def print(prefix=None):  # noqa: T202
+        keys = list(timeit._REG)
+        keys.sort()
+        for name in keys:
+            strings = []
+            if prefix:
+                strings.append(prefix)
+            strings.append(
+                f"{name} took {timeit._REG[name][0] * 1000:4.4} msec (total = {timeit._REG[name][1]} sec)"
+            )
+            logger.info(" -- ".join(strings))
+
+    @staticmethod
+    def erase():
+        for k in timeit._REG:
+            timeit._REG[k] = [0.0, 0.0, 0]
+
+
+def int_generator(seed):
+    """A pseudo-random chaing generator.
+
+    To be used to produce deterministic integer sequences
+
+    Examples:
+        >>> for _ in range(2):
+        ...     init_int = 10
+        ...     for _ in range(10):
+        ...        init_int = int_generator(init_int)
+        ...        print(init_int, end=", ")
+        ...     print("")
+        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
+        6756, 1717, 4410, 9740, 9611, 9716, 5397, 7745, 4521, 7523,
+    """
+    max_seed_val = 10_000
+    rng = np.random.default_rng(seed)
+    seed = int.from_bytes(rng.bytes(8), "big")
+    return seed % max_seed_val
+
+
+def _is_lis_of_list_of_bools(index, first_level=True):
+    # determines if an index is a list of list of bools.
+    # this is aimed at catching a deprecation feature where list of list
+    # of bools are valid indices
+    if first_level:
+        if not isinstance(index, list):
+            return False
+        if not len(index):
+            return False
+        if isinstance(index[0], list):
+            return _is_lis_of_list_of_bools(index[0], False)
+        return False
+    # then we know it is a list of lists
+    if isinstance(index[0], bool):
+        return True
+    if isinstance(index[0], list):
+        return _is_lis_of_list_of_bools(index[0], False)
+    return False
+
+
+def is_tensorclass(obj: type | Any) -> bool:
+    """Returns True if obj is either a tensorclass or an instance of a tensorclass."""
+    cls = obj if isinstance(obj, type) else type(obj)
+    return _is_tensorclass(cls)
+
+
+def _is_tensorclass(cls: type) -> bool:
+    return getattr(cls, "_is_tensorclass", False)
+
+
+class implement_for:
+    """A version decorator that checks the version in the environment and implements a function with the fitting one.
+
+    If specified module is missing or there is no fitting implementation, call of the decorated function
+    will lead to the explicit error.
+    In case of intersected ranges, last fitting implementation is used.
+
+    Args:
+        module_name (str or callable): version is checked for the module with this
+            name (e.g. "gym"). If a callable is provided, it should return the
+            module.
+        from_version: version from which implementation is compatible. Can be open (None).
+        to_version: version from which implementation is no longer compatible. Can be open (None).
+
+    Examples:
+        >>> @implement_for("torch", None, "1.13")
+        >>> def fun(self, x):
+        ...     # Older torch versions will return x + 1
+        ...     return x + 1
+        ...
+        >>> @implement_for("torch", "0.13", "2.0")
+        >>> def fun(self, x):
+        ...     # More recent torch versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for(lambda: import_module("torch"), "0.", None)
+        >>> def fun(self, x):
+        ...     # More recent gym versions will return x + 2
+        ...     return x + 2
+        ...
+        >>> @implement_for("gymnasium", "0.27", None)
+        >>> def fun(self, x):
+        ...     # If gymnasium is to be used instead of gym, x+3 will be returned
+        ...     return x + 3
+        ...
+
+        This indicates that the function is compatible with gym 0.13+, but doesn't with gym 0.14+.
+    """
+
+    # Stores pointers to fitting implementations: dict[func_name] = func_pointer
+    _implementations = {}
+    _setters = []
+    _cache_modules = {}
+
+    def __init__(
+        self,
+        module_name: Union[str, Callable],
+        from_version: str = None,
+        to_version: str = None,
+    ):
+        self.module_name = module_name
+        self.from_version = from_version
+        self.to_version = to_version
+        implement_for._setters.append(self)
+
+    @staticmethod
+    def check_version(version: str, from_version: str | None, to_version: str | None):
+        version = parse(".".join([str(v) for v in parse(version).release]))
+        return (from_version is None or version >= parse(from_version)) and (
+            to_version is None or version < parse(to_version)
+        )
+
+    @staticmethod
+    def get_class_that_defined_method(f):
+        """Returns the class of a method, if it is defined, and None otherwise."""
+        return f.__globals__.get(f.__qualname__.split(".")[0], None)
+
+    @classmethod
+    def get_func_name(cls, fn):
+        # produces a name like torchrl.module.Class.method or torchrl.module.function
+        first = str(fn).split(".")[0][len("<function ") :]
+        last = str(fn).split(".")[1:]
+        if last:
+            first = [first]
+            last[-1] = last[-1].split(" ")[0]
+        else:
+            last = [first.split(" ")[0]]
+            first = []
+        return ".".join([fn.__module__] + first + last)
+
+    def _get_cls(self, fn):
+        cls = self.get_class_that_defined_method(fn)
+        if cls is None:
+            # class not yet defined
+            return
+        if cls.__class__.__name__ == "function":
+            cls = inspect.getmodule(fn)
+        return cls
+
+    def module_set(self):
+        """Sets the function in its module, if it exists already."""
+        prev_setter = type(self)._implementations.get(self.get_func_name(self.fn), None)
+        if prev_setter is not None:
+            prev_setter.do_set = False
+        type(self)._implementations[self.get_func_name(self.fn)] = self
+        cls = self.get_class_that_defined_method(self.fn)
+        if cls is not None:
+            if cls.__class__.__name__ == "function":
+                cls = inspect.getmodule(self.fn)
+        else:
+            # class not yet defined
+            return
+        setattr(cls, self.fn.__name__, self.fn)
+
+    @classmethod
+    def import_module(cls, module_name: Union[Callable, str]) -> str:
+        """Imports module and returns its version."""
+        if not callable(module_name):
+            module = cls._cache_modules.get(module_name, None)
+            if module is None:
+                if module_name in sys.modules:
+                    sys.modules[module_name] = module = import_module(module_name)
+                else:
+                    cls._cache_modules[module_name] = module = import_module(
+                        module_name
+                    )
+        else:
+            module = module_name()
+        return module.__version__
+
+    _lazy_impl = collections.defaultdict(list)
+
+    def _delazify(self, func_name):
+        for local_call in implement_for._lazy_impl[func_name]:
+            out = local_call()
+        return out
+
+    def __call__(self, fn):
+        # function names are unique
+        self.func_name = self.get_func_name(fn)
+        self.fn = fn
+        implement_for._lazy_impl[self.func_name].append(self._call)
+
+        @wraps(fn)
+        def _lazy_call_fn(*args, **kwargs):
+            # first time we call the function, we also do the replacement.
+            # This will cause the imports to occur only during the first call to fn
+            return self._delazify(self.func_name)(*args, **kwargs)
+
+        return _lazy_call_fn
+
+    def _call(self):
+
+        # If the module is missing replace the function with the mock.
+        fn = self.fn
+        func_name = self.func_name
+        implementations = implement_for._implementations
+
+        @wraps(fn)
+        def unsupported(*args, **kwargs):
+            raise ModuleNotFoundError(
+                f"Supported version of '{func_name}' has not been found."
+            )
+
+        self.do_set = False
+        # Return fitting implementation if it was encountered before.
+        if func_name in implementations:
+            try:
+                # check that backends don't conflict
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    self.do_set = True
+                if not self.do_set:
+                    return implementations[func_name].fn
+            except ModuleNotFoundError:
+                # then it's ok, there is no conflict
+                return implementations[func_name].fn
+        else:
+            try:
+                version = self.import_module(self.module_name)
+                if self.check_version(version, self.from_version, self.to_version):
+                    self.do_set = True
+            except ModuleNotFoundError:
+                return unsupported
+        if self.do_set:
+            self.module_set()
+            return fn
+        return unsupported
+
+    @classmethod
+    def reset(cls, setters_dict: Dict[str, implement_for] = None):
+        """Resets the setters in setter_dict.
+
+        ``setter_dict`` is a copy of implementations. We just need to iterate through its
+        values and call :meth:`~.module_set` for each.
+
+        """
+        if setters_dict is None:
+            setters_dict = copy(cls._implementations)
+        for setter in setters_dict.values():
+            setter.module_set()
+
+    def __repr__(self):
+        return (
+            f"{self.__class__.__name__}("
+            f"module_name={self.module_name}({self.from_version, self.to_version}), "
+            f"fn_name={self.fn.__name__}, cls={self._get_cls(self.fn)}, is_set={self.do_set})"
+        )
+
+
+def _unfold_sequence(seq):
+    for item in seq:
+        if isinstance(item, (list, tuple)):
+            yield tuple(_unfold_sequence(item))
+        else:
+            if isinstance(item, (str, int, slice)) or item is Ellipsis:
+                yield item
+            else:
+                yield id(item)
+
+
+def _make_cache_key(args, kwargs):
+    """Creates a key for the cache such that memory footprint is minimized."""
+    # Fast path for the common args
+    if not args and not kwargs:
+        return ((), ())
+    elif not kwargs and len(args) == 1 and type(args[0]) is str:
+        return (args, ())
+    else:
+        return (
+            tuple(_unfold_sequence(args)),
+            tuple(_unfold_sequence(sorted(kwargs.items()))),
+        )
+
+
+def cache(fun):
+    """A cache for TensorDictBase subclasses.
+
+    This decorator will cache the values returned by a method as long as the
+    input arguments match.
+    Leaves (tensors and such) are not cached.
+    The cache is stored within the tensordict such that it can be erased at any
+    point in time.
+
+    Examples:
+        >>> import timeit
+        >>> from tensordict import TensorDict
+        >>> class SomeOtherTd(TensorDict):
+        ...     @cache
+        ...     def all_keys(self):
+        ...         return set(self.keys(include_nested=True))
+        >>> td = SomeOtherTd({("a", "b", "c", "d", "e", "f", "g"): 1.0}, [])
+        >>> td.lock_()
+        >>> print(timeit.timeit("set(td.keys(True))", globals={'td': td}))
+        11.057
+        >>> print(timeit.timeit("set(td.all_keys())", globals={'td': td}))
+        0.88
+    """
+
+    @wraps(fun)
+    def newfun(_self: "TensorDictBase", *args, **kwargs):
+        if not _self.is_locked:
+            return fun(_self, *args, **kwargs)
+        cache = _self._cache
+        if cache is None:
+            cache = _self._cache = defaultdict(dict)
+        cache = cache[fun.__name__]
+        key = _make_cache_key(args, kwargs)
+        if key not in cache:
+            out = fun(_self, *args, **kwargs)
+            if not isinstance(out, (Tensor, KeyedJaggedTensor)):
+                # we don't cache tensors to avoid filling the mem and / or
+                # stacking them from their origin
+                cache[key] = out
+        else:
+            out = cache[key]
+        return out
+
+    return newfun
+
+
+def erase_cache(fun):
+    """A decorator to erase the cache at each call."""
+
+    @wraps(fun)
+    def new_fun(self, *args, **kwargs):
+        self._erase_cache()
+        return fun(self, *args, **kwargs)
+
+    return new_fun
+
+
+_NON_STR_KEY_TUPLE_ERR = "Nested membership checks with tuples of strings is only supported when setting `include_nested=True`."
+_NON_STR_KEY_ERR = "TensorDict keys are always strings. Membership checks are only supported for strings or non-empty tuples of strings (for nested TensorDicts)"
+_GENERIC_NESTED_ERR = "Only NestedKeys are supported. Got key {}."
+
+
+class _StringKeys(KeysView):
+    """A key view where contains is restricted to strings."""
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            try:
+                unravel_item = _unravel_key_to_tuple(item)
+                if not unravel_item:  # catch errors during unravel
+                    raise TypeError
+            except Exception:
+                raise TypeError(_NON_STR_KEY_ERR)
+            if len(unravel_item) > 1:
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+            else:
+                item = unravel_item[0]
+        return super().__contains__(item)
+
+
+class _StringOnlyDict(dict):
+    """A dict class where contains is restricted to strings."""
+
+    # kept here for debugging
+    # def __setitem__(self, key, value):
+    #     if not isinstance(key, str):
+    #         raise RuntimeError
+    #     return super().__setitem__(key, value)
+
+    def __contains__(self, item):
+        if not isinstance(item, str):
+            try:
+                unravel_item = _unravel_key_to_tuple(item)
+                if not unravel_item:  # catch errors during unravel
+                    raise TypeError
+            except Exception:
+                raise TypeError(_NON_STR_KEY_ERR)
+            if len(unravel_item) > 1:
+                raise TypeError(_NON_STR_KEY_TUPLE_ERR)
+            else:
+                item = unravel_item[0]
+        return super().__contains__(item)
+
+    def keys(self):
+        return _StringKeys(self)
+
+
+def lock_blocked(func):
+    """Checks that the tensordict is unlocked before executing a function."""
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        if self.is_locked:
+            raise RuntimeError(_LOCK_ERROR)
+        return func(self, *args, **kwargs)
+
+    return new_func
+
+
+class as_decorator:
+    """Converts a method to a decorator.
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> data = TensorDict({}, [])
+        >>> with data.lock_(): # lock_ is decorated
+        ...     assert data.is_locked
+        >>> assert not data.is_locked
+    """
+
+    def __init__(self, attr=None):
+        self.attr = attr
+
+    def __call__(self, func):
+        if self.attr is not None:
+
+            @wraps(func)
+            def new_func(_self, *args, **kwargs):
+                _attr_pre = getattr(_self, self.attr)
+                out = func(_self, *args, **kwargs)
+                _attr_post = getattr(_self, self.attr)
+                if out is not None:
+                    if _attr_post is not _attr_pre:
+                        out._last_op = (new_func.__name__, (args, kwargs, _self))
+                    else:
+                        out._last_op = None
+                return out
+
+        else:
+
+            @wraps(func)
+            def new_func(_self, *args, **kwargs):
+                out = func(_self, *args, **kwargs)
+                if out is not None:
+                    out._last_op = (new_func.__name__, (args, kwargs, _self))
+                return out
+
+        return new_func
+
+
+def _split_tensordict(
+    td,
+    chunksize,
+    num_chunks,
+    num_workers,
+    dim,
+    use_generator=False,
+    to_tensordict=False,
+):
+    if chunksize is None and num_chunks is None:
+        num_chunks = num_workers
+    if chunksize is not None and num_chunks is not None:
+        raise ValueError(
+            "Either chunksize or num_chunks must be provided, but not both."
+        )
+    if num_chunks is not None:
+        num_chunks = min(td.shape[dim], num_chunks)
+        if use_generator:
+
+            def _chunk_generator():
+                chunksize = -(td.shape[dim] // -num_chunks)
+                idx_start = 0
+                base = (slice(None),) * dim
+                for _ in range(num_chunks):
+                    idx_end = idx_start + chunksize
+                    out = td[base + (slice(idx_start, idx_end),)]
+                    if to_tensordict:
+                        out = out.to_tensordict()
+                    yield out
+                    idx_start = idx_end
+
+            return _chunk_generator()
+        return td.chunk(num_chunks, dim=dim)
+    else:
+        if chunksize == 0:
+            if use_generator:
+
+                def _unbind_generator():
+                    base = (slice(None),) * dim
+                    for i in range(td.shape[dim]):
+                        out = td[base + (i,)]
+                        if to_tensordict:
+                            out = out.to_tensordict()
+                        yield out
+
+                return _unbind_generator()
+            return td.unbind(dim=dim)
+        if use_generator:
+
+            def _split_generator():
+                idx_start = 0
+                base = (slice(None),) * dim
+                for _ in range(num_chunks):
+                    idx_end = idx_start + chunksize
+                    out = td[base + (slice(idx_start, idx_end),)]
+                    if to_tensordict:
+                        out = out.to_tensordict()
+                    yield out
+                    idx_start = idx_end
+
+            return _split_generator()
+        chunksize = min(td.shape[dim], chunksize)
+        return td.split(chunksize, dim=dim)
+
+
+def _parse_to(*args, **kwargs):
+    batch_size = kwargs.pop("batch_size", None)
+    other = kwargs.pop("other", None)
+    device, dtype, non_blocking, convert_to_format = torch._C._nn._parse_to(
+        *args, **kwargs
+    )
+    if other is not None:
+        if device is not None and device != other.device:
+            raise ValueError("other and device cannot be both passed")
+        device = other.device
+        dtypes = {val.dtype for val in other.values(True, True)}
+        if len(dtypes) > 1 or len(dtypes) == 0:
+            dtype = None
+        elif len(dtypes) == 1:
+            dtype = list(dtypes)[0]
+    return device, dtype, non_blocking, convert_to_format, batch_size
+
+
+class _ErrorInteceptor:
+    """Context manager for catching errors and modifying message.
+
+    Intended for use with stacking / concatenation operations applied to TensorDicts.
+
+    """
+
+    DEFAULT_EXC_MSG = "Expected all tensors to be on the same device"
+
+    def __init__(
+        self,
+        key: NestedKey,
+        prefix: str,
+        exc_msg: str | None = None,
+        exc_type: type[Exception] | None = None,
+    ) -> None:
+        self.exc_type = exc_type if exc_type is not None else RuntimeError
+        self.exc_msg = exc_msg if exc_msg is not None else self.DEFAULT_EXC_MSG
+        self.prefix = prefix
+        self.key = key
+
+    def _add_key_to_error_msg(self, msg: str) -> str:
+        if msg.startswith(self.prefix):
+            return f'{self.prefix} "{self.key}" /{msg[len(self.prefix):]}'
+        return f'{self.prefix} "{self.key}". {msg}'
+
+    def __enter__(self):
+        pass
+
+    def __exit__(self, exc_type, exc_value, _):
+        if exc_type is self.exc_type and (
+            self.exc_msg is None or self.exc_msg in str(exc_value)
+        ):
+            exc_value.args = (self._add_key_to_error_msg(str(exc_value)),)
+
+
+def _nested_keys_to_dict(keys: Iterator[NestedKey]) -> dict[str, Any]:
+    nested_keys = {}
+    for key in keys:
+        if isinstance(key, str):
+            nested_keys.setdefault(key, {})
+        else:
+            d = nested_keys
+            for subkey in key:
+                d = d.setdefault(subkey, {})
+    return nested_keys
+
+
+def _dict_to_nested_keys(
+    nested_keys: dict[NestedKey, NestedKey], prefix: tuple[str, ...] = ()
+) -> tuple[str, ...]:
+    for key, subkeys in nested_keys.items():
+        if subkeys:
+            yield from _dict_to_nested_keys(subkeys, prefix=(*prefix, key))
+        elif prefix:
+            yield (*prefix, key)
+        else:
+            yield key
+
+
+def _default_hook(td: T, key: tuple[str, ...]) -> None:
+    """Used to populate a tensordict.
+
+    For example, ``td.set(("a", "b"))`` may require to create ``"a"``.
+
+    """
+    out = td.get(key[0], None)
+    if out is None:
+        td._create_nested_str(key[0])
+        out = td._get_str(key[0], None)
+    return out
+
+
+def _get_leaf_tensordict(
+    tensordict: T, key: tuple[str, ...], hook: Callable = None
+) -> tuple[TensorDictBase, str]:
+    # utility function for traversing nested tensordicts
+    # hook should return the default value for tensordit.get(key)
+    while len(key) > 1:
+        if hook is not None:
+            tensordict = hook(tensordict, key)
+        else:
+            tensordict = tensordict.get(key[0])
+        key = key[1:]
+    return tensordict, key[0]
+
+
+def assert_allclose_td(
+    actual: T,
+    expected: T,
+    rtol: float | None = None,
+    atol: float | None = None,
+    equal_nan: bool = True,
+    msg: str = "",
+) -> bool:
+    """Compares two tensordicts and raise an exception if their content does not match exactly."""
+    from tensordict.base import _is_tensor_collection
+
+    if not _is_tensor_collection(actual.__class__) or not _is_tensor_collection(
+        expected.__class__
+    ):
+        raise TypeError("assert_allclose inputs must be of TensorDict type")
+
+    from tensordict._lazy import LazyStackedTensorDict
+
+    if isinstance(actual, LazyStackedTensorDict) and isinstance(
+        expected, LazyStackedTensorDict
+    ):
+        for sub_actual, sub_expected in zip(actual.tensordicts, expected.tensordicts):
+            assert_allclose_td(sub_actual, sub_expected, rtol=rtol, atol=atol)
+        return True
+
+    try:
+        set1 = set(
+            actual.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
+        )
+        set2 = set(
+            expected.keys(is_leaf=lambda x: not is_non_tensor(x), leaves_only=True)
+        )
+    except ValueError:
+        # Persistent tensordicts do not work with is_leaf
+        set1 = set(actual.keys())
+        set2 = set(expected.keys())
+    if not (len(set1.difference(set2)) == 0 and len(set2) == len(set1)):
+        raise KeyError(
+            "actual and expected tensordict keys mismatch, "
+            f"keys {(set1 - set2).union(set2 - set1)} appear in one but not "
+            f"the other."
+        )
+    keys = sorted(actual.keys(), key=str)
+    for key in keys:
+        input1 = actual.get(key)
+        input2 = expected.get(key)
+        if _is_tensor_collection(input1.__class__):
+            if is_non_tensor(input1):
+                # We skip non-tensor data
+                continue
+            assert_allclose_td(input1, input2, rtol=rtol, atol=atol)
+            continue
+
+        mse = (input1.to(torch.float) - input2.to(torch.float)).pow(2).sum()
+        mse = mse.div(input1.numel()).sqrt().item()
+
+        default_msg = f"key {key} does not match, got mse = {mse:4.4f}"
+        msg = "\t".join([default_msg, msg]) if len(msg) else default_msg
+        torch.testing.assert_close(
+            input1, input2, rtol=rtol, atol=atol, equal_nan=equal_nan, msg=msg
+        )
+    return True
+
+
+def _get_repr(tensor: Tensor) -> str:
+    s = ", ".join(
+        [
+            f"shape={_shape(tensor)}",
+            f"device={_device(tensor)}",
+            f"dtype={_dtype(tensor)}",
+            f"is_shared={_is_shared(tensor)}",
+        ]
+    )
+    return f"{tensor.__class__.__name__}({s})"
+
+
+def _get_repr_custom(cls, shape, device, dtype, is_shared) -> str:
+    s = ", ".join(
+        [
+            f"shape={shape}",
+            f"device={device}",
+            f"dtype={dtype}",
+            f"is_shared={is_shared}",
+        ]
+    )
+    return f"{cls.__name__}({s})"
+
+
+def _make_repr(key: NestedKey, item, tensordict: T) -> str:
+    from tensordict.base import _is_tensor_collection
+
+    if _is_tensor_collection(type(item)):
+        return f"{key}: {repr(tensordict.get(key))}"
+    return f"{key}: {_get_repr(item)}"
+
+
+def _td_fields(td: T, keys=None) -> str:
+    strs = []
+    if keys is None:
+        keys = td.keys()
+    for key in keys:
+        shape = td.get_item_shape(key)
+        if -1 not in shape:
+            item = td.get(key)
+            strs.append(_make_repr(key, item, td))
+        else:
+            # we know td is lazy stacked and the key is a leaf
+            # so we can get the shape and escape the error
+            temp_td = td
+            from tensordict import LazyStackedTensorDict, TensorDictBase
+
+            while isinstance(
+                temp_td, LazyStackedTensorDict
+            ):  # we need to grab the het tensor from the inner nesting level
+                temp_td = temp_td.tensordicts[0]
+            tensor = temp_td.get(key)
+
+            if isinstance(tensor, TensorDictBase):
+                substr = _td_fields(tensor)
+            else:
+                is_shared = (
+                    tensor.is_shared()
+                    if not isinstance(tensor, UninitializedTensorMixin)
+                    else None
+                )
+                substr = _get_repr_custom(
+                    tensor.__class__,
+                    shape=shape,
+                    device=tensor.device,
+                    dtype=tensor.dtype,
+                    is_shared=is_shared,
+                )
+            strs.append(f"{key}: {substr}")
+
+    return indent(
+        "\n" + ",\n".join(sorted(strs)),
+        4 * " ",
+    )
+
+
+def _check_keys(
+    list_of_tensordicts: Sequence[TensorDictBase],
+    strict: bool = False,
+    include_nested: bool = False,
+    leaves_only: bool = False,
+) -> set[str]:
+    from tensordict.base import _is_leaf_nontensor
+
+    if not len(list_of_tensordicts):
+        return set()
+    keys: set[str] = set(
+        list_of_tensordicts[0].keys(
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+            is_leaf=_is_leaf_nontensor,
+        )
+    )
+    for td in list_of_tensordicts[1:]:
+        k = td.keys(
+            include_nested=include_nested,
+            leaves_only=leaves_only,
+            is_leaf=_is_leaf_nontensor,
+        )
+        if not strict:
+            keys = keys.intersection(k)
+        else:
+            if set(k) != keys:
+                raise KeyError(
+                    f"got keys {keys} and {set(td.keys())} which are incompatible"
+                )
+    return keys
+
+
+def _expand_to_match_shape(
+    parent_batch_size: torch.Size,
+    tensor: Tensor,
+    self_batch_dims: int,
+    self_device: DeviceType,
+) -> Tensor | TensorDictBase:
+    from tensordict.base import _is_tensor_collection
+
+    if not _is_tensor_collection(type(tensor)):
+        return torch.zeros(
+            (
+                *parent_batch_size,
+                *_shape(tensor)[self_batch_dims:],
+            ),
+            dtype=tensor.dtype,
+            device=self_device,
+        )
+    else:
+        # tensordict
+        out = tensor.empty()
+        out.batch_size = torch.Size(
+            [*parent_batch_size, *_shape(tensor)[self_batch_dims:]]
+        )
+        return out
+
+
+def _set_max_batch_size(source: T, batch_dims=None):
+    """Updates a tensordict with its maximum batch size."""
+    from tensordict.base import _is_tensor_collection
+
+    tensor_data = [val for val in source.values() if not is_non_tensor(val)]
+
+    for val in tensor_data:
+        if _is_tensor_collection(val.__class__):
+            _set_max_batch_size(val, batch_dims=batch_dims)
+
+    batch_size = []
+    if not tensor_data:  # when source is empty
+        if batch_dims:
+            source.batch_size = source.batch_size[:batch_dims]
+            return source
+        else:
+            return source
+
+    curr_dim = 0
+    tensor_shapes = [_shape(_tensor_data) for _tensor_data in tensor_data]
+
+    while True:
+        if len(tensor_shapes[0]) > curr_dim:
+            curr_dim_size = tensor_shapes[0][curr_dim]
+        else:
+            source.batch_size = batch_size
+            return
+        for leaf, shape in zip(tensor_data[1:], tensor_shapes[1:]):
+            # if we have a nested empty tensordict we can modify its batch size at will
+            if _is_tensor_collection(type(leaf)) and leaf.is_empty():
+                continue
+            if (len(shape) <= curr_dim) or (shape[curr_dim] != curr_dim_size):
+                source.batch_size = batch_size
+                return
+        if batch_dims is None or len(batch_size) < batch_dims:
+            batch_size.append(curr_dim_size)
+        curr_dim += 1
+
+
+def _clone_value(value, recurse: bool):
+    from tensordict.base import _is_tensor_collection
+
+    if recurse:
+        # this is not a problem for locked tds as we will not lock it
+        return value.clone()
+    elif _is_tensor_collection(value.__class__):
+        return value._clone(recurse=False)
+    else:
+        return value
+
+
+def _is_number(item):
+    if isinstance(item, (Number, ftdim.Dim)):
+        return True
+    if isinstance(item, Tensor) and item.ndim == 0:
+        return True
+    if isinstance(item, np.ndarray) and item.ndim == 0:
+        return True
+    return False
+
+
+def _expand_index(index, batch_size):
+    len_index = sum(True for idx in index if idx is not None)
+    if len_index > len(batch_size):
+        raise ValueError
+    if len_index < len(batch_size):
+        index = index + (slice(None),) * (len(batch_size) - len_index)
+    return index
+
+
+def _renamed_inplace_method(fn):
+    def wrapper(*args, **kwargs):
+        warnings.warn(
+            f"{fn.__name__.rstrip('_')} has been deprecated, use {fn.__name__} instead"
+        )
+        return fn(*args, **kwargs)
+
+    return wrapper
+
+
+def _broadcast_tensors(index):
+    # tensors and range need to be broadcast
+    tensors = {
+        i: torch.as_tensor(tensor)
+        for i, tensor in enumerate(index)
+        if isinstance(tensor, (range, list, np.ndarray, Tensor))
+    }
+    if tensors:
+        shape = torch.broadcast_shapes(*[tensor.shape for tensor in tensors.values()])
+        tensors = {i: tensor.expand(shape) for i, tensor in tensors.items()}
+        index = tuple(
+            idx if i not in tensors else tensors[i] for i, idx in enumerate(index)
+        )
+    return index
+
+
+def _reduce_index(index):
+    if all(
+        idx is Ellipsis or (isinstance(idx, slice) and idx == slice(None))
+        for idx in index
+    ):
+        index = ()
+    return index
+
+
+def _get_shape_from_args(*args, kwarg_name="size", **kwargs):
+    if not args and not kwargs:
+        return ()
+    if args:
+        if len(args) > 1 or isinstance(args[0], Number):
+            size = args
+        else:
+            size = args[0]
+        if len(kwargs):
+            raise TypeError(
+                f"Either the kwarg `{kwarg_name}`, a single shape argument or a sequence of integers can be passed. Got args={args} and kwargs={kwargs}."
+            )
+    else:
+        size = kwargs.pop(kwarg_name, None)
+        if size is None:
+            raise TypeError(
+                f"Either the kwarg `{kwarg_name}`, a single shape argument or a sequence of integers can be passed. Got args={args} and kwargs={kwargs}."
+            )
+    return size
+
+
+class Buffer(Tensor, metaclass=_ParameterMeta):
+    r"""A kind of Tensor that is to be considered a module buffer.
+
+    Args:
+        data (Tensor): buffer tensor.
+        requires_grad (bool, optional): if the buffer requires gradient. See
+            :ref:`locally-disable-grad-doc` for more details. Default: `False`
+    """
+
+    def __new__(cls, data=None, requires_grad=False):
+        if data is None:
+            data = torch.empty(0)
+        if type(data) is Tensor or type(data) is Buffer:
+            # For ease of BC maintenance, keep this path for standard Tensor.
+            # Eventually (tm), we should change the behavior for standard Tensor to match.
+            return Tensor._make_subclass(cls, data, requires_grad)
+
+        # Path for custom tensors: set a flag on the instance to indicate parameter-ness.
+        t = data.detach().requires_grad_(requires_grad)
+        t._is_buffer = True
+        return t
+
+    def __deepcopy__(self, memo):
+        if id(self) in memo:
+            return memo[id(self)]
+        else:
+            result = type(self)(
+                self.data.clone(memory_format=torch.preserve_format), self.requires_grad
+            )
+            memo[id(self)] = result
+            return result
+
+    def __repr__(self):
+        return "Buffer containing:\n" + super(Buffer, self).__repr__()
+
+    def __reduce_ex__(self, proto):
+        # See Note [Don't serialize hooks]
+        return (
+            torch._utils._rebuild_parameter,
+            (self.data, self.requires_grad, OrderedDict()),
+        )
+
+    __torch_function__ = _disabled_torch_function_impl
+
+
+def _getitem_batch_size(batch_size, index):
+    """Given an input shape and an index, returns the size of the resulting indexed tensor.
+
+    This function is aimed to be used when indexing is an
+    expensive operation.
+    Args:
+        shape (torch.Size): Input shape
+        items (index): Index of the hypothetical tensor
+
+    Returns:
+        Size of the resulting object (tensor or tensordict)
+
+    Examples:
+        >>> idx = (None, ..., None)
+        >>> torch.zeros(4, 3, 2, 1)[idx].shape
+        torch.Size([1, 4, 3, 2, 1, 1])
+        >>> _getitem_batch_size([4, 3, 2, 1], idx)
+        torch.Size([1, 4, 3, 2, 1, 1])
+    """
+    if not isinstance(index, tuple):
+        if isinstance(index, int):
+            return batch_size[1:]
+        if isinstance(index, slice) and index == slice(None):
+            return batch_size
+        index = (index,)
+    # index = convert_ellipsis_to_idx(index, batch_size)
+    # broadcast shapes
+    shapes_dict = {}
+    look_for_disjoint = False
+    disjoint = False
+    bools = []
+    for i, idx in enumerate(index):
+        boolean = False
+        if isinstance(idx, (range, list)):
+            shape = len(idx)
+        elif isinstance(idx, (torch.Tensor, np.ndarray)):
+            if idx.dtype == torch.bool or idx.dtype == np.dtype("bool"):
+                shape = torch.Size([idx.sum()])
+                boolean = True
+            else:
+                shape = idx.shape
+        elif isinstance(idx, slice):
+            look_for_disjoint = not disjoint and (len(shapes_dict) > 0)
+            shape = None
+        else:
+            shape = None
+        if shape is not None:
+            if look_for_disjoint:
+                disjoint = True
+            shapes_dict[i] = shape
+        bools.append(boolean)
+    bs_shape = None
+    if shapes_dict:
+        bs_shape = torch.broadcast_shapes(*shapes_dict.values())
+    out = []
+    count = -1
+    for i, idx in enumerate(index):
+        if idx is None:
+            out.append(1)
+            continue
+        count += 1 if not bools[i] else idx.ndim
+        if i in shapes_dict:
+            if bs_shape is not None:
+                if disjoint:
+                    # the indices will be put at the beginning
+                    out = list(bs_shape) + out
+                else:
+                    # if there is a single tensor or similar, we just extend
+                    out.extend(bs_shape)
+                bs_shape = None
+            continue
+        elif isinstance(idx, (int, ftdim.Dim)):
+            # could be spared for efficiency
+            continue
+        elif isinstance(idx, slice):
+            batch = batch_size[count]
+            out.append(len(range(*idx.indices(batch))))
+    count += 1
+    if batch_size[count:]:
+        out.extend(batch_size[count:])
+    return torch.Size(out)
+
+
+# Lazy classes control (legacy feature)
+_DEFAULT_LAZY_OP = False
+_LAZY_OP = os.environ.get("LAZY_LEGACY_OP", None)
+
+
+class set_lazy_legacy(_DecoratorContextManager):
+    """Sets the behaviour of some methods to a lazy transform.
+
+    These methods include :meth:`~tensordict.TensorDict.view`, :meth:`~tensordict.TensorDict.permute`,
+    :meth:`~tensordict.TensorDict.transpose`, :meth:`~tensordict.TensorDict.squeeze`
+    and :meth:`~tensordict.TensorDict.unsqueeze`.
+
+    This property is dynamic, ie. it can be changed during the code execution, but
+    it won't propagate to sub-processes unless it has been called before the process
+    has been created.
+
+    """
+
+    def __init__(self, mode: bool) -> None:
+        super().__init__()
+        self.mode = mode
+
+    def clone(self) -> set_lazy_legacy:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.mode)
+
+    def __enter__(self) -> None:
+        self.set()
+
+    def set(self) -> None:
+        global _LAZY_OP
+        self._old_mode = _LAZY_OP
+        _LAZY_OP = bool(self.mode)
+        # we do this such that sub-processes see the same lazy op than the main one
+        os.environ["LAZY_LEGACY_OP"] = str(_LAZY_OP)
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _LAZY_OP
+        _LAZY_OP = bool(self._old_mode)
+        os.environ["LAZY_LEGACY_OP"] = str(_LAZY_OP)
+
+
+def lazy_legacy(allow_none=False):
+    """Returns `True` if lazy representations will be used for selected methods."""
+    global _LAZY_OP
+    if _LAZY_OP is None and allow_none:
+        return None
+    elif _LAZY_OP is None:
+        return _DEFAULT_LAZY_OP
+    return strtobool(_LAZY_OP) if isinstance(_LAZY_OP, str) else _LAZY_OP
+
+
+def _legacy_lazy(func):
+    if not func.__name__.startswith("_legacy_"):
+        raise NameError(
+            f"The function name {func.__name__} must start with _legacy_ if it's decorated with _legacy_lazy."
+        )
+    func.LEGACY = True
+    return func
+
+
+# Process initializer for map
+def _proc_init(base_seed, queue, num_threads):
+    worker_id = queue.get(timeout=120)
+    seed = base_seed + worker_id
+    torch.manual_seed(seed)
+    np_seed = _generate_state(base_seed, worker_id)
+    np.random.seed(np_seed)
+    torch.set_num_threads(num_threads)
+
+
+def _prune_selected_keys(keys_to_update, prefix):
+    if keys_to_update is None:
+        return None
+    return tuple(
+        key[1:] for key in keys_to_update if isinstance(key, tuple) and key[0] == prefix
+    )
+
+
+class TensorDictFuture:
+    """A custom future class for TensorDict multithreaded operations.
+
+    Args:
+        futures (list of futures): a list of concurrent.futures.Future objects to wait for.
+        resulting_td (TensorDictBase): instance that will result from the futures
+            completing.
+
+    """
+
+    def __init__(self, futures, resulting_td):
+        self.futures = futures
+        self.resulting_td = resulting_td
+
+    def result(self):
+        """Wait and returns the resulting tensordict."""
+        concurrent.futures.wait(self.futures)
+        return self.resulting_td
+
+
+def _is_json_serializable(item):
+    if isinstance(item, dict):
+        for key, val in item.items():
+            # Per se, int, float and bool are serializable but not recoverable
+            # as such
+            if not isinstance(key, (str,)) or not _is_json_serializable(val):
+                return False
+        else:
+            return True
+    if isinstance(item, (list, tuple, set)):
+        for val in item:
+            if not _is_json_serializable(val):
+                return False
+        else:
+            return True
+    return isinstance(item, (str, int, float, bool)) or item is None
+
+
+def print_directory_tree(path, indent="", display_metadata=True):
+    """Prints the directory tree starting from the specified path.
+
+    Args:
+        path (str): The path of the directory to print.
+        indent (str): The current indentation level for formatting.
+        display_metadata (bool): if ``True``, metadata of the dir will be
+            displayed too.
+
+    """
+    if display_metadata:
+
+        def get_directory_size(path="."):
+            total_size = 0
+
+            for dirpath, _, filenames in os.walk(path):
+                for filename in filenames:
+                    file_path = os.path.join(dirpath, filename)
+                    total_size += os.path.getsize(file_path)
+
+            return total_size
+
+        def format_size(size):
+            # Convert size to a human-readable format
+            for unit in ["B", "KB", "MB", "GB", "TB"]:
+                if size < 1024.0:
+                    return f"{size:.2f} {unit}"
+                size /= 1024.0
+
+        total_size_bytes = get_directory_size(path)
+        formatted_size = format_size(total_size_bytes)
+        logger.info(f"Directory size: {formatted_size}")
+
+    if os.path.isdir(path):
+        logger.info(indent + os.path.basename(path) + "/")
+        indent += "    "
+        for item in os.listdir(path):
+            print_directory_tree(
+                os.path.join(path, item), indent=indent, display_metadata=False
+            )
+    else:
+        logger.info(indent + os.path.basename(path))
+
+
+def isin(
+    input: TensorDictBase,
+    reference: TensorDictBase,
+    key: NestedKey,
+    dim: int = 0,
+) -> Tensor:
+    """Tests if each element of ``key`` in input ``dim`` is also present in the reference.
+
+    This function returns a boolean tensor of length  ``input.batch_size[dim]`` that is ``True`` for elements in
+    the entry ``key`` that are also present in the ``reference``. This function assumes that both ``input`` and
+    ``reference`` have the same batch size and contain the specified entry, otherwise an error will be raised.
+
+    Args:
+        input (TensorDictBase): Input TensorDict.
+        reference (TensorDictBase): Target TensorDict against which to test.
+        key (Nestedkey): The key to test.
+        dim (int, optional): The dimension along which to test. Defaults to ``0``.
+
+    Returns:
+        out (Tensor): A boolean tensor of length ``input.batch_size[dim]`` that is ``True`` for elements in
+            the ``input`` ``key`` tensor that are also present in the ``reference``.
+
+    Examples:
+        >>> td = TensorDict(
+        ...     {
+        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]]),
+        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [40, 50], [50, 60]]),
+        ...     },
+        ...     batch_size=[4],
+        ... )
+        >>> td_ref = TensorDict(
+        ...     {
+        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [10, 11, 12]]),
+        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [50, 60]]),
+        ...     },
+        ...     batch_size=[3],
+        ... )
+        >>> in_reference = isin(td, td_ref, key="tensor1")
+        >>> expected_in_reference = torch.tensor([True, True, True, False])
+        >>> torch.testing.assert_close(in_reference, expected_in_reference)
+    """
+    # Get the data
+    reference_tensor = reference.get(key, default=None)
+    target_tensor = input.get(key, default=None)
+
+    # Check key is present in both tensordict and reference_tensordict
+    if not isinstance(target_tensor, torch.Tensor):
+        raise KeyError(f"Key '{key}' not found in input or not a tensor.")
+    if not isinstance(reference_tensor, torch.Tensor):
+        raise KeyError(f"Key '{key}' not found in reference or not a tensor.")
+
+    # Check that both TensorDicts have the same number of dimensions
+    if len(input.batch_size) != len(reference.batch_size):
+        raise ValueError(
+            "The number of dimensions in the batch size of the input and reference must be the same."
+        )
+
+    # Check dim is valid
+    batch_dims = input.ndim
+    if dim >= batch_dims or dim < -batch_dims or batch_dims == 0:
+        raise ValueError(
+            f"The specified dimension '{dim}' is invalid for an input TensorDict with batch size '{input.batch_size}'."
+        )
+
+    # Convert negative dimension to its positive equivalent
+    if dim < 0:
+        dim = batch_dims + dim
+
+    # Find the common indices
+    N = reference_tensor.shape[dim]
+    cat_data = torch.cat([reference_tensor, target_tensor], dim=dim)
+    _, unique_indices = torch.unique(
+        cat_data, dim=dim, sorted=True, return_inverse=True
+    )
+    out = torch.isin(unique_indices[N:], unique_indices[:N], assume_unique=True)
+
+    return out
+
+
+def _index_preserve_data_ptr(index):
+    if isinstance(index, tuple):
+        return all(_index_preserve_data_ptr(idx) for idx in index)
+    # we can't use a list comprehension here because it fails with tensor indices
+    if index is None or index is Ellipsis:
+        return True
+    if isinstance(index, int):
+        return True
+    if isinstance(index, slice) and (index.start == 0 or index.start is None):
+        return True
+    return False
+
+
+def remove_duplicates(
+    input: TensorDictBase,
+    key: NestedKey,
+    dim: int = 0,
+    *,
+    return_indices: bool = False,
+) -> TensorDictBase:
+    """Removes indices duplicated in `key` along the specified dimension.
+
+    This method detects duplicate elements in the tensor associated with the specified `key` along the specified
+    `dim` and removes elements in the same indices in all other tensors within the TensorDict. It is expected for
+    `dim` to be one of the dimensions within the batch size of the input TensorDict to ensure consistency in all
+    tensors. Otherwise, an error will be raised.
+
+    Args:
+        input (TensorDictBase): The TensorDict containing potentially duplicate elements.
+        key (NestedKey): The key of the tensor along which duplicate elements should be identified and removed. It
+            must be one of the leaf keys within the TensorDict, pointing to a tensor and not to another TensorDict.
+        dim (int, optional): The dimension along which duplicate elements should be identified and removed. It must be one of
+            the dimensions within the batch size of the input TensorDict. Defaults to ``0``.
+        return_indices (bool, optional): If ``True``, the indices of the unique elements in the input tensor will be
+            returned as well. Defaults to ``False``.
+
+    Returns:
+        output (TensorDictBase): input tensordict with the indices corrsponding to duplicated elements
+            in tensor `key` along dimension `dim` removed.
+        unique_indices (torch.Tensor, optional): The indices of the first occurrences of the unique elements in the
+            input tensordict for the specified `key` along the specified `dim`. Only provided if return_index is True.
+
+    Example:
+        >>> td = TensorDict(
+        ...     {
+        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [7, 8, 9]]),
+        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [40, 50], [50, 60]]),
+        ...     }
+        ...     batch_size=[4],
+        ... )
+        >>> output_tensordict = remove_duplicate_elements(td, key="tensor1", dim=0)
+        >>> expected_output = TensorDict(
+        ...     {
+        ...         "tensor1": torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]]),
+        ...         "tensor2": torch.tensor([[10, 20], [30, 40], [50, 60]]),
+        ...     },
+        ...     batch_size=[3],
+        ... )
+        >>> assert (td == expected_output).all()
+    """
+    tensor = input.get(key, default=None)
+
+    # Check if the key is a TensorDict
+    if tensor is None:
+        raise KeyError(f"The key '{key}' does not exist in the TensorDict.")
+
+    # Check that the key points to a tensor
+    if not isinstance(tensor, torch.Tensor):
+        raise KeyError(f"The key '{key}' does not point to a tensor in the TensorDict.")
+
+    # Check dim is valid
+    batch_dims = input.ndim
+    if dim >= batch_dims or dim < -batch_dims or batch_dims == 0:
+        raise ValueError(
+            f"The specified dimension '{dim}' is invalid for a TensorDict with batch size '{input.batch_size}'."
+        )
+
+    # Convert negative dimension to its positive equivalent
+    if dim < 0:
+        dim = batch_dims + dim
+
+    # Get indices of unique elements (e.g. [0, 1, 0, 2])
+    _, unique_indices, counts = torch.unique(
+        tensor, dim=dim, sorted=True, return_inverse=True, return_counts=True
+    )
+
+    # Find first occurrence of each index  (e.g. [0, 1, 3])
+    _, unique_indices_sorted = torch.sort(unique_indices, stable=True)
+    cum_sum = counts.cumsum(0, dtype=torch.long)
+    cum_sum = torch.cat(
+        (torch.zeros(1, device=input.device, dtype=torch.long), cum_sum[:-1])
+    )
+    first_indices = unique_indices_sorted[cum_sum]
+
+    # Remove duplicate elements in the TensorDict
+    output = input[(slice(None),) * dim + (first_indices,)]
+
+    if return_indices:
+        return output, unique_indices
+
+    return output
+
+
+class _CloudpickleWrapper(object):
+    def __init__(self, fn):
+        self.fn = fn
+
+    def __getstate__(self):
+        import cloudpickle
+
+        return cloudpickle.dumps(self.fn)
+
+    def __setstate__(self, ob: bytes):
+        import pickle
+
+        self.fn = pickle.loads(ob)
+
+    def __call__(self, *args, **kwargs):
+        return self.fn(*args, **kwargs)
+
+
+class _BatchedUninitializedParameter(UninitializedParameter):
+    batch_size: torch.Size
+    in_dim: int | None = None
+    vmap_level: int | None = None
+
+    def materialize(self, shape, device=None, dtype=None):
+        UninitializedParameter.materialize(
+            self, (*self.batch_size, *shape), device=device, dtype=dtype
+        )
+
+
+class _BatchedUninitializedBuffer(UninitializedBuffer):
+    batch_size: torch.Size
+    in_dim: int | None = None
+    vmap_level: int | None = None
+
+    def materialize(self, shape, device=None, dtype=None):
+        UninitializedBuffer.materialize(
+            self, (*self.batch_size, *shape), device=device, dtype=dtype
+        )
+
+
+class _add_batch_dim_pre_hook:
+    def __call__(self, mod: torch.nn.Module, args, kwargs):
+        for name, param in list(mod.named_parameters(recurse=False)):
+            if hasattr(param, "in_dim") and hasattr(param, "vmap_level"):
+                from torch._C._functorch import _add_batch_dim
+
+                param = _add_batch_dim(param, param.in_dim, param.vmap_level)
+                delattr(mod, name)
+                setattr(mod, name, param)
+        for key, val in list(mod._forward_pre_hooks.items()):
+            if val is self:
+                del mod._forward_pre_hooks[key]
+                return
+        else:
+            raise RuntimeError("did not find pre-hook")
+
+
+def is_non_tensor(data):
+    """Checks if an item is a non-tensor."""
+    return getattr(type(data), "_is_non_tensor", False)
+
+
+def _is_non_tensor(cls: type):
+    return getattr(cls, "_is_non_tensor", False)
+
+
+class KeyDependentDefaultDict(collections.defaultdict):
+    """A key-dependent default dict.
+
+    Examples:
+        >>> my_dict = KeyDependentDefaultDict(lambda key: "foo_" + key)
+        >>> print(my_dict["bar"])
+        foo_bar
+    """
+
+    def __init__(self, fun):
+        self.fun = fun
+        super().__init__()
+
+    def __missing__(self, key):
+        value = self.fun(key)
+        self[key] = value
+        return value
```

## tensordict/version.py

```diff
@@ -1,2 +1,2 @@
-__version__ = '2024.06.02'
-git_version = 'a35430776245e67d31ac87b539b91c6b24c21c39'
+__version__ = '2024.06.03'
+git_version = 'Unknown'
```

## tensordict/nn/__init__.py

 * *Ordering differences only*

```diff
@@ -1,62 +1,62 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict.nn.common import (
-    dispatch,
-    make_tensordict,
-    TensorDictModule,
-    TensorDictModuleBase,
-    TensorDictModuleWrapper,
-)
-from tensordict.nn.distributions import (
-    AddStateIndependentNormalScale,
-    CompositeDistribution,
-    NormalParamExtractor,
-    OneHotCategorical,
-    rand_one_hot,
-    TruncatedNormal,
-)
-from tensordict.nn.ensemble import EnsembleModule
-from tensordict.nn.functional_modules import (
-    get_functional,
-    is_functional,
-    make_functional,
-    repopulate_module,
-)
-from tensordict.nn.params import TensorDictParams
-from tensordict.nn.probabilistic import (
-    InteractionType,
-    ProbabilisticTensorDictModule,
-    ProbabilisticTensorDictSequential,
-    set_interaction_mode,
-    set_interaction_type,
-)
-from tensordict.nn.sequence import TensorDictSequential
-from tensordict.nn.utils import (
-    biased_softplus,
-    inv_softplus,
-    set_skip_existing,
-    skip_existing,
-)
-
-__all__ = [
-    "dispatch",
-    "TensorDictModule",
-    "TensorDictModuleWrapper",
-    "get_functional",
-    "make_functional",
-    "repopulate_module",
-    "InteractionType",
-    "ProbabilisticTensorDictModule",
-    "ProbabilisticTensorDictSequential",
-    "set_interaction_mode",
-    "set_interaction_type",
-    "TensorDictSequential",
-    "make_tensordict",
-    "biased_softplus",
-    "inv_softplus",
-    "TensorDictParams",
-    "is_functional",
-]
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict.nn.common import (
+    dispatch,
+    make_tensordict,
+    TensorDictModule,
+    TensorDictModuleBase,
+    TensorDictModuleWrapper,
+)
+from tensordict.nn.distributions import (
+    AddStateIndependentNormalScale,
+    CompositeDistribution,
+    NormalParamExtractor,
+    OneHotCategorical,
+    rand_one_hot,
+    TruncatedNormal,
+)
+from tensordict.nn.ensemble import EnsembleModule
+from tensordict.nn.functional_modules import (
+    get_functional,
+    is_functional,
+    make_functional,
+    repopulate_module,
+)
+from tensordict.nn.params import TensorDictParams
+from tensordict.nn.probabilistic import (
+    InteractionType,
+    ProbabilisticTensorDictModule,
+    ProbabilisticTensorDictSequential,
+    set_interaction_mode,
+    set_interaction_type,
+)
+from tensordict.nn.sequence import TensorDictSequential
+from tensordict.nn.utils import (
+    biased_softplus,
+    inv_softplus,
+    set_skip_existing,
+    skip_existing,
+)
+
+__all__ = [
+    "dispatch",
+    "TensorDictModule",
+    "TensorDictModuleWrapper",
+    "get_functional",
+    "make_functional",
+    "repopulate_module",
+    "InteractionType",
+    "ProbabilisticTensorDictModule",
+    "ProbabilisticTensorDictSequential",
+    "set_interaction_mode",
+    "set_interaction_type",
+    "TensorDictSequential",
+    "make_tensordict",
+    "biased_softplus",
+    "inv_softplus",
+    "TensorDictParams",
+    "is_functional",
+]
```

## tensordict/nn/common.py

 * *Ordering differences only*

```diff
@@ -1,1300 +1,1300 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-import inspect
-import warnings
-from textwrap import indent
-from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
-
-import torch
-from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
-
-from tensordict._td import is_tensor_collection, TensorDictBase
-from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
-from tensordict.functional import make_tensordict
-from tensordict.nn.functional_modules import (
-    _swap_state,
-    extract_weights_and_buffers,
-    is_functional,
-    make_functional,
-    repopulate_module,
-)
-from tensordict.nn.utils import (
-    _auto_make_functional,
-    _dispatch_td_nn_modules,
-    set_skip_existing,
-)
-from tensordict.utils import implement_for, NestedKey
-from torch import nn, Tensor
-
-try:
-    from functorch import FunctionalModule, FunctionalModuleWithBuffers
-
-    _has_functorch = True
-except ImportError:
-    _has_functorch = False
-
-    class FunctionalModule:
-        pass
-
-    class FunctionalModuleWithBuffers:
-        pass
-
-
-__all__ = [
-    "TensorDictModule",
-    "TensorDictModuleWrapper",
-]
-
-
-class dispatch:
-    """Allows for a function expecting a TensorDict to be called using kwargs.
-
-    :func:`dispatch` must be used within modules that have an ``in_keys`` (or
-    another source of keys indicated by the ``source`` keyword argument) and
-    ``out_keys`` (or another ``dest`` key list) attributes indicating what keys
-    to be read and written from the tensordict. The wrapped function should
-    also have a ``tensordict`` leading argument.
-
-    The resulting function will return a single tensor (if there is a single
-    element in out_keys), otherwise it will return a tuple sorted as the ``out_keys``
-    of the module.
-
-    :func:`dispatch` can be used either as a method or as a class when extra arguments
-    need to be passed.
-
-    Args:
-        separator (str, optional): separator that combines sub-keys together
-            for ``in_keys`` that are tuples of strings.
-            Defaults to ``"_"``.
-        source (str or list of keys, optional): if a string is provided,
-            it points to the module attribute that contains the
-            list of input keys to be used. If a list is provided instead, it
-            will contain the keys used as input to the module.
-            Defaults to ``"in_keys"`` which is the attribute name of
-            :class:`~.TensorDictModule` list of input keys.
-        dest (str or list of keys, optional): if a string is provided,
-            it points to the module attribute that contains the
-            list of output keys to be used. If a list is provided instead, it
-            will contain the keys used as output to the module.
-            Defaults to ``"out_keys"`` which is the attribute name of
-            :class:`~.TensorDictModule` list of output keys.
-        auto_batch_size (bool, optional): if ``True``, the batch-size of the
-            input tensordict is determined automatically as the maximum number
-            of common dimensions across all the input tensors.
-            Defaults to ``True``.
-
-    Examples:
-        >>> class MyModule(nn.Module):
-        ...     in_keys = ["a"]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-        >>> # equivalently
-        >>> class MyModule(nn.Module):
-        ...     keys_in = ["a"]
-        ...     keys_out = ["b"]
-        ...
-        ...     @dispatch(source="keys_in", dest="keys_out")
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-        >>> # or this
-        >>> class MyModule(nn.Module):
-        ...     @dispatch(source=["a"], dest=["b"])
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModule()
-        >>> b = module(a=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-    :func:`dispatch_kwargs` will also work with nested keys with the default
-    ``"_"`` separator.
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c")]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(a_c=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-    If another separator is wanted, it can be indicated with the ``separator``
-    argument in the constructor:
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c")]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch(separator="sep")
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + 1
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(asepc=torch.zeros(1, 2))
-        >>> assert (b == 1).all()
-
-
-    Since the input keys is a sorted sequence of strings,
-    :func:`dispatch` can also be used with unnamed arguments where the order
-    must match the order of the input keys.
-
-    .. note::
-
-        If the first argument is a :class:`~.TensorDictBase` instance, it is
-        assumed that dispatch is __not__ being used and that this tensordict
-        contains all the necessary information to be run through the module.
-        In other words, one cannot decompose a tensordict with the first key
-        of the module inputs pointing to a tensordict instance.
-        In general, it is preferred to use :func:`dispatch` with tensordict
-        leaves only.
-
-    Examples:
-        >>> class MyModuleNest(nn.Module):
-        ...     in_keys = [("a", "c"), "d"]
-        ...     out_keys = ["b"]
-        ...
-        ...     @dispatch
-        ...     def forward(self, tensordict):
-        ...         tensordict['b'] = tensordict['a', 'c'] + tensordict["d"]
-        ...         return tensordict
-        ...
-        >>> module = MyModuleNest()
-        >>> b, = module(torch.zeros(1, 2), d=torch.ones(1, 2))  # works
-        >>> assert (b == 1).all()
-        >>> b, = module(torch.zeros(1, 2), torch.ones(1, 2))  # works
-        >>> assert (b == 1).all()
-        >>> try:
-        ...     b, = module(torch.zeros(1, 2), a_c=torch.ones(1, 2))  # fails
-        ... except:
-        ...     print("oopsy!")
-        ...
-
-    """
-
-    DEFAULT_SEPARATOR = "_"
-    DEFAULT_SOURCE = "in_keys"
-    DEFAULT_DEST = "out_keys"
-
-    def __new__(
-        cls,
-        separator=DEFAULT_SEPARATOR,
-        source=DEFAULT_SOURCE,
-        dest=DEFAULT_DEST,
-        auto_batch_size: bool = True,
-    ):
-        if callable(separator):
-            func = separator
-            separator = dispatch.DEFAULT_SEPARATOR
-            self = super().__new__(cls)
-            self.__init__(separator, source, dest)
-            return self.__call__(func)
-        return super().__new__(cls)
-
-    def __init__(
-        self,
-        separator=DEFAULT_SEPARATOR,
-        source=DEFAULT_SOURCE,
-        dest=DEFAULT_DEST,
-        auto_batch_size: bool = True,
-    ):
-        self.separator = separator
-        self.source = source
-        self.dest = dest
-        self.auto_batch_size = auto_batch_size
-
-    def __call__(self, func: Callable) -> Callable:
-        # sanity check
-        for i, key in enumerate(inspect.signature(func).parameters):
-            if i == 0:
-                # skip self
-                continue
-            if key != "tensordict":
-                raise RuntimeError(
-                    "the first argument of the wrapped function must be "
-                    "named 'tensordict'."
-                )
-            break
-        # if the env variable was used, we can skip the wrapper altogether
-        if not _dispatch_td_nn_modules():
-            return func
-
-        @functools.wraps(func)
-        def wrapper(_self, *args: Any, **kwargs: Any) -> Any:
-            if not _dispatch_td_nn_modules():
-                return func(_self, *args, **kwargs)
-
-            source = self.source
-            if isinstance(source, str):
-                source = getattr(_self, source)
-            tensordict = None
-            if len(args):
-                if is_tensor_collection(args[0]):
-                    tensordict, args = args[0], args[1:]
-            if tensordict is None:
-                tensordict_values = {}
-                dest = self.dest
-                if isinstance(dest, str):
-                    dest = getattr(_self, dest)
-                for key in source:
-                    expected_key = self.separator.join(_unravel_key_to_tuple(key))
-                    if len(args):
-                        tensordict_values[key] = args[0]
-                        args = args[1:]
-                        if expected_key in kwargs:
-                            raise RuntimeError(
-                                "Duplicated argument in args and kwargs."
-                            )
-                    elif expected_key in kwargs:
-                        try:
-                            tensordict_values[key] = kwargs.pop(expected_key)
-                        except KeyError:
-                            raise KeyError(
-                                f"The key {expected_key} wasn't found in the keyword arguments "
-                                f"but is expected to execute that function."
-                            )
-                tensordict = make_tensordict(
-                    tensordict_values,
-                    batch_size=torch.Size([]) if not self.auto_batch_size else None,
-                )
-                out = func(_self, tensordict, *args, **kwargs)
-                out = tuple(out[key] for key in dest)
-                return out[0] if len(out) == 1 else out
-            return func(_self, tensordict, *args, **kwargs)
-
-        return wrapper
-
-
-class _OutKeysSelect:
-    def __init__(self, out_keys):
-        self.out_keys = out_keys
-        self._initialized = False
-
-    def _init(self, module):
-        if self._initialized:
-            return
-        self._initialized = True
-        self.module = module
-        module.out_keys = list(self.out_keys)
-
-    @implement_for("torch", None, "2.0")
-    def __call__(  # noqa: F811
-        self,
-        module: TensorDictModuleBase,
-        tensordict_in: TensorDictBase,
-        tensordict_out: TensorDictBase,
-    ):
-        if not isinstance(tensordict_out, TensorDictBase):
-            raise RuntimeError(
-                "You are likely using tensordict.nn.dispatch with keyword arguments with an older (< 2.0) version of pytorch. "
-                "This is currently not supported. Please use unnamed arguments or upgrade pytorch."
-            )
-        # detect dispatch calls
-        in_keys = module.in_keys
-        is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
-        out_keys = self.out_keys
-        # if dispatch filtered the out keys as they should we're happy
-        if is_dispatched:
-            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
-                len(out_keys) == len(tensordict_out)
-            ):
-                return tensordict_out
-        self._init(module)
-        if is_dispatched:
-            # it might be the case that dispatch was not aware of what the out-keys were.
-            if isinstance(tensordict_out, tuple):
-                out = tuple(
-                    item
-                    for i, item in enumerate(tensordict_out)
-                    if module._out_keys[i] in module.out_keys
-                )
-                if len(out) == 1:
-                    return out[0]
-                return out
-            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
-                return tensordict_out
-            elif (
-                module._out_keys[0] not in module.out_keys
-                and len(module._out_keys) == 1
-            ):
-                return ()
-            else:
-                raise RuntimeError(
-                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
-                )
-        if tensordict_out is tensordict_in:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-            )
-        else:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-                strict=False,
-            )
-
-    @implement_for("torch", "2.0", None)
-    def __call__(  # noqa: F811
-        self,
-        module: TensorDictModuleBase,
-        tensordict_in: TensorDictBase,
-        kwargs: Dict,
-        tensordict_out: TensorDictBase,
-    ):
-        # detect dispatch calls
-        in_keys = module.in_keys
-        if not tensordict_in and kwargs.get("tensordict", None) is not None:
-            tensordict_in = kwargs.pop("tensordict")
-        is_dispatched = self._detect_dispatch(tensordict_in, kwargs, in_keys)
-        out_keys = self.out_keys
-        # if dispatch filtered the out keys as they should we're happy
-        if is_dispatched:
-            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
-                len(out_keys) == len(tensordict_out)
-            ):
-                return tensordict_out
-        self._init(module)
-        if is_dispatched:
-            # it might be the case that dispatch was not aware of what the out-keys were.
-            if isinstance(tensordict_out, tuple):
-                out = tuple(
-                    item
-                    for i, item in enumerate(tensordict_out)
-                    if module._out_keys[i] in module.out_keys
-                )
-                if len(out) == 1:
-                    return out[0]
-                return out
-            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
-                return tensordict_out
-            elif (
-                module._out_keys[0] not in module.out_keys
-                and len(module._out_keys) == 1
-            ):
-                return ()
-            else:
-                raise RuntimeError(
-                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
-                )
-        if tensordict_out is tensordict_in:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-            )
-        else:
-            return tensordict_out.select(
-                *in_keys,
-                *out_keys,
-                inplace=True,
-                strict=False,
-            )
-
-    @implement_for("torch", None, "2.0")
-    def _detect_dispatch(self, tensordict_in, in_keys):  # noqa: F811
-        if isinstance(tensordict_in, TensorDictBase) and all(
-            key in tensordict_in.keys() for key in in_keys
-        ):
-            return False
-        elif isinstance(tensordict_in, tuple):
-            if len(tensordict_in):
-                if isinstance(tensordict_in[0], TensorDictBase):
-                    return self._detect_dispatch(tensordict_in[0], in_keys)
-                return True
-            return not len(in_keys)
-        # not a TDBase: must be True
-        return True
-
-    @implement_for("torch", "2.0", None)
-    def _detect_dispatch(self, tensordict_in, kwargs, in_keys):  # noqa: F811
-        if isinstance(tensordict_in, TensorDictBase) and all(
-            key in tensordict_in.keys(include_nested=True) for key in in_keys
-        ):
-            return False
-        elif isinstance(tensordict_in, tuple):
-            if len(tensordict_in) or len(kwargs):
-                if len(tensordict_in) and isinstance(tensordict_in[0], TensorDictBase):
-                    return self._detect_dispatch(tensordict_in[0], kwargs, in_keys)
-                elif (
-                    not len(tensordict_in)
-                    and len(kwargs)
-                    and isinstance(kwargs.get("tensordict", None), TensorDictBase)
-                ):
-                    return self._detect_dispatch(kwargs["tensordict"], in_keys)
-                return True
-            return not len(in_keys)
-        # not a TDBase: must be True
-        return True
-
-    def remove(self):
-        # reset ground truth
-        if self.module._out_keys is not None:
-            self.module.out_keys = self.module._out_keys
-
-    def __del__(self):
-        self.remove()
-
-
-class TensorDictModuleBase(nn.Module):
-    """Base class to TensorDict modules.
-
-    TensorDictModule subclasses are characterized by ``in_keys`` and ``out_keys``
-    key-lists that indicate what input entries are to be read and what output
-    entries should be expected to be written.
-
-    The forward method input/output signature should always follow the
-    convention:
-
-        >>> tensordict_out = module.forward(tensordict_in)
-
-    """
-
-    def __new__(cls, *args, **kwargs):
-        # check the out_keys and in_keys in the dict
-        if "in_keys" in cls.__dict__ and not isinstance(
-            cls.__dict__.get("in_keys"), property
-        ):
-            in_keys = cls.__dict__.get("in_keys")
-            # now let's remove it
-            delattr(cls, "in_keys")
-            cls._in_keys = unravel_key_list(in_keys)
-            cls.in_keys = TensorDictModuleBase.in_keys
-        if "out_keys" in cls.__dict__ and not isinstance(
-            cls.__dict__.get("out_keys"), property
-        ):
-            out_keys = cls.__dict__.get("out_keys")
-            # now let's remove it
-            delattr(cls, "out_keys")
-            out_keys = unravel_key_list(out_keys)
-            cls._out_keys = out_keys
-            cls._out_keys_apparent = out_keys
-            cls.out_keys = TensorDictModuleBase.out_keys
-        out = super().__new__(cls)
-        return out
-
-    @property
-    def in_keys(self):
-        return self._in_keys
-
-    @in_keys.setter
-    def in_keys(self, value: List[Union[str, Tuple[str]]]):
-        self._in_keys = unravel_key_list(value)
-
-    @property
-    def out_keys(self):
-        return self._out_keys_apparent
-
-    @property
-    def out_keys_source(self):
-        return self._out_keys
-
-    @out_keys.setter
-    def out_keys(self, value: List[Union[str, Tuple[str]]]):
-        # the first time out_keys are set, they are marked as ground truth
-        value = unravel_key_list(list(value))
-        if not hasattr(self, "_out_keys"):
-            self._out_keys = value
-        self._out_keys_apparent = value
-
-    @implement_for("torch", None, "2.0")
-    def select_out_keys(self, *out_keys):  # noqa: F811
-        """Selects the keys that will be found in the output tensordict.
-
-        This is useful whenever one wants to get rid of intermediate keys in a
-        complicated graph, or when the presence of these keys may trigger unexpected
-        behaviours.
-
-        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
-
-        Args:
-            *out_keys (a sequence of strings or tuples of strings): the
-                out_keys that should be found in the output tensordict.
-
-        Returns: the same module, modified in-place with updated ``out_keys``.
-
-        The simplest usage is with :class:`~.TensorDictModule`:
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This feature will also work with dispatched arguments:
-        Examples:
-            >>> mod(torch.zeros(()), torch.ones(()))
-            tensor(2.)
-
-        This change will occur in-place (ie the same module will be returned
-        with an updated list of out_keys). It can be reverted using the
-        :meth:`TensorDictModuleBase.reset_out_keys` method.
-
-        Examples:
-            >>> mod.reset_out_keys()
-            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This will work with other classes too, such as Sequential:
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential
-            >>> seq = TensorDictSequential(
-            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
-            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
-            ... )
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> seq.select_out_keys("z")
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        out_keys = unravel_key_list(list(out_keys))
-        if len(out_keys) == 1:
-            if out_keys[0] not in self.out_keys:
-                err_msg = f"Can't select non existent key: {out_keys[0]}. "
-                if (
-                    out_keys[0]
-                    and isinstance(out_keys[0], (tuple, list))
-                    and out_keys[0][0] in self.out_keys
-                ):
-                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
-                raise ValueError(err_msg)
-        self.register_forward_hook(_OutKeysSelect(out_keys))
-        for hook in self._forward_hooks.values():
-            if isinstance(hook, _OutKeysSelect):
-                hook._init(self)
-        return self
-
-    @implement_for("torch", "2.0", None)
-    def select_out_keys(self, *out_keys):  # noqa: F811
-        """Selects the keys that will be found in the output tensordict.
-
-        This is useful whenever one wants to get rid of intermediate keys in a
-        complicated graph, or when the presence of these keys may trigger unexpected
-        behaviours.
-
-        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
-
-        Args:
-            *out_keys (a sequence of strings or tuples of strings): the
-                out_keys that should be found in the output tensordict.
-
-        Returns: the same module, modified in-place with updated ``out_keys``.
-
-        The simplest usage is with :class:`~.TensorDictModule`:
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This feature will also work with dispatched arguments:
-        Examples:
-            >>> mod(torch.zeros(()), torch.ones(()))
-            tensor(2.)
-
-        This change will occur in-place (ie the same module will be returned
-        with an updated list of out_keys). It can be reverted using the
-        :meth:`TensorDictModuleBase.reset_out_keys` method.
-
-        Examples:
-            >>> mod.reset_out_keys()
-            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        This will work with other classes too, such as Sequential:
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential
-            >>> seq = TensorDictSequential(
-            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
-            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
-            ... )
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> seq.select_out_keys("z")
-            >>> td = TensorDict({"x": torch.zeros(())}, [])
-            >>> seq(td)
-            TensorDict(
-                fields={
-                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-        """
-        out_keys = unravel_key_list(list(out_keys))
-        if len(out_keys) == 1:
-            if out_keys[0] not in self.out_keys:
-                err_msg = f"Can't select non existent key: {out_keys[0]}. "
-                if (
-                    out_keys[0]
-                    and isinstance(out_keys[0], (tuple, list))
-                    and out_keys[0][0] in self.out_keys
-                ):
-                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
-                raise ValueError(err_msg)
-        self.register_forward_hook(_OutKeysSelect(out_keys), with_kwargs=True)
-        for hook in self._forward_hooks.values():
-            if isinstance(hook, _OutKeysSelect):
-                hook._init(self)
-        return self
-
-    def reset_out_keys(self):
-        """Resets the ``out_keys`` attribute to its orignal value.
-
-        Returns: the same module, with its original ``out_keys`` values.
-
-        Examples:
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-            >>> import torch
-            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
-            >>> mod.select_out_keys("d")
-            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-            >>> mod.reset_out_keys()
-            >>> mod(td)
-            TensorDict(
-                fields={
-                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-        """
-        for i, hook in list(self._forward_hooks.items()):
-            if isinstance(hook, _OutKeysSelect):
-                del self._forward_hooks[i]
-        return self
-
-    def reset_parameters_recursive(
-        self, parameters: Optional[TensorDictBase] = None
-    ) -> Optional[TensorDictBase]:
-        """Recursively reset the parameters of the module and its children.
-
-        Args:
-            parameters (TensorDict of parameters, optional): If set to None, the module will reset using self.parameters().
-                Otherwise, we will reset the parameters in the tensordict in-place. This is
-                useful for functional modules where the parameters are not stored in the module itself.
-
-        Returns:
-            A tensordict of the new parameters, only if parameters was not None.
-
-        Examples:
-            >>> from tensordict.nn import TensorDictModule
-            >>> from torch import nn
-            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
-            >>> old_param = net[0].weight.clone()
-            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
-            >>> module.reset_parameters()
-            >>> (old_param == net[0].weight).any()
-            tensor(False)
-
-        This method also supports functional parameter sampling:
-
-            >>> from tensordict import TensorDict
-            >>> from tensordict.nn import TensorDictModule
-            >>> from torch import nn
-            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
-            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
-            >>> params = TensorDict.from_module(module)
-            >>> old_params = params.clone(recurse=True)
-            >>> module.reset_parameters(params)
-            >>> (old_params == params).any()
-            False
-        """
-        if parameters is None:
-            any_reset = self._reset_parameters(self)
-            if not any_reset:
-                warnings.warn(
-                    "reset_parameters_recursive was called without the parameters argument and did not find any parameters to reset"
-                )
-            return
-        elif parameters.ndim:
-            raise RuntimeError(
-                "reset_parameters_recursive does not support batched TensorDicts, ensure `batch_size` is empty and the parameters shape match their original shape."
-            )
-
-        sanitized_parameters = parameters.apply(
-            lambda x: x.detach().requires_grad_(), inplace=False
-        )
-
-        if _auto_make_functional() and not is_functional(self):
-            make_functional(self, keep_params=True)
-            is_stateless = self._is_stateless
-            if is_stateless:
-                repopulate_module(self, sanitized_parameters)
-            else:
-                old_params = _swap_state(
-                    self,
-                    sanitized_parameters,
-                    is_stateless=False,
-                    return_old_tensordict=True,
-                )
-
-            self._reset_parameters(self)
-
-            if is_stateless:
-                new_parameters = extract_weights_and_buffers(self)
-            else:
-                new_parameters = _swap_state(
-                    self, old_params, is_stateless=False, return_old_tensordict=True
-                )
-            return new_parameters
-        else:
-            with sanitized_parameters.to_module(self):
-                self._reset_parameters(self)
-            return sanitized_parameters
-
-    def _reset_parameters(self, module: nn.Module) -> bool:
-        any_reset = False
-        for child in module.children():
-            if isinstance(child, nn.Module):
-                any_reset |= self._reset_parameters(child)
-
-            if hasattr(child, "reset_parameters"):
-                child.reset_parameters()
-                any_reset |= True
-        return any_reset
-
-
-class TensorDictModule(TensorDictModuleBase):
-    """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
-
-    Args:
-        module (Callable): a callable, typically a :class:`torch.nn.Module`,
-            used to map the input to the output parameter space. Its forward method
-            can return a single tensor, a tuple of tensors or even a dictionary.
-            In the latter case, the output keys of the :class:`TensorDictModule`
-            will be used to populate the output tensordict (ie. the keys present
-            in ``out_keys`` should be present in the dictionary returned by the
-            ``module`` forward method).
-        in_keys (iterable of NestedKeys, Dict[NestedStr, str]): keys to be read
-            from input tensordict and passed to the module. If it
-            contains more than one element, the values will be passed in the
-            order given by the in_keys iterable.
-            If ``in_keys`` is a dictionary, its keys must correspond to the key
-            to be read in the tensordict and its values must match the name of
-            the keyword argument in the function signature.
-        out_keys (iterable of str): keys to be written to the input tensordict. The length of out_keys must match the
-            number of tensors returned by the embedded module. Using "_" as a key avoid writing tensor to output.
-
-    Embedding a neural network in a TensorDictModule only requires to specify the input
-    and output keys. TensorDictModule support functional and regular :obj:`nn.Module`
-    objects. In the functional case, the 'params' (and 'buffers') keyword argument must
-    be specified:
-
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> # one can wrap regular nn.Module
-        >>> module = TensorDictModule(nn.Transformer(128), in_keys=["input", "tgt"], out_keys=["out"])
-        >>> input = torch.ones(2, 3, 128)
-        >>> tgt = torch.zeros(2, 3, 128)
-        >>> data = TensorDict({"input": input, "tgt": tgt}, batch_size=[2, 3])
-        >>> data = module(data)
-        >>> print(data)
-        TensorDict(
-            fields={
-                input: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
-                out: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
-                tgt: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([2, 3]),
-            device=None,
-            is_shared=False)
-
-    We can also pass directly the tensors
-
-    Examples:
-        >>> out = module(input, tgt)
-        >>> assert out.shape == input.shape
-        >>> # we can also wrap regular functions
-        >>> module = TensorDictModule(lambda x: (x-1, x+1), in_keys=[("input", "x")], out_keys=[("output", "x-1"), ("output", "x+1")])
-        >>> module(TensorDict({("input", "x"): torch.zeros(())}, batch_size=[]))
-        TensorDict(
-            fields={
-                input: TensorDict(
-                    fields={
-                        x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False),
-                output: TensorDict(
-                    fields={
-                        x+1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
-                        x-1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    We can use TensorDictModule to populate a tensordict:
-
-    Examples:
-        >>> module = TensorDictModule(lambda: torch.randn(3), in_keys=[], out_keys=["x"])
-        >>> print(module(TensorDict({}, batch_size=[])))
-        TensorDict(
-            fields={
-                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    Another feature is passing a dictionary as input keys, to control the
-    dispatching of values to specific keyword arguments.
-
-    Examples:
-        >>> module = TensorDictModule(lambda x, *, y: x+y,
-        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'],
-        ...     )
-        >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))
-        >>> td['z']
-        tensor(3.)
-
-    Functional calls to a tensordict module is easy:
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import TensorDictModule
-        >>> td = TensorDict({"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3,])
-        >>> module = torch.nn.GRUCell(4, 8)
-        >>> td_module = TensorDictModule(
-        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
-        ... )
-        >>> params = TensorDict.from_module(td_module)
-        >>> # functional API
-        >>> with params.to_module(td_module):
-        ...     td_functional = td_module(td.clone())
-        >>> print(td_functional)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    In the stateful case:
-        >>> module = torch.nn.GRUCell(4, 8)
-        >>> td_module = TensorDictModule(
-        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
-        ... )
-        >>> td_stateful = td_module(td.clone())
-        >>> print(td_stateful)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    One can use a vmap operator to call the functional module.
-
-    Examples:
-        >>> from torch import vmap
-        >>> from tensordict.nn.functional_modules import extract_weights_and_buffers
-        >>> params = extract_weights_and_buffers(td_module)
-        >>> params_repeat = params.expand(4)
-        >>> print(params_repeat)
-        TensorDict(
-            fields={
-                module: TensorDict(
-                    fields={
-                        bias_hh: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
-                        bias_ih: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight_hh: Tensor(shape=torch.Size([4, 24, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight_ih: Tensor(shape=torch.Size([4, 24, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([4]),
-            device=None,
-            is_shared=False)
-        >>> def func(td, params):
-        ...     with params.to_module(td_module):
-        ...         return td_module(td)
-        >>> td_vmap = vmap(func, (None, 0))(td.clone(), params_repeat)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    _IN_KEY_ERR = "in_keys must be of type list, str or tuples of str, or dict."
-    _OUT_KEY_ERR = "out_keys must be of type list, str or tuples of str."
-
-    def __init__(
-        self,
-        module: Callable,
-        in_keys: NestedKey | List[NestedKey] | Dict[NestedKey:str],
-        out_keys: NestedKey | List[NestedKey],
-    ) -> None:
-        super().__init__()
-
-        if isinstance(in_keys, dict):
-            # write the kwargs and create a list instead
-            _in_keys = []
-            self._kwargs = []
-            for key, value in in_keys.items():
-                self._kwargs.append(value)
-                _in_keys.append(key)
-            in_keys = _in_keys
-        else:
-            if isinstance(in_keys, (str, tuple)):
-                in_keys = [in_keys]
-            elif not isinstance(in_keys, list):
-                raise ValueError(self._IN_KEY_ERR)
-            self._kwargs = None
-
-        if isinstance(out_keys, (str, tuple)):
-            out_keys = [out_keys]
-        elif not isinstance(out_keys, list):
-            raise ValueError(self._OUT_KEY_ERR)
-        try:
-            in_keys = unravel_key_list(in_keys)
-        except Exception:
-            raise ValueError(self._IN_KEY_ERR)
-        try:
-            out_keys = unravel_key_list(out_keys)
-        except Exception:
-            raise ValueError(self._OUT_KEY_ERR)
-
-        if type(module) is type or not callable(module):
-            raise ValueError(
-                f"Module {module} if type {type(module)} is not callable. "
-                f"Typical accepted types are nn.Module or TensorDictModule."
-            )
-        self.out_keys = out_keys
-        self.in_keys = in_keys
-
-        if "_" in self.in_keys:
-            warnings.warn(
-                'key "_" is for ignoring output, it should not be used in input keys',
-                stacklevel=2,
-            )
-
-        self.module = module
-        if _auto_make_functional():
-            make_functional(self, keep_params=True, return_params=False)
-
-    @property
-    def is_functional(self) -> bool:
-        return _has_functorch and isinstance(
-            self.module,
-            (FunctionalModule, FunctionalModuleWithBuffers),
-        )
-
-    def _write_to_tensordict(
-        self,
-        tensordict: TensorDictBase,
-        tensors: list[Tensor],
-        tensordict_out: TensorDictBase | None = None,
-        out_keys: Iterable[NestedKey] | None = None,
-    ) -> TensorDictBase:
-        if out_keys is None:
-            out_keys = self.out_keys_source
-        if tensordict_out is None:
-            tensordict_out = tensordict
-        for _out_key, _tensor in zip(out_keys, tensors):
-            if _out_key != "_":
-                tensordict_out.set(_out_key, _tensor)
-        return tensordict_out
-
-    def _call_module(
-        self, tensors: Sequence[Tensor], **kwargs: Any
-    ) -> Tensor | Sequence[Tensor]:
-        out = self.module(*tensors, **kwargs)
-        return out
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        *args,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs: Any,
-    ) -> TensorDictBase:
-        """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
-        try:
-            if len(args):
-                tensordict_out = args[0]
-                args = args[1:]
-                # we will get rid of tensordict_out as a regular arg, because it
-                # blocks us when using vmap
-                # with stateful but functional modules: the functional module checks if
-                # it still contains parameters. If so it considers that only a "params" kwarg
-                # is indicative of what the params are, when we could potentially make a
-                # special rule for TensorDictModule that states that the second arg is
-                # likely to be the module params.
-                warnings.warn(
-                    "tensordict_out will be deprecated soon.",
-                    category=DeprecationWarning,
-                )
-            if len(args):
-                raise ValueError(
-                    "Got a non-empty list of extra agruments, when none was expected."
-                )
-            if self._kwargs is not None:
-                kwargs.update(
-                    {
-                        kwarg: tensordict.get(in_key, None)
-                        for kwarg, in_key in zip(self._kwargs, self.in_keys)
-                    }
-                )
-                tensors = ()
-            else:
-                tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
-            try:
-                tensors = self._call_module(tensors, **kwargs)
-            except Exception as err:
-                if any(tensor is None for tensor in tensors) and "None" in str(err):
-                    none_set = {
-                        key
-                        for key, tensor in zip(self.in_keys, tensors)
-                        if tensor is None
-                    }
-                    raise KeyError(
-                        "Some tensors that are necessary for the module call may "
-                        "not have not been found in the input tensordict: "
-                        f"the following inputs are None: {none_set}."
-                    ) from err
-                else:
-                    raise err
-            if isinstance(tensors, (dict, TensorDictBase)):
-                if isinstance(tensors, dict):
-                    keys = unravel_key_list(list(tensors.keys()))
-                    values = tensors.values()
-                    tensors = dict(zip(keys, values))
-                tensors = tuple(tensors.get(key, None) for key in self.out_keys)
-            if not isinstance(tensors, tuple):
-                tensors = (tensors,)
-            tensordict_out = self._write_to_tensordict(
-                tensordict, tensors, tensordict_out
-            )
-            return tensordict_out
-        except Exception as err:
-            module = self.module
-            if not isinstance(module, nn.Module):
-                try:
-                    import inspect
-
-                    module = inspect.getsource(module)
-                except OSError:
-                    # then we can't print the source code
-                    pass
-            module = indent(str(module), 4 * " ")
-            in_keys = indent(f"in_keys={self.in_keys}", 4 * " ")
-            out_keys = indent(f"out_keys={self.out_keys}", 4 * " ")
-            raise err from RuntimeError(
-                f"TensorDictModule failed with operation\n{module}\n{in_keys}\n{out_keys}."
-            )
-
-    @property
-    def device(self) -> torch.device:
-        for p in self.parameters():
-            return p.device
-        return torch.device("cpu")
-
-    def __repr__(self) -> str:
-        fields = indent(
-            f"module={self.module},\n"
-            f"device={self.device},\n"
-            f"in_keys={self.in_keys},\n"
-            f"out_keys={self.out_keys}",
-            4 * " ",
-        )
-
-        return f"{self.__class__.__name__}(\n{fields})"
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError as err1:
-            if not name.startswith("_"):
-                # no fallback for private attributes
-                try:
-                    return getattr(super().__getattr__("module"), name)
-                except Exception as err2:
-                    raise err2 from err1
-            raise
-
-    def __getstate__(self):
-        state = self.__dict__.copy()
-        if not isinstance(self.module, nn.Module):
-            state["module"] = cloudpickle_dumps(state["module"])
-        return state
-
-    def __setstate__(self, state):
-        if "module" in state:
-            state["module"] = cloudpickle_loads(state["module"])
-        self.__dict__ = state
-
-
-class TensorDictModuleWrapper(TensorDictModuleBase):
-    """Wrapper class for TensorDictModule objects.
-
-    Once created, a TensorDictModuleWrapper will behave exactly as the
-    TensorDictModule it contains except for the methods that are
-    overwritten.
-
-    Args:
-        td_module (TensorDictModule): operator to be wrapped.
-
-    """
-
-    def __init__(self, td_module: TensorDictModule) -> None:
-        super().__init__()
-        self.td_module = td_module
-        if len(self.td_module._forward_hooks):
-            for pre_hook in self.td_module._forward_hooks:
-                self.register_forward_hook(self.td_module._forward_hooks[pre_hook])
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError:
-            if name not in self.__dict__ and not name.startswith("__"):
-                return getattr(self._modules["td_module"], name)
-            else:
-                raise AttributeError(
-                    f"attribute {name} not recognised in {type(self).__name__}"
-                )
-
-    def forward(self, *args: Any, **kwargs: Any) -> TensorDictBase:
-        return self.td_module.forward(*args, **kwargs)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+import inspect
+import warnings
+from textwrap import indent
+from typing import Any, Callable, Dict, Iterable, List, Optional, Sequence, Tuple, Union
+
+import torch
+from cloudpickle import dumps as cloudpickle_dumps, loads as cloudpickle_loads
+
+from tensordict._td import is_tensor_collection, TensorDictBase
+from tensordict._tensordict import _unravel_key_to_tuple, unravel_key_list
+from tensordict.functional import make_tensordict
+from tensordict.nn.functional_modules import (
+    _swap_state,
+    extract_weights_and_buffers,
+    is_functional,
+    make_functional,
+    repopulate_module,
+)
+from tensordict.nn.utils import (
+    _auto_make_functional,
+    _dispatch_td_nn_modules,
+    set_skip_existing,
+)
+from tensordict.utils import implement_for, NestedKey
+from torch import nn, Tensor
+
+try:
+    from functorch import FunctionalModule, FunctionalModuleWithBuffers
+
+    _has_functorch = True
+except ImportError:
+    _has_functorch = False
+
+    class FunctionalModule:
+        pass
+
+    class FunctionalModuleWithBuffers:
+        pass
+
+
+__all__ = [
+    "TensorDictModule",
+    "TensorDictModuleWrapper",
+]
+
+
+class dispatch:
+    """Allows for a function expecting a TensorDict to be called using kwargs.
+
+    :func:`dispatch` must be used within modules that have an ``in_keys`` (or
+    another source of keys indicated by the ``source`` keyword argument) and
+    ``out_keys`` (or another ``dest`` key list) attributes indicating what keys
+    to be read and written from the tensordict. The wrapped function should
+    also have a ``tensordict`` leading argument.
+
+    The resulting function will return a single tensor (if there is a single
+    element in out_keys), otherwise it will return a tuple sorted as the ``out_keys``
+    of the module.
+
+    :func:`dispatch` can be used either as a method or as a class when extra arguments
+    need to be passed.
+
+    Args:
+        separator (str, optional): separator that combines sub-keys together
+            for ``in_keys`` that are tuples of strings.
+            Defaults to ``"_"``.
+        source (str or list of keys, optional): if a string is provided,
+            it points to the module attribute that contains the
+            list of input keys to be used. If a list is provided instead, it
+            will contain the keys used as input to the module.
+            Defaults to ``"in_keys"`` which is the attribute name of
+            :class:`~.TensorDictModule` list of input keys.
+        dest (str or list of keys, optional): if a string is provided,
+            it points to the module attribute that contains the
+            list of output keys to be used. If a list is provided instead, it
+            will contain the keys used as output to the module.
+            Defaults to ``"out_keys"`` which is the attribute name of
+            :class:`~.TensorDictModule` list of output keys.
+        auto_batch_size (bool, optional): if ``True``, the batch-size of the
+            input tensordict is determined automatically as the maximum number
+            of common dimensions across all the input tensors.
+            Defaults to ``True``.
+
+    Examples:
+        >>> class MyModule(nn.Module):
+        ...     in_keys = ["a"]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+        >>> # equivalently
+        >>> class MyModule(nn.Module):
+        ...     keys_in = ["a"]
+        ...     keys_out = ["b"]
+        ...
+        ...     @dispatch(source="keys_in", dest="keys_out")
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+        >>> # or this
+        >>> class MyModule(nn.Module):
+        ...     @dispatch(source=["a"], dest=["b"])
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModule()
+        >>> b = module(a=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+    :func:`dispatch_kwargs` will also work with nested keys with the default
+    ``"_"`` separator.
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c")]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(a_c=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+    If another separator is wanted, it can be indicated with the ``separator``
+    argument in the constructor:
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c")]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch(separator="sep")
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + 1
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(asepc=torch.zeros(1, 2))
+        >>> assert (b == 1).all()
+
+
+    Since the input keys is a sorted sequence of strings,
+    :func:`dispatch` can also be used with unnamed arguments where the order
+    must match the order of the input keys.
+
+    .. note::
+
+        If the first argument is a :class:`~.TensorDictBase` instance, it is
+        assumed that dispatch is __not__ being used and that this tensordict
+        contains all the necessary information to be run through the module.
+        In other words, one cannot decompose a tensordict with the first key
+        of the module inputs pointing to a tensordict instance.
+        In general, it is preferred to use :func:`dispatch` with tensordict
+        leaves only.
+
+    Examples:
+        >>> class MyModuleNest(nn.Module):
+        ...     in_keys = [("a", "c"), "d"]
+        ...     out_keys = ["b"]
+        ...
+        ...     @dispatch
+        ...     def forward(self, tensordict):
+        ...         tensordict['b'] = tensordict['a', 'c'] + tensordict["d"]
+        ...         return tensordict
+        ...
+        >>> module = MyModuleNest()
+        >>> b, = module(torch.zeros(1, 2), d=torch.ones(1, 2))  # works
+        >>> assert (b == 1).all()
+        >>> b, = module(torch.zeros(1, 2), torch.ones(1, 2))  # works
+        >>> assert (b == 1).all()
+        >>> try:
+        ...     b, = module(torch.zeros(1, 2), a_c=torch.ones(1, 2))  # fails
+        ... except:
+        ...     print("oopsy!")
+        ...
+
+    """
+
+    DEFAULT_SEPARATOR = "_"
+    DEFAULT_SOURCE = "in_keys"
+    DEFAULT_DEST = "out_keys"
+
+    def __new__(
+        cls,
+        separator=DEFAULT_SEPARATOR,
+        source=DEFAULT_SOURCE,
+        dest=DEFAULT_DEST,
+        auto_batch_size: bool = True,
+    ):
+        if callable(separator):
+            func = separator
+            separator = dispatch.DEFAULT_SEPARATOR
+            self = super().__new__(cls)
+            self.__init__(separator, source, dest)
+            return self.__call__(func)
+        return super().__new__(cls)
+
+    def __init__(
+        self,
+        separator=DEFAULT_SEPARATOR,
+        source=DEFAULT_SOURCE,
+        dest=DEFAULT_DEST,
+        auto_batch_size: bool = True,
+    ):
+        self.separator = separator
+        self.source = source
+        self.dest = dest
+        self.auto_batch_size = auto_batch_size
+
+    def __call__(self, func: Callable) -> Callable:
+        # sanity check
+        for i, key in enumerate(inspect.signature(func).parameters):
+            if i == 0:
+                # skip self
+                continue
+            if key != "tensordict":
+                raise RuntimeError(
+                    "the first argument of the wrapped function must be "
+                    "named 'tensordict'."
+                )
+            break
+        # if the env variable was used, we can skip the wrapper altogether
+        if not _dispatch_td_nn_modules():
+            return func
+
+        @functools.wraps(func)
+        def wrapper(_self, *args: Any, **kwargs: Any) -> Any:
+            if not _dispatch_td_nn_modules():
+                return func(_self, *args, **kwargs)
+
+            source = self.source
+            if isinstance(source, str):
+                source = getattr(_self, source)
+            tensordict = None
+            if len(args):
+                if is_tensor_collection(args[0]):
+                    tensordict, args = args[0], args[1:]
+            if tensordict is None:
+                tensordict_values = {}
+                dest = self.dest
+                if isinstance(dest, str):
+                    dest = getattr(_self, dest)
+                for key in source:
+                    expected_key = self.separator.join(_unravel_key_to_tuple(key))
+                    if len(args):
+                        tensordict_values[key] = args[0]
+                        args = args[1:]
+                        if expected_key in kwargs:
+                            raise RuntimeError(
+                                "Duplicated argument in args and kwargs."
+                            )
+                    elif expected_key in kwargs:
+                        try:
+                            tensordict_values[key] = kwargs.pop(expected_key)
+                        except KeyError:
+                            raise KeyError(
+                                f"The key {expected_key} wasn't found in the keyword arguments "
+                                f"but is expected to execute that function."
+                            )
+                tensordict = make_tensordict(
+                    tensordict_values,
+                    batch_size=torch.Size([]) if not self.auto_batch_size else None,
+                )
+                out = func(_self, tensordict, *args, **kwargs)
+                out = tuple(out[key] for key in dest)
+                return out[0] if len(out) == 1 else out
+            return func(_self, tensordict, *args, **kwargs)
+
+        return wrapper
+
+
+class _OutKeysSelect:
+    def __init__(self, out_keys):
+        self.out_keys = out_keys
+        self._initialized = False
+
+    def _init(self, module):
+        if self._initialized:
+            return
+        self._initialized = True
+        self.module = module
+        module.out_keys = list(self.out_keys)
+
+    @implement_for("torch", None, "2.0")
+    def __call__(  # noqa: F811
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        tensordict_out: TensorDictBase,
+    ):
+        if not isinstance(tensordict_out, TensorDictBase):
+            raise RuntimeError(
+                "You are likely using tensordict.nn.dispatch with keyword arguments with an older (< 2.0) version of pytorch. "
+                "This is currently not supported. Please use unnamed arguments or upgrade pytorch."
+            )
+        # detect dispatch calls
+        in_keys = module.in_keys
+        is_dispatched = self._detect_dispatch(tensordict_in, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    @implement_for("torch", "2.0", None)
+    def __call__(  # noqa: F811
+        self,
+        module: TensorDictModuleBase,
+        tensordict_in: TensorDictBase,
+        kwargs: Dict,
+        tensordict_out: TensorDictBase,
+    ):
+        # detect dispatch calls
+        in_keys = module.in_keys
+        if not tensordict_in and kwargs.get("tensordict", None) is not None:
+            tensordict_in = kwargs.pop("tensordict")
+        is_dispatched = self._detect_dispatch(tensordict_in, kwargs, in_keys)
+        out_keys = self.out_keys
+        # if dispatch filtered the out keys as they should we're happy
+        if is_dispatched:
+            if (not isinstance(tensordict_out, tuple) and len(out_keys) == 1) or (
+                len(out_keys) == len(tensordict_out)
+            ):
+                return tensordict_out
+        self._init(module)
+        if is_dispatched:
+            # it might be the case that dispatch was not aware of what the out-keys were.
+            if isinstance(tensordict_out, tuple):
+                out = tuple(
+                    item
+                    for i, item in enumerate(tensordict_out)
+                    if module._out_keys[i] in module.out_keys
+                )
+                if len(out) == 1:
+                    return out[0]
+                return out
+            elif module._out_keys[0] in module.out_keys and len(module._out_keys) == 1:
+                return tensordict_out
+            elif (
+                module._out_keys[0] not in module.out_keys
+                and len(module._out_keys) == 1
+            ):
+                return ()
+            else:
+                raise RuntimeError(
+                    f"Selecting out-keys failed. Original out_keys: {module._out_keys}, selected: {module.out_keys}."
+                )
+        if tensordict_out is tensordict_in:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+            )
+        else:
+            return tensordict_out.select(
+                *in_keys,
+                *out_keys,
+                inplace=True,
+                strict=False,
+            )
+
+    @implement_for("torch", None, "2.0")
+    def _detect_dispatch(self, tensordict_in, in_keys):  # noqa: F811
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys() for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in):
+                if isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
+    @implement_for("torch", "2.0", None)
+    def _detect_dispatch(self, tensordict_in, kwargs, in_keys):  # noqa: F811
+        if isinstance(tensordict_in, TensorDictBase) and all(
+            key in tensordict_in.keys(include_nested=True) for key in in_keys
+        ):
+            return False
+        elif isinstance(tensordict_in, tuple):
+            if len(tensordict_in) or len(kwargs):
+                if len(tensordict_in) and isinstance(tensordict_in[0], TensorDictBase):
+                    return self._detect_dispatch(tensordict_in[0], kwargs, in_keys)
+                elif (
+                    not len(tensordict_in)
+                    and len(kwargs)
+                    and isinstance(kwargs.get("tensordict", None), TensorDictBase)
+                ):
+                    return self._detect_dispatch(kwargs["tensordict"], in_keys)
+                return True
+            return not len(in_keys)
+        # not a TDBase: must be True
+        return True
+
+    def remove(self):
+        # reset ground truth
+        if self.module._out_keys is not None:
+            self.module.out_keys = self.module._out_keys
+
+    def __del__(self):
+        self.remove()
+
+
+class TensorDictModuleBase(nn.Module):
+    """Base class to TensorDict modules.
+
+    TensorDictModule subclasses are characterized by ``in_keys`` and ``out_keys``
+    key-lists that indicate what input entries are to be read and what output
+    entries should be expected to be written.
+
+    The forward method input/output signature should always follow the
+    convention:
+
+        >>> tensordict_out = module.forward(tensordict_in)
+
+    """
+
+    def __new__(cls, *args, **kwargs):
+        # check the out_keys and in_keys in the dict
+        if "in_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("in_keys"), property
+        ):
+            in_keys = cls.__dict__.get("in_keys")
+            # now let's remove it
+            delattr(cls, "in_keys")
+            cls._in_keys = unravel_key_list(in_keys)
+            cls.in_keys = TensorDictModuleBase.in_keys
+        if "out_keys" in cls.__dict__ and not isinstance(
+            cls.__dict__.get("out_keys"), property
+        ):
+            out_keys = cls.__dict__.get("out_keys")
+            # now let's remove it
+            delattr(cls, "out_keys")
+            out_keys = unravel_key_list(out_keys)
+            cls._out_keys = out_keys
+            cls._out_keys_apparent = out_keys
+            cls.out_keys = TensorDictModuleBase.out_keys
+        out = super().__new__(cls)
+        return out
+
+    @property
+    def in_keys(self):
+        return self._in_keys
+
+    @in_keys.setter
+    def in_keys(self, value: List[Union[str, Tuple[str]]]):
+        self._in_keys = unravel_key_list(value)
+
+    @property
+    def out_keys(self):
+        return self._out_keys_apparent
+
+    @property
+    def out_keys_source(self):
+        return self._out_keys
+
+    @out_keys.setter
+    def out_keys(self, value: List[Union[str, Tuple[str]]]):
+        # the first time out_keys are set, they are marked as ground truth
+        value = unravel_key_list(list(value))
+        if not hasattr(self, "_out_keys"):
+            self._out_keys = value
+        self._out_keys_apparent = value
+
+    @implement_for("torch", None, "2.0")
+    def select_out_keys(self, *out_keys):  # noqa: F811
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        out_keys = unravel_key_list(list(out_keys))
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
+        self.register_forward_hook(_OutKeysSelect(out_keys))
+        for hook in self._forward_hooks.values():
+            if isinstance(hook, _OutKeysSelect):
+                hook._init(self)
+        return self
+
+    @implement_for("torch", "2.0", None)
+    def select_out_keys(self, *out_keys):  # noqa: F811
+        """Selects the keys that will be found in the output tensordict.
+
+        This is useful whenever one wants to get rid of intermediate keys in a
+        complicated graph, or when the presence of these keys may trigger unexpected
+        behaviours.
+
+        The original ``out_keys`` can still be accessed via ``module.out_keys_source``.
+
+        Args:
+            *out_keys (a sequence of strings or tuples of strings): the
+                out_keys that should be found in the output tensordict.
+
+        Returns: the same module, modified in-place with updated ``out_keys``.
+
+        The simplest usage is with :class:`~.TensorDictModule`:
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This feature will also work with dispatched arguments:
+        Examples:
+            >>> mod(torch.zeros(()), torch.ones(()))
+            tensor(2.)
+
+        This change will occur in-place (ie the same module will be returned
+        with an updated list of out_keys). It can be reverted using the
+        :meth:`TensorDictModuleBase.reset_out_keys` method.
+
+        Examples:
+            >>> mod.reset_out_keys()
+            >>> mod(TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, []))
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        This will work with other classes too, such as Sequential:
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential
+            >>> seq = TensorDictSequential(
+            ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["y"]),
+            ...     TensorDictModule(lambda x: x+1, in_keys=["y"], out_keys=["z"]),
+            ... )
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    y: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> seq.select_out_keys("z")
+            >>> td = TensorDict({"x": torch.zeros(())}, [])
+            >>> seq(td)
+            TensorDict(
+                fields={
+                    x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    z: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+        """
+        out_keys = unravel_key_list(list(out_keys))
+        if len(out_keys) == 1:
+            if out_keys[0] not in self.out_keys:
+                err_msg = f"Can't select non existent key: {out_keys[0]}. "
+                if (
+                    out_keys[0]
+                    and isinstance(out_keys[0], (tuple, list))
+                    and out_keys[0][0] in self.out_keys
+                ):
+                    err_msg += f"Are you passing the keys in a list? Try unpacking as: `{', '.join(out_keys[0])}`"
+                raise ValueError(err_msg)
+        self.register_forward_hook(_OutKeysSelect(out_keys), with_kwargs=True)
+        for hook in self._forward_hooks.values():
+            if isinstance(hook, _OutKeysSelect):
+                hook._init(self)
+        return self
+
+    def reset_out_keys(self):
+        """Resets the ``out_keys`` attribute to its orignal value.
+
+        Returns: the same module, with its original ``out_keys`` values.
+
+        Examples:
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+            >>> import torch
+            >>> mod = TensorDictModule(lambda x, y: (x+2, y+2), in_keys=["a", "b"], out_keys=["c", "d"])
+            >>> mod.select_out_keys("d")
+            >>> td = TensorDict({"a": torch.zeros(()), "b": torch.ones(())}, [])
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+            >>> mod.reset_out_keys()
+            >>> mod(td)
+            TensorDict(
+                fields={
+                    a: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    b: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    c: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                    d: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+        """
+        for i, hook in list(self._forward_hooks.items()):
+            if isinstance(hook, _OutKeysSelect):
+                del self._forward_hooks[i]
+        return self
+
+    def reset_parameters_recursive(
+        self, parameters: Optional[TensorDictBase] = None
+    ) -> Optional[TensorDictBase]:
+        """Recursively reset the parameters of the module and its children.
+
+        Args:
+            parameters (TensorDict of parameters, optional): If set to None, the module will reset using self.parameters().
+                Otherwise, we will reset the parameters in the tensordict in-place. This is
+                useful for functional modules where the parameters are not stored in the module itself.
+
+        Returns:
+            A tensordict of the new parameters, only if parameters was not None.
+
+        Examples:
+            >>> from tensordict.nn import TensorDictModule
+            >>> from torch import nn
+            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
+            >>> old_param = net[0].weight.clone()
+            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
+            >>> module.reset_parameters()
+            >>> (old_param == net[0].weight).any()
+            tensor(False)
+
+        This method also supports functional parameter sampling:
+
+            >>> from tensordict import TensorDict
+            >>> from tensordict.nn import TensorDictModule
+            >>> from torch import nn
+            >>> net = nn.Sequential(nn.Linear(2,3), nn.ReLU())
+            >>> module = TensorDictModule(net, in_keys=['bork'], out_keys=['dork'])
+            >>> params = TensorDict.from_module(module)
+            >>> old_params = params.clone(recurse=True)
+            >>> module.reset_parameters(params)
+            >>> (old_params == params).any()
+            False
+        """
+        if parameters is None:
+            any_reset = self._reset_parameters(self)
+            if not any_reset:
+                warnings.warn(
+                    "reset_parameters_recursive was called without the parameters argument and did not find any parameters to reset"
+                )
+            return
+        elif parameters.ndim:
+            raise RuntimeError(
+                "reset_parameters_recursive does not support batched TensorDicts, ensure `batch_size` is empty and the parameters shape match their original shape."
+            )
+
+        sanitized_parameters = parameters.apply(
+            lambda x: x.detach().requires_grad_(), inplace=False
+        )
+
+        if _auto_make_functional() and not is_functional(self):
+            make_functional(self, keep_params=True)
+            is_stateless = self._is_stateless
+            if is_stateless:
+                repopulate_module(self, sanitized_parameters)
+            else:
+                old_params = _swap_state(
+                    self,
+                    sanitized_parameters,
+                    is_stateless=False,
+                    return_old_tensordict=True,
+                )
+
+            self._reset_parameters(self)
+
+            if is_stateless:
+                new_parameters = extract_weights_and_buffers(self)
+            else:
+                new_parameters = _swap_state(
+                    self, old_params, is_stateless=False, return_old_tensordict=True
+                )
+            return new_parameters
+        else:
+            with sanitized_parameters.to_module(self):
+                self._reset_parameters(self)
+            return sanitized_parameters
+
+    def _reset_parameters(self, module: nn.Module) -> bool:
+        any_reset = False
+        for child in module.children():
+            if isinstance(child, nn.Module):
+                any_reset |= self._reset_parameters(child)
+
+            if hasattr(child, "reset_parameters"):
+                child.reset_parameters()
+                any_reset |= True
+        return any_reset
+
+
+class TensorDictModule(TensorDictModuleBase):
+    """A TensorDictModule, is a python wrapper around a :obj:`nn.Module` that reads and writes to a TensorDict.
+
+    Args:
+        module (Callable): a callable, typically a :class:`torch.nn.Module`,
+            used to map the input to the output parameter space. Its forward method
+            can return a single tensor, a tuple of tensors or even a dictionary.
+            In the latter case, the output keys of the :class:`TensorDictModule`
+            will be used to populate the output tensordict (ie. the keys present
+            in ``out_keys`` should be present in the dictionary returned by the
+            ``module`` forward method).
+        in_keys (iterable of NestedKeys, Dict[NestedStr, str]): keys to be read
+            from input tensordict and passed to the module. If it
+            contains more than one element, the values will be passed in the
+            order given by the in_keys iterable.
+            If ``in_keys`` is a dictionary, its keys must correspond to the key
+            to be read in the tensordict and its values must match the name of
+            the keyword argument in the function signature.
+        out_keys (iterable of str): keys to be written to the input tensordict. The length of out_keys must match the
+            number of tensors returned by the embedded module. Using "_" as a key avoid writing tensor to output.
+
+    Embedding a neural network in a TensorDictModule only requires to specify the input
+    and output keys. TensorDictModule support functional and regular :obj:`nn.Module`
+    objects. In the functional case, the 'params' (and 'buffers') keyword argument must
+    be specified:
+
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> # one can wrap regular nn.Module
+        >>> module = TensorDictModule(nn.Transformer(128), in_keys=["input", "tgt"], out_keys=["out"])
+        >>> input = torch.ones(2, 3, 128)
+        >>> tgt = torch.zeros(2, 3, 128)
+        >>> data = TensorDict({"input": input, "tgt": tgt}, batch_size=[2, 3])
+        >>> data = module(data)
+        >>> print(data)
+        TensorDict(
+            fields={
+                input: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                out: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False),
+                tgt: Tensor(shape=torch.Size([2, 3, 128]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([2, 3]),
+            device=None,
+            is_shared=False)
+
+    We can also pass directly the tensors
+
+    Examples:
+        >>> out = module(input, tgt)
+        >>> assert out.shape == input.shape
+        >>> # we can also wrap regular functions
+        >>> module = TensorDictModule(lambda x: (x-1, x+1), in_keys=[("input", "x")], out_keys=[("output", "x-1"), ("output", "x+1")])
+        >>> module(TensorDict({("input", "x"): torch.zeros(())}, batch_size=[]))
+        TensorDict(
+            fields={
+                input: TensorDict(
+                    fields={
+                        x: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False),
+                output: TensorDict(
+                    fields={
+                        x+1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),
+                        x-1: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    We can use TensorDictModule to populate a tensordict:
+
+    Examples:
+        >>> module = TensorDictModule(lambda: torch.randn(3), in_keys=[], out_keys=["x"])
+        >>> print(module(TensorDict({}, batch_size=[])))
+        TensorDict(
+            fields={
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Another feature is passing a dictionary as input keys, to control the
+    dispatching of values to specific keyword arguments.
+
+    Examples:
+        >>> module = TensorDictModule(lambda x, *, y: x+y,
+        ...     in_keys={'1': 'x', '2': 'y'}, out_keys=['z'],
+        ...     )
+        >>> td = module(TensorDict({'1': torch.ones(()), '2': torch.ones(())*2}, []))
+        >>> td['z']
+        tensor(3.)
+
+    Functional calls to a tensordict module is easy:
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import TensorDictModule
+        >>> td = TensorDict({"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3,])
+        >>> module = torch.nn.GRUCell(4, 8)
+        >>> td_module = TensorDictModule(
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
+        ... )
+        >>> params = TensorDict.from_module(td_module)
+        >>> # functional API
+        >>> with params.to_module(td_module):
+        ...     td_functional = td_module(td.clone())
+        >>> print(td_functional)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    In the stateful case:
+        >>> module = torch.nn.GRUCell(4, 8)
+        >>> td_module = TensorDictModule(
+        ...    module=module, in_keys=["input", "hidden"], out_keys=["output"]
+        ... )
+        >>> td_stateful = td_module(td.clone())
+        >>> print(td_stateful)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    One can use a vmap operator to call the functional module.
+
+    Examples:
+        >>> from torch import vmap
+        >>> from tensordict.nn.functional_modules import extract_weights_and_buffers
+        >>> params = extract_weights_and_buffers(td_module)
+        >>> params_repeat = params.expand(4)
+        >>> print(params_repeat)
+        TensorDict(
+            fields={
+                module: TensorDict(
+                    fields={
+                        bias_hh: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        bias_ih: Tensor(shape=torch.Size([4, 24]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_hh: Tensor(shape=torch.Size([4, 24, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight_ih: Tensor(shape=torch.Size([4, 24, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([4]),
+            device=None,
+            is_shared=False)
+        >>> def func(td, params):
+        ...     with params.to_module(td_module):
+        ...         return td_module(td)
+        >>> td_vmap = vmap(func, (None, 0))(td.clone(), params_repeat)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    _IN_KEY_ERR = "in_keys must be of type list, str or tuples of str, or dict."
+    _OUT_KEY_ERR = "out_keys must be of type list, str or tuples of str."
+
+    def __init__(
+        self,
+        module: Callable,
+        in_keys: NestedKey | List[NestedKey] | Dict[NestedKey:str],
+        out_keys: NestedKey | List[NestedKey],
+    ) -> None:
+        super().__init__()
+
+        if isinstance(in_keys, dict):
+            # write the kwargs and create a list instead
+            _in_keys = []
+            self._kwargs = []
+            for key, value in in_keys.items():
+                self._kwargs.append(value)
+                _in_keys.append(key)
+            in_keys = _in_keys
+        else:
+            if isinstance(in_keys, (str, tuple)):
+                in_keys = [in_keys]
+            elif not isinstance(in_keys, list):
+                raise ValueError(self._IN_KEY_ERR)
+            self._kwargs = None
+
+        if isinstance(out_keys, (str, tuple)):
+            out_keys = [out_keys]
+        elif not isinstance(out_keys, list):
+            raise ValueError(self._OUT_KEY_ERR)
+        try:
+            in_keys = unravel_key_list(in_keys)
+        except Exception:
+            raise ValueError(self._IN_KEY_ERR)
+        try:
+            out_keys = unravel_key_list(out_keys)
+        except Exception:
+            raise ValueError(self._OUT_KEY_ERR)
+
+        if type(module) is type or not callable(module):
+            raise ValueError(
+                f"Module {module} if type {type(module)} is not callable. "
+                f"Typical accepted types are nn.Module or TensorDictModule."
+            )
+        self.out_keys = out_keys
+        self.in_keys = in_keys
+
+        if "_" in self.in_keys:
+            warnings.warn(
+                'key "_" is for ignoring output, it should not be used in input keys',
+                stacklevel=2,
+            )
+
+        self.module = module
+        if _auto_make_functional():
+            make_functional(self, keep_params=True, return_params=False)
+
+    @property
+    def is_functional(self) -> bool:
+        return _has_functorch and isinstance(
+            self.module,
+            (FunctionalModule, FunctionalModuleWithBuffers),
+        )
+
+    def _write_to_tensordict(
+        self,
+        tensordict: TensorDictBase,
+        tensors: list[Tensor],
+        tensordict_out: TensorDictBase | None = None,
+        out_keys: Iterable[NestedKey] | None = None,
+    ) -> TensorDictBase:
+        if out_keys is None:
+            out_keys = self.out_keys_source
+        if tensordict_out is None:
+            tensordict_out = tensordict
+        for _out_key, _tensor in zip(out_keys, tensors):
+            if _out_key != "_":
+                tensordict_out.set(_out_key, _tensor)
+        return tensordict_out
+
+    def _call_module(
+        self, tensors: Sequence[Tensor], **kwargs: Any
+    ) -> Tensor | Sequence[Tensor]:
+        out = self.module(*tensors, **kwargs)
+        return out
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        *args,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs: Any,
+    ) -> TensorDictBase:
+        """When the tensordict parameter is not set, kwargs are used to create an instance of TensorDict."""
+        try:
+            if len(args):
+                tensordict_out = args[0]
+                args = args[1:]
+                # we will get rid of tensordict_out as a regular arg, because it
+                # blocks us when using vmap
+                # with stateful but functional modules: the functional module checks if
+                # it still contains parameters. If so it considers that only a "params" kwarg
+                # is indicative of what the params are, when we could potentially make a
+                # special rule for TensorDictModule that states that the second arg is
+                # likely to be the module params.
+                warnings.warn(
+                    "tensordict_out will be deprecated soon.",
+                    category=DeprecationWarning,
+                )
+            if len(args):
+                raise ValueError(
+                    "Got a non-empty list of extra agruments, when none was expected."
+                )
+            if self._kwargs is not None:
+                kwargs.update(
+                    {
+                        kwarg: tensordict.get(in_key, None)
+                        for kwarg, in_key in zip(self._kwargs, self.in_keys)
+                    }
+                )
+                tensors = ()
+            else:
+                tensors = tuple(tensordict.get(in_key, None) for in_key in self.in_keys)
+            try:
+                tensors = self._call_module(tensors, **kwargs)
+            except Exception as err:
+                if any(tensor is None for tensor in tensors) and "None" in str(err):
+                    none_set = {
+                        key
+                        for key, tensor in zip(self.in_keys, tensors)
+                        if tensor is None
+                    }
+                    raise KeyError(
+                        "Some tensors that are necessary for the module call may "
+                        "not have not been found in the input tensordict: "
+                        f"the following inputs are None: {none_set}."
+                    ) from err
+                else:
+                    raise err
+            if isinstance(tensors, (dict, TensorDictBase)):
+                if isinstance(tensors, dict):
+                    keys = unravel_key_list(list(tensors.keys()))
+                    values = tensors.values()
+                    tensors = dict(zip(keys, values))
+                tensors = tuple(tensors.get(key, None) for key in self.out_keys)
+            if not isinstance(tensors, tuple):
+                tensors = (tensors,)
+            tensordict_out = self._write_to_tensordict(
+                tensordict, tensors, tensordict_out
+            )
+            return tensordict_out
+        except Exception as err:
+            module = self.module
+            if not isinstance(module, nn.Module):
+                try:
+                    import inspect
+
+                    module = inspect.getsource(module)
+                except OSError:
+                    # then we can't print the source code
+                    pass
+            module = indent(str(module), 4 * " ")
+            in_keys = indent(f"in_keys={self.in_keys}", 4 * " ")
+            out_keys = indent(f"out_keys={self.out_keys}", 4 * " ")
+            raise err from RuntimeError(
+                f"TensorDictModule failed with operation\n{module}\n{in_keys}\n{out_keys}."
+            )
+
+    @property
+    def device(self) -> torch.device:
+        for p in self.parameters():
+            return p.device
+        return torch.device("cpu")
+
+    def __repr__(self) -> str:
+        fields = indent(
+            f"module={self.module},\n"
+            f"device={self.device},\n"
+            f"in_keys={self.in_keys},\n"
+            f"out_keys={self.out_keys}",
+            4 * " ",
+        )
+
+        return f"{self.__class__.__name__}(\n{fields})"
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError as err1:
+            if not name.startswith("_"):
+                # no fallback for private attributes
+                try:
+                    return getattr(super().__getattr__("module"), name)
+                except Exception as err2:
+                    raise err2 from err1
+            raise
+
+    def __getstate__(self):
+        state = self.__dict__.copy()
+        if not isinstance(self.module, nn.Module):
+            state["module"] = cloudpickle_dumps(state["module"])
+        return state
+
+    def __setstate__(self, state):
+        if "module" in state:
+            state["module"] = cloudpickle_loads(state["module"])
+        self.__dict__ = state
+
+
+class TensorDictModuleWrapper(TensorDictModuleBase):
+    """Wrapper class for TensorDictModule objects.
+
+    Once created, a TensorDictModuleWrapper will behave exactly as the
+    TensorDictModule it contains except for the methods that are
+    overwritten.
+
+    Args:
+        td_module (TensorDictModule): operator to be wrapped.
+
+    """
+
+    def __init__(self, td_module: TensorDictModule) -> None:
+        super().__init__()
+        self.td_module = td_module
+        if len(self.td_module._forward_hooks):
+            for pre_hook in self.td_module._forward_hooks:
+                self.register_forward_hook(self.td_module._forward_hooks[pre_hook])
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError:
+            if name not in self.__dict__ and not name.startswith("__"):
+                return getattr(self._modules["td_module"], name)
+            else:
+                raise AttributeError(
+                    f"attribute {name} not recognised in {type(self).__name__}"
+                )
+
+    def forward(self, *args: Any, **kwargs: Any) -> TensorDictBase:
+        return self.td_module.forward(*args, **kwargs)
```

## tensordict/nn/ensemble.py

 * *Ordering differences only*

```diff
@@ -1,129 +1,129 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import warnings
-
-import torch
-from tensordict import LazyStackedTensorDict, TensorDict
-from tensordict.nn.common import TensorDictBase, TensorDictModuleBase
-
-from tensordict.nn.params import TensorDictParams
-
-
-class EnsembleModule(TensorDictModuleBase):
-    """Module that wraps a module and repeats it to form an ensemble.
-
-    Args:
-        module (nn.Module): The nn.module to duplicate and wrap.
-        num_copies (int): The number of copies of module to make.
-        parameter_init_function (Callable): A function that takes a module copy and initializes its parameters.
-        expand_input (bool): Whether to expand the input TensorDict to match the number of copies. This should be
-            True unless you are chaining ensemble modules together, e.g. EnsembleModule(cnn) -> EnsembleModule(mlp).
-            If False, EnsembleModule(mlp) will expected the previous module(s) to have already expanded the input.
-
-    Examples:
-        >>> import torch
-        >>> from torch import nn
-        >>> from tensordict.nn import TensorDictModule, EnsembleModule
-        >>> from tensordict import TensorDict
-        >>> net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))
-        >>> mod = TensorDictModule(net, in_keys=['a'], out_keys=['b'])
-        >>> ensemble = EnsembleModule(mod, num_copies=3)
-        >>> data = TensorDict({'a': torch.randn(10, 4)}, batch_size=[10])
-        >>> ensemble(data)
-        TensorDict(
-            fields={
-                a: Tensor(shape=torch.Size([3, 10, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                b: Tensor(shape=torch.Size([3, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3, 10]),
-            device=None,
-            is_shared=False)
-
-    To stack EnsembleModules together, we should be mindful of turning off `expand_input` from the second module and on.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict.nn import TensorDictModule, TensorDictSequential, EnsembleModule
-        >>> from tensordict import TensorDict
-        >>> module = TensorDictModule(torch.nn.Linear(2,3), in_keys=['bork'], out_keys=['dork'])
-        >>> next_module = TensorDictModule(torch.nn.Linear(3,1), in_keys=['dork'], out_keys=['spork'])
-        >>> e0 = EnsembleModule(module, num_copies=4, expand_input=True)
-        >>> e1 = EnsembleModule(next_module, num_copies=4, expand_input=False)
-        >>> seq = TensorDictSequential(e0, e1)
-        >>> data = TensorDict({'bork': torch.randn(5,2)}, batch_size=[5])
-        >>> seq(data)
-        TensorDict(
-            fields={
-                bork: Tensor(shape=torch.Size([4, 5, 2]), device=cpu, dtype=torch.float32, is_shared=False),
-                dork: Tensor(shape=torch.Size([4, 5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
-                spork: Tensor(shape=torch.Size([4, 5, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 5]),
-            device=None,
-            is_shared=False)
-    """
-
-    def __init__(
-        self,
-        module: TensorDictModuleBase,
-        num_copies: int,
-        expand_input: bool = True,
-    ):
-        super().__init__()
-        self.in_keys = module.in_keys
-        self.out_keys = module.out_keys
-        params_td = TensorDict.from_module(module).expand(num_copies).to_tensordict()
-
-        self.module = module
-        if expand_input:
-            self.vmapped_forward = torch.vmap(self._func_module_call, (None, 0))
-        else:
-            self.vmapped_forward = torch.vmap(self._func_module_call, 0)
-
-        self.reset_parameters_recursive(params_td)
-        self.params_td = TensorDictParams(params_td)
-
-    def _func_module_call(self, input, params):
-        with params.to_module(self.module):
-            return self.module(input)
-
-    def forward(self, tensordict: TensorDict) -> TensorDict:
-        return self.vmapped_forward(tensordict, self.params_td)
-
-    def reset_parameters_recursive(
-        self, parameters: TensorDictBase = None
-    ) -> TensorDictBase:
-        """Resets the parameters of all the copies of the module.
-
-        Args:
-            parameters (TensorDict): A TensorDict of parameters for self.module. The batch dimension(s) of the tensordict
-                denote the number of module copies to reset.
-
-        Returns:
-            A TensorDict of pointers to the reset parameters.
-        """
-        if parameters is None:
-            raise ValueError(
-                "Ensembles are functional and require passing a TensorDict of parameters to reset_parameters_recursive"
-            )
-        if parameters.ndim:
-            params_pointers = []
-            for params_copy in parameters.unbind(0):
-                self.reset_parameters_recursive(params_copy)
-                params_pointers.append(params_copy)
-            return LazyStackedTensorDict.lazy_stack(params_pointers, -1)
-        else:
-            # In case the user has added other neural networks to the EnsembleModule
-            # besides those in self.module
-            child_mods = [
-                mod
-                for name, mod in self.named_children()
-                if name != "module" and name != "ensemble_parameters"
-            ]
-            if child_mods:
-                warnings.warn(
-                    "EnsembleModule.reset_parameters_recursive() only resets parameters of self.module, but other parameters were detected. These parameters will not be reset."
-                )
-            # Reset all self.module descendant parameters
-            return self.module.reset_parameters_recursive(parameters)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import warnings
+
+import torch
+from tensordict import LazyStackedTensorDict, TensorDict
+from tensordict.nn.common import TensorDictBase, TensorDictModuleBase
+
+from tensordict.nn.params import TensorDictParams
+
+
+class EnsembleModule(TensorDictModuleBase):
+    """Module that wraps a module and repeats it to form an ensemble.
+
+    Args:
+        module (nn.Module): The nn.module to duplicate and wrap.
+        num_copies (int): The number of copies of module to make.
+        parameter_init_function (Callable): A function that takes a module copy and initializes its parameters.
+        expand_input (bool): Whether to expand the input TensorDict to match the number of copies. This should be
+            True unless you are chaining ensemble modules together, e.g. EnsembleModule(cnn) -> EnsembleModule(mlp).
+            If False, EnsembleModule(mlp) will expected the previous module(s) to have already expanded the input.
+
+    Examples:
+        >>> import torch
+        >>> from torch import nn
+        >>> from tensordict.nn import TensorDictModule, EnsembleModule
+        >>> from tensordict import TensorDict
+        >>> net = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))
+        >>> mod = TensorDictModule(net, in_keys=['a'], out_keys=['b'])
+        >>> ensemble = EnsembleModule(mod, num_copies=3)
+        >>> data = TensorDict({'a': torch.randn(10, 4)}, batch_size=[10])
+        >>> ensemble(data)
+        TensorDict(
+            fields={
+                a: Tensor(shape=torch.Size([3, 10, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                b: Tensor(shape=torch.Size([3, 10, 2]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3, 10]),
+            device=None,
+            is_shared=False)
+
+    To stack EnsembleModules together, we should be mindful of turning off `expand_input` from the second module and on.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict.nn import TensorDictModule, TensorDictSequential, EnsembleModule
+        >>> from tensordict import TensorDict
+        >>> module = TensorDictModule(torch.nn.Linear(2,3), in_keys=['bork'], out_keys=['dork'])
+        >>> next_module = TensorDictModule(torch.nn.Linear(3,1), in_keys=['dork'], out_keys=['spork'])
+        >>> e0 = EnsembleModule(module, num_copies=4, expand_input=True)
+        >>> e1 = EnsembleModule(next_module, num_copies=4, expand_input=False)
+        >>> seq = TensorDictSequential(e0, e1)
+        >>> data = TensorDict({'bork': torch.randn(5,2)}, batch_size=[5])
+        >>> seq(data)
+        TensorDict(
+            fields={
+                bork: Tensor(shape=torch.Size([4, 5, 2]), device=cpu, dtype=torch.float32, is_shared=False),
+                dork: Tensor(shape=torch.Size([4, 5, 3]), device=cpu, dtype=torch.float32, is_shared=False),
+                spork: Tensor(shape=torch.Size([4, 5, 1]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 5]),
+            device=None,
+            is_shared=False)
+    """
+
+    def __init__(
+        self,
+        module: TensorDictModuleBase,
+        num_copies: int,
+        expand_input: bool = True,
+    ):
+        super().__init__()
+        self.in_keys = module.in_keys
+        self.out_keys = module.out_keys
+        params_td = TensorDict.from_module(module).expand(num_copies).to_tensordict()
+
+        self.module = module
+        if expand_input:
+            self.vmapped_forward = torch.vmap(self._func_module_call, (None, 0))
+        else:
+            self.vmapped_forward = torch.vmap(self._func_module_call, 0)
+
+        self.reset_parameters_recursive(params_td)
+        self.params_td = TensorDictParams(params_td)
+
+    def _func_module_call(self, input, params):
+        with params.to_module(self.module):
+            return self.module(input)
+
+    def forward(self, tensordict: TensorDict) -> TensorDict:
+        return self.vmapped_forward(tensordict, self.params_td)
+
+    def reset_parameters_recursive(
+        self, parameters: TensorDictBase = None
+    ) -> TensorDictBase:
+        """Resets the parameters of all the copies of the module.
+
+        Args:
+            parameters (TensorDict): A TensorDict of parameters for self.module. The batch dimension(s) of the tensordict
+                denote the number of module copies to reset.
+
+        Returns:
+            A TensorDict of pointers to the reset parameters.
+        """
+        if parameters is None:
+            raise ValueError(
+                "Ensembles are functional and require passing a TensorDict of parameters to reset_parameters_recursive"
+            )
+        if parameters.ndim:
+            params_pointers = []
+            for params_copy in parameters.unbind(0):
+                self.reset_parameters_recursive(params_copy)
+                params_pointers.append(params_copy)
+            return LazyStackedTensorDict.lazy_stack(params_pointers, -1)
+        else:
+            # In case the user has added other neural networks to the EnsembleModule
+            # besides those in self.module
+            child_mods = [
+                mod
+                for name, mod in self.named_children()
+                if name != "module" and name != "ensemble_parameters"
+            ]
+            if child_mods:
+                warnings.warn(
+                    "EnsembleModule.reset_parameters_recursive() only resets parameters of self.module, but other parameters were detected. These parameters will not be reset."
+                )
+            # Reset all self.module descendant parameters
+            return self.module.reset_parameters_recursive(parameters)
```

## tensordict/nn/functional_modules.py

 * *Ordering differences only*

```diff
@@ -1,658 +1,658 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import inspect
-import re
-import types
-import warnings
-from copy import deepcopy
-from functools import wraps
-from typing import Any, Callable, Iterable
-
-import torch
-from tensordict._pytree import PYTREE_REGISTERED_LAZY_TDS, PYTREE_REGISTERED_TDS
-
-from tensordict._td import TensorDict
-from tensordict.base import _is_tensor_collection, TensorDictBase
-
-from tensordict.utils import implement_for
-from torch import nn
-from torch.utils._pytree import SUPPORTED_NODES
-
-try:
-    from torch.nn.modules.module import _global_parameter_registration_hooks
-except ImportError:
-    # old torch version, passing
-    pass
-
-__base__setattr__ = nn.Module.__setattr__
-
-
-@implement_for("torch", "2.0", None)
-def _register_params(self, name, param):
-    """A simplified version of register_param where checks are skipped."""
-    for hook in _global_parameter_registration_hooks.values():
-        output = hook(self, name, param)
-        if output is not None:
-            param = output
-    self._parameters[name] = param
-
-
-@implement_for("torch", None, "2.0")
-def _register_params(self, name, param):  # noqa: F811
-    self.register_parameter(name, param)
-
-
-def set_tensor(module: "torch.nn.Module", name: str, tensor: torch.Tensor) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module._parameters:
-        del module._parameters[name]  # type: ignore[assignment]
-    was_buffer = name in module._buffers
-    if was_buffer:
-        del module._buffers[name]
-    if isinstance(tensor, nn.Parameter):
-        module.__dict__.pop(name, None)
-        # module.register_parameter(name, tensor)
-        _register_params(module, name, tensor)
-    elif was_buffer and isinstance(tensor, Tensor):
-        module._buffers[name] = tensor
-    else:
-        module.__dict__[name] = tensor
-
-
-@implement_for("torch", "2.0", None)
-def set_tensor_dict(  # noqa: F811
-    module_dict, module, name: str, tensor: torch.Tensor
-) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module_dict["_parameters"]:
-        del module_dict["_parameters"][name]  # type: ignore[assignment]
-    was_buffer = name in module_dict["_buffers"]
-    if was_buffer:
-        del module_dict["_buffers"][name]
-    if isinstance(tensor, nn.Parameter):
-        module_dict.pop(name, None)
-        # module.register_parameter(name, tensor)
-        for hook in _global_parameter_registration_hooks.values():
-            output = hook(module, name, tensor)
-            if output is not None:
-                tensor = output
-        module_dict["_parameters"][name] = tensor
-    elif was_buffer and isinstance(tensor, Tensor):
-        module_dict["_buffers"][name] = tensor
-    else:
-        module_dict[name] = tensor
-
-
-@implement_for("torch", None, "2.0")
-def set_tensor_dict(  # noqa: F811
-    module_dict, module, name: str, tensor: torch.Tensor
-) -> None:
-    """Simplified version of torch.nn.utils._named_member_accessor."""
-    if name in module_dict["_parameters"]:
-        del module_dict["_parameters"][name]  # type: ignore[assignment]
-    was_buffer = name in module_dict["_buffers"]
-    if was_buffer:
-        del module_dict["_buffers"][name]
-    if isinstance(tensor, nn.Parameter):
-        module_dict.pop(name, None)
-        module.register_parameter(name, tensor)
-    elif was_buffer and isinstance(tensor, Tensor):
-        module_dict["_buffers"][name] = tensor
-    else:
-        module_dict[name] = tensor
-
-
-_RESET_OLD_TENSORDICT = True
-try:
-    import torch._functorch.vmap as vmap_src
-    from torch._functorch.vmap import (
-        _add_batch_dim,
-        _broadcast_to_and_flatten,
-        _get_name,
-        _remove_batch_dim,
-        _validate_and_get_batch_size,
-        Tensor,
-        tree_flatten,
-        tree_unflatten,
-    )
-
-    _has_functorch = True
-except ImportError:
-    try:
-        from functorch._src.vmap import (
-            _add_batch_dim,
-            _broadcast_to_and_flatten,
-            _get_name,
-            _remove_batch_dim,
-            _validate_and_get_batch_size,
-            Tensor,
-            tree_flatten,
-            tree_unflatten,
-        )
-
-        _has_functorch = True
-        import functorch._src.vmap as vmap_src
-    except ImportError:
-        _has_functorch = False
-
-
-class _exclude_td_from_pytree:
-    def __init__(self):
-        self.tdnodes = {}
-
-    def __enter__(self):
-        for tdtype in PYTREE_REGISTERED_TDS + PYTREE_REGISTERED_LAZY_TDS:
-            self.tdnodes[tdtype] = SUPPORTED_NODES.pop(tdtype)
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        for tdtype in PYTREE_REGISTERED_TDS + PYTREE_REGISTERED_LAZY_TDS:
-            SUPPORTED_NODES[tdtype] = self.tdnodes[tdtype]
-
-
-# Monkey-patch functorch, mainly for cases where a "isinstance(obj, Tensor) is invoked
-if _has_functorch:
-    # Monkey-patches
-
-    def _process_batched_inputs(
-        in_dims: int | tuple[int, ...], args: Any, func: Callable
-    ) -> tuple[Any, ...]:
-        if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
-            raise ValueError(
-                f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-expected `in_dims` to be int or a (potentially nested) tuple
-matching the structure of inputs, got: {type(in_dims)}."""
-            )
-        if len(args) == 0:
-            raise ValueError(
-                f"""vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add
-inputs, or you are trying to vmap over a function with no inputs.
-The latter is unsupported."""
-            )
-
-        # we want to escape TensorDicts as they take care of adding the batch dimension
-        with _exclude_td_from_pytree():
-            flat_args, args_spec = tree_flatten(args)
-            flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
-            if flat_in_dims is None:
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-    in_dims is not compatible with the structure of `inputs`.
-    in_dims has structure {tree_flatten(in_dims)[1]} but inputs
-    has structure {args_spec}."""
-                )
-
-        for i, (arg, in_dim) in enumerate(zip(flat_args, flat_in_dims)):
-            if not isinstance(in_dim, int) and in_dim is not None:
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for an input but in_dim must be either
-an integer dimension or None."""
-                )
-            if isinstance(in_dim, int) and not isinstance(
-                arg, (Tensor, TensorDictBase)
-            ):
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for an input but the input is of type
-{type(arg)}. We cannot vmap over non-Tensor arguments,
-please use None as the respective in_dim"""
-                )
-            if in_dim is not None and (in_dim < -arg.dim() or in_dim >= arg.dim()):
-                raise ValueError(
-                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
-Got in_dim={in_dim} for some input, but that input is a Tensor
-of dimensionality {arg.dim()} so expected in_dim to satisfy
--{arg.dim()} <= in_dim < {arg.dim()}."""
-                )
-            if in_dim is not None and in_dim < 0:
-                flat_in_dims[i] = in_dim % arg.dim()
-
-        return (
-            _validate_and_get_batch_size(flat_in_dims, flat_args),
-            flat_in_dims,
-            flat_args,
-            args_spec,
-        )
-
-    vmap_src._process_batched_inputs = _process_batched_inputs
-
-    def _create_batched_inputs(
-        flat_in_dims: list[int], flat_args: list[Any], vmap_level: int, args_spec
-    ) -> Any:
-        # See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
-        # If tensordict, we remove the dim at batch_size[in_dim] such that the TensorDict can accept
-        # the batched tensors. This will be added in _unwrap_batched
-
-        batched_inputs = []
-        for in_dim, arg in zip(flat_in_dims, flat_args):
-            if in_dim is None:
-                if isinstance(arg, TensorDictBase):
-                    # this may be a perf bottleneck and could benefit from caching
-                    # arg = cache(arg.clone)(False)
-                    arg = arg.clone(False)
-
-                batched_input = arg
-            else:
-                if isinstance(arg, TensorDictBase):
-                    batched_input = arg._add_batch_dim(
-                        in_dim=in_dim, vmap_level=vmap_level
-                    )
-                else:
-                    batched_input = _add_batch_dim(arg, in_dim, vmap_level)
-            batched_inputs.append(batched_input)
-        with _exclude_td_from_pytree():
-            return tree_unflatten(batched_inputs, args_spec)
-
-    vmap_src._create_batched_inputs = _create_batched_inputs
-
-    def _unwrap_batched(
-        batched_outputs: Any,
-        out_dims: int | tuple[int, ...],
-        vmap_level: int,
-        batch_size: int,
-        func: Callable,
-    ) -> Any:
-        with _exclude_td_from_pytree():
-            flat_batched_outputs, output_spec = tree_flatten(batched_outputs)
-
-        for out in flat_batched_outputs:
-            # Change here:
-            if isinstance(out, (TensorDictBase, torch.Tensor)):
-                continue
-            raise ValueError(
-                f"vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return "
-                f"Tensors, got type {type(out)} as a return."
-            )
-
-        def incompatible_error():
-            raise ValueError(
-                f"vmap({_get_name(func)}, ..., out_dims={out_dims})(<inputs>): "
-                f"out_dims is not compatible with the structure of `outputs`. "
-                f"out_dims has structure {tree_flatten(out_dims)[1]} but outputs "
-                f"has structure {output_spec}."
-            )
-
-        # Here:
-        if isinstance(batched_outputs, (TensorDictBase, torch.Tensor)):
-            # Some weird edge case requires us to spell out the following
-            # see test_out_dims_edge_case
-            if isinstance(out_dims, int):
-                flat_out_dims = [out_dims]
-            elif isinstance(out_dims, tuple) and len(out_dims) == 1:
-                flat_out_dims = out_dims
-                out_dims = out_dims[0]
-            else:
-                incompatible_error()
-        else:
-            flat_out_dims = _broadcast_to_and_flatten(out_dims, output_spec)
-            if flat_out_dims is None:
-                incompatible_error()
-        flat_outputs = []
-        for batched_output, out_dim in zip(flat_batched_outputs, flat_out_dims):
-            if not isinstance(batched_output, TensorDictBase):
-                out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
-            else:
-                out = batched_output._remove_batch_dim(
-                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
-                )
-            flat_outputs.append(out)
-        with _exclude_td_from_pytree():
-            return tree_unflatten(flat_outputs, output_spec)
-
-    vmap_src._unwrap_batched = _unwrap_batched
-
-
-# Tensordict-compatible Functional modules
-
-
-def _decorate_funs(
-    model: nn.Module,
-    make_stateless: bool,
-    funs_to_decorate: Iterable[str] | None = None,
-) -> None:
-    if funs_to_decorate is None:
-        funs_to_decorate = ["forward"]
-    _is_functional = model.__dict__.get("_functionalized", False)
-    if not _is_functional:
-        model.__dict__["_functionalized"] = True
-        model.__dict__["_decorated_funs"] = set()
-
-    for fun_to_decorate in funs_to_decorate:
-        if fun_to_decorate in model.__dict__["_decorated_funs"]:
-            continue
-        try:
-            setattr(
-                model,
-                fun_to_decorate,
-                types.MethodType(_make_decorator(model, fun_to_decorate), model),
-            )
-            model.__dict__["_decorated_funs"].add(fun_to_decorate)
-        except AttributeError:
-            continue
-    if not model.__dict__.get("_is_stateless", False):
-        model.__dict__["_is_stateless"] = make_stateless
-
-    for module in model.children():
-        # we decorate forward for the sub-modules
-        _decorate_funs(module, make_stateless=make_stateless)
-
-
-def extract_weights_and_buffers(
-    model: nn.Module,
-) -> TensorDict:
-    """Extracts the weights and buffers of a model in a tensordict, and adapts the modules to read those inputs."""
-    tensordict = {}
-    for name, param in list(model.named_parameters(recurse=False)):
-        setattr(model, name, None)
-        tensordict[name] = param
-
-    for name, param in list(model.named_buffers(recurse=False)):
-        setattr(model, name, None)
-        tensordict[name] = param
-
-    for name, module in model.named_children():
-        module_tensordict = extract_weights_and_buffers(module)
-        if module_tensordict is not None:
-            tensordict[name] = module_tensordict
-    model.__dict__["_is_stateless"] = True
-    return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
-
-
-# For bookkeeping: this function seems to have the same runtime but will not access
-# modules that don't have parameters if they're not registered as empty tensordicts
-# in the input. Hence they won't be turned as stateful, which could cause some bugs.
-def _swap_state(
-    model: nn.Module,
-    tensordict: TensorDict,
-    is_stateless: bool,
-    return_old_tensordict: bool = False,
-    old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
-) -> dict[str, torch.Tensor] | TensorDict | None:
-    __dict__ = model.__dict__
-    was_stateless = __dict__.get("_is_stateless", None)
-    if was_stateless is None:
-        raise Exception(f"{model}\nhas no stateless attribute.")
-    __dict__["_is_stateless"] = is_stateless
-    # return_old_tensordict = return_old_tensordict and not was_stateless
-    if old_tensordict is None:
-        old_tensordict_dict = old_tensordict = {}
-    else:
-        old_tensordict_dict = {}
-    for key, value in tensordict.items():
-        cls = value.__class__
-        if _is_tensor_collection(cls) or issubclass(cls, dict):
-            _old_value = old_tensordict.get(key, None)
-            _old_value = _swap_state(
-                __dict__["_modules"][key],
-                value,
-                is_stateless=is_stateless,
-                old_tensordict=_old_value,
-                return_old_tensordict=return_old_tensordict,
-            )
-            old_tensordict_dict[key] = _old_value
-        else:
-            _old_value = None
-            if return_old_tensordict:
-                _old_value = __dict__["_parameters"].get(key, None)
-                if _old_value is None:
-                    _old_value = __dict__["_buffers"].get(key, None)
-                if _old_value is None:
-                    _old_value = __dict__.get(key, None)
-                if _old_value is None:
-                    pass
-                    # _old_value = torch.zeros(*value.shape, 0)
-                old_tensordict_dict[key] = _old_value
-                # old_tensordict_dict[key] = _old_value
-            if model.__class__.__setattr__ is __base__setattr__:
-                set_tensor_dict(__dict__, model, key, value)
-            else:
-                setattr(model, key, value)
-    old_tensordict.update(old_tensordict_dict)
-    if was_stateless or not return_old_tensordict:
-        return old_tensordict
-    else:
-        return TensorDict(old_tensordict, [], _run_checks=False)
-
-
-# def _swap_state(
-#     model: nn.Module,
-#     tensordict: TensorDict,
-#     is_stateless: bool,
-#     return_old_tensordict: bool = False,
-#     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
-# ) -> dict[str, torch.Tensor] | TensorDict | None:
-#     __dict__ = model.__dict__
-#     was_stateless = __dict__.get("_is_stateless", None)
-#     if was_stateless is None:
-#         raise Exception(f"{model}\nhas no stateless attribute.")
-#     __dict__["_is_stateless"] = is_stateless
-#     # return_old_tensordict = return_old_tensordict and not was_stateless
-#     if old_tensordict is None:
-#         old_tensordict_dict = old_tensordict = {}
-#     else:
-#         old_tensordict_dict = {}
-#     # keys = set(tensordict.keys())
-#     children = set()
-#     # this loop ignores the memo from named children
-#     for key, child in __dict__["_modules"].items():  # model.named_children():
-#         children.add(key)
-#         value = tensordict.get(key, None)
-#         if value is None:
-#             # faster than get(key, Tensordict(...))
-#             value = {}
-#
-#         _old_value = old_tensordict.get(key, None)
-#         _old_value = _swap_state(
-#             child,
-#             value,
-#             return_old_tensordict=return_old_tensordict,
-#             old_tensordict=_old_value,
-#             is_stateless=is_stateless,
-#         )
-#         old_tensordict_dict[key] = _old_value
-#     for key in tensordict.keys():
-#         if key in children:
-#             continue
-#         value = tensordict.get(key)
-#         if return_old_tensordict:
-#             old_attr = __dict__["_parameters"].get(key, None)
-#             if old_attr is None:
-#                 old_attr = __dict__["_buffers"].get(key, None)
-#             if old_attr is None:
-#                 old_attr = __dict__.get(key, None)
-#             if old_attr is None:
-#                 old_attr = torch.zeros(*value.shape, 0)
-#             old_tensordict_dict[key] = old_attr
-#         # is_param = key in model.__dict__.get("_parameters")
-#         # if is_param:
-#         #     delattr(model, key)
-#         #     print(value)
-#         set_tensor_dict(__dict__, model, key, value)
-#     old_tensordict.update(old_tensordict_dict)
-#     if was_stateless or not return_old_tensordict:
-#         return old_tensordict
-#     else:
-#         return TensorDict(old_tensordict, [])
-
-
-def is_functional(module: nn.Module):
-    """Checks if :func:`make_functional` has been called on the module."""
-    return "_functionalized" in module.__dict__
-
-
-def make_functional(
-    module: nn.Module,
-    funs_to_decorate: Iterable[str] | None = None,
-    keep_params: bool = False,
-    return_params: bool = True,
-) -> TensorDict:
-    """Converts a nn.Module to a functional module in-place, and returns its params.
-
-    Args:
-        module (torch.nn.Module): module that is to be made functional.
-        funs_to_decorate (iterable of str, optional): each string must correspond
-            to a function belonging to module. For nested modules, the
-            :meth:`torch.nn.Module.forward` method will be decorated.
-            Defaults to ``"forward"``.
-        keep_params (bool, optional): if ``True``, the module will keep its
-            parameters. Defaults to ``False``.
-        return_params (bool, optional): if ``True``, the parameters will
-            be collected in a nested tensordict and returned. If ``False``,
-            the module will be made functional but still be stateful.
-
-    """
-    _is_stateless = module.__dict__.get("_is_stateless", False)
-    _decorate_funs(
-        module,
-        funs_to_decorate=funs_to_decorate,
-        make_stateless=not keep_params,
-    )
-    if return_params and not _is_stateless:
-        params = extract_weights_and_buffers(
-            module,
-        )
-        if keep_params:
-            repopulate_module(module, params)
-        return params.lock_()
-    elif return_params and _is_stateless:
-        raise RuntimeError(
-            "Calling make_functional with return_params=True on a functional, stateless module. "
-        )
-    elif not keep_params:
-        extract_weights_and_buffers(module)
-
-
-def get_functional(
-    module: nn.Module,
-    funs_to_decorate: Iterable[str] | None = None,
-) -> nn.Module:
-    """Converts a nn.Module to a functional module in-place, and returns a stateful version of this module that can be used in functional settings."""
-    params = make_functional(module, funs_to_decorate=funs_to_decorate)
-    out = deepcopy(module)
-    repopulate_module(module, params)
-    return out
-
-
-def _make_decorator(module: nn.Module, fun_name: str) -> Callable:
-    fun = getattr(module, fun_name)
-
-    from tensordict.nn.common import TensorDictModuleBase
-
-    @wraps(fun)
-    def new_fun(self, *args, **kwargs):
-        # 3 use cases: (1) params is the last arg, (2) params is in kwargs, (3) no params
-        _is_stateless = self.__dict__.get("_is_stateless", False)
-        params = kwargs.pop("params", None)
-
-        if isinstance(self, TensorDictModuleBase):
-            if (
-                params is None
-                and len(args) == 2
-                and all(_is_tensor_collection(item.__class__) for item in args)
-            ):
-                params = args[1]
-                args = args[:1]
-        elif (
-            len(args) and _is_tensor_collection(args[0].__class__)
-        ) or "tensordict" in kwargs:
-            warnings.warn(
-                "You are passing a tensordict/tensorclass instance to a module that "
-                "does not inherit from TensorDictModuleBase. This may lead to unexpected "
-                "behaviours with functional calls."
-            )
-        if _is_stateless or params is not None:
-            if params is None:
-                params = args[-1]
-                args = args[:-1]
-                # get the previous params, and tell the submodules not to look for params anymore
-            old_params = _assign_params(
-                self, params, make_stateless=False, return_old_tensordict=True
-            )
-            try:
-                out = getattr(type(self), fun_name)(self, *args, **kwargs)
-            finally:
-                # reset the previous params, and tell the submodules to look for params
-                _assign_params(
-                    self,
-                    old_params,
-                    make_stateless=_is_stateless,
-                    return_old_tensordict=False,
-                )
-            return out
-        else:
-            try:
-                return getattr(type(self), fun_name)(self, *args, **kwargs)
-            except TypeError as err:
-                pattern = r".*takes \d+ positional arguments but \d+ were given|got multiple values for argument"
-                pattern = re.compile(pattern)
-                if pattern.search(str(err)) and isinstance(args[-1], TensorDictBase):
-                    # this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)
-                    raise TypeError(
-                        "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. "
-                        "If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` "
-                        "syntax."
-                    ) from err
-                else:
-                    raise err
-
-    # we need to update the signature so that params can be the last positional arg
-    oldsig = inspect.signature(fun)
-    if "_forward_unimplemented" in fun.__name__:
-        raise AttributeError("_forward_unimplemented not supported")
-    # search if a VAR_POSITIONAL or VAR_KEYWORD is present
-    # if yes insert step parameter before it, else insert it in last position
-    params = list(oldsig.parameters.values())
-    for i, param in enumerate(params):
-        if param.kind == inspect.Parameter.KEYWORD_ONLY:
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-        if param.kind == inspect.Parameter.VAR_POSITIONAL:
-            out_type = inspect.Parameter.KEYWORD_ONLY
-            i = i + 1
-            break
-        if param.kind == inspect.Parameter.VAR_KEYWORD:
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-        if (
-            param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
-            and param.default is not inspect._empty
-        ):
-            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-            break
-    else:
-        out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
-        i = len(params)
-    # new parameter name is params or params_[_...] if params if already present
-    name = "params"
-    while name in oldsig.parameters:
-        name += "_"
-    newparam = inspect.Parameter(name, out_type, default=None)
-    params.insert(i, newparam)
-    # we can now build the signature for the wrapper function
-    sig = oldsig.replace(parameters=params)
-
-    new_fun.__signature__ = sig
-    return new_fun
-
-
-def _assign_params(
-    module: nn.Module,
-    params: TensorDict,
-    make_stateless: bool,
-    return_old_tensordict: bool,
-) -> TensorDict | None:
-    if params is not None:
-        return _swap_state(module, params, make_stateless, return_old_tensordict)
-
-    return None
-
-
-def repopulate_module(model: nn.Module, tensordict: TensorDict) -> nn.Module:
-    """Repopulates a module with its parameters, presented as a nested TensorDict."""
-    _swap_state(model, tensordict, is_stateless=False)
-    return model
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import inspect
+import re
+import types
+import warnings
+from copy import deepcopy
+from functools import wraps
+from typing import Any, Callable, Iterable
+
+import torch
+from tensordict._pytree import PYTREE_REGISTERED_LAZY_TDS, PYTREE_REGISTERED_TDS
+
+from tensordict._td import TensorDict
+from tensordict.base import _is_tensor_collection, TensorDictBase
+
+from tensordict.utils import implement_for
+from torch import nn
+from torch.utils._pytree import SUPPORTED_NODES
+
+try:
+    from torch.nn.modules.module import _global_parameter_registration_hooks
+except ImportError:
+    # old torch version, passing
+    pass
+
+__base__setattr__ = nn.Module.__setattr__
+
+
+@implement_for("torch", "2.0", None)
+def _register_params(self, name, param):
+    """A simplified version of register_param where checks are skipped."""
+    for hook in _global_parameter_registration_hooks.values():
+        output = hook(self, name, param)
+        if output is not None:
+            param = output
+    self._parameters[name] = param
+
+
+@implement_for("torch", None, "2.0")
+def _register_params(self, name, param):  # noqa: F811
+    self.register_parameter(name, param)
+
+
+def set_tensor(module: "torch.nn.Module", name: str, tensor: torch.Tensor) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module._parameters:
+        del module._parameters[name]  # type: ignore[assignment]
+    was_buffer = name in module._buffers
+    if was_buffer:
+        del module._buffers[name]
+    if isinstance(tensor, nn.Parameter):
+        module.__dict__.pop(name, None)
+        # module.register_parameter(name, tensor)
+        _register_params(module, name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
+        module._buffers[name] = tensor
+    else:
+        module.__dict__[name] = tensor
+
+
+@implement_for("torch", "2.0", None)
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        # module.register_parameter(name, tensor)
+        for hook in _global_parameter_registration_hooks.values():
+            output = hook(module, name, tensor)
+            if output is not None:
+                tensor = output
+        module_dict["_parameters"][name] = tensor
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+
+
+@implement_for("torch", None, "2.0")
+def set_tensor_dict(  # noqa: F811
+    module_dict, module, name: str, tensor: torch.Tensor
+) -> None:
+    """Simplified version of torch.nn.utils._named_member_accessor."""
+    if name in module_dict["_parameters"]:
+        del module_dict["_parameters"][name]  # type: ignore[assignment]
+    was_buffer = name in module_dict["_buffers"]
+    if was_buffer:
+        del module_dict["_buffers"][name]
+    if isinstance(tensor, nn.Parameter):
+        module_dict.pop(name, None)
+        module.register_parameter(name, tensor)
+    elif was_buffer and isinstance(tensor, Tensor):
+        module_dict["_buffers"][name] = tensor
+    else:
+        module_dict[name] = tensor
+
+
+_RESET_OLD_TENSORDICT = True
+try:
+    import torch._functorch.vmap as vmap_src
+    from torch._functorch.vmap import (
+        _add_batch_dim,
+        _broadcast_to_and_flatten,
+        _get_name,
+        _remove_batch_dim,
+        _validate_and_get_batch_size,
+        Tensor,
+        tree_flatten,
+        tree_unflatten,
+    )
+
+    _has_functorch = True
+except ImportError:
+    try:
+        from functorch._src.vmap import (
+            _add_batch_dim,
+            _broadcast_to_and_flatten,
+            _get_name,
+            _remove_batch_dim,
+            _validate_and_get_batch_size,
+            Tensor,
+            tree_flatten,
+            tree_unflatten,
+        )
+
+        _has_functorch = True
+        import functorch._src.vmap as vmap_src
+    except ImportError:
+        _has_functorch = False
+
+
+class _exclude_td_from_pytree:
+    def __init__(self):
+        self.tdnodes = {}
+
+    def __enter__(self):
+        for tdtype in PYTREE_REGISTERED_TDS + PYTREE_REGISTERED_LAZY_TDS:
+            self.tdnodes[tdtype] = SUPPORTED_NODES.pop(tdtype)
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        for tdtype in PYTREE_REGISTERED_TDS + PYTREE_REGISTERED_LAZY_TDS:
+            SUPPORTED_NODES[tdtype] = self.tdnodes[tdtype]
+
+
+# Monkey-patch functorch, mainly for cases where a "isinstance(obj, Tensor) is invoked
+if _has_functorch:
+    # Monkey-patches
+
+    def _process_batched_inputs(
+        in_dims: int | tuple[int, ...], args: Any, func: Callable
+    ) -> tuple[Any, ...]:
+        if not isinstance(in_dims, int) and not isinstance(in_dims, tuple):
+            raise ValueError(
+                f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+expected `in_dims` to be int or a (potentially nested) tuple
+matching the structure of inputs, got: {type(in_dims)}."""
+            )
+        if len(args) == 0:
+            raise ValueError(
+                f"""vmap({_get_name(func)})(<inputs>): got no inputs. Maybe you forgot to add
+inputs, or you are trying to vmap over a function with no inputs.
+The latter is unsupported."""
+            )
+
+        # we want to escape TensorDicts as they take care of adding the batch dimension
+        with _exclude_td_from_pytree():
+            flat_args, args_spec = tree_flatten(args)
+            flat_in_dims = _broadcast_to_and_flatten(in_dims, args_spec)
+            if flat_in_dims is None:
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+    in_dims is not compatible with the structure of `inputs`.
+    in_dims has structure {tree_flatten(in_dims)[1]} but inputs
+    has structure {args_spec}."""
+                )
+
+        for i, (arg, in_dim) in enumerate(zip(flat_args, flat_in_dims)):
+            if not isinstance(in_dim, int) and in_dim is not None:
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for an input but in_dim must be either
+an integer dimension or None."""
+                )
+            if isinstance(in_dim, int) and not isinstance(
+                arg, (Tensor, TensorDictBase)
+            ):
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for an input but the input is of type
+{type(arg)}. We cannot vmap over non-Tensor arguments,
+please use None as the respective in_dim"""
+                )
+            if in_dim is not None and (in_dim < -arg.dim() or in_dim >= arg.dim()):
+                raise ValueError(
+                    f"""vmap({_get_name(func)}, in_dims={in_dims}, ...)(<inputs>):
+Got in_dim={in_dim} for some input, but that input is a Tensor
+of dimensionality {arg.dim()} so expected in_dim to satisfy
+-{arg.dim()} <= in_dim < {arg.dim()}."""
+                )
+            if in_dim is not None and in_dim < 0:
+                flat_in_dims[i] = in_dim % arg.dim()
+
+        return (
+            _validate_and_get_batch_size(flat_in_dims, flat_args),
+            flat_in_dims,
+            flat_args,
+            args_spec,
+        )
+
+    vmap_src._process_batched_inputs = _process_batched_inputs
+
+    def _create_batched_inputs(
+        flat_in_dims: list[int], flat_args: list[Any], vmap_level: int, args_spec
+    ) -> Any:
+        # See NOTE [Ignored _remove_batch_dim, _add_batch_dim]
+        # If tensordict, we remove the dim at batch_size[in_dim] such that the TensorDict can accept
+        # the batched tensors. This will be added in _unwrap_batched
+
+        batched_inputs = []
+        for in_dim, arg in zip(flat_in_dims, flat_args):
+            if in_dim is None:
+                if isinstance(arg, TensorDictBase):
+                    # this may be a perf bottleneck and could benefit from caching
+                    # arg = cache(arg.clone)(False)
+                    arg = arg.clone(False)
+
+                batched_input = arg
+            else:
+                if isinstance(arg, TensorDictBase):
+                    batched_input = arg._add_batch_dim(
+                        in_dim=in_dim, vmap_level=vmap_level
+                    )
+                else:
+                    batched_input = _add_batch_dim(arg, in_dim, vmap_level)
+            batched_inputs.append(batched_input)
+        with _exclude_td_from_pytree():
+            return tree_unflatten(batched_inputs, args_spec)
+
+    vmap_src._create_batched_inputs = _create_batched_inputs
+
+    def _unwrap_batched(
+        batched_outputs: Any,
+        out_dims: int | tuple[int, ...],
+        vmap_level: int,
+        batch_size: int,
+        func: Callable,
+    ) -> Any:
+        with _exclude_td_from_pytree():
+            flat_batched_outputs, output_spec = tree_flatten(batched_outputs)
+
+        for out in flat_batched_outputs:
+            # Change here:
+            if isinstance(out, (TensorDictBase, torch.Tensor)):
+                continue
+            raise ValueError(
+                f"vmap({_get_name(func)}, ...): `{_get_name(func)}` must only return "
+                f"Tensors, got type {type(out)} as a return."
+            )
+
+        def incompatible_error():
+            raise ValueError(
+                f"vmap({_get_name(func)}, ..., out_dims={out_dims})(<inputs>): "
+                f"out_dims is not compatible with the structure of `outputs`. "
+                f"out_dims has structure {tree_flatten(out_dims)[1]} but outputs "
+                f"has structure {output_spec}."
+            )
+
+        # Here:
+        if isinstance(batched_outputs, (TensorDictBase, torch.Tensor)):
+            # Some weird edge case requires us to spell out the following
+            # see test_out_dims_edge_case
+            if isinstance(out_dims, int):
+                flat_out_dims = [out_dims]
+            elif isinstance(out_dims, tuple) and len(out_dims) == 1:
+                flat_out_dims = out_dims
+                out_dims = out_dims[0]
+            else:
+                incompatible_error()
+        else:
+            flat_out_dims = _broadcast_to_and_flatten(out_dims, output_spec)
+            if flat_out_dims is None:
+                incompatible_error()
+        flat_outputs = []
+        for batched_output, out_dim in zip(flat_batched_outputs, flat_out_dims):
+            if not isinstance(batched_output, TensorDictBase):
+                out = _remove_batch_dim(batched_output, vmap_level, batch_size, out_dim)
+            else:
+                out = batched_output._remove_batch_dim(
+                    vmap_level=vmap_level, batch_size=batch_size, out_dim=out_dim
+                )
+            flat_outputs.append(out)
+        with _exclude_td_from_pytree():
+            return tree_unflatten(flat_outputs, output_spec)
+
+    vmap_src._unwrap_batched = _unwrap_batched
+
+
+# Tensordict-compatible Functional modules
+
+
+def _decorate_funs(
+    model: nn.Module,
+    make_stateless: bool,
+    funs_to_decorate: Iterable[str] | None = None,
+) -> None:
+    if funs_to_decorate is None:
+        funs_to_decorate = ["forward"]
+    _is_functional = model.__dict__.get("_functionalized", False)
+    if not _is_functional:
+        model.__dict__["_functionalized"] = True
+        model.__dict__["_decorated_funs"] = set()
+
+    for fun_to_decorate in funs_to_decorate:
+        if fun_to_decorate in model.__dict__["_decorated_funs"]:
+            continue
+        try:
+            setattr(
+                model,
+                fun_to_decorate,
+                types.MethodType(_make_decorator(model, fun_to_decorate), model),
+            )
+            model.__dict__["_decorated_funs"].add(fun_to_decorate)
+        except AttributeError:
+            continue
+    if not model.__dict__.get("_is_stateless", False):
+        model.__dict__["_is_stateless"] = make_stateless
+
+    for module in model.children():
+        # we decorate forward for the sub-modules
+        _decorate_funs(module, make_stateless=make_stateless)
+
+
+def extract_weights_and_buffers(
+    model: nn.Module,
+) -> TensorDict:
+    """Extracts the weights and buffers of a model in a tensordict, and adapts the modules to read those inputs."""
+    tensordict = {}
+    for name, param in list(model.named_parameters(recurse=False)):
+        setattr(model, name, None)
+        tensordict[name] = param
+
+    for name, param in list(model.named_buffers(recurse=False)):
+        setattr(model, name, None)
+        tensordict[name] = param
+
+    for name, module in model.named_children():
+        module_tensordict = extract_weights_and_buffers(module)
+        if module_tensordict is not None:
+            tensordict[name] = module_tensordict
+    model.__dict__["_is_stateless"] = True
+    return TensorDict(tensordict, batch_size=torch.Size([]), _run_checks=False)
+
+
+# For bookkeeping: this function seems to have the same runtime but will not access
+# modules that don't have parameters if they're not registered as empty tensordicts
+# in the input. Hence they won't be turned as stateful, which could cause some bugs.
+def _swap_state(
+    model: nn.Module,
+    tensordict: TensorDict,
+    is_stateless: bool,
+    return_old_tensordict: bool = False,
+    old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
+) -> dict[str, torch.Tensor] | TensorDict | None:
+    __dict__ = model.__dict__
+    was_stateless = __dict__.get("_is_stateless", None)
+    if was_stateless is None:
+        raise Exception(f"{model}\nhas no stateless attribute.")
+    __dict__["_is_stateless"] = is_stateless
+    # return_old_tensordict = return_old_tensordict and not was_stateless
+    if old_tensordict is None:
+        old_tensordict_dict = old_tensordict = {}
+    else:
+        old_tensordict_dict = {}
+    for key, value in tensordict.items():
+        cls = value.__class__
+        if _is_tensor_collection(cls) or issubclass(cls, dict):
+            _old_value = old_tensordict.get(key, None)
+            _old_value = _swap_state(
+                __dict__["_modules"][key],
+                value,
+                is_stateless=is_stateless,
+                old_tensordict=_old_value,
+                return_old_tensordict=return_old_tensordict,
+            )
+            old_tensordict_dict[key] = _old_value
+        else:
+            _old_value = None
+            if return_old_tensordict:
+                _old_value = __dict__["_parameters"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__["_buffers"].get(key, None)
+                if _old_value is None:
+                    _old_value = __dict__.get(key, None)
+                if _old_value is None:
+                    pass
+                    # _old_value = torch.zeros(*value.shape, 0)
+                old_tensordict_dict[key] = _old_value
+                # old_tensordict_dict[key] = _old_value
+            if model.__class__.__setattr__ is __base__setattr__:
+                set_tensor_dict(__dict__, model, key, value)
+            else:
+                setattr(model, key, value)
+    old_tensordict.update(old_tensordict_dict)
+    if was_stateless or not return_old_tensordict:
+        return old_tensordict
+    else:
+        return TensorDict(old_tensordict, [], _run_checks=False)
+
+
+# def _swap_state(
+#     model: nn.Module,
+#     tensordict: TensorDict,
+#     is_stateless: bool,
+#     return_old_tensordict: bool = False,
+#     old_tensordict: dict[str, torch.Tensor] | TensorDict | None = None,
+# ) -> dict[str, torch.Tensor] | TensorDict | None:
+#     __dict__ = model.__dict__
+#     was_stateless = __dict__.get("_is_stateless", None)
+#     if was_stateless is None:
+#         raise Exception(f"{model}\nhas no stateless attribute.")
+#     __dict__["_is_stateless"] = is_stateless
+#     # return_old_tensordict = return_old_tensordict and not was_stateless
+#     if old_tensordict is None:
+#         old_tensordict_dict = old_tensordict = {}
+#     else:
+#         old_tensordict_dict = {}
+#     # keys = set(tensordict.keys())
+#     children = set()
+#     # this loop ignores the memo from named children
+#     for key, child in __dict__["_modules"].items():  # model.named_children():
+#         children.add(key)
+#         value = tensordict.get(key, None)
+#         if value is None:
+#             # faster than get(key, Tensordict(...))
+#             value = {}
+#
+#         _old_value = old_tensordict.get(key, None)
+#         _old_value = _swap_state(
+#             child,
+#             value,
+#             return_old_tensordict=return_old_tensordict,
+#             old_tensordict=_old_value,
+#             is_stateless=is_stateless,
+#         )
+#         old_tensordict_dict[key] = _old_value
+#     for key in tensordict.keys():
+#         if key in children:
+#             continue
+#         value = tensordict.get(key)
+#         if return_old_tensordict:
+#             old_attr = __dict__["_parameters"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__["_buffers"].get(key, None)
+#             if old_attr is None:
+#                 old_attr = __dict__.get(key, None)
+#             if old_attr is None:
+#                 old_attr = torch.zeros(*value.shape, 0)
+#             old_tensordict_dict[key] = old_attr
+#         # is_param = key in model.__dict__.get("_parameters")
+#         # if is_param:
+#         #     delattr(model, key)
+#         #     print(value)
+#         set_tensor_dict(__dict__, model, key, value)
+#     old_tensordict.update(old_tensordict_dict)
+#     if was_stateless or not return_old_tensordict:
+#         return old_tensordict
+#     else:
+#         return TensorDict(old_tensordict, [])
+
+
+def is_functional(module: nn.Module):
+    """Checks if :func:`make_functional` has been called on the module."""
+    return "_functionalized" in module.__dict__
+
+
+def make_functional(
+    module: nn.Module,
+    funs_to_decorate: Iterable[str] | None = None,
+    keep_params: bool = False,
+    return_params: bool = True,
+) -> TensorDict:
+    """Converts a nn.Module to a functional module in-place, and returns its params.
+
+    Args:
+        module (torch.nn.Module): module that is to be made functional.
+        funs_to_decorate (iterable of str, optional): each string must correspond
+            to a function belonging to module. For nested modules, the
+            :meth:`torch.nn.Module.forward` method will be decorated.
+            Defaults to ``"forward"``.
+        keep_params (bool, optional): if ``True``, the module will keep its
+            parameters. Defaults to ``False``.
+        return_params (bool, optional): if ``True``, the parameters will
+            be collected in a nested tensordict and returned. If ``False``,
+            the module will be made functional but still be stateful.
+
+    """
+    _is_stateless = module.__dict__.get("_is_stateless", False)
+    _decorate_funs(
+        module,
+        funs_to_decorate=funs_to_decorate,
+        make_stateless=not keep_params,
+    )
+    if return_params and not _is_stateless:
+        params = extract_weights_and_buffers(
+            module,
+        )
+        if keep_params:
+            repopulate_module(module, params)
+        return params.lock_()
+    elif return_params and _is_stateless:
+        raise RuntimeError(
+            "Calling make_functional with return_params=True on a functional, stateless module. "
+        )
+    elif not keep_params:
+        extract_weights_and_buffers(module)
+
+
+def get_functional(
+    module: nn.Module,
+    funs_to_decorate: Iterable[str] | None = None,
+) -> nn.Module:
+    """Converts a nn.Module to a functional module in-place, and returns a stateful version of this module that can be used in functional settings."""
+    params = make_functional(module, funs_to_decorate=funs_to_decorate)
+    out = deepcopy(module)
+    repopulate_module(module, params)
+    return out
+
+
+def _make_decorator(module: nn.Module, fun_name: str) -> Callable:
+    fun = getattr(module, fun_name)
+
+    from tensordict.nn.common import TensorDictModuleBase
+
+    @wraps(fun)
+    def new_fun(self, *args, **kwargs):
+        # 3 use cases: (1) params is the last arg, (2) params is in kwargs, (3) no params
+        _is_stateless = self.__dict__.get("_is_stateless", False)
+        params = kwargs.pop("params", None)
+
+        if isinstance(self, TensorDictModuleBase):
+            if (
+                params is None
+                and len(args) == 2
+                and all(_is_tensor_collection(item.__class__) for item in args)
+            ):
+                params = args[1]
+                args = args[:1]
+        elif (
+            len(args) and _is_tensor_collection(args[0].__class__)
+        ) or "tensordict" in kwargs:
+            warnings.warn(
+                "You are passing a tensordict/tensorclass instance to a module that "
+                "does not inherit from TensorDictModuleBase. This may lead to unexpected "
+                "behaviours with functional calls."
+            )
+        if _is_stateless or params is not None:
+            if params is None:
+                params = args[-1]
+                args = args[:-1]
+                # get the previous params, and tell the submodules not to look for params anymore
+            old_params = _assign_params(
+                self, params, make_stateless=False, return_old_tensordict=True
+            )
+            try:
+                out = getattr(type(self), fun_name)(self, *args, **kwargs)
+            finally:
+                # reset the previous params, and tell the submodules to look for params
+                _assign_params(
+                    self,
+                    old_params,
+                    make_stateless=_is_stateless,
+                    return_old_tensordict=False,
+                )
+            return out
+        else:
+            try:
+                return getattr(type(self), fun_name)(self, *args, **kwargs)
+            except TypeError as err:
+                pattern = r".*takes \d+ positional arguments but \d+ were given|got multiple values for argument"
+                pattern = re.compile(pattern)
+                if pattern.search(str(err)) and isinstance(args[-1], TensorDictBase):
+                    # this is raised whenever the module is an nn.Module (not a TensorDictModuleBase)
+                    raise TypeError(
+                        "It seems you tried to provide the parameters as an argument to the module when the module was not stateless. "
+                        "If this is the case, this error should vanish by providing the parameters using the ``module(..., params=params)`` "
+                        "syntax."
+                    ) from err
+                else:
+                    raise err
+
+    # we need to update the signature so that params can be the last positional arg
+    oldsig = inspect.signature(fun)
+    if "_forward_unimplemented" in fun.__name__:
+        raise AttributeError("_forward_unimplemented not supported")
+    # search if a VAR_POSITIONAL or VAR_KEYWORD is present
+    # if yes insert step parameter before it, else insert it in last position
+    params = list(oldsig.parameters.values())
+    for i, param in enumerate(params):
+        if param.kind == inspect.Parameter.KEYWORD_ONLY:
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+        if param.kind == inspect.Parameter.VAR_POSITIONAL:
+            out_type = inspect.Parameter.KEYWORD_ONLY
+            i = i + 1
+            break
+        if param.kind == inspect.Parameter.VAR_KEYWORD:
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+        if (
+            param.kind == inspect.Parameter.POSITIONAL_OR_KEYWORD
+            and param.default is not inspect._empty
+        ):
+            out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+            break
+    else:
+        out_type = inspect.Parameter.POSITIONAL_OR_KEYWORD
+        i = len(params)
+    # new parameter name is params or params_[_...] if params if already present
+    name = "params"
+    while name in oldsig.parameters:
+        name += "_"
+    newparam = inspect.Parameter(name, out_type, default=None)
+    params.insert(i, newparam)
+    # we can now build the signature for the wrapper function
+    sig = oldsig.replace(parameters=params)
+
+    new_fun.__signature__ = sig
+    return new_fun
+
+
+def _assign_params(
+    module: nn.Module,
+    params: TensorDict,
+    make_stateless: bool,
+    return_old_tensordict: bool,
+) -> TensorDict | None:
+    if params is not None:
+        return _swap_state(module, params, make_stateless, return_old_tensordict)
+
+    return None
+
+
+def repopulate_module(model: nn.Module, tensordict: TensorDict) -> nn.Module:
+    """Repopulates a module with its parameters, presented as a nested TensorDict."""
+    _swap_state(model, tensordict, is_stateless=False)
+    return model
```

## tensordict/nn/params.py

 * *Ordering differences only*

```diff
@@ -1,1282 +1,1282 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from __future__ import annotations
-
-import functools
-import inspect
-import numbers
-import re
-import weakref
-from copy import copy
-from functools import wraps
-from typing import Any, Callable, Iterator, OrderedDict, Sequence, Type
-
-import torch
-
-from tensordict._lazy import _CustomOpTensorDict, LazyStackedTensorDict
-from tensordict._td import _SubTensorDict, TensorDict
-from tensordict._torch_func import TD_HANDLED_FUNCTIONS
-
-from tensordict.base import (
-    _default_is_leaf,
-    _is_tensor_collection,
-    _register_tensor_class,
-    CompatibleType,
-    NO_DEFAULT,
-    T,
-    TensorDictBase,
-)
-
-from tensordict.memmap import MemoryMappedTensor
-from tensordict.utils import (
-    _LOCK_ERROR,
-    Buffer,
-    erase_cache,
-    IndexType,
-    lock_blocked,
-    NestedKey,
-)
-from torch import multiprocessing as mp, nn, Tensor
-from torch.utils._pytree import tree_map
-
-
-try:
-    from functorch import dim as ftdim
-
-    _has_funcdim = True
-except ImportError:
-    from tensordict.utils import _ftdim_mock as ftdim
-
-    _has_funcdim = False
-
-
-def _apply_leaves(data, fn):
-    if isinstance(data, TensorDict):
-        with data.unlock_():
-            for key, val in list(data.items()):
-                data._set_str(
-                    key,
-                    _apply_leaves(val, fn),
-                    validated=True,
-                    inplace=False,
-                    non_blocking=False,
-                )
-        return data
-    elif isinstance(data, LazyStackedTensorDict):
-        # this is currently not implemented as the registration of params will only work
-        # with plain TensorDict. The solution will be using pytree to get each independent
-        # leaf
-        raise RuntimeError(
-            "Using a LazyStackedTensorDict within a TensorDictParams isn't permitted."
-        )
-        # for _data in data.tensordicts:
-        #     _apply_leaves(_data, fn)
-        # return data
-    elif isinstance(data, _CustomOpTensorDict):
-        _apply_leaves(data._source, fn)
-        return data
-    elif isinstance(data, _SubTensorDict):
-        raise RuntimeError(
-            "Using a _SubTensorDict within a TensorDictParams isn't permitted."
-        )
-    else:
-        return fn(data)
-
-
-def _get_args_dict(func, args, kwargs):
-    signature = inspect.signature(func)
-    bound_arguments = signature.bind(*args, **kwargs)
-    bound_arguments.apply_defaults()
-
-    args_dict = dict(bound_arguments.arguments)
-    return args_dict
-
-
-def _maybe_make_param(tensor):
-    if (
-        isinstance(tensor, (Tensor, ftdim.Tensor))
-        and not isinstance(tensor, nn.Parameter)
-        and tensor.dtype in (torch.float, torch.double, torch.half)
-    ):
-        tensor = nn.Parameter(tensor)
-    return tensor
-
-
-def _maybe_make_param_or_buffer(tensor):
-    if (
-        isinstance(tensor, (Tensor, ftdim.Tensor))
-        and not isinstance(tensor, nn.Parameter)
-        and tensor.dtype in (torch.float, torch.double, torch.half)
-    ):
-        # convert all non-parameters to buffers
-        tensor = Buffer(tensor)
-    return tensor
-
-
-class _unlock_and_set:
-    # temporarily unlocks the nested tensordict to execute a function
-    def __new__(cls, *args, **kwargs):
-        if len(args) and callable(args[0]):
-            return cls(**kwargs)(args[0])
-        return super().__new__(cls)
-
-    def __init__(self, **only_for_kwargs):
-        self.only_for_kwargs = only_for_kwargs
-
-    def __call__(self, func):
-        name = func.__name__
-
-        @wraps(func)
-        def new_func(_self, *args, **kwargs):
-            if self.only_for_kwargs:
-                arg_dict = _get_args_dict(func, (_self, *args), kwargs)
-                for kwarg, exp_value in self.only_for_kwargs.items():
-                    cur_val = arg_dict.get(kwarg, NO_DEFAULT)
-                    if cur_val != exp_value:
-                        # escape
-                        meth = getattr(_self._param_td, name)
-                        out = meth(*args, **kwargs)
-                        return out
-            if not _self.no_convert:
-                args = tree_map(_maybe_make_param, args)
-                kwargs = tree_map(_maybe_make_param, kwargs)
-            else:
-                args = tree_map(_maybe_make_param_or_buffer, args)
-                kwargs = tree_map(_maybe_make_param_or_buffer, kwargs)
-            if _self.is_locked:
-                # if the root (TensorDictParams) is locked, we still want to raise an exception
-                raise RuntimeError(_LOCK_ERROR)
-            with _self._param_td.unlock_():
-                meth = getattr(_self._param_td, name)
-                out = meth(*args, **kwargs)
-            _self._reset_params()
-            if out is _self._param_td:
-                return _self
-            return out
-
-        return new_func
-
-
-def _get_post_hook(func):
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = func(self, *args, **kwargs)
-        return self._apply_get_post_hook(out)
-
-    return new_func
-
-
-def _fallback(func):
-    """Calls the method on the nested tensordict."""
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        if out is self._param_td:
-            # if the output does not change, return the wrapper
-            return self
-        return out
-
-    return new_func
-
-
-def _fallback_property(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self):
-        out = getattr(self._param_td, name)
-        if out is self._param_td:
-            return self
-        return out
-
-    def setter(self, value):
-        return getattr(type(self._param_td), name).fset(self._param_td, value)
-
-    return property(new_func, setter)
-
-
-def _replace(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        if out is self._param_td:
-            return self
-        self._param_td = out
-        return self
-
-    return new_func
-
-
-def _carry_over(func):
-    name = func.__name__
-
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        out = getattr(self._param_td, name)(*args, **kwargs)
-        if out is self._param_td:
-            return self
-        if not isinstance(out, TensorDictParams):
-            out = TensorDictParams(out, no_convert=True)
-            out.no_convert = self.no_convert
-        return out
-
-    return new_func
-
-
-def _apply_on_data(func):
-    @wraps(func)
-    def new_func(self, *args, **kwargs):
-        getattr(self.data, func.__name__)(*args, **kwargs)
-        return self
-
-    return new_func
-
-
-class TensorDictParams(TensorDictBase, nn.Module):
-    r"""Holds a TensorDictBase instance full of parameters.
-
-    This class exposes the contained parameters to a parent nn.Module
-    such that iterating over the parameters of the module also iterates over
-    the leaves of the tensordict.
-
-    Indexing works exactly as the indexing of the wrapped tensordict.
-    The parameter names will be registered within this module using :meth:`~.TensorDict.flatten_keys("_")`.
-    Therefore, the result of :meth:`~.named_parameters()` and the content of the
-    tensordict will differ slightly in term of key names.
-
-    Any operation that sets a tensor in the tensordict will be augmented by
-    a :class:`torch.nn.Parameter` conversion.
-
-    Args:
-        parameters (TensorDictBase): a tensordict to represent as parameters.
-            Values will be converted to parameters unless ``no_convert=True``.
-
-    Keyword Args:
-        no_convert (bool): if ``True``, no conversion to ``nn.Parameter`` will
-            occur at construction and after (unless the ``no_convert`` attribute is changed).
-            If ``no_convert`` is ``True`` and if non-parameters are present, they
-            will be registered as buffers.
-            Defaults to ``False``.
-        lock (bool): if ``True``, the tensordict hosted by TensorDictParams will
-            be locked. This can be useful to avoid unwanted modifications, but
-            also restricts the operations that can be done over the object (and
-            can have significant performance impact when `unlock_()` is required).
-            Defaults to ``False``.
-
-    Examples:
-        >>> from torch import nn
-        >>> from tensordict import TensorDict
-        >>> module = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 4))
-        >>> params = TensorDict.from_module(module)
-        >>> params.lock_()
-        >>> p = TensorDictParams(params)
-        >>> print(p)
-        TensorDictParams(params=TensorDict(
-            fields={
-                0: TensorDict(
-                    fields={
-                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight: Parameter(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False),
-                1: TensorDict(
-                    fields={
-                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                        weight: Parameter(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False))
-        >>> class CustomModule(nn.Module):
-        ...     def __init__(self, params):
-        ...         super().__init__()
-        ...         self.params = params
-        >>> m = CustomModule(p)
-        >>> # the wrapper supports assignment and values are turned in Parameter
-        >>> m.params['other'] = torch.randn(3)
-        >>> assert isinstance(m.params['other'], nn.Parameter)
-
-    """
-
-    def __init__(
-        self, parameters: TensorDictBase, *, no_convert=False, lock: bool = False
-    ):
-        super().__init__()
-        if isinstance(parameters, TensorDictParams):
-            parameters = parameters._param_td
-        self._param_td = parameters
-        self.no_convert = no_convert
-        if not no_convert:
-            func = _maybe_make_param
-        else:
-            func = _maybe_make_param_or_buffer
-        self._param_td = _apply_leaves(self._param_td, lambda x: func(x))
-        self._lock_content = lock
-        if lock:
-            self._param_td.lock_()
-        self._reset_params()
-        self._is_locked = False
-        self._locked_tensordicts = []
-        self.__last_op_queue = None
-        self._get_post_hook = []
-
-    def register_get_post_hook(self, hook):
-        """Register a hook to be called after any get operation on leaf tensors."""
-        if not callable(hook):
-            raise ValueError("Hooks must be callables.")
-        self._get_post_hook.append(hook)
-
-    def _apply_get_post_hook(self, val):
-        if not _is_tensor_collection(type(val)):
-            for hook in self._get_post_hook:
-                new_val = hook(self, val)
-                if new_val is not None:
-                    val = new_val
-        return val
-
-    def _reset_params(self):
-        parameters = self._param_td
-        param_keys = []
-        params = []
-        buffer_keys = []
-        buffers = []
-        for key, value in parameters.items(True, True):
-            # flatten key
-            if isinstance(key, tuple):
-                key = ".".join(key)
-            if isinstance(value, nn.Parameter):
-                param_keys.append(key)
-                params.append(value)
-            else:
-                buffer_keys.append(key)
-                buffers.append(value)
-        self.__dict__["_parameters"] = dict(zip(param_keys, params))
-        self.__dict__["_buffers"] = dict(zip(buffer_keys, buffers))
-
-    @classmethod
-    def __torch_function__(
-        cls,
-        func: Callable,
-        types: tuple[type, ...],
-        args: tuple[Any, ...] = (),
-        kwargs: dict[str, Any] | None = None,
-    ) -> Callable:
-        if kwargs is None:
-            kwargs = {}
-        if func not in TDPARAM_HANDLED_FUNCTIONS or not all(
-            issubclass(t, (Tensor, ftdim.Tensor, TensorDictBase)) for t in types
-        ):
-            return NotImplemented
-        return TDPARAM_HANDLED_FUNCTIONS[func](*args, **kwargs)
-
-    @classmethod
-    def _flatten_key(cls, key):
-        def make_valid_identifier(s):
-            # Replace invalid characters with underscores
-            s = re.sub(r"\W|^(?=\d)", "_", s)
-
-            # Ensure the string starts with a letter or underscore
-            if not s[0].isalpha() and s[0] != "_":
-                s = "_" + s
-
-            return s
-
-        key_flat = "_".join(key)
-        if not key_flat.isidentifier():
-            key_flat = make_valid_identifier(key_flat)
-        return key_flat
-
-    @lock_blocked
-    @_unlock_and_set
-    def __setitem__(
-        self,
-        index: IndexType,
-        value: TensorDictBase | dict | numbers.Number | CompatibleType,
-    ) -> None:
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def set(
-        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
-    ) -> TensorDictBase:
-        ...
-
-    @lock_blocked
-    def update(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
-        clone: bool = False,
-        inplace: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> TensorDictBase:
-        # Deprecating this since _set_tuple will do it thx to the decorator
-        # if not self.no_convert:
-        #     func = _maybe_make_param
-        # else:
-        #     func = _maybe_make_param_or_buffer
-        # if _is_tensor_collection(type(input_dict_or_td)):
-        #     input_dict_or_td = input_dict_or_td.apply(func)
-        # else:
-        #     input_dict_or_td = tree_map(func, input_dict_or_td)
-        with self._param_td.unlock_():
-            TensorDictBase.update(
-                self,
-                input_dict_or_td,
-                clone=clone,
-                inplace=inplace,
-                keys_to_update=keys_to_update,
-                non_blocking=non_blocking,
-            )
-            self._reset_params()
-        return self
-
-    @lock_blocked
-    @_unlock_and_set
-    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def popitem(self):
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def rename_key_(
-        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    def map(
-        self,
-        fn: Callable,
-        dim: int = 0,
-        num_workers: int = None,
-        chunksize: int = None,
-        num_chunks: int = None,
-        pool: mp.Pool = None,
-        generator: torch.Generator | None = None,
-        max_tasks_per_child: int | None = None,
-        worker_threads: int = 1,
-        mp_start_method: str | None = None,
-    ):
-        raise RuntimeError(
-            "Cannot call map on a TensorDictParams object. Convert it "
-            "to a detached tensordict first (through ``tensordict.data`` or ``tensordict.to_tensordict()``) and call "
-            "map in a second time."
-        )
-
-    @_unlock_and_set(inplace=True)
-    def apply(
-        self,
-        fn: Callable,
-        *others: TensorDictBase,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        default: Any = NO_DEFAULT,
-        filter_empty: bool | None = None,
-        call_on_nested: bool = False,
-        **constructor_kwargs,
-    ) -> TensorDictBase | None:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def named_apply(
-        self,
-        fn: Callable,
-        *others: TensorDictBase,
-        batch_size: Sequence[int] | None = None,
-        device: torch.device | None = NO_DEFAULT,
-        names: Sequence[str] | None = None,
-        inplace: bool = False,
-        default: Any = NO_DEFAULT,
-        filter_empty: bool | None = None,
-        call_on_nested: bool = False,
-        **constructor_kwargs,
-    ) -> TensorDictBase | None:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def _apply_nest(*args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def __getitem__(self, index: IndexType) -> TensorDictBase:
-        ...
-
-    __getitems__ = __getitem__
-
-    def to(self, *args, **kwargs) -> TensorDictBase:
-        params = self._param_td.to(*args, **kwargs)
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def cpu(self):
-        params = self._param_td.cpu()
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def cuda(self, device=None):
-        params = self._param_td.cuda(device=device)
-        if params is self._param_td:
-            return self
-        return TensorDictParams(params)
-
-    def _clone(self, recurse: bool = True) -> TensorDictBase:
-        """Clones the TensorDictParams.
-
-        .. warning::
-            The effect of this call is different from a regular torch.Tensor.clone call
-            in that it will create a TensorDictParams instance with a new copy of the
-            parameters and buffers __detached__ from the current graph. For a
-            regular clone (ie, cloning leaf parameters onto a new tensor that
-            is part of the graph), simply call
-
-                >>> params.apply(torch.clone)
-
-        .. note::
-            If a parameter is duplicated in the tree, ``clone`` will preserve this
-            identity (ie, parameter tying is preserved).
-
-        See :meth:`tensordict.TensorDictBase.clone` for more info on the clone
-        method.
-
-        """
-        if not recurse:
-            return TensorDictParams(self._param_td._clone(False), no_convert=True)
-
-        memo = {}
-
-        def _clone(tensor, memo=memo):
-            result = memo.get(tensor, None)
-            if result is not None:
-                return result
-
-            if isinstance(tensor, nn.Parameter):
-                result = nn.Parameter(
-                    tensor.data.clone(), requires_grad=tensor.requires_grad
-                )
-            else:
-                result = Buffer(tensor.data.clone(), requires_grad=tensor.requires_grad)
-            memo[tensor] = result
-            return result
-
-        return TensorDictParams(self._param_td.apply(_clone), no_convert=True)
-
-    @_fallback
-    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
-        ...
-
-    @_fallback
-    def _unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
-        ...
-
-    @_fallback
-    def to_tensordict(self):
-        ...
-
-    @_fallback
-    def to_h5(
-        self,
-        filename,
-        **kwargs,
-    ):
-        ...
-
-    def __hash__(self):
-        return hash((id(self), id(self.__dict__.get("_param_td", None))))
-
-    @_fallback
-    def __eq__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __ne__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __xor__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __or__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __ge__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __gt__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __le__(self, other: object) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def __lt__(self, other: object) -> TensorDictBase:
-        ...
-
-    def __getattr__(self, item: str) -> Any:
-        if not item.startswith("_"):
-            try:
-                return getattr(self.__dict__["_param_td"], item)
-            except AttributeError:
-                return super().__getattr__(item)
-        else:
-            return super().__getattr__(item)
-
-    @_fallback
-    def _change_batch_size(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _erase_names(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_str(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_tuple(self, *args, **kwargs):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_at_str(self, key, idx, default):
-        ...
-
-    @_get_post_hook
-    @_fallback
-    def _get_at_tuple(self, key, idx, default):
-        ...
-
-    @_fallback
-    def _add_batch_dim(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _convert_to_tensordict(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _get_names_idx(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _index_tensordict(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _remove_batch_dim(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _has_names(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _rename_subtds(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_at_str(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def _set_at_tuple(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_str(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _set_tuple(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _create_nested_str(self, *args, **kwargs):
-        ...
-
-    @_fallback_property
-    def batch_size(self) -> torch.Size:
-        ...
-
-    @_fallback
-    def contiguous(self, *args, **kwargs):
-        ...
-
-    @lock_blocked
-    @_unlock_and_set
-    def del_(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def detach_(self, *args, **kwargs):
-        ...
-
-    @_fallback_property
-    def device(self):
-        ...
-
-    @_fallback
-    def entry_class(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def is_contiguous(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def keys(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def masked_fill(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def masked_fill_(self, *args, **kwargs):
-        ...
-
-    def memmap_(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        num_threads: int = 0,
-    ) -> TensorDictBase:
-        raise RuntimeError(
-            "Cannot build a memmap TensorDict in-place. Use memmap or memmap_like instead."
-        )
-
-    _memmap_ = TensorDict._memmap_
-
-    _load_memmap = TensorDict._load_memmap
-
-    def make_memmap(
-        self,
-        key: NestedKey,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_storage(
-        self,
-        key: NestedKey,
-        storage: torch.UntypedStorage,
-        shape: torch.Size | torch.Tensor,
-        *,
-        dtype: torch.dtype | None = None,
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    def make_memmap_from_tensor(
-        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
-    ) -> MemoryMappedTensor:
-        raise RuntimeError(
-            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
-            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
-        )
-
-    @_fallback_property
-    def names(self):
-        ...
-
-    @_fallback
-    def pin_memory(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def _select(self, *args, **kwargs):
-        ...
-
-    @_fallback
-    def share_memory_(self, *args, **kwargs):
-        ...
-
-    @property
-    def is_locked(self) -> bool:
-        # Cannot be locked
-        return self._is_locked
-
-    @is_locked.setter
-    def is_locked(self, value):
-        self._is_locked = bool(value)
-
-    @_fallback_property
-    def is_shared(self) -> bool:
-        ...
-
-    @_fallback_property
-    def is_memmap(self) -> bool:
-        ...
-
-    @property
-    def _is_shared(self) -> bool:
-        return self._param_td._is_shared
-
-    @property
-    def _is_memmap(self) -> bool:
-        return self._param_td._is_memmap
-
-    @_fallback_property
-    def shape(self) -> torch.Size:
-        ...
-
-    def _propagate_lock(self, _lock_parents_weakrefs=None):
-        """Registers the parent tensordict that handles the lock."""
-        self._is_locked = True
-        if _lock_parents_weakrefs is None:
-            _lock_parents_weakrefs = []
-        self._lock_parents_weakrefs += _lock_parents_weakrefs
-        _lock_parents_weakrefs.append(weakref.ref(self))
-        # we don't want to double-lock the _param_td attrbute which is locked by default
-        if not self._param_td.is_locked:
-            self._param_td._propagate_lock(_lock_parents_weakrefs)
-
-    @erase_cache
-    def _propagate_unlock(self):
-        # if we end up here, we can clear the graph associated with this td
-        self._is_locked = False
-
-        if not self._lock_content:
-            return self._param_td._propagate_unlock()
-
-    unlock_ = TensorDict.unlock_
-    lock_ = TensorDict.lock_
-
-    @property
-    def data(self):
-        return self._param_td._data()
-
-    @property
-    def grad(self):
-        return self._param_td._grad()
-
-    @_unlock_and_set(inplace=True)
-    def flatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def unflatten_keys(
-        self, separator: str = ".", inplace: bool = False
-    ) -> TensorDictBase:
-        ...
-
-    @_unlock_and_set(inplace=True)
-    def _exclude(
-        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
-    ) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def from_dict_instance(
-        self, input_dict, batch_size=None, device=None, batch_dims=None
-    ):
-        ...
-
-    @_carry_over
-    def _legacy_transpose(self, dim0, dim1):
-        ...
-
-    @_fallback
-    def _transpose(self, dim0, dim1):
-        ...
-
-    @_fallback
-    def where(self, condition, other, *, out=None, pad=None):
-        ...
-
-    @_fallback
-    def _permute(
-        self,
-        *dims_list: int,
-        dims: list[int] | None = None,
-    ) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def _legacy_permute(
-        self,
-        *dims_list: int,
-        dims: list[int] | None = None,
-    ) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def _squeeze(self, dim: int | None = None) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def _legacy_squeeze(self, dim: int | None = None) -> TensorDictBase:
-        ...
-
-    @_fallback
-    def _unsqueeze(self, dim: int) -> TensorDictBase:
-        ...
-
-    @_carry_over
-    def _legacy_unsqueeze(self, dim: int) -> TensorDictBase:
-        ...
-
-    _check_device = TensorDict._check_device
-    _check_is_shared = TensorDict._check_is_shared
-
-    @_fallback
-    def _cast_reduction(self, **kwargs):
-        ...
-
-    @_fallback
-    def all(self, dim: int = None) -> bool | TensorDictBase:
-        ...
-
-    @_fallback
-    def any(self, dim: int = None) -> bool | TensorDictBase:
-        ...
-
-    @_fallback
-    def expand(self, *args, **kwargs) -> T:
-        ...
-
-    @_fallback
-    def masked_select(self, mask: Tensor) -> T:
-        ...
-
-    @_fallback
-    def memmap_like(
-        self,
-        prefix: str | None = None,
-        copy_existing: bool = False,
-        num_threads: int = 0,
-    ) -> T:
-        ...
-
-    @_fallback
-    def reshape(self, *shape: int):
-        ...
-
-    @_fallback
-    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
-        ...
-
-    @_fallback
-    def _to_module(
-        self,
-        module,
-        *,
-        inplace: bool = False,
-        return_swap: bool = True,
-        swap_dest=None,
-        memo=None,
-        use_state_dict: bool = False,
-        non_blocking: bool = False,
-    ):
-        ...
-
-    @_fallback
-    def _view(self, *args, **kwargs):
-        ...
-
-    @_carry_over
-    def _legacy_view(self, *args, **kwargs):
-        ...
-
-    @_unlock_and_set
-    def create_nested(self, key):
-        ...
-
-    def __repr__(self):
-        return f"TensorDictParams(params={self._param_td})"
-
-    def values(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> Iterator[CompatibleType]:
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-        for v in self._param_td.values(include_nested, leaves_only):
-            if not is_leaf(type(v)):
-                yield v
-                continue
-            yield self._apply_get_post_hook(v)
-
-    def state_dict(
-        self, *args, destination=None, prefix="", keep_vars=False, flatten=True
-    ):
-        # flatten must be True by default to comply with module's state-dict API
-        # since we want all params to be visible at root
-        return self._param_td.state_dict(
-            destination=destination,
-            prefix=prefix,
-            keep_vars=keep_vars,
-            flatten=flatten,
-        )
-
-    def load_state_dict(
-        self, state_dict: OrderedDict[str, Any], strict=True, assign=False
-    ):
-        # The state-dict is presumably the result of a call to TensorDictParams.state_dict
-        # but can't be sure.
-
-        state_dict_tensors = {}
-        state_dict = dict(state_dict)
-        for k, v in list(state_dict.items()):
-            if isinstance(v, torch.Tensor):
-                del state_dict[k]
-                state_dict_tensors[k] = v
-        state_dict_tensors = dict(
-            TensorDict(state_dict_tensors, []).unflatten_keys(".")
-        )
-        state_dict.update(state_dict_tensors)
-        self.data.load_state_dict(state_dict, strict=True, assign=False)
-        return self
-
-    def _load_from_state_dict(
-        self,
-        state_dict,
-        prefix,
-        local_metadata,
-        strict,
-        missing_keys,
-        unexpected_keys,
-        error_msgs,
-    ):
-        data = TensorDict(
-            {
-                key: val
-                for key, val in state_dict.items()
-                if key.startswith(prefix) and val is not None
-            },
-            [],
-        ).unflatten_keys(".")
-        prefix = tuple(key for key in prefix.split(".") if key)
-        if prefix:
-            data = data.get(prefix)
-        self.data.load_state_dict(data)
-
-    def items(
-        self,
-        include_nested: bool = False,
-        leaves_only: bool = False,
-        is_leaf: Callable[[Type], bool] | None = None,
-    ) -> Iterator[CompatibleType]:
-        if is_leaf is None:
-            is_leaf = _default_is_leaf
-        for k, v in self._param_td.items(include_nested, leaves_only):
-            if not is_leaf(type(v)):
-                yield k, v
-                continue
-            yield k, self._apply_get_post_hook(v)
-
-    @_apply_on_data
-    def zero_(self) -> T:
-        ...
-
-    @_apply_on_data
-    def fill_(self, key: NestedKey, value: float | bool) -> T:
-        ...
-
-    @_apply_on_data
-    def copy_(self, tensordict: T, non_blocking: bool = None) -> T:
-        ...
-
-    @_apply_on_data
-    def set_at_(self, key: NestedKey, value: CompatibleType, index: IndexType) -> T:
-        ...
-
-    @_apply_on_data
-    def set_(
-        self,
-        key: NestedKey,
-        item: CompatibleType,
-    ) -> T:
-        ...
-
-    @_apply_on_data
-    def _stack_onto_(
-        self,
-        list_item: list[CompatibleType],
-        dim: int,
-    ) -> T:
-        ...
-
-    @_apply_on_data
-    def _stack_onto_at_(
-        self,
-        key: NestedKey,
-        list_item: list[CompatibleType],
-        dim: int,
-        idx: IndexType,
-    ) -> T:
-        ...
-
-    @_apply_on_data
-    def update_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        ...
-
-    @_apply_on_data
-    def update_at_(
-        self,
-        input_dict_or_td: dict[str, CompatibleType] | T,
-        idx: IndexType,
-        clone: bool = False,
-        *,
-        non_blocking: bool = False,
-        keys_to_update: Sequence[NestedKey] | None = None,
-    ) -> T:
-        ...
-
-    @_apply_on_data
-    def apply_(self, fn: Callable, *others, **kwargs) -> T:
-        ...
-
-    def _apply(self, fn, recurse=True):
-        """Modifies torch.nn.Module._apply to work with Buffer class."""
-        if recurse:
-            for module in self.children():
-                module._apply(fn)
-
-        def compute_should_use_set_data(tensor, tensor_applied):
-            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
-                # If the new tensor has compatible tensor type as the existing tensor,
-                # the current behavior is to change the tensor in-place using `.data =`,
-                # and the future behavior is to overwrite the existing tensor. However,
-                # changing the current behavior is a BC-breaking change, and we want it
-                # to happen in future releases. So for now we introduce the
-                # `torch.__future__.get_overwrite_module_params_on_conversion()`
-                # global flag to let the user control whether they want the future
-                # behavior of overwriting the existing tensor or not.
-                return not torch.__future__.get_overwrite_module_params_on_conversion()
-            else:
-                return False
-
-        for key, param in self._parameters.items():
-            if param is None:
-                continue
-            # Tensors stored in modules are graph leaves, and we don't want to
-            # track autograd history of `param_applied`, so we have to use
-            # `with torch.no_grad():`
-            with torch.no_grad():
-                param_applied = fn(param)
-            should_use_set_data = compute_should_use_set_data(param, param_applied)
-            if should_use_set_data:
-                param.data = param_applied
-                out_param = param
-            else:
-                out_param = nn.Parameter(param_applied, param.requires_grad)
-                self._parameters[key] = out_param
-
-            if param.grad is not None:
-                with torch.no_grad():
-                    grad_applied = fn(param.grad)
-                should_use_set_data = compute_should_use_set_data(
-                    param.grad, grad_applied
-                )
-                if should_use_set_data:
-                    out_param.grad.data = grad_applied
-                else:
-                    out_param.grad = grad_applied.requires_grad_(
-                        param.grad.requires_grad
-                    )
-
-        for key, buffer in self._buffers.items():
-            if buffer is None:
-                continue
-            # Tensors stored in modules are graph leaves, and we don't want to
-            # track autograd history of `buffer_applied`, so we have to use
-            # `with torch.no_grad():`
-            with torch.no_grad():
-                buffer_applied = fn(buffer)
-            should_use_set_data = compute_should_use_set_data(buffer, buffer_applied)
-            if should_use_set_data:
-                buffer.data = buffer_applied
-                out_buffer = buffer
-            else:
-                out_buffer = Buffer(buffer_applied, buffer.requires_grad)
-                self._buffers[key] = out_buffer
-
-            if buffer.grad is not None:
-                with torch.no_grad():
-                    grad_applied = fn(buffer.grad)
-                should_use_set_data = compute_should_use_set_data(
-                    buffer.grad, grad_applied
-                )
-                if should_use_set_data:
-                    out_buffer.grad.data = grad_applied
-                else:
-                    out_buffer.grad = grad_applied.requires_grad_(
-                        buffer.grad.requires_grad
-                    )
-
-        return self
-
-
-TDPARAM_HANDLED_FUNCTIONS = copy(TD_HANDLED_FUNCTIONS)
-
-
-def implements_for_tdparam(torch_function: Callable) -> Callable[[Callable], Callable]:
-    """Register a torch function override for TensorDictParams."""
-
-    @functools.wraps(torch_function)
-    def decorator(func: Callable) -> Callable:
-        TDPARAM_HANDLED_FUNCTIONS[torch_function] = func
-        return func
-
-    return decorator
-
-
-@implements_for_tdparam(torch.empty_like)
-def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
-    return td.apply(
-        lambda x: torch.empty_like(x, *args, **kwargs),
-        device=kwargs.pop("device", NO_DEFAULT),
-    )
-
-
-_register_tensor_class(TensorDictParams)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from __future__ import annotations
+
+import functools
+import inspect
+import numbers
+import re
+import weakref
+from copy import copy
+from functools import wraps
+from typing import Any, Callable, Iterator, OrderedDict, Sequence, Type
+
+import torch
+
+from tensordict._lazy import _CustomOpTensorDict, LazyStackedTensorDict
+from tensordict._td import _SubTensorDict, TensorDict
+from tensordict._torch_func import TD_HANDLED_FUNCTIONS
+
+from tensordict.base import (
+    _default_is_leaf,
+    _is_tensor_collection,
+    _register_tensor_class,
+    CompatibleType,
+    NO_DEFAULT,
+    T,
+    TensorDictBase,
+)
+
+from tensordict.memmap import MemoryMappedTensor
+from tensordict.utils import (
+    _LOCK_ERROR,
+    Buffer,
+    erase_cache,
+    IndexType,
+    lock_blocked,
+    NestedKey,
+)
+from torch import multiprocessing as mp, nn, Tensor
+from torch.utils._pytree import tree_map
+
+
+try:
+    from functorch import dim as ftdim
+
+    _has_funcdim = True
+except ImportError:
+    from tensordict.utils import _ftdim_mock as ftdim
+
+    _has_funcdim = False
+
+
+def _apply_leaves(data, fn):
+    if isinstance(data, TensorDict):
+        with data.unlock_():
+            for key, val in list(data.items()):
+                data._set_str(
+                    key,
+                    _apply_leaves(val, fn),
+                    validated=True,
+                    inplace=False,
+                    non_blocking=False,
+                )
+        return data
+    elif isinstance(data, LazyStackedTensorDict):
+        # this is currently not implemented as the registration of params will only work
+        # with plain TensorDict. The solution will be using pytree to get each independent
+        # leaf
+        raise RuntimeError(
+            "Using a LazyStackedTensorDict within a TensorDictParams isn't permitted."
+        )
+        # for _data in data.tensordicts:
+        #     _apply_leaves(_data, fn)
+        # return data
+    elif isinstance(data, _CustomOpTensorDict):
+        _apply_leaves(data._source, fn)
+        return data
+    elif isinstance(data, _SubTensorDict):
+        raise RuntimeError(
+            "Using a _SubTensorDict within a TensorDictParams isn't permitted."
+        )
+    else:
+        return fn(data)
+
+
+def _get_args_dict(func, args, kwargs):
+    signature = inspect.signature(func)
+    bound_arguments = signature.bind(*args, **kwargs)
+    bound_arguments.apply_defaults()
+
+    args_dict = dict(bound_arguments.arguments)
+    return args_dict
+
+
+def _maybe_make_param(tensor):
+    if (
+        isinstance(tensor, (Tensor, ftdim.Tensor))
+        and not isinstance(tensor, nn.Parameter)
+        and tensor.dtype in (torch.float, torch.double, torch.half)
+    ):
+        tensor = nn.Parameter(tensor)
+    return tensor
+
+
+def _maybe_make_param_or_buffer(tensor):
+    if (
+        isinstance(tensor, (Tensor, ftdim.Tensor))
+        and not isinstance(tensor, nn.Parameter)
+        and tensor.dtype in (torch.float, torch.double, torch.half)
+    ):
+        # convert all non-parameters to buffers
+        tensor = Buffer(tensor)
+    return tensor
+
+
+class _unlock_and_set:
+    # temporarily unlocks the nested tensordict to execute a function
+    def __new__(cls, *args, **kwargs):
+        if len(args) and callable(args[0]):
+            return cls(**kwargs)(args[0])
+        return super().__new__(cls)
+
+    def __init__(self, **only_for_kwargs):
+        self.only_for_kwargs = only_for_kwargs
+
+    def __call__(self, func):
+        name = func.__name__
+
+        @wraps(func)
+        def new_func(_self, *args, **kwargs):
+            if self.only_for_kwargs:
+                arg_dict = _get_args_dict(func, (_self, *args), kwargs)
+                for kwarg, exp_value in self.only_for_kwargs.items():
+                    cur_val = arg_dict.get(kwarg, NO_DEFAULT)
+                    if cur_val != exp_value:
+                        # escape
+                        meth = getattr(_self._param_td, name)
+                        out = meth(*args, **kwargs)
+                        return out
+            if not _self.no_convert:
+                args = tree_map(_maybe_make_param, args)
+                kwargs = tree_map(_maybe_make_param, kwargs)
+            else:
+                args = tree_map(_maybe_make_param_or_buffer, args)
+                kwargs = tree_map(_maybe_make_param_or_buffer, kwargs)
+            if _self.is_locked:
+                # if the root (TensorDictParams) is locked, we still want to raise an exception
+                raise RuntimeError(_LOCK_ERROR)
+            with _self._param_td.unlock_():
+                meth = getattr(_self._param_td, name)
+                out = meth(*args, **kwargs)
+            _self._reset_params()
+            if out is _self._param_td:
+                return _self
+            return out
+
+        return new_func
+
+
+def _get_post_hook(func):
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = func(self, *args, **kwargs)
+        return self._apply_get_post_hook(out)
+
+    return new_func
+
+
+def _fallback(func):
+    """Calls the method on the nested tensordict."""
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        if out is self._param_td:
+            # if the output does not change, return the wrapper
+            return self
+        return out
+
+    return new_func
+
+
+def _fallback_property(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self):
+        out = getattr(self._param_td, name)
+        if out is self._param_td:
+            return self
+        return out
+
+    def setter(self, value):
+        return getattr(type(self._param_td), name).fset(self._param_td, value)
+
+    return property(new_func, setter)
+
+
+def _replace(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        if out is self._param_td:
+            return self
+        self._param_td = out
+        return self
+
+    return new_func
+
+
+def _carry_over(func):
+    name = func.__name__
+
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        out = getattr(self._param_td, name)(*args, **kwargs)
+        if out is self._param_td:
+            return self
+        if not isinstance(out, TensorDictParams):
+            out = TensorDictParams(out, no_convert=True)
+            out.no_convert = self.no_convert
+        return out
+
+    return new_func
+
+
+def _apply_on_data(func):
+    @wraps(func)
+    def new_func(self, *args, **kwargs):
+        getattr(self.data, func.__name__)(*args, **kwargs)
+        return self
+
+    return new_func
+
+
+class TensorDictParams(TensorDictBase, nn.Module):
+    r"""Holds a TensorDictBase instance full of parameters.
+
+    This class exposes the contained parameters to a parent nn.Module
+    such that iterating over the parameters of the module also iterates over
+    the leaves of the tensordict.
+
+    Indexing works exactly as the indexing of the wrapped tensordict.
+    The parameter names will be registered within this module using :meth:`~.TensorDict.flatten_keys("_")`.
+    Therefore, the result of :meth:`~.named_parameters()` and the content of the
+    tensordict will differ slightly in term of key names.
+
+    Any operation that sets a tensor in the tensordict will be augmented by
+    a :class:`torch.nn.Parameter` conversion.
+
+    Args:
+        parameters (TensorDictBase): a tensordict to represent as parameters.
+            Values will be converted to parameters unless ``no_convert=True``.
+
+    Keyword Args:
+        no_convert (bool): if ``True``, no conversion to ``nn.Parameter`` will
+            occur at construction and after (unless the ``no_convert`` attribute is changed).
+            If ``no_convert`` is ``True`` and if non-parameters are present, they
+            will be registered as buffers.
+            Defaults to ``False``.
+        lock (bool): if ``True``, the tensordict hosted by TensorDictParams will
+            be locked. This can be useful to avoid unwanted modifications, but
+            also restricts the operations that can be done over the object (and
+            can have significant performance impact when `unlock_()` is required).
+            Defaults to ``False``.
+
+    Examples:
+        >>> from torch import nn
+        >>> from tensordict import TensorDict
+        >>> module = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 4))
+        >>> params = TensorDict.from_module(module)
+        >>> params.lock_()
+        >>> p = TensorDictParams(params)
+        >>> print(p)
+        TensorDictParams(params=TensorDict(
+            fields={
+                0: TensorDict(
+                    fields={
+                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight: Parameter(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False),
+                1: TensorDict(
+                    fields={
+                        bias: Parameter(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                        weight: Parameter(shape=torch.Size([4, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False))
+        >>> class CustomModule(nn.Module):
+        ...     def __init__(self, params):
+        ...         super().__init__()
+        ...         self.params = params
+        >>> m = CustomModule(p)
+        >>> # the wrapper supports assignment and values are turned in Parameter
+        >>> m.params['other'] = torch.randn(3)
+        >>> assert isinstance(m.params['other'], nn.Parameter)
+
+    """
+
+    def __init__(
+        self, parameters: TensorDictBase, *, no_convert=False, lock: bool = False
+    ):
+        super().__init__()
+        if isinstance(parameters, TensorDictParams):
+            parameters = parameters._param_td
+        self._param_td = parameters
+        self.no_convert = no_convert
+        if not no_convert:
+            func = _maybe_make_param
+        else:
+            func = _maybe_make_param_or_buffer
+        self._param_td = _apply_leaves(self._param_td, lambda x: func(x))
+        self._lock_content = lock
+        if lock:
+            self._param_td.lock_()
+        self._reset_params()
+        self._is_locked = False
+        self._locked_tensordicts = []
+        self.__last_op_queue = None
+        self._get_post_hook = []
+
+    def register_get_post_hook(self, hook):
+        """Register a hook to be called after any get operation on leaf tensors."""
+        if not callable(hook):
+            raise ValueError("Hooks must be callables.")
+        self._get_post_hook.append(hook)
+
+    def _apply_get_post_hook(self, val):
+        if not _is_tensor_collection(type(val)):
+            for hook in self._get_post_hook:
+                new_val = hook(self, val)
+                if new_val is not None:
+                    val = new_val
+        return val
+
+    def _reset_params(self):
+        parameters = self._param_td
+        param_keys = []
+        params = []
+        buffer_keys = []
+        buffers = []
+        for key, value in parameters.items(True, True):
+            # flatten key
+            if isinstance(key, tuple):
+                key = ".".join(key)
+            if isinstance(value, nn.Parameter):
+                param_keys.append(key)
+                params.append(value)
+            else:
+                buffer_keys.append(key)
+                buffers.append(value)
+        self.__dict__["_parameters"] = dict(zip(param_keys, params))
+        self.__dict__["_buffers"] = dict(zip(buffer_keys, buffers))
+
+    @classmethod
+    def __torch_function__(
+        cls,
+        func: Callable,
+        types: tuple[type, ...],
+        args: tuple[Any, ...] = (),
+        kwargs: dict[str, Any] | None = None,
+    ) -> Callable:
+        if kwargs is None:
+            kwargs = {}
+        if func not in TDPARAM_HANDLED_FUNCTIONS or not all(
+            issubclass(t, (Tensor, ftdim.Tensor, TensorDictBase)) for t in types
+        ):
+            return NotImplemented
+        return TDPARAM_HANDLED_FUNCTIONS[func](*args, **kwargs)
+
+    @classmethod
+    def _flatten_key(cls, key):
+        def make_valid_identifier(s):
+            # Replace invalid characters with underscores
+            s = re.sub(r"\W|^(?=\d)", "_", s)
+
+            # Ensure the string starts with a letter or underscore
+            if not s[0].isalpha() and s[0] != "_":
+                s = "_" + s
+
+            return s
+
+        key_flat = "_".join(key)
+        if not key_flat.isidentifier():
+            key_flat = make_valid_identifier(key_flat)
+        return key_flat
+
+    @lock_blocked
+    @_unlock_and_set
+    def __setitem__(
+        self,
+        index: IndexType,
+        value: TensorDictBase | dict | numbers.Number | CompatibleType,
+    ) -> None:
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def set(
+        self, key: NestedKey, item: CompatibleType, inplace: bool = False, **kwargs: Any
+    ) -> TensorDictBase:
+        ...
+
+    @lock_blocked
+    def update(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | TensorDictBase,
+        clone: bool = False,
+        inplace: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> TensorDictBase:
+        # Deprecating this since _set_tuple will do it thx to the decorator
+        # if not self.no_convert:
+        #     func = _maybe_make_param
+        # else:
+        #     func = _maybe_make_param_or_buffer
+        # if _is_tensor_collection(type(input_dict_or_td)):
+        #     input_dict_or_td = input_dict_or_td.apply(func)
+        # else:
+        #     input_dict_or_td = tree_map(func, input_dict_or_td)
+        with self._param_td.unlock_():
+            TensorDictBase.update(
+                self,
+                input_dict_or_td,
+                clone=clone,
+                inplace=inplace,
+                keys_to_update=keys_to_update,
+                non_blocking=non_blocking,
+            )
+            self._reset_params()
+        return self
+
+    @lock_blocked
+    @_unlock_and_set
+    def pop(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def popitem(self):
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def rename_key_(
+        self, old_key: NestedKey, new_key: NestedKey, safe: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    def map(
+        self,
+        fn: Callable,
+        dim: int = 0,
+        num_workers: int = None,
+        chunksize: int = None,
+        num_chunks: int = None,
+        pool: mp.Pool = None,
+        generator: torch.Generator | None = None,
+        max_tasks_per_child: int | None = None,
+        worker_threads: int = 1,
+        mp_start_method: str | None = None,
+    ):
+        raise RuntimeError(
+            "Cannot call map on a TensorDictParams object. Convert it "
+            "to a detached tensordict first (through ``tensordict.data`` or ``tensordict.to_tensordict()``) and call "
+            "map in a second time."
+        )
+
+    @_unlock_and_set(inplace=True)
+    def apply(
+        self,
+        fn: Callable,
+        *others: TensorDictBase,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
+        call_on_nested: bool = False,
+        **constructor_kwargs,
+    ) -> TensorDictBase | None:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def named_apply(
+        self,
+        fn: Callable,
+        *others: TensorDictBase,
+        batch_size: Sequence[int] | None = None,
+        device: torch.device | None = NO_DEFAULT,
+        names: Sequence[str] | None = None,
+        inplace: bool = False,
+        default: Any = NO_DEFAULT,
+        filter_empty: bool | None = None,
+        call_on_nested: bool = False,
+        **constructor_kwargs,
+    ) -> TensorDictBase | None:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def _apply_nest(*args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def get(self, key: NestedKey, default: Any = NO_DEFAULT) -> CompatibleType:
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def __getitem__(self, index: IndexType) -> TensorDictBase:
+        ...
+
+    __getitems__ = __getitem__
+
+    def to(self, *args, **kwargs) -> TensorDictBase:
+        params = self._param_td.to(*args, **kwargs)
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def cpu(self):
+        params = self._param_td.cpu()
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def cuda(self, device=None):
+        params = self._param_td.cuda(device=device)
+        if params is self._param_td:
+            return self
+        return TensorDictParams(params)
+
+    def _clone(self, recurse: bool = True) -> TensorDictBase:
+        """Clones the TensorDictParams.
+
+        .. warning::
+            The effect of this call is different from a regular torch.Tensor.clone call
+            in that it will create a TensorDictParams instance with a new copy of the
+            parameters and buffers __detached__ from the current graph. For a
+            regular clone (ie, cloning leaf parameters onto a new tensor that
+            is part of the graph), simply call
+
+                >>> params.apply(torch.clone)
+
+        .. note::
+            If a parameter is duplicated in the tree, ``clone`` will preserve this
+            identity (ie, parameter tying is preserved).
+
+        See :meth:`tensordict.TensorDictBase.clone` for more info on the clone
+        method.
+
+        """
+        if not recurse:
+            return TensorDictParams(self._param_td._clone(False), no_convert=True)
+
+        memo = {}
+
+        def _clone(tensor, memo=memo):
+            result = memo.get(tensor, None)
+            if result is not None:
+                return result
+
+            if isinstance(tensor, nn.Parameter):
+                result = nn.Parameter(
+                    tensor.data.clone(), requires_grad=tensor.requires_grad
+                )
+            else:
+                result = Buffer(tensor.data.clone(), requires_grad=tensor.requires_grad)
+            memo[tensor] = result
+            return result
+
+        return TensorDictParams(self._param_td.apply(_clone), no_convert=True)
+
+    @_fallback
+    def chunk(self, chunks: int, dim: int = 0) -> tuple[TensorDictBase, ...]:
+        ...
+
+    @_fallback
+    def _unbind(self, dim: int) -> tuple[TensorDictBase, ...]:
+        ...
+
+    @_fallback
+    def to_tensordict(self):
+        ...
+
+    @_fallback
+    def to_h5(
+        self,
+        filename,
+        **kwargs,
+    ):
+        ...
+
+    def __hash__(self):
+        return hash((id(self), id(self.__dict__.get("_param_td", None))))
+
+    @_fallback
+    def __eq__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __ne__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __xor__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __or__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __ge__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __gt__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __le__(self, other: object) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def __lt__(self, other: object) -> TensorDictBase:
+        ...
+
+    def __getattr__(self, item: str) -> Any:
+        if not item.startswith("_"):
+            try:
+                return getattr(self.__dict__["_param_td"], item)
+            except AttributeError:
+                return super().__getattr__(item)
+        else:
+            return super().__getattr__(item)
+
+    @_fallback
+    def _change_batch_size(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _erase_names(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_str(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_tuple(self, *args, **kwargs):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_at_str(self, key, idx, default):
+        ...
+
+    @_get_post_hook
+    @_fallback
+    def _get_at_tuple(self, key, idx, default):
+        ...
+
+    @_fallback
+    def _add_batch_dim(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _convert_to_tensordict(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _get_names_idx(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _index_tensordict(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _remove_batch_dim(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _has_names(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _rename_subtds(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_at_str(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def _set_at_tuple(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_str(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _set_tuple(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _create_nested_str(self, *args, **kwargs):
+        ...
+
+    @_fallback_property
+    def batch_size(self) -> torch.Size:
+        ...
+
+    @_fallback
+    def contiguous(self, *args, **kwargs):
+        ...
+
+    @lock_blocked
+    @_unlock_and_set
+    def del_(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def detach_(self, *args, **kwargs):
+        ...
+
+    @_fallback_property
+    def device(self):
+        ...
+
+    @_fallback
+    def entry_class(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def is_contiguous(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def keys(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def masked_fill(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def masked_fill_(self, *args, **kwargs):
+        ...
+
+    def memmap_(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        num_threads: int = 0,
+    ) -> TensorDictBase:
+        raise RuntimeError(
+            "Cannot build a memmap TensorDict in-place. Use memmap or memmap_like instead."
+        )
+
+    _memmap_ = TensorDict._memmap_
+
+    _load_memmap = TensorDict._load_memmap
+
+    def make_memmap(
+        self,
+        key: NestedKey,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_storage(
+        self,
+        key: NestedKey,
+        storage: torch.UntypedStorage,
+        shape: torch.Size | torch.Tensor,
+        *,
+        dtype: torch.dtype | None = None,
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    def make_memmap_from_tensor(
+        self, key: NestedKey, tensor: torch.Tensor, *, copy_data: bool = True
+    ) -> MemoryMappedTensor:
+        raise RuntimeError(
+            "Making a memory-mapped tensor after instantiation isn't currently allowed for TensorDictParams."
+            "If this feature is required, open an issue on GitHub to trigger a discussion on the topic!"
+        )
+
+    @_fallback_property
+    def names(self):
+        ...
+
+    @_fallback
+    def pin_memory(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def _select(self, *args, **kwargs):
+        ...
+
+    @_fallback
+    def share_memory_(self, *args, **kwargs):
+        ...
+
+    @property
+    def is_locked(self) -> bool:
+        # Cannot be locked
+        return self._is_locked
+
+    @is_locked.setter
+    def is_locked(self, value):
+        self._is_locked = bool(value)
+
+    @_fallback_property
+    def is_shared(self) -> bool:
+        ...
+
+    @_fallback_property
+    def is_memmap(self) -> bool:
+        ...
+
+    @property
+    def _is_shared(self) -> bool:
+        return self._param_td._is_shared
+
+    @property
+    def _is_memmap(self) -> bool:
+        return self._param_td._is_memmap
+
+    @_fallback_property
+    def shape(self) -> torch.Size:
+        ...
+
+    def _propagate_lock(self, _lock_parents_weakrefs=None):
+        """Registers the parent tensordict that handles the lock."""
+        self._is_locked = True
+        if _lock_parents_weakrefs is None:
+            _lock_parents_weakrefs = []
+        self._lock_parents_weakrefs += _lock_parents_weakrefs
+        _lock_parents_weakrefs.append(weakref.ref(self))
+        # we don't want to double-lock the _param_td attrbute which is locked by default
+        if not self._param_td.is_locked:
+            self._param_td._propagate_lock(_lock_parents_weakrefs)
+
+    @erase_cache
+    def _propagate_unlock(self):
+        # if we end up here, we can clear the graph associated with this td
+        self._is_locked = False
+
+        if not self._lock_content:
+            return self._param_td._propagate_unlock()
+
+    unlock_ = TensorDict.unlock_
+    lock_ = TensorDict.lock_
+
+    @property
+    def data(self):
+        return self._param_td._data()
+
+    @property
+    def grad(self):
+        return self._param_td._grad()
+
+    @_unlock_and_set(inplace=True)
+    def flatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def unflatten_keys(
+        self, separator: str = ".", inplace: bool = False
+    ) -> TensorDictBase:
+        ...
+
+    @_unlock_and_set(inplace=True)
+    def _exclude(
+        self, *keys: NestedKey, inplace: bool = False, set_shared: bool = True
+    ) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def from_dict_instance(
+        self, input_dict, batch_size=None, device=None, batch_dims=None
+    ):
+        ...
+
+    @_carry_over
+    def _legacy_transpose(self, dim0, dim1):
+        ...
+
+    @_fallback
+    def _transpose(self, dim0, dim1):
+        ...
+
+    @_fallback
+    def where(self, condition, other, *, out=None, pad=None):
+        ...
+
+    @_fallback
+    def _permute(
+        self,
+        *dims_list: int,
+        dims: list[int] | None = None,
+    ) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def _legacy_permute(
+        self,
+        *dims_list: int,
+        dims: list[int] | None = None,
+    ) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def _squeeze(self, dim: int | None = None) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def _legacy_squeeze(self, dim: int | None = None) -> TensorDictBase:
+        ...
+
+    @_fallback
+    def _unsqueeze(self, dim: int) -> TensorDictBase:
+        ...
+
+    @_carry_over
+    def _legacy_unsqueeze(self, dim: int) -> TensorDictBase:
+        ...
+
+    _check_device = TensorDict._check_device
+    _check_is_shared = TensorDict._check_is_shared
+
+    @_fallback
+    def _cast_reduction(self, **kwargs):
+        ...
+
+    @_fallback
+    def all(self, dim: int = None) -> bool | TensorDictBase:
+        ...
+
+    @_fallback
+    def any(self, dim: int = None) -> bool | TensorDictBase:
+        ...
+
+    @_fallback
+    def expand(self, *args, **kwargs) -> T:
+        ...
+
+    @_fallback
+    def masked_select(self, mask: Tensor) -> T:
+        ...
+
+    @_fallback
+    def memmap_like(
+        self,
+        prefix: str | None = None,
+        copy_existing: bool = False,
+        num_threads: int = 0,
+    ) -> T:
+        ...
+
+    @_fallback
+    def reshape(self, *shape: int):
+        ...
+
+    @_fallback
+    def split(self, split_size: int | list[int], dim: int = 0) -> list[TensorDictBase]:
+        ...
+
+    @_fallback
+    def _to_module(
+        self,
+        module,
+        *,
+        inplace: bool = False,
+        return_swap: bool = True,
+        swap_dest=None,
+        memo=None,
+        use_state_dict: bool = False,
+        non_blocking: bool = False,
+    ):
+        ...
+
+    @_fallback
+    def _view(self, *args, **kwargs):
+        ...
+
+    @_carry_over
+    def _legacy_view(self, *args, **kwargs):
+        ...
+
+    @_unlock_and_set
+    def create_nested(self, key):
+        ...
+
+    def __repr__(self):
+        return f"TensorDictParams(params={self._param_td})"
+
+    def values(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> Iterator[CompatibleType]:
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+        for v in self._param_td.values(include_nested, leaves_only):
+            if not is_leaf(type(v)):
+                yield v
+                continue
+            yield self._apply_get_post_hook(v)
+
+    def state_dict(
+        self, *args, destination=None, prefix="", keep_vars=False, flatten=True
+    ):
+        # flatten must be True by default to comply with module's state-dict API
+        # since we want all params to be visible at root
+        return self._param_td.state_dict(
+            destination=destination,
+            prefix=prefix,
+            keep_vars=keep_vars,
+            flatten=flatten,
+        )
+
+    def load_state_dict(
+        self, state_dict: OrderedDict[str, Any], strict=True, assign=False
+    ):
+        # The state-dict is presumably the result of a call to TensorDictParams.state_dict
+        # but can't be sure.
+
+        state_dict_tensors = {}
+        state_dict = dict(state_dict)
+        for k, v in list(state_dict.items()):
+            if isinstance(v, torch.Tensor):
+                del state_dict[k]
+                state_dict_tensors[k] = v
+        state_dict_tensors = dict(
+            TensorDict(state_dict_tensors, []).unflatten_keys(".")
+        )
+        state_dict.update(state_dict_tensors)
+        self.data.load_state_dict(state_dict, strict=True, assign=False)
+        return self
+
+    def _load_from_state_dict(
+        self,
+        state_dict,
+        prefix,
+        local_metadata,
+        strict,
+        missing_keys,
+        unexpected_keys,
+        error_msgs,
+    ):
+        data = TensorDict(
+            {
+                key: val
+                for key, val in state_dict.items()
+                if key.startswith(prefix) and val is not None
+            },
+            [],
+        ).unflatten_keys(".")
+        prefix = tuple(key for key in prefix.split(".") if key)
+        if prefix:
+            data = data.get(prefix)
+        self.data.load_state_dict(data)
+
+    def items(
+        self,
+        include_nested: bool = False,
+        leaves_only: bool = False,
+        is_leaf: Callable[[Type], bool] | None = None,
+    ) -> Iterator[CompatibleType]:
+        if is_leaf is None:
+            is_leaf = _default_is_leaf
+        for k, v in self._param_td.items(include_nested, leaves_only):
+            if not is_leaf(type(v)):
+                yield k, v
+                continue
+            yield k, self._apply_get_post_hook(v)
+
+    @_apply_on_data
+    def zero_(self) -> T:
+        ...
+
+    @_apply_on_data
+    def fill_(self, key: NestedKey, value: float | bool) -> T:
+        ...
+
+    @_apply_on_data
+    def copy_(self, tensordict: T, non_blocking: bool = None) -> T:
+        ...
+
+    @_apply_on_data
+    def set_at_(self, key: NestedKey, value: CompatibleType, index: IndexType) -> T:
+        ...
+
+    @_apply_on_data
+    def set_(
+        self,
+        key: NestedKey,
+        item: CompatibleType,
+    ) -> T:
+        ...
+
+    @_apply_on_data
+    def _stack_onto_(
+        self,
+        list_item: list[CompatibleType],
+        dim: int,
+    ) -> T:
+        ...
+
+    @_apply_on_data
+    def _stack_onto_at_(
+        self,
+        key: NestedKey,
+        list_item: list[CompatibleType],
+        dim: int,
+        idx: IndexType,
+    ) -> T:
+        ...
+
+    @_apply_on_data
+    def update_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        ...
+
+    @_apply_on_data
+    def update_at_(
+        self,
+        input_dict_or_td: dict[str, CompatibleType] | T,
+        idx: IndexType,
+        clone: bool = False,
+        *,
+        non_blocking: bool = False,
+        keys_to_update: Sequence[NestedKey] | None = None,
+    ) -> T:
+        ...
+
+    @_apply_on_data
+    def apply_(self, fn: Callable, *others, **kwargs) -> T:
+        ...
+
+    def _apply(self, fn, recurse=True):
+        """Modifies torch.nn.Module._apply to work with Buffer class."""
+        if recurse:
+            for module in self.children():
+                module._apply(fn)
+
+        def compute_should_use_set_data(tensor, tensor_applied):
+            if torch._has_compatible_shallow_copy_type(tensor, tensor_applied):
+                # If the new tensor has compatible tensor type as the existing tensor,
+                # the current behavior is to change the tensor in-place using `.data =`,
+                # and the future behavior is to overwrite the existing tensor. However,
+                # changing the current behavior is a BC-breaking change, and we want it
+                # to happen in future releases. So for now we introduce the
+                # `torch.__future__.get_overwrite_module_params_on_conversion()`
+                # global flag to let the user control whether they want the future
+                # behavior of overwriting the existing tensor or not.
+                return not torch.__future__.get_overwrite_module_params_on_conversion()
+            else:
+                return False
+
+        for key, param in self._parameters.items():
+            if param is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `param_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                param_applied = fn(param)
+            should_use_set_data = compute_should_use_set_data(param, param_applied)
+            if should_use_set_data:
+                param.data = param_applied
+                out_param = param
+            else:
+                out_param = nn.Parameter(param_applied, param.requires_grad)
+                self._parameters[key] = out_param
+
+            if param.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(param.grad)
+                should_use_set_data = compute_should_use_set_data(
+                    param.grad, grad_applied
+                )
+                if should_use_set_data:
+                    out_param.grad.data = grad_applied
+                else:
+                    out_param.grad = grad_applied.requires_grad_(
+                        param.grad.requires_grad
+                    )
+
+        for key, buffer in self._buffers.items():
+            if buffer is None:
+                continue
+            # Tensors stored in modules are graph leaves, and we don't want to
+            # track autograd history of `buffer_applied`, so we have to use
+            # `with torch.no_grad():`
+            with torch.no_grad():
+                buffer_applied = fn(buffer)
+            should_use_set_data = compute_should_use_set_data(buffer, buffer_applied)
+            if should_use_set_data:
+                buffer.data = buffer_applied
+                out_buffer = buffer
+            else:
+                out_buffer = Buffer(buffer_applied, buffer.requires_grad)
+                self._buffers[key] = out_buffer
+
+            if buffer.grad is not None:
+                with torch.no_grad():
+                    grad_applied = fn(buffer.grad)
+                should_use_set_data = compute_should_use_set_data(
+                    buffer.grad, grad_applied
+                )
+                if should_use_set_data:
+                    out_buffer.grad.data = grad_applied
+                else:
+                    out_buffer.grad = grad_applied.requires_grad_(
+                        buffer.grad.requires_grad
+                    )
+
+        return self
+
+
+TDPARAM_HANDLED_FUNCTIONS = copy(TD_HANDLED_FUNCTIONS)
+
+
+def implements_for_tdparam(torch_function: Callable) -> Callable[[Callable], Callable]:
+    """Register a torch function override for TensorDictParams."""
+
+    @functools.wraps(torch_function)
+    def decorator(func: Callable) -> Callable:
+        TDPARAM_HANDLED_FUNCTIONS[torch_function] = func
+        return func
+
+    return decorator
+
+
+@implements_for_tdparam(torch.empty_like)
+def _empty_like(td: TensorDictBase, *args, **kwargs) -> TensorDictBase:
+    return td.apply(
+        lambda x: torch.empty_like(x, *args, **kwargs),
+        device=kwargs.pop("device", NO_DEFAULT),
+    )
+
+
+_register_tensor_class(TensorDictParams)
```

## tensordict/nn/probabilistic.py

 * *Ordering differences only*

```diff
@@ -1,577 +1,577 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import re
-import warnings
-from enum import auto, Enum
-from textwrap import indent
-from typing import Any, Callable, Dict, List, Optional
-from warnings import warn
-
-from tensordict._contextlib import _DecoratorContextManager
-from tensordict.nn import CompositeDistribution
-
-from tensordict.nn.common import dispatch, TensorDictModule, TensorDictModuleBase
-from tensordict.nn.distributions import Delta, distributions_maps
-from tensordict.nn.sequence import TensorDictSequential
-
-from tensordict.nn.utils import set_skip_existing
-from tensordict.tensordict import TensorDictBase
-from tensordict.utils import NestedKey
-from torch import distributions as D, Tensor
-
-__all__ = ["ProbabilisticTensorDictModule", "ProbabilisticTensorDictSequential"]
-
-
-class InteractionType(Enum):
-    MODE = auto()
-    MEDIAN = auto()
-    MEAN = auto()
-    RANDOM = auto()
-
-    @classmethod
-    def from_str(cls, type_str: str) -> InteractionType:
-        """Return the interaction_type with name matched to the provided string (case insensitive)."""
-        for member_type in cls:
-            if member_type.name == type_str.upper():
-                return member_type
-        raise ValueError(f"The provided interaction type {type_str} is unsupported!")
-
-
-_INTERACTION_TYPE: InteractionType | None = None
-
-
-def _insert_interaction_mode_deprecation_warning(
-    prefix: str = "",
-) -> Callable[[str, Warning, int], None]:
-    return warn(
-        (
-            f"{prefix}interaction_mode is deprecated for naming clarity. "
-            f"Please use {prefix}interaction_type with InteractionType enum instead."
-        ),
-        DeprecationWarning,
-        stacklevel=2,
-    )
-
-
-def interaction_type() -> InteractionType | None:
-    """Returns the current sampling type."""
-    return _INTERACTION_TYPE
-
-
-def interaction_mode() -> str | None:
-    """*Deprecated* Returns the current sampling mode."""
-    _insert_interaction_mode_deprecation_warning()
-    type = interaction_type()
-    return type.name.lower() if type else None
-
-
-class set_interaction_mode(_DecoratorContextManager):
-    """*Deprecated* Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
-
-    Args:
-        mode (str): mode to use when the policy is being called.
-    """
-
-    def __init__(self, mode: str | None = "mode") -> None:
-        _insert_interaction_mode_deprecation_warning("set_")
-        super().__init__()
-        self.mode = InteractionType.from_str(mode) if mode else None
-
-    def clone(self) -> set_interaction_mode:
-        # override this method if your children class takes __init__ parameters
-        return self.__class__(self.mode)
-
-    def __enter__(self) -> None:
-        global _INTERACTION_TYPE
-        self.prev = _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.mode
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.prev
-
-
-class set_interaction_type(_DecoratorContextManager):
-    """Sets all ProbabilisticTDModules sampling to the desired type.
-
-    Args:
-        type (InteractionType): sampling type to use when the policy is being called.
-    """
-
-    def __init__(self, type: InteractionType | None = InteractionType.MODE) -> None:
-        super().__init__()
-        self.type = type
-
-    def clone(self) -> set_interaction_type:
-        # override this method if your children class takes __init__ parameters
-        return self.__class__(self.type)
-
-    def __enter__(self) -> None:
-        global _INTERACTION_TYPE
-        self.prev = _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.type
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _INTERACTION_TYPE
-        _INTERACTION_TYPE = self.prev
-
-
-class ProbabilisticTensorDictModule(TensorDictModuleBase):
-    """A probabilistic TD Module.
-
-    `ProbabilisticTensorDictModule` is a non-parametric module representing a
-    probability distribution. It reads the distribution parameters from an input
-    TensorDict using the specified `in_keys`. The output is sampled given some rule,
-    specified by the input :obj:`default_interaction_type` argument and the
-    :obj:`interaction_type()` global function.
-
-    :obj:`ProbabilisticTensorDictModule` can be used to construct the distribution
-    (through the :obj:`get_dist()` method) and/or sampling from this distribution
-    (through a regular :obj:`__call__()` to the module).
-
-    A :obj:`ProbabilisticTensorDictModule` instance has two main features:
-    - It reads and writes TensorDict objects
-    - It uses a real mapping R^n -> R^m to create a distribution in R^d from
-    which values can be sampled or computed.
-
-    When the :obj:`__call__` / :obj:`forward` method is called, a distribution is
-    created, and a value computed (using the 'mean', 'mode', 'median' attribute or
-    the 'rsample', 'sample' method). The sampling step is skipped if the supplied
-    TensorDict has all of the desired key-value pairs already.
-
-    By default, ProbabilisticTensorDictModule distribution class is a Delta
-    distribution, making ProbabilisticTensorDictModule a simple wrapper around
-    a deterministic mapping function.
-
-    Args:
-        in_keys (NestedKey or list of NestedKey or dict): key(s) that will be read from the
-            input TensorDict and used to build the distribution. Importantly, if it's an
-            list of NestedKey or a NestedKey, the leaf (last element) of those keys must match the keywords used by
-            the distribution class of interest, e.g. :obj:`"loc"` and :obj:`"scale"` for
-            the Normal distribution and similar. If in_keys is a dictionary, the keys
-            are the keys of the distribution and the values are the keys in the
-            tensordict that will get match to the corresponding distribution keys.
-        out_keys (NestedKey or list of NestedKey): keys where the sampled values will be
-            written. Importantly, if these keys are found in the input TensorDict, the
-            sampling step will be skipped.
-        default_interaction_mode (str, optional): *Deprecated* keyword-only argument.
-            Please use default_interaction_type instead.
-        default_interaction_type (InteractionType, optional): keyword-only argument.
-            Default method to be used to retrieve
-            the output value. Should be one of InteractionType: MODE, MEDIAN, MEAN or RANDOM
-            (in which case the value is sampled randomly from the distribution). Default
-            is MODE.
-
-            .. note:: When a sample is drawn, the
-              :class:`ProbabilisticTensorDictModule` instance will
-              first look for the interaction mode dictated by the
-              :func:`~tensordict.nn.probabilistic.interaction_type`
-              global function. If this returns `None` (its default value), then the
-              `default_interaction_type` of the `ProbabilisticTDModule`
-              instance will be used. Note that
-              :class:`~torchrl.collectors.collectors.DataCollectorBase`
-              instances will use `set_interaction_type` to
-              :class:`tensordict.nn.InteractionType.RANDOM` by default.
-
-        distribution_class (Type, optional): keyword-only argument.
-            A :class:`torch.distributions.Distribution` class to
-            be used for sampling.
-            Default is :class:`tensordict.nn.distributions.Delta`.
-        distribution_kwargs (dict, optional): keyword-only argument.
-            Keyword-argument pairs to be passed to the distribution.
-        return_log_prob (bool, optional): keyword-only argument.
-            If ``True``, the log-probability of the
-            distribution sample will be written in the tensordict with the key
-            `log_prob_key`. Default is ``False``.
-        log_prob_key (NestedKey, optional): key where to write the log_prob if return_log_prob = True.
-            Defaults to `'sample_log_prob'`.
-        cache_dist (bool, optional): keyword-only argument.
-            EXPERIMENTAL: if ``True``, the parameters of the
-            distribution (i.e. the output of the module) will be written to the
-            tensordict along with the sample. Those parameters can be used to re-compute
-            the original distribution later on (e.g. to compute the divergence between
-            the distribution used to sample the action and the updated distribution in
-            PPO). Default is ``False``.
-        n_empirical_estimate (int, optional): keyword-only argument.
-            Number of samples to compute the empirical
-            mean when it is not available. Defaults to 1000.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import (
-        ...     ProbabilisticTensorDictModule,
-        ...     ProbabilisticTensorDictSequential,
-        ...     TensorDictModule,
-        ... )
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from tensordict.nn.functional_modules import make_functional
-        >>> from torch.distributions import Normal, Independent
-        >>> td = TensorDict(
-        ...     {"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3]
-        ... )
-        >>> net = torch.nn.GRUCell(4, 8)
-        >>> module = TensorDictModule(
-        ...     net, in_keys=["input", "hidden"], out_keys=["params"]
-        ... )
-        >>> normal_params = TensorDictModule(
-        ...     NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
-        ... )
-        >>> def IndepNormal(**kwargs):
-        ...     return Independent(Normal(**kwargs), 1)
-        >>> prob_module = ProbabilisticTensorDictModule(
-        ...     in_keys=["loc", "scale"],
-        ...     out_keys=["action"],
-        ...     distribution_class=IndepNormal,
-        ...     return_log_prob=True,
-        ... )
-        >>> td_module = ProbabilisticTensorDictSequential(
-        ...     module, normal_params, prob_module
-        ... )
-        >>> params = TensorDict.from_module(td_module)
-        >>> with params.to_module(td_module):
-        ...     _ = td_module(td)
-        >>> print(td)
-        TensorDict(
-            fields={
-                action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-        >>> with params.to_module(td_module):
-        ...     dist = td_module.get_dist(td)
-        >>> print(dist)
-        Independent(Normal(loc: torch.Size([3, 4]), scale: torch.Size([3, 4])), 1)
-        >>> # we can also apply the module to the TensorDict with vmap
-        >>> from torch import vmap
-        >>> params = params.expand(4)
-        >>> def func(td, params):
-        ...     with params.to_module(td_module):
-        ...         return td_module(td)
-        >>> td_vmap = vmap(func, (None, 0))(td, params)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                action: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    def __init__(
-        self,
-        in_keys: NestedKey | List[NestedKey] | Dict[str, NestedKey],
-        out_keys: NestedKey | List[NestedKey] | None = None,
-        *,
-        default_interaction_mode: str | None = None,
-        default_interaction_type: InteractionType = InteractionType.MODE,
-        distribution_class: type = Delta,
-        distribution_kwargs: dict | None = None,
-        return_log_prob: bool = False,
-        log_prob_key: Optional[NestedKey] = "sample_log_prob",
-        cache_dist: bool = False,
-        n_empirical_estimate: int = 1000,
-    ) -> None:
-        super().__init__()
-        if isinstance(in_keys, (str, tuple)):
-            in_keys = [in_keys]
-        if isinstance(out_keys, (str, tuple)):
-            out_keys = [out_keys]
-        elif out_keys is None:
-            out_keys = ["_"]
-        if isinstance(in_keys, dict):
-            dist_keys, in_keys = zip(*in_keys.items())
-            if set(map(type, dist_keys)) != {str}:
-                raise ValueError(
-                    f"If in_keys is dict, its keys must be strings matching to the distribution kwargs."
-                    f"{self.__class__.__name__} got {dist_keys}"
-                )
-        else:
-            dist_keys = in_keys
-
-        self.out_keys = out_keys
-        self.in_keys = in_keys
-        self.dist_keys = dist_keys
-        if log_prob_key is None:
-            log_prob_key = "sample_log_prob"
-        self.log_prob_key = log_prob_key
-
-        if default_interaction_mode is not None:
-            _insert_interaction_mode_deprecation_warning("default_")
-            self.default_interaction_type = InteractionType.from_str(
-                default_interaction_mode
-            )
-        else:
-            self.default_interaction_type = default_interaction_type
-
-        if isinstance(distribution_class, str):
-            distribution_class = distributions_maps.get(distribution_class.lower())
-        self.distribution_class = distribution_class
-        self.distribution_kwargs = (
-            distribution_kwargs if distribution_kwargs is not None else {}
-        )
-        self.n_empirical_estimate = n_empirical_estimate
-        self._dist = None
-        self.cache_dist = cache_dist if hasattr(distribution_class, "update") else False
-        self.return_log_prob = return_log_prob
-        if self.return_log_prob and self.log_prob_key not in self.out_keys:
-            self.out_keys.append(self.log_prob_key)
-
-    def get_dist(self, tensordict: TensorDictBase) -> D.Distribution:
-        """Creates a :class:`torch.distribution.Distribution` instance with the parameters provided in the input tensordict."""
-        try:
-            dist_kwargs = {}
-            for dist_key, td_key in zip(self.dist_keys, self.in_keys):
-                if isinstance(dist_key, tuple):
-                    dist_key = dist_key[-1]
-                dist_kwargs[dist_key] = tensordict.get(td_key)
-            dist = self.distribution_class(**dist_kwargs, **self.distribution_kwargs)
-        except TypeError as err:
-            if "an unexpected keyword argument" in str(err):
-                raise TypeError(
-                    "distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match."
-                    f"Got this error message: \n{indent(str(err), 4 * ' ')}\nwith dist_keys={self.dist_keys}"
-                )
-            elif re.search(r"missing.*required positional arguments", str(err)):
-                raise TypeError(
-                    f"TensorDict with keys {tensordict.keys()} does not match the distribution {self.distribution_class} keywords."
-                )
-            else:
-                raise err
-        return dist
-
-    def log_prob(self, tensordict):
-        """Writes the log-probability of the distribution sample."""
-        dist = self.get_dist(tensordict)
-        if isinstance(dist, CompositeDistribution):
-            tensordict = dist.log_prob(tensordict)
-            return tensordict.get("sample_log_prob")
-        else:
-            return dist.log_prob(tensordict.get(self.out_keys[0]))
-
-    @property
-    def SAMPLE_LOG_PROB_KEY(self):
-        warnings.warn(
-            "SAMPLE_LOG_PROB_KEY will be deprecated soon."
-            "Use 'obj.log_prob_key' instead",
-            category=DeprecationWarning,
-        )
-        return self.log_prob_key
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        _requires_sample: bool = True,
-    ) -> TensorDictBase:
-        if tensordict_out is None:
-            tensordict_out = tensordict
-
-        dist = self.get_dist(tensordict)
-        if _requires_sample:
-            out_tensors = self._dist_sample(dist, interaction_type=interaction_type())
-            if isinstance(out_tensors, TensorDictBase):
-                tensordict_out.update(out_tensors)
-                if self.return_log_prob:
-                    tensordict_out = dist.log_prob(tensordict_out)
-            else:
-                if isinstance(out_tensors, Tensor):
-                    out_tensors = (out_tensors,)
-                tensordict_out.update(
-                    {key: value for key, value in zip(self.out_keys, out_tensors)}
-                )
-                if self.return_log_prob:
-                    log_prob = dist.log_prob(*out_tensors)
-                    tensordict_out.set(self.log_prob_key, log_prob)
-        elif self.return_log_prob:
-            out_tensors = [
-                tensordict.get(key) for key in self.out_keys if key != self.log_prob_key
-            ]
-            log_prob = dist.log_prob(*out_tensors)
-            tensordict_out.set(self.log_prob_key, log_prob)
-            # raise RuntimeError(
-            #     "ProbabilisticTensorDictModule.return_log_prob = True is incompatible with settings in which "
-            #     "the submodule is responsible for sampling. To manually gather the log-probability, call first "
-            #     "\n>>> dist, tensordict = tensordict_module.get_dist(tensordict)"
-            #     "\n>>> tensordict.set('sample_log_prob', dist.log_prob(tensordict.get(sample_key))"
-            # )
-        return tensordict_out
-
-    def _dist_sample(
-        self,
-        dist: D.Distribution,
-        interaction_type: InteractionType | None = None,
-    ) -> tuple[Tensor, ...] | Tensor:
-        if interaction_type is None:
-            interaction_type = self.default_interaction_type
-
-        if interaction_type is InteractionType.MODE:
-            try:
-                return dist.mode
-            except AttributeError:
-                raise NotImplementedError(
-                    f"method {type(dist)}.mode is not implemented"
-                )
-
-        elif interaction_type is InteractionType.MEDIAN:
-            try:
-                return dist.median
-            except AttributeError:
-                raise NotImplementedError(
-                    f"method {type(dist)}.median is not implemented"
-                )
-
-        elif interaction_type is InteractionType.MEAN:
-            try:
-                return dist.mean
-            except (AttributeError, NotImplementedError):
-                if dist.has_rsample:
-                    return dist.rsample((self.n_empirical_estimate,)).mean(0)
-                else:
-                    return dist.sample((self.n_empirical_estimate,)).mean(0)
-
-        elif interaction_type is InteractionType.RANDOM:
-            if dist.has_rsample:
-                return dist.rsample()
-            else:
-                return dist.sample()
-        else:
-            raise NotImplementedError(f"unknown interaction_type {interaction_type}")
-
-
-class ProbabilisticTensorDictSequential(TensorDictSequential):
-    """A sequence of TensorDictModules ending in a ProbabilistictTensorDictModule.
-
-    Similarly to :obj:`TensorDictSequential`, but enforces that the final module in the
-    sequence is an :obj:`ProbabilisticTensorDictModule` and also exposes ``get_dist``
-    method to recover the distribution object from the ``ProbabilisticTensorDictModule``
-
-    Args:
-         modules (sequence of TensorDictModules): ordered sequence of TensorDictModule
-            instances, terminating in ProbabilisticTensorDictModule, to be run
-            sequentially.
-         partial_tolerant (bool, optional): if True, the input tensordict can miss some
-            of the input keys. If so, the only module that will be executed are those
-            who can be executed given the keys that are present. Also, if the input
-            tensordict is a lazy stack of tensordicts AND if partial_tolerant is
-            :obj:`True` AND if the stack does not have the required keys, then
-            TensorDictSequential will scan through the sub-tensordicts looking for those
-            that have the required keys, if any.
-
-    """
-
-    def __init__(
-        self,
-        *modules: TensorDictModule | ProbabilisticTensorDictModule,
-        partial_tolerant: bool = False,
-    ) -> None:
-        if len(modules) == 0:
-            raise ValueError(
-                "ProbabilisticTensorDictSequential must consist of zero or more "
-                "TensorDictModules followed by a ProbabilisticTensorDictModule"
-            )
-        if not isinstance(
-            modules[-1],
-            (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential),
-        ):
-            raise TypeError(
-                "The final module passed to ProbabilisticTensorDictSequential must be "
-                "an instance of ProbabilisticTensorDictModule or another "
-                "ProbabilisticTensorDictSequential"
-            )
-        # if the modules not including the final probabilistic module return the sampled
-        # key we wont be sampling it again, in that case
-        # ProbabilisticTensorDictSequential is presumably used to return the
-        # distribution using `get_dist` or to sample log_probabilities
-        _, out_keys = self._compute_in_and_out_keys(modules[:-1])
-        self._requires_sample = modules[-1].out_keys[0] not in set(out_keys)
-        super().__init__(*modules, partial_tolerant=partial_tolerant)
-
-    @property
-    def det_part(self):
-        if not hasattr(self, "_det_part"):
-            # we use a list to avoid having the submodules listed in module.modules()
-            self._det_part = [TensorDictSequential(*self.module[:-1])]
-        return self._det_part[0]
-
-    def get_dist_params(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> tuple[D.Distribution, TensorDictBase]:
-        tds = self.det_part
-        type = interaction_type()
-        if type is None:
-            type = self.module[-1].default_interaction_type
-        with set_interaction_type(type):
-            return tds(tensordict, tensordict_out, **kwargs)
-
-    def get_dist(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> D.Distribution:
-        """Get the distribution that results from passing the input tensordict through the sequence, and then using the resulting parameters."""
-        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
-        return self.build_dist_from_params(tensordict_out)
-
-    def log_prob(
-        self, tensordict, tensordict_out: TensorDictBase | None = None, **kwargs
-    ):
-        tensordict_out = self.get_dist_params(
-            tensordict,
-            tensordict_out,
-            **kwargs,
-        )
-        return self.module[-1].log_prob(tensordict_out)
-
-    def build_dist_from_params(self, tensordict: TensorDictBase) -> D.Distribution:
-        """Construct a distribution from the input parameters. Other modules in the sequence are not evaluated.
-
-        This method will look for the last ProbabilisticTensorDictModule contained in the
-        sequence and use it to build the distribution.
-
-        """
-        dest_module = None
-        for module in reversed(list(self.modules())):
-            if isinstance(module, ProbabilisticTensorDictModule):
-                dest_module = module
-                break
-        if dest_module is None:
-            raise RuntimeError(
-                "Could not find any ProbabilisticTensorDictModule in the sequence."
-            )
-        return dest_module.get_dist(tensordict)
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> TensorDictBase:
-        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
-        return self.module[-1](tensordict_out, _requires_sample=self._requires_sample)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import re
+import warnings
+from enum import auto, Enum
+from textwrap import indent
+from typing import Any, Callable, Dict, List, Optional
+from warnings import warn
+
+from tensordict._contextlib import _DecoratorContextManager
+from tensordict.nn import CompositeDistribution
+
+from tensordict.nn.common import dispatch, TensorDictModule, TensorDictModuleBase
+from tensordict.nn.distributions import Delta, distributions_maps
+from tensordict.nn.sequence import TensorDictSequential
+
+from tensordict.nn.utils import set_skip_existing
+from tensordict.tensordict import TensorDictBase
+from tensordict.utils import NestedKey
+from torch import distributions as D, Tensor
+
+__all__ = ["ProbabilisticTensorDictModule", "ProbabilisticTensorDictSequential"]
+
+
+class InteractionType(Enum):
+    MODE = auto()
+    MEDIAN = auto()
+    MEAN = auto()
+    RANDOM = auto()
+
+    @classmethod
+    def from_str(cls, type_str: str) -> InteractionType:
+        """Return the interaction_type with name matched to the provided string (case insensitive)."""
+        for member_type in cls:
+            if member_type.name == type_str.upper():
+                return member_type
+        raise ValueError(f"The provided interaction type {type_str} is unsupported!")
+
+
+_INTERACTION_TYPE: InteractionType | None = None
+
+
+def _insert_interaction_mode_deprecation_warning(
+    prefix: str = "",
+) -> Callable[[str, Warning, int], None]:
+    return warn(
+        (
+            f"{prefix}interaction_mode is deprecated for naming clarity. "
+            f"Please use {prefix}interaction_type with InteractionType enum instead."
+        ),
+        DeprecationWarning,
+        stacklevel=2,
+    )
+
+
+def interaction_type() -> InteractionType | None:
+    """Returns the current sampling type."""
+    return _INTERACTION_TYPE
+
+
+def interaction_mode() -> str | None:
+    """*Deprecated* Returns the current sampling mode."""
+    _insert_interaction_mode_deprecation_warning()
+    type = interaction_type()
+    return type.name.lower() if type else None
+
+
+class set_interaction_mode(_DecoratorContextManager):
+    """*Deprecated* Sets the sampling mode of all ProbabilisticTDModules to the desired mode.
+
+    Args:
+        mode (str): mode to use when the policy is being called.
+    """
+
+    def __init__(self, mode: str | None = "mode") -> None:
+        _insert_interaction_mode_deprecation_warning("set_")
+        super().__init__()
+        self.mode = InteractionType.from_str(mode) if mode else None
+
+    def clone(self) -> set_interaction_mode:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.mode)
+
+    def __enter__(self) -> None:
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.mode
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class set_interaction_type(_DecoratorContextManager):
+    """Sets all ProbabilisticTDModules sampling to the desired type.
+
+    Args:
+        type (InteractionType): sampling type to use when the policy is being called.
+    """
+
+    def __init__(self, type: InteractionType | None = InteractionType.MODE) -> None:
+        super().__init__()
+        self.type = type
+
+    def clone(self) -> set_interaction_type:
+        # override this method if your children class takes __init__ parameters
+        return self.__class__(self.type)
+
+    def __enter__(self) -> None:
+        global _INTERACTION_TYPE
+        self.prev = _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.type
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _INTERACTION_TYPE
+        _INTERACTION_TYPE = self.prev
+
+
+class ProbabilisticTensorDictModule(TensorDictModuleBase):
+    """A probabilistic TD Module.
+
+    `ProbabilisticTensorDictModule` is a non-parametric module representing a
+    probability distribution. It reads the distribution parameters from an input
+    TensorDict using the specified `in_keys`. The output is sampled given some rule,
+    specified by the input :obj:`default_interaction_type` argument and the
+    :obj:`interaction_type()` global function.
+
+    :obj:`ProbabilisticTensorDictModule` can be used to construct the distribution
+    (through the :obj:`get_dist()` method) and/or sampling from this distribution
+    (through a regular :obj:`__call__()` to the module).
+
+    A :obj:`ProbabilisticTensorDictModule` instance has two main features:
+    - It reads and writes TensorDict objects
+    - It uses a real mapping R^n -> R^m to create a distribution in R^d from
+    which values can be sampled or computed.
+
+    When the :obj:`__call__` / :obj:`forward` method is called, a distribution is
+    created, and a value computed (using the 'mean', 'mode', 'median' attribute or
+    the 'rsample', 'sample' method). The sampling step is skipped if the supplied
+    TensorDict has all of the desired key-value pairs already.
+
+    By default, ProbabilisticTensorDictModule distribution class is a Delta
+    distribution, making ProbabilisticTensorDictModule a simple wrapper around
+    a deterministic mapping function.
+
+    Args:
+        in_keys (NestedKey or list of NestedKey or dict): key(s) that will be read from the
+            input TensorDict and used to build the distribution. Importantly, if it's an
+            list of NestedKey or a NestedKey, the leaf (last element) of those keys must match the keywords used by
+            the distribution class of interest, e.g. :obj:`"loc"` and :obj:`"scale"` for
+            the Normal distribution and similar. If in_keys is a dictionary, the keys
+            are the keys of the distribution and the values are the keys in the
+            tensordict that will get match to the corresponding distribution keys.
+        out_keys (NestedKey or list of NestedKey): keys where the sampled values will be
+            written. Importantly, if these keys are found in the input TensorDict, the
+            sampling step will be skipped.
+        default_interaction_mode (str, optional): *Deprecated* keyword-only argument.
+            Please use default_interaction_type instead.
+        default_interaction_type (InteractionType, optional): keyword-only argument.
+            Default method to be used to retrieve
+            the output value. Should be one of InteractionType: MODE, MEDIAN, MEAN or RANDOM
+            (in which case the value is sampled randomly from the distribution). Default
+            is MODE.
+
+            .. note:: When a sample is drawn, the
+              :class:`ProbabilisticTensorDictModule` instance will
+              first look for the interaction mode dictated by the
+              :func:`~tensordict.nn.probabilistic.interaction_type`
+              global function. If this returns `None` (its default value), then the
+              `default_interaction_type` of the `ProbabilisticTDModule`
+              instance will be used. Note that
+              :class:`~torchrl.collectors.collectors.DataCollectorBase`
+              instances will use `set_interaction_type` to
+              :class:`tensordict.nn.InteractionType.RANDOM` by default.
+
+        distribution_class (Type, optional): keyword-only argument.
+            A :class:`torch.distributions.Distribution` class to
+            be used for sampling.
+            Default is :class:`tensordict.nn.distributions.Delta`.
+        distribution_kwargs (dict, optional): keyword-only argument.
+            Keyword-argument pairs to be passed to the distribution.
+        return_log_prob (bool, optional): keyword-only argument.
+            If ``True``, the log-probability of the
+            distribution sample will be written in the tensordict with the key
+            `log_prob_key`. Default is ``False``.
+        log_prob_key (NestedKey, optional): key where to write the log_prob if return_log_prob = True.
+            Defaults to `'sample_log_prob'`.
+        cache_dist (bool, optional): keyword-only argument.
+            EXPERIMENTAL: if ``True``, the parameters of the
+            distribution (i.e. the output of the module) will be written to the
+            tensordict along with the sample. Those parameters can be used to re-compute
+            the original distribution later on (e.g. to compute the divergence between
+            the distribution used to sample the action and the updated distribution in
+            PPO). Default is ``False``.
+        n_empirical_estimate (int, optional): keyword-only argument.
+            Number of samples to compute the empirical
+            mean when it is not available. Defaults to 1000.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import (
+        ...     ProbabilisticTensorDictModule,
+        ...     ProbabilisticTensorDictSequential,
+        ...     TensorDictModule,
+        ... )
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from tensordict.nn.functional_modules import make_functional
+        >>> from torch.distributions import Normal, Independent
+        >>> td = TensorDict(
+        ...     {"input": torch.randn(3, 4), "hidden": torch.randn(3, 8)}, [3]
+        ... )
+        >>> net = torch.nn.GRUCell(4, 8)
+        >>> module = TensorDictModule(
+        ...     net, in_keys=["input", "hidden"], out_keys=["params"]
+        ... )
+        >>> normal_params = TensorDictModule(
+        ...     NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
+        ... )
+        >>> def IndepNormal(**kwargs):
+        ...     return Independent(Normal(**kwargs), 1)
+        >>> prob_module = ProbabilisticTensorDictModule(
+        ...     in_keys=["loc", "scale"],
+        ...     out_keys=["action"],
+        ...     distribution_class=IndepNormal,
+        ...     return_log_prob=True,
+        ... )
+        >>> td_module = ProbabilisticTensorDictSequential(
+        ...     module, normal_params, prob_module
+        ... )
+        >>> params = TensorDict.from_module(td_module)
+        >>> with params.to_module(td_module):
+        ...     _ = td_module(td)
+        >>> print(td)
+        TensorDict(
+            fields={
+                action: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                hidden: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+        >>> with params.to_module(td_module):
+        ...     dist = td_module.get_dist(td)
+        >>> print(dist)
+        Independent(Normal(loc: torch.Size([3, 4]), scale: torch.Size([3, 4])), 1)
+        >>> # we can also apply the module to the TensorDict with vmap
+        >>> from torch import vmap
+        >>> params = params.expand(4)
+        >>> def func(td, params):
+        ...     with params.to_module(td_module):
+        ...         return td_module(td)
+        >>> td_vmap = vmap(func, (None, 0))(td, params)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                action: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                hidden: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    def __init__(
+        self,
+        in_keys: NestedKey | List[NestedKey] | Dict[str, NestedKey],
+        out_keys: NestedKey | List[NestedKey] | None = None,
+        *,
+        default_interaction_mode: str | None = None,
+        default_interaction_type: InteractionType = InteractionType.MODE,
+        distribution_class: type = Delta,
+        distribution_kwargs: dict | None = None,
+        return_log_prob: bool = False,
+        log_prob_key: Optional[NestedKey] = "sample_log_prob",
+        cache_dist: bool = False,
+        n_empirical_estimate: int = 1000,
+    ) -> None:
+        super().__init__()
+        if isinstance(in_keys, (str, tuple)):
+            in_keys = [in_keys]
+        if isinstance(out_keys, (str, tuple)):
+            out_keys = [out_keys]
+        elif out_keys is None:
+            out_keys = ["_"]
+        if isinstance(in_keys, dict):
+            dist_keys, in_keys = zip(*in_keys.items())
+            if set(map(type, dist_keys)) != {str}:
+                raise ValueError(
+                    f"If in_keys is dict, its keys must be strings matching to the distribution kwargs."
+                    f"{self.__class__.__name__} got {dist_keys}"
+                )
+        else:
+            dist_keys = in_keys
+
+        self.out_keys = out_keys
+        self.in_keys = in_keys
+        self.dist_keys = dist_keys
+        if log_prob_key is None:
+            log_prob_key = "sample_log_prob"
+        self.log_prob_key = log_prob_key
+
+        if default_interaction_mode is not None:
+            _insert_interaction_mode_deprecation_warning("default_")
+            self.default_interaction_type = InteractionType.from_str(
+                default_interaction_mode
+            )
+        else:
+            self.default_interaction_type = default_interaction_type
+
+        if isinstance(distribution_class, str):
+            distribution_class = distributions_maps.get(distribution_class.lower())
+        self.distribution_class = distribution_class
+        self.distribution_kwargs = (
+            distribution_kwargs if distribution_kwargs is not None else {}
+        )
+        self.n_empirical_estimate = n_empirical_estimate
+        self._dist = None
+        self.cache_dist = cache_dist if hasattr(distribution_class, "update") else False
+        self.return_log_prob = return_log_prob
+        if self.return_log_prob and self.log_prob_key not in self.out_keys:
+            self.out_keys.append(self.log_prob_key)
+
+    def get_dist(self, tensordict: TensorDictBase) -> D.Distribution:
+        """Creates a :class:`torch.distribution.Distribution` instance with the parameters provided in the input tensordict."""
+        try:
+            dist_kwargs = {}
+            for dist_key, td_key in zip(self.dist_keys, self.in_keys):
+                if isinstance(dist_key, tuple):
+                    dist_key = dist_key[-1]
+                dist_kwargs[dist_key] = tensordict.get(td_key)
+            dist = self.distribution_class(**dist_kwargs, **self.distribution_kwargs)
+        except TypeError as err:
+            if "an unexpected keyword argument" in str(err):
+                raise TypeError(
+                    "distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match."
+                    f"Got this error message: \n{indent(str(err), 4 * ' ')}\nwith dist_keys={self.dist_keys}"
+                )
+            elif re.search(r"missing.*required positional arguments", str(err)):
+                raise TypeError(
+                    f"TensorDict with keys {tensordict.keys()} does not match the distribution {self.distribution_class} keywords."
+                )
+            else:
+                raise err
+        return dist
+
+    def log_prob(self, tensordict):
+        """Writes the log-probability of the distribution sample."""
+        dist = self.get_dist(tensordict)
+        if isinstance(dist, CompositeDistribution):
+            tensordict = dist.log_prob(tensordict)
+            return tensordict.get("sample_log_prob")
+        else:
+            return dist.log_prob(tensordict.get(self.out_keys[0]))
+
+    @property
+    def SAMPLE_LOG_PROB_KEY(self):
+        warnings.warn(
+            "SAMPLE_LOG_PROB_KEY will be deprecated soon."
+            "Use 'obj.log_prob_key' instead",
+            category=DeprecationWarning,
+        )
+        return self.log_prob_key
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        _requires_sample: bool = True,
+    ) -> TensorDictBase:
+        if tensordict_out is None:
+            tensordict_out = tensordict
+
+        dist = self.get_dist(tensordict)
+        if _requires_sample:
+            out_tensors = self._dist_sample(dist, interaction_type=interaction_type())
+            if isinstance(out_tensors, TensorDictBase):
+                tensordict_out.update(out_tensors)
+                if self.return_log_prob:
+                    tensordict_out = dist.log_prob(tensordict_out)
+            else:
+                if isinstance(out_tensors, Tensor):
+                    out_tensors = (out_tensors,)
+                tensordict_out.update(
+                    {key: value for key, value in zip(self.out_keys, out_tensors)}
+                )
+                if self.return_log_prob:
+                    log_prob = dist.log_prob(*out_tensors)
+                    tensordict_out.set(self.log_prob_key, log_prob)
+        elif self.return_log_prob:
+            out_tensors = [
+                tensordict.get(key) for key in self.out_keys if key != self.log_prob_key
+            ]
+            log_prob = dist.log_prob(*out_tensors)
+            tensordict_out.set(self.log_prob_key, log_prob)
+            # raise RuntimeError(
+            #     "ProbabilisticTensorDictModule.return_log_prob = True is incompatible with settings in which "
+            #     "the submodule is responsible for sampling. To manually gather the log-probability, call first "
+            #     "\n>>> dist, tensordict = tensordict_module.get_dist(tensordict)"
+            #     "\n>>> tensordict.set('sample_log_prob', dist.log_prob(tensordict.get(sample_key))"
+            # )
+        return tensordict_out
+
+    def _dist_sample(
+        self,
+        dist: D.Distribution,
+        interaction_type: InteractionType | None = None,
+    ) -> tuple[Tensor, ...] | Tensor:
+        if interaction_type is None:
+            interaction_type = self.default_interaction_type
+
+        if interaction_type is InteractionType.MODE:
+            try:
+                return dist.mode
+            except AttributeError:
+                raise NotImplementedError(
+                    f"method {type(dist)}.mode is not implemented"
+                )
+
+        elif interaction_type is InteractionType.MEDIAN:
+            try:
+                return dist.median
+            except AttributeError:
+                raise NotImplementedError(
+                    f"method {type(dist)}.median is not implemented"
+                )
+
+        elif interaction_type is InteractionType.MEAN:
+            try:
+                return dist.mean
+            except (AttributeError, NotImplementedError):
+                if dist.has_rsample:
+                    return dist.rsample((self.n_empirical_estimate,)).mean(0)
+                else:
+                    return dist.sample((self.n_empirical_estimate,)).mean(0)
+
+        elif interaction_type is InteractionType.RANDOM:
+            if dist.has_rsample:
+                return dist.rsample()
+            else:
+                return dist.sample()
+        else:
+            raise NotImplementedError(f"unknown interaction_type {interaction_type}")
+
+
+class ProbabilisticTensorDictSequential(TensorDictSequential):
+    """A sequence of TensorDictModules ending in a ProbabilistictTensorDictModule.
+
+    Similarly to :obj:`TensorDictSequential`, but enforces that the final module in the
+    sequence is an :obj:`ProbabilisticTensorDictModule` and also exposes ``get_dist``
+    method to recover the distribution object from the ``ProbabilisticTensorDictModule``
+
+    Args:
+         modules (sequence of TensorDictModules): ordered sequence of TensorDictModule
+            instances, terminating in ProbabilisticTensorDictModule, to be run
+            sequentially.
+         partial_tolerant (bool, optional): if True, the input tensordict can miss some
+            of the input keys. If so, the only module that will be executed are those
+            who can be executed given the keys that are present. Also, if the input
+            tensordict is a lazy stack of tensordicts AND if partial_tolerant is
+            :obj:`True` AND if the stack does not have the required keys, then
+            TensorDictSequential will scan through the sub-tensordicts looking for those
+            that have the required keys, if any.
+
+    """
+
+    def __init__(
+        self,
+        *modules: TensorDictModule | ProbabilisticTensorDictModule,
+        partial_tolerant: bool = False,
+    ) -> None:
+        if len(modules) == 0:
+            raise ValueError(
+                "ProbabilisticTensorDictSequential must consist of zero or more "
+                "TensorDictModules followed by a ProbabilisticTensorDictModule"
+            )
+        if not isinstance(
+            modules[-1],
+            (ProbabilisticTensorDictModule, ProbabilisticTensorDictSequential),
+        ):
+            raise TypeError(
+                "The final module passed to ProbabilisticTensorDictSequential must be "
+                "an instance of ProbabilisticTensorDictModule or another "
+                "ProbabilisticTensorDictSequential"
+            )
+        # if the modules not including the final probabilistic module return the sampled
+        # key we wont be sampling it again, in that case
+        # ProbabilisticTensorDictSequential is presumably used to return the
+        # distribution using `get_dist` or to sample log_probabilities
+        _, out_keys = self._compute_in_and_out_keys(modules[:-1])
+        self._requires_sample = modules[-1].out_keys[0] not in set(out_keys)
+        super().__init__(*modules, partial_tolerant=partial_tolerant)
+
+    @property
+    def det_part(self):
+        if not hasattr(self, "_det_part"):
+            # we use a list to avoid having the submodules listed in module.modules()
+            self._det_part = [TensorDictSequential(*self.module[:-1])]
+        return self._det_part[0]
+
+    def get_dist_params(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> tuple[D.Distribution, TensorDictBase]:
+        tds = self.det_part
+        type = interaction_type()
+        if type is None:
+            type = self.module[-1].default_interaction_type
+        with set_interaction_type(type):
+            return tds(tensordict, tensordict_out, **kwargs)
+
+    def get_dist(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> D.Distribution:
+        """Get the distribution that results from passing the input tensordict through the sequence, and then using the resulting parameters."""
+        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
+        return self.build_dist_from_params(tensordict_out)
+
+    def log_prob(
+        self, tensordict, tensordict_out: TensorDictBase | None = None, **kwargs
+    ):
+        tensordict_out = self.get_dist_params(
+            tensordict,
+            tensordict_out,
+            **kwargs,
+        )
+        return self.module[-1].log_prob(tensordict_out)
+
+    def build_dist_from_params(self, tensordict: TensorDictBase) -> D.Distribution:
+        """Construct a distribution from the input parameters. Other modules in the sequence are not evaluated.
+
+        This method will look for the last ProbabilisticTensorDictModule contained in the
+        sequence and use it to build the distribution.
+
+        """
+        dest_module = None
+        for module in reversed(list(self.modules())):
+            if isinstance(module, ProbabilisticTensorDictModule):
+                dest_module = module
+                break
+        if dest_module is None:
+            raise RuntimeError(
+                "Could not find any ProbabilisticTensorDictModule in the sequence."
+            )
+        return dest_module.get_dist(tensordict)
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> TensorDictBase:
+        tensordict_out = self.get_dist_params(tensordict, tensordict_out, **kwargs)
+        return self.module[-1](tensordict_out, _requires_sample=self._requires_sample)
```

## tensordict/nn/sequence.py

 * *Ordering differences only*

```diff
@@ -1,451 +1,451 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import logging
-
-from copy import deepcopy
-from typing import Any, Iterable
-
-from tensordict.nn.utils import set_skip_existing
-
-_has_functorch = False
-try:
-    import functorch
-
-    _has_functorch = True
-except ImportError:
-    logging.info(
-        "failed to import functorch. TensorDict's features that do not require "
-        "functional programming should work, but functionality and performance "
-        "may be affected. Consider installing functorch and/or upgrating pytorch."
-    )
-    FUNCTORCH_ERROR = "functorch not installed. Consider installing functorch to use this functionality."
-
-
-from tensordict._tensordict import unravel_key_list
-from tensordict.nn.common import dispatch, TensorDictModule
-from tensordict.tensordict import LazyStackedTensorDict, TensorDictBase
-from tensordict.utils import NestedKey
-from torch import nn
-
-__all__ = ["TensorDictSequential"]
-
-
-class TensorDictSequential(TensorDictModule):
-    """A sequence of TensorDictModules.
-
-    Similarly to :obj:`nn.Sequence` which passes a tensor through a chain of mappings that read and write a single tensor
-    each, this module will read and write over a tensordict by querying each of the input modules.
-    When calling a :obj:`TensorDictSequencial` instance with a functional module, it is expected that the parameter lists (and
-    buffers) will be concatenated in a single list.
-
-    Args:
-         modules (iterable of TensorDictModules): ordered sequence of TensorDictModule instances to be run sequentially.
-         partial_tolerant (bool, optional): if True, the input tensordict can miss some of the input keys.
-            If so, the only module that will be executed are those who can be executed given the keys that
-            are present.
-            Also, if the input tensordict is a lazy stack of tensordicts AND if partial_tolerant is :obj:`True` AND if the
-            stack does not have the required keys, then TensorDictSequential will scan through the sub-tensordicts
-            looking for those that have the required keys, if any.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
-        >>> torch.manual_seed(0)
-        >>> module = TensorDictSequential(
-        ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["x+1"]),
-        ...     TensorDictModule(nn.Linear(3, 4), in_keys=["x+1"], out_keys=["w*(x+1)+b"]),
-        ... )
-        >>> # with tensordict input
-        >>> print(module(TensorDict({"x": torch.zeros(3)}, [])))
-        TensorDict(
-            fields={
-                w*(x+1)+b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
-                x+1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
-                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-        >>> # with tensor input: returns all the output keys in the order of the modules, ie "x+1" and "w*(x+1)+b"
-        >>> module(x=torch.zeros(3))
-        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
-        >>> module(torch.zeros(3))
-        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
-
-    TensorDictSequence supports functional, modular and vmap coding:
-    Examples:
-        >>> import torch
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import (
-        ...     ProbabilisticTensorDictModule,
-        ...     ProbabilisticTensorDictSequential,
-        ...     TensorDictModule,
-        ...     TensorDictSequential,
-        ... )
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from tensordict.nn.functional_modules import make_functional
-        >>> from torch.distributions import Normal
-        >>> td = TensorDict({"input": torch.randn(3, 4)}, [3,])
-        >>> net1 = torch.nn.Linear(4, 8)
-        >>> module1 = TensorDictModule(net1, in_keys=["input"], out_keys=["params"])
-        >>> normal_params = TensorDictModule(
-        ...      NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
-        ...  )
-        >>> td_module1 = ProbabilisticTensorDictSequential(
-        ...     module1,
-        ...     normal_params,
-        ...     ProbabilisticTensorDictModule(
-        ...         in_keys=["loc", "scale"],
-        ...         out_keys=["hidden"],
-        ...         distribution_class=Normal,
-        ...         return_log_prob=True,
-        ...     )
-        ... )
-        >>> module2 = torch.nn.Linear(4, 8)
-        >>> td_module2 = TensorDictModule(
-        ...    module=module2, in_keys=["hidden"], out_keys=["output"]
-        ... )
-        >>> td_module = TensorDictSequential(td_module1, td_module2)
-        >>> params = TensorDict.from_module(td_module)
-        >>> with params.to_module(td_module):
-        ...     _ = td_module(td)
-        >>> print(td)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([3]),
-            device=None,
-            is_shared=False)
-
-    In the vmap case:
-        >>> from torch import vmap
-        >>> params = params.expand(4)
-        >>> def func(td, params):
-        ...     with params.to_module(td_module):
-        ...         return td_module(td)
-        >>> td_vmap = vmap(func, (None, 0))(td, params)
-        >>> print(td_vmap)
-        TensorDict(
-            fields={
-                hidden: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
-                sample_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([4, 3]),
-            device=None,
-            is_shared=False)
-
-    """
-
-    module: nn.ModuleList
-
-    def __init__(
-        self,
-        *modules: TensorDictModule,
-        partial_tolerant: bool = False,
-    ) -> None:
-        in_keys, out_keys = self._compute_in_and_out_keys(modules)
-
-        super().__init__(
-            module=nn.ModuleList(list(modules)), in_keys=in_keys, out_keys=out_keys
-        )
-
-        self.partial_tolerant = partial_tolerant
-
-    def _compute_in_and_out_keys(
-        self, modules: list[TensorDictModule]
-    ) -> tuple[list[NestedKey], list[NestedKey]]:
-        in_keys = []
-        out_keys = []
-        for module in modules:
-            # we sometimes use in_keys to select keys of a tensordict that are
-            # necessary to run a TensorDictModule. If a key is an intermediary in
-            # the chain, there is no reason why it should belong to the input
-            # TensorDict.
-            for in_key in module.in_keys:
-                if in_key not in (out_keys + in_keys):
-                    in_keys.append(in_key)
-            out_keys += module.out_keys
-
-        out_keys = [
-            out_key
-            for i, out_key in enumerate(out_keys)
-            if out_key not in out_keys[i + 1 :]
-        ]
-        return in_keys, out_keys
-
-    @staticmethod
-    def _find_functional_module(module: TensorDictModule) -> nn.Module:
-        if not _has_functorch:
-            raise ImportError(FUNCTORCH_ERROR)
-        fmodule = module
-        while not isinstance(
-            fmodule, (functorch.FunctionalModule, functorch.FunctionalModuleWithBuffers)
-        ):
-            try:
-                fmodule = fmodule.module
-            except AttributeError:
-                raise AttributeError(
-                    f"couldn't find a functional module in module of type {type(module)}"
-                )
-        return fmodule
-
-    def select_subsequence(
-        self,
-        in_keys: Iterable[NestedKey] | None = None,
-        out_keys: Iterable[NestedKey] | None = None,
-    ) -> TensorDictSequential:
-        """Returns a new TensorDictSequential with only the modules that are necessary to compute the given output keys with the given input keys.
-
-        Args:
-            in_keys: input keys of the subsequence we want to select.
-                All the keys absent from ``in_keys`` will be considered as
-                non-relevant, and modules that *just* take these keys as inputs
-                will be discarded.
-                The resulting sequential module will follow the pattern "all
-                the modules which output will be affected by a different value
-                for any in <in_keys>".
-                If none is provided, the module's ``in_keys`` are assumed.
-            out_keys: output keys of the subsequence we want to select.
-                Only the modules that are necessary to get the ``out_keys``
-                will be found in the resulting sequence.
-                The resulting sequential module will follow the pattern "all
-                the modules that condition the value or <out_keys> entries."
-                If none is provided, the module's ``out_keys`` are assumed.
-
-        Returns:
-            A new TensorDictSequential with only the modules that are necessary acording to the given input and output keys.
-
-        Examples:
-            >>> from tensordict.nn import TensorDictSequential as Seq, TensorDictModule as Mod
-            >>> idn = lambda x: x
-            >>> module = Seq(
-            ...     Mod(idn, in_keys=["a"], out_keys=["b"]),
-            ...     Mod(idn, in_keys=["b"], out_keys=["c"]),
-            ...     Mod(idn, in_keys=["c"], out_keys=["d"]),
-            ...     Mod(idn, in_keys=["a"], out_keys=["e"]),
-            ... )
-            >>> # select all modules whose output depend on "a"
-            >>> module.select_subsequence(in_keys=["a"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['b'])
-                  (1): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['c'])
-                  (2): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['c'],
-                      out_keys=['d'])
-                  (3): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['e'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['b', 'c', 'd', 'e'])
-            >>> # select all modules whose output depend on "c"
-            >>> module.select_subsequence(in_keys=["c"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['c'],
-                      out_keys=['d'])
-                ),
-                device=cpu,
-                in_keys=['c'],
-                out_keys=['d'])
-            >>> # select all modules that affect the value of "c"
-            >>> module.select_subsequence(out_keys=["c"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['b'])
-                  (1): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['c'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['b', 'c'])
-            >>> # select all modules that affect the value of "e"
-            >>> module.select_subsequence(out_keys=["e"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictModule(
-                      module=<function <lambda> at 0x126ed1ca0>,
-                      device=cpu,
-                      in_keys=['a'],
-                      out_keys=['e'])
-                ),
-                device=cpu,
-                in_keys=['a'],
-                out_keys=['e'])
-
-        This method propagates to nested sequential:
-
-            >>> module = Seq(
-            ...     Seq(
-            ...         Mod(idn, in_keys=["a"], out_keys=["b"]),
-            ...         Mod(idn, in_keys=["b"], out_keys=["c"]),
-            ...     ),
-            ...     Seq(
-            ...         Mod(idn, in_keys=["b"], out_keys=["d"]),
-            ...         Mod(idn, in_keys=["d"], out_keys=["e"]),
-            ...     ),
-            ... )
-            >>> # select submodules whose output will be affected by a change in "b" or "d" AND which output is "e"
-            >>> module.select_subsequence(in_keys=["b", "d"], out_keys=["e"])
-            TensorDictSequential(
-                module=ModuleList(
-                  (0): TensorDictSequential(
-                      module=ModuleList(
-                        (0): TensorDictModule(
-                            module=<function <lambda> at 0x129efae50>,
-                            device=cpu,
-                            in_keys=['b'],
-                            out_keys=['d'])
-                        (1): TensorDictModule(
-                            module=<function <lambda> at 0x129efae50>,
-                            device=cpu,
-                            in_keys=['d'],
-                            out_keys=['e'])
-                      ),
-                      device=cpu,
-                      in_keys=['b'],
-                      out_keys=['d', 'e'])
-                ),
-                device=cpu,
-                in_keys=['b'],
-                out_keys=['d', 'e'])
-
-        """
-        if in_keys is None:
-            in_keys = deepcopy(self.in_keys)
-        in_keys = unravel_key_list(in_keys)
-        if out_keys is None:
-            out_keys = deepcopy(self.out_keys)
-        out_keys = unravel_key_list(out_keys)
-
-        module_list = list(self.module)
-        id_to_keep = set(range(len(module_list)))
-        for i, module in enumerate(module_list):
-            if (
-                type(module) is TensorDictSequential
-            ):  # no isinstance because we don't want to mess up subclasses
-                try:
-                    module = module_list[i] = module.select_subsequence(in_keys=in_keys)
-                except ValueError:
-                    # then the module can be removed
-                    id_to_keep.remove(i)
-                    continue
-
-            if all(key in in_keys for key in module.in_keys):
-                in_keys.extend(module.out_keys)
-            else:
-                id_to_keep.remove(i)
-        for i, module in reversed(list(enumerate(module_list))):
-            if i in id_to_keep:
-                if any(key in out_keys for key in module.out_keys):
-                    if (
-                        type(module) is TensorDictSequential
-                    ):  # no isinstance because we don't want to mess up subclasses
-                        module = module_list[i] = module.select_subsequence(
-                            out_keys=out_keys
-                        )
-                    out_keys.extend(module.in_keys)
-                else:
-                    id_to_keep.remove(i)
-        id_to_keep = sorted(id_to_keep)
-
-        modules = [module_list[i] for i in id_to_keep]
-
-        if modules == []:
-            raise ValueError(
-                "No modules left after selection. Make sure that in_keys and out_keys are coherent."
-            )
-
-        return self.__class__(*modules)
-
-    def _run_module(
-        self,
-        module: TensorDictModule,
-        tensordict: TensorDictBase,
-        **kwargs: Any,
-    ) -> Any:
-        if not self.partial_tolerant or all(
-            key in tensordict.keys(include_nested=True) for key in module.in_keys
-        ):
-            tensordict = module(tensordict, **kwargs)
-        elif self.partial_tolerant and isinstance(tensordict, LazyStackedTensorDict):
-            for sub_td in tensordict.tensordicts:
-                if all(
-                    key in sub_td.keys(include_nested=True) for key in module.in_keys
-                ):
-                    module(sub_td, **kwargs)
-        return tensordict
-
-    @dispatch(auto_batch_size=False)
-    @set_skip_existing(None)
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs: Any,
-    ) -> TensorDictBase:
-        if not len(kwargs):
-            for module in self.module:
-                tensordict = self._run_module(module, tensordict, **kwargs)
-        else:
-            raise RuntimeError(
-                f"TensorDictSequential does not support keyword arguments other than 'tensordict_out' or in_keys: {self.in_keys}. Got {kwargs.keys()} instead."
-            )
-        if tensordict_out is not None:
-            tensordict_out.update(tensordict, inplace=True)
-            return tensordict_out
-        return tensordict
-
-    def __len__(self) -> int:
-        return len(self.module)
-
-    def __getitem__(self, index: int | slice) -> TensorDictModule:
-        if isinstance(index, int):
-            return self.module.__getitem__(index)
-        else:
-            return self.__class__(*self.module.__getitem__(index))
-
-    def __setitem__(self, index: int, tensordict_module: TensorDictModule) -> None:
-        return self.module.__setitem__(idx=index, module=tensordict_module)
-
-    def __delitem__(self, index: int | slice) -> None:
-        self.module.__delitem__(idx=index)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import logging
+
+from copy import deepcopy
+from typing import Any, Iterable
+
+from tensordict.nn.utils import set_skip_existing
+
+_has_functorch = False
+try:
+    import functorch
+
+    _has_functorch = True
+except ImportError:
+    logging.info(
+        "failed to import functorch. TensorDict's features that do not require "
+        "functional programming should work, but functionality and performance "
+        "may be affected. Consider installing functorch and/or upgrating pytorch."
+    )
+    FUNCTORCH_ERROR = "functorch not installed. Consider installing functorch to use this functionality."
+
+
+from tensordict._tensordict import unravel_key_list
+from tensordict.nn.common import dispatch, TensorDictModule
+from tensordict.tensordict import LazyStackedTensorDict, TensorDictBase
+from tensordict.utils import NestedKey
+from torch import nn
+
+__all__ = ["TensorDictSequential"]
+
+
+class TensorDictSequential(TensorDictModule):
+    """A sequence of TensorDictModules.
+
+    Similarly to :obj:`nn.Sequence` which passes a tensor through a chain of mappings that read and write a single tensor
+    each, this module will read and write over a tensordict by querying each of the input modules.
+    When calling a :obj:`TensorDictSequencial` instance with a functional module, it is expected that the parameter lists (and
+    buffers) will be concatenated in a single list.
+
+    Args:
+         modules (iterable of TensorDictModules): ordered sequence of TensorDictModule instances to be run sequentially.
+         partial_tolerant (bool, optional): if True, the input tensordict can miss some of the input keys.
+            If so, the only module that will be executed are those who can be executed given the keys that
+            are present.
+            Also, if the input tensordict is a lazy stack of tensordicts AND if partial_tolerant is :obj:`True` AND if the
+            stack does not have the required keys, then TensorDictSequential will scan through the sub-tensordicts
+            looking for those that have the required keys, if any.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import TensorDictModule, TensorDictSequential
+        >>> torch.manual_seed(0)
+        >>> module = TensorDictSequential(
+        ...     TensorDictModule(lambda x: x+1, in_keys=["x"], out_keys=["x+1"]),
+        ...     TensorDictModule(nn.Linear(3, 4), in_keys=["x+1"], out_keys=["w*(x+1)+b"]),
+        ... )
+        >>> # with tensordict input
+        >>> print(module(TensorDict({"x": torch.zeros(3)}, [])))
+        TensorDict(
+            fields={
+                w*(x+1)+b: Tensor(shape=torch.Size([4]), device=cpu, dtype=torch.float32, is_shared=False),
+                x+1: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False),
+                x: Tensor(shape=torch.Size([3]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> # with tensor input: returns all the output keys in the order of the modules, ie "x+1" and "w*(x+1)+b"
+        >>> module(x=torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+        >>> module(torch.zeros(3))
+        (tensor([1., 1., 1.]), tensor([-0.7214, -0.8748,  0.1571, -0.1138], grad_fn=<AddBackward0>))
+
+    TensorDictSequence supports functional, modular and vmap coding:
+    Examples:
+        >>> import torch
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import (
+        ...     ProbabilisticTensorDictModule,
+        ...     ProbabilisticTensorDictSequential,
+        ...     TensorDictModule,
+        ...     TensorDictSequential,
+        ... )
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from tensordict.nn.functional_modules import make_functional
+        >>> from torch.distributions import Normal
+        >>> td = TensorDict({"input": torch.randn(3, 4)}, [3,])
+        >>> net1 = torch.nn.Linear(4, 8)
+        >>> module1 = TensorDictModule(net1, in_keys=["input"], out_keys=["params"])
+        >>> normal_params = TensorDictModule(
+        ...      NormalParamExtractor(), in_keys=["params"], out_keys=["loc", "scale"]
+        ...  )
+        >>> td_module1 = ProbabilisticTensorDictSequential(
+        ...     module1,
+        ...     normal_params,
+        ...     ProbabilisticTensorDictModule(
+        ...         in_keys=["loc", "scale"],
+        ...         out_keys=["hidden"],
+        ...         distribution_class=Normal,
+        ...         return_log_prob=True,
+        ...     )
+        ... )
+        >>> module2 = torch.nn.Linear(4, 8)
+        >>> td_module2 = TensorDictModule(
+        ...    module=module2, in_keys=["hidden"], out_keys=["output"]
+        ... )
+        >>> td_module = TensorDictSequential(td_module1, td_module2)
+        >>> params = TensorDict.from_module(td_module)
+        >>> with params.to_module(td_module):
+        ...     _ = td_module(td)
+        >>> print(td)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([3]),
+            device=None,
+            is_shared=False)
+
+    In the vmap case:
+        >>> from torch import vmap
+        >>> params = params.expand(4)
+        >>> def func(td, params):
+        ...     with params.to_module(td_module):
+        ...         return td_module(td)
+        >>> td_vmap = vmap(func, (None, 0))(td, params)
+        >>> print(td_vmap)
+        TensorDict(
+            fields={
+                hidden: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                input: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                loc: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                output: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                params: Tensor(shape=torch.Size([4, 3, 8]), device=cpu, dtype=torch.float32, is_shared=False),
+                sample_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                scale: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([4, 3]),
+            device=None,
+            is_shared=False)
+
+    """
+
+    module: nn.ModuleList
+
+    def __init__(
+        self,
+        *modules: TensorDictModule,
+        partial_tolerant: bool = False,
+    ) -> None:
+        in_keys, out_keys = self._compute_in_and_out_keys(modules)
+
+        super().__init__(
+            module=nn.ModuleList(list(modules)), in_keys=in_keys, out_keys=out_keys
+        )
+
+        self.partial_tolerant = partial_tolerant
+
+    def _compute_in_and_out_keys(
+        self, modules: list[TensorDictModule]
+    ) -> tuple[list[NestedKey], list[NestedKey]]:
+        in_keys = []
+        out_keys = []
+        for module in modules:
+            # we sometimes use in_keys to select keys of a tensordict that are
+            # necessary to run a TensorDictModule. If a key is an intermediary in
+            # the chain, there is no reason why it should belong to the input
+            # TensorDict.
+            for in_key in module.in_keys:
+                if in_key not in (out_keys + in_keys):
+                    in_keys.append(in_key)
+            out_keys += module.out_keys
+
+        out_keys = [
+            out_key
+            for i, out_key in enumerate(out_keys)
+            if out_key not in out_keys[i + 1 :]
+        ]
+        return in_keys, out_keys
+
+    @staticmethod
+    def _find_functional_module(module: TensorDictModule) -> nn.Module:
+        if not _has_functorch:
+            raise ImportError(FUNCTORCH_ERROR)
+        fmodule = module
+        while not isinstance(
+            fmodule, (functorch.FunctionalModule, functorch.FunctionalModuleWithBuffers)
+        ):
+            try:
+                fmodule = fmodule.module
+            except AttributeError:
+                raise AttributeError(
+                    f"couldn't find a functional module in module of type {type(module)}"
+                )
+        return fmodule
+
+    def select_subsequence(
+        self,
+        in_keys: Iterable[NestedKey] | None = None,
+        out_keys: Iterable[NestedKey] | None = None,
+    ) -> TensorDictSequential:
+        """Returns a new TensorDictSequential with only the modules that are necessary to compute the given output keys with the given input keys.
+
+        Args:
+            in_keys: input keys of the subsequence we want to select.
+                All the keys absent from ``in_keys`` will be considered as
+                non-relevant, and modules that *just* take these keys as inputs
+                will be discarded.
+                The resulting sequential module will follow the pattern "all
+                the modules which output will be affected by a different value
+                for any in <in_keys>".
+                If none is provided, the module's ``in_keys`` are assumed.
+            out_keys: output keys of the subsequence we want to select.
+                Only the modules that are necessary to get the ``out_keys``
+                will be found in the resulting sequence.
+                The resulting sequential module will follow the pattern "all
+                the modules that condition the value or <out_keys> entries."
+                If none is provided, the module's ``out_keys`` are assumed.
+
+        Returns:
+            A new TensorDictSequential with only the modules that are necessary acording to the given input and output keys.
+
+        Examples:
+            >>> from tensordict.nn import TensorDictSequential as Seq, TensorDictModule as Mod
+            >>> idn = lambda x: x
+            >>> module = Seq(
+            ...     Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...     Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     Mod(idn, in_keys=["c"], out_keys=["d"]),
+            ...     Mod(idn, in_keys=["a"], out_keys=["e"]),
+            ... )
+            >>> # select all modules whose output depend on "a"
+            >>> module.select_subsequence(in_keys=["a"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                  (2): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                  (3): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c', 'd', 'e'])
+            >>> # select all modules whose output depend on "c"
+            >>> module.select_subsequence(in_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['c'],
+                      out_keys=['d'])
+                ),
+                device=cpu,
+                in_keys=['c'],
+                out_keys=['d'])
+            >>> # select all modules that affect the value of "c"
+            >>> module.select_subsequence(out_keys=["c"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['b'])
+                  (1): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['c'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['b', 'c'])
+            >>> # select all modules that affect the value of "e"
+            >>> module.select_subsequence(out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictModule(
+                      module=<function <lambda> at 0x126ed1ca0>,
+                      device=cpu,
+                      in_keys=['a'],
+                      out_keys=['e'])
+                ),
+                device=cpu,
+                in_keys=['a'],
+                out_keys=['e'])
+
+        This method propagates to nested sequential:
+
+            >>> module = Seq(
+            ...     Seq(
+            ...         Mod(idn, in_keys=["a"], out_keys=["b"]),
+            ...         Mod(idn, in_keys=["b"], out_keys=["c"]),
+            ...     ),
+            ...     Seq(
+            ...         Mod(idn, in_keys=["b"], out_keys=["d"]),
+            ...         Mod(idn, in_keys=["d"], out_keys=["e"]),
+            ...     ),
+            ... )
+            >>> # select submodules whose output will be affected by a change in "b" or "d" AND which output is "e"
+            >>> module.select_subsequence(in_keys=["b", "d"], out_keys=["e"])
+            TensorDictSequential(
+                module=ModuleList(
+                  (0): TensorDictSequential(
+                      module=ModuleList(
+                        (0): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['b'],
+                            out_keys=['d'])
+                        (1): TensorDictModule(
+                            module=<function <lambda> at 0x129efae50>,
+                            device=cpu,
+                            in_keys=['d'],
+                            out_keys=['e'])
+                      ),
+                      device=cpu,
+                      in_keys=['b'],
+                      out_keys=['d', 'e'])
+                ),
+                device=cpu,
+                in_keys=['b'],
+                out_keys=['d', 'e'])
+
+        """
+        if in_keys is None:
+            in_keys = deepcopy(self.in_keys)
+        in_keys = unravel_key_list(in_keys)
+        if out_keys is None:
+            out_keys = deepcopy(self.out_keys)
+        out_keys = unravel_key_list(out_keys)
+
+        module_list = list(self.module)
+        id_to_keep = set(range(len(module_list)))
+        for i, module in enumerate(module_list):
+            if (
+                type(module) is TensorDictSequential
+            ):  # no isinstance because we don't want to mess up subclasses
+                try:
+                    module = module_list[i] = module.select_subsequence(in_keys=in_keys)
+                except ValueError:
+                    # then the module can be removed
+                    id_to_keep.remove(i)
+                    continue
+
+            if all(key in in_keys for key in module.in_keys):
+                in_keys.extend(module.out_keys)
+            else:
+                id_to_keep.remove(i)
+        for i, module in reversed(list(enumerate(module_list))):
+            if i in id_to_keep:
+                if any(key in out_keys for key in module.out_keys):
+                    if (
+                        type(module) is TensorDictSequential
+                    ):  # no isinstance because we don't want to mess up subclasses
+                        module = module_list[i] = module.select_subsequence(
+                            out_keys=out_keys
+                        )
+                    out_keys.extend(module.in_keys)
+                else:
+                    id_to_keep.remove(i)
+        id_to_keep = sorted(id_to_keep)
+
+        modules = [module_list[i] for i in id_to_keep]
+
+        if modules == []:
+            raise ValueError(
+                "No modules left after selection. Make sure that in_keys and out_keys are coherent."
+            )
+
+        return self.__class__(*modules)
+
+    def _run_module(
+        self,
+        module: TensorDictModule,
+        tensordict: TensorDictBase,
+        **kwargs: Any,
+    ) -> Any:
+        if not self.partial_tolerant or all(
+            key in tensordict.keys(include_nested=True) for key in module.in_keys
+        ):
+            tensordict = module(tensordict, **kwargs)
+        elif self.partial_tolerant and isinstance(tensordict, LazyStackedTensorDict):
+            for sub_td in tensordict.tensordicts:
+                if all(
+                    key in sub_td.keys(include_nested=True) for key in module.in_keys
+                ):
+                    module(sub_td, **kwargs)
+        return tensordict
+
+    @dispatch(auto_batch_size=False)
+    @set_skip_existing(None)
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs: Any,
+    ) -> TensorDictBase:
+        if not len(kwargs):
+            for module in self.module:
+                tensordict = self._run_module(module, tensordict, **kwargs)
+        else:
+            raise RuntimeError(
+                f"TensorDictSequential does not support keyword arguments other than 'tensordict_out' or in_keys: {self.in_keys}. Got {kwargs.keys()} instead."
+            )
+        if tensordict_out is not None:
+            tensordict_out.update(tensordict, inplace=True)
+            return tensordict_out
+        return tensordict
+
+    def __len__(self) -> int:
+        return len(self.module)
+
+    def __getitem__(self, index: int | slice) -> TensorDictModule:
+        if isinstance(index, int):
+            return self.module.__getitem__(index)
+        else:
+            return self.__class__(*self.module.__getitem__(index))
+
+    def __setitem__(self, index: int, tensordict_module: TensorDictModule) -> None:
+        return self.module.__setitem__(idx=index, module=tensordict_module)
+
+    def __delitem__(self, index: int | slice) -> None:
+        self.module.__delitem__(idx=index)
```

## tensordict/nn/utils.py

 * *Ordering differences only*

```diff
@@ -1,347 +1,347 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import functools
-import inspect
-import os
-from distutils.util import strtobool
-from typing import Any, Callable
-
-import torch
-from torch import nn
-
-AUTO_MAKE_FUNCTIONAL = strtobool(os.environ.get("AUTO_MAKE_FUNCTIONAL", "False"))
-
-
-DISPATCH_TDNN_MODULES = strtobool(os.environ.get("DISPATCH_TDNN_MODULES", "True"))
-
-__all__ = ["mappings", "inv_softplus", "biased_softplus"]
-
-_SKIP_EXISTING = False
-
-from tensordict._contextlib import _DecoratorContextManager
-
-
-def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:
-    """Inverse softplus function.
-
-    Args:
-        bias (float or tensor): the value to be softplus-inverted.
-    """
-    is_tensor = True
-    if not isinstance(bias, torch.Tensor):
-        is_tensor = False
-        bias = torch.tensor(bias)
-    out = bias.expm1().clamp_min(1e-6).log()
-    if not is_tensor and out.numel() == 1:
-        return out.item()
-    return out
-
-
-class biased_softplus(nn.Module):
-    """A biased softplus module.
-
-    The bias indicates the value that is to be returned when a zero-tensor is
-    passed through the transform.
-
-    Args:
-        bias (scalar): 'bias' of the softplus transform. If bias=1.0, then a _bias shift will be computed such that
-            softplus(0.0 + _bias) = bias.
-        min_val (scalar): minimum value of the transform.
-            default: 0.1
-    """
-
-    def __init__(self, bias: float, min_val: float = 0.01) -> None:
-        super().__init__()
-        self.bias = inv_softplus(bias - min_val)
-        self.min_val = min_val
-
-    def forward(self, x: torch.Tensor) -> torch.Tensor:
-        return torch.nn.functional.softplus(x + self.bias) + self.min_val
-
-
-def mappings(key: str) -> Callable:
-    """Given an input string, returns a surjective function f(x): R -> R^+.
-
-    Args:
-        key (str): one of "softplus", "exp", "relu", "expln",
-            or "biased_softplus". If the key beggins with "biased_softplus",
-            then it needs to take the following form:
-            ```"biased_softplus_{bias}"``` where ```bias``` can be converted to a floating point number that will be used to bias the softplus function.
-            Alternatively, the ```"biased_softplus_{bias}_{min_val}"``` syntax can be used. In that case, the additional ```min_val``` term is a floating point
-            number that will be used to encode the minimum value of the softplus transform.
-            In practice, the equation used is softplus(x + bias) + min_val, where bias and min_val are values computed such that the conditions above are met.
-
-    Returns:
-         a Callable
-
-    """
-    _mappings: dict[str, Callable] = {
-        "softplus": torch.nn.functional.softplus,
-        "exp": torch.exp,
-        "relu": torch.relu,
-        "biased_softplus": biased_softplus(1.0),
-    }
-    if key in _mappings:
-        return _mappings[key]
-    elif key.startswith("biased_softplus"):
-        stripped_key = key.split("_")
-        if len(stripped_key) == 3:
-            return biased_softplus(float(stripped_key[-1]))
-        elif len(stripped_key) == 4:
-            return biased_softplus(
-                float(stripped_key[-2]), min_val=float(stripped_key[-1])
-            )
-        else:
-            raise ValueError(f"Invalid number of args in  {key}")
-
-    else:
-        raise NotImplementedError(f"Unknown mapping {key}")
-
-
-class set_skip_existing(_DecoratorContextManager):
-    """A context manager for skipping existing nodes in a TensorDict graph.
-
-    When used as a context manager, it will set the `skip_existing()` value
-    to the ``mode`` indicated, leaving the user able to code up methods that
-    will check the global value and execute the code accordingly.
-
-    When used as a method decorator, it will check the tensordict input keys
-    and if the ``skip_existing()`` call returns ``True``, it will skip the method
-    if all the output keys are already present.
-    This not not expected to be used as a decorator for methods that do not
-    respect the following signature: ``def fun(self, tensordict, *args, **kwargs)``.
-
-    Args:
-        mode (bool, optional):
-            If ``True``, it indicates that existing entries in the graph
-            won't be overwritten, unless they are only partially present. :func:`~.skip_existing`
-            will return ``True``.
-            If ``False``, no check will be performed.
-            If ``None``, the value of :func:`~.skip_existing` will not be
-            changed. This is intended to be used exclusively for decorating
-            methods and allow their behaviour to depend on the same class
-            when used as a context manager (see example below).
-            Defaults to ``True``.
-        in_key_attr (str, optional): the name of the input key list attribute
-            in the module's method being decorated. Defaults to ``in_keys``.
-        out_key_attr (str, optional): the name of the output key list attribute
-            in the module's method being decorated. Defaults to ``out_keys``.
-
-    Examples:
-        >>> with set_skip_existing():
-        ...     if skip_existing():
-        ...         print("True")
-        ...     else:
-        ...         print("False")
-        ...
-        True
-        >>> print("calling from outside:", skip_existing())
-        calling from outside: False
-
-    This class can also be used as a decorator:
-    Examples:
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
-        >>> class MyModule(TensorDictModuleBase):
-        ...     in_keys = []
-        ...     out_keys = ["out"]
-        ...     @set_skip_existing()
-        ...     def forward(self, tensordict):
-        ...         print("hello")
-        ...         tensordict.set("out", torch.zeros(()))
-        ...         return tensordict
-        >>> module = MyModule()
-        >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
-        TensorDict(
-            fields={
-                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-        >>> module(TensorDict({}, []))  # prints hello
-        hello
-        TensorDict(
-            fields={
-                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-            batch_size=torch.Size([]),
-            device=None,
-            is_shared=False)
-
-    Decorating a method with the mode set to ``None`` is useful whenever one
-    wants ot let the context manager take care of skipping things from the outside:
-        >>> from tensordict import TensorDict
-        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
-        >>> class MyModule(TensorDictModuleBase):
-        ...     in_keys = []
-        ...     out_keys = ["out"]
-        ...     @set_skip_existing(None)
-        ...     def forward(self, tensordict):
-        ...         print("hello")
-        ...         tensordict.set("out", torch.zeros(()))
-        ...         return tensordict
-        >>> module = MyModule()
-        >>> _ = module(TensorDict({"out": torch.zeros(())}, []))  # prints "hello"
-        hello
-        >>> with set_skip_existing(True):
-        ...     _ = module(TensorDict({"out": torch.zeros(())}, []))  # no print
-
-
-    .. note::
-        To allow for modules to have the same input and output keys and not
-        mistakenly ignoring subgraphs, ``@set_skip_existing(True)`` will be
-        deactivated whenever the output keys are also the input keys:
-
-            >>> class MyModule(TensorDictModuleBase):
-            ...     in_keys = ["out"]
-            ...     out_keys = ["out"]
-            ...     @set_skip_existing()
-            ...     def forward(self, tensordict):
-            ...         print("calling the method!")
-            ...         return tensordict
-            ...
-            >>> module = MyModule()
-            >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
-            calling the method!
-            TensorDict(
-                fields={
-                    out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
-                batch_size=torch.Size([]),
-                device=None,
-                is_shared=False)
-
-
-    """
-
-    def __init__(
-        self, mode: bool | None = True, in_key_attr="in_keys", out_key_attr="out_keys"
-    ):
-        self.mode = mode
-        self.in_key_attr = in_key_attr
-        self.out_key_attr = out_key_attr
-        self._called = False
-
-    def clone(self) -> set_skip_existing:
-        # override this method if your children class takes __init__ parameters
-        out = self.__class__(self.mode)
-        out._called = self._called
-        return out
-
-    def __call__(self, func: Callable):
-
-        self._called = True
-
-        # sanity check
-        for i, key in enumerate(inspect.signature(func).parameters):
-            if i == 0:
-                # skip self
-                continue
-            if key != "tensordict":
-                raise RuntimeError(
-                    "the first argument of the wrapped function must be "
-                    "named 'tensordict'."
-                )
-            break
-
-        @functools.wraps(func)
-        def wrapper(_self, tensordict, *args: Any, **kwargs: Any) -> Any:
-            in_keys = getattr(_self, self.in_key_attr)
-            out_keys = getattr(_self, self.out_key_attr)
-            # we use skip_existing to allow users to override the mode internally
-            if (
-                skip_existing()
-                and all(key in tensordict.keys(True) for key in out_keys)
-                and not any(key in out_keys for key in in_keys)
-            ):
-                return tensordict
-            return func(_self, tensordict, *args, **kwargs)
-
-        return super().__call__(wrapper)
-
-    def __enter__(self) -> None:
-        global _SKIP_EXISTING
-        self.prev = _SKIP_EXISTING
-        if self.mode is not None:
-            _SKIP_EXISTING = self.mode
-        elif not self._called:
-            raise RuntimeError(
-                f"It seems you are using {self.__class__.__name__} as a decorator with ``None`` input. "
-                f"This behaviour is not allowed."
-            )
-
-    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
-        global _SKIP_EXISTING
-        _SKIP_EXISTING = self.prev
-
-
-def skip_existing():
-    """Returns whether or not existing entries in a tensordict should be re-computed by a module."""
-    return _SKIP_EXISTING
-
-
-def _rebuild_buffer(data, requires_grad, backward_hooks):
-    buffer = Buffer(data, requires_grad)
-    # NB: This line exists only for backwards compatibility; the
-    # general expectation is that backward_hooks is an empty
-    # OrderedDict.  See Note [Don't serialize hooks]
-    buffer._backward_hooks = backward_hooks
-
-    return buffer
-
-
-# For backward compatibility in imports
-from tensordict.utils import Buffer  # noqa
-
-
-def _auto_make_functional():
-    """Returns ``True`` if TensorDictModuleBase subclasses are automatically made functional with the old API."""
-    global AUTO_MAKE_FUNCTIONAL
-    return AUTO_MAKE_FUNCTIONAL
-
-
-class _set_auto_make_functional(_DecoratorContextManager):
-    """Controls if TensorDictModule subclasses should be made functional automatically with the old API."""
-
-    def __init__(self, mode):
-        self.mode = mode
-
-    def clone(self):
-        return self.__class__(self.mode)
-
-    def __enter__(self):
-        global AUTO_MAKE_FUNCTIONAL
-        self._saved_mode = AUTO_MAKE_FUNCTIONAL
-        AUTO_MAKE_FUNCTIONAL = self.mode
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        global AUTO_MAKE_FUNCTIONAL
-        AUTO_MAKE_FUNCTIONAL = self._saved_mode
-
-
-def _dispatch_td_nn_modules():
-    """Returns ``True`` if @dispatch should be used. Not using dispatch is faster and also better compatible with torch.compile."""
-    global DISPATCH_TDNN_MODULES
-    return DISPATCH_TDNN_MODULES
-
-
-class _set_dispatch_td_nn_modules(_DecoratorContextManager):
-    """Controls whether @dispatch should be used. Not using dispatch is faster and also better compatible with torch.compile."""
-
-    def __init__(self, mode):
-        self.mode = mode
-
-    def clone(self):
-        return self.__class__(self.mode)
-
-    def __enter__(self):
-        global DISPATCH_TDNN_MODULES
-        self._saved_mode = DISPATCH_TDNN_MODULES
-        DISPATCH_TDNN_MODULES = self.mode
-
-    def __exit__(self, exc_type, exc_val, exc_tb):
-        global DISPATCH_TDNN_MODULES
-        DISPATCH_TDNN_MODULES = self._saved_mode
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import functools
+import inspect
+import os
+from distutils.util import strtobool
+from typing import Any, Callable
+
+import torch
+from torch import nn
+
+AUTO_MAKE_FUNCTIONAL = strtobool(os.environ.get("AUTO_MAKE_FUNCTIONAL", "False"))
+
+
+DISPATCH_TDNN_MODULES = strtobool(os.environ.get("DISPATCH_TDNN_MODULES", "True"))
+
+__all__ = ["mappings", "inv_softplus", "biased_softplus"]
+
+_SKIP_EXISTING = False
+
+from tensordict._contextlib import _DecoratorContextManager
+
+
+def inv_softplus(bias: float | torch.Tensor) -> float | torch.Tensor:
+    """Inverse softplus function.
+
+    Args:
+        bias (float or tensor): the value to be softplus-inverted.
+    """
+    is_tensor = True
+    if not isinstance(bias, torch.Tensor):
+        is_tensor = False
+        bias = torch.tensor(bias)
+    out = bias.expm1().clamp_min(1e-6).log()
+    if not is_tensor and out.numel() == 1:
+        return out.item()
+    return out
+
+
+class biased_softplus(nn.Module):
+    """A biased softplus module.
+
+    The bias indicates the value that is to be returned when a zero-tensor is
+    passed through the transform.
+
+    Args:
+        bias (scalar): 'bias' of the softplus transform. If bias=1.0, then a _bias shift will be computed such that
+            softplus(0.0 + _bias) = bias.
+        min_val (scalar): minimum value of the transform.
+            default: 0.1
+    """
+
+    def __init__(self, bias: float, min_val: float = 0.01) -> None:
+        super().__init__()
+        self.bias = inv_softplus(bias - min_val)
+        self.min_val = min_val
+
+    def forward(self, x: torch.Tensor) -> torch.Tensor:
+        return torch.nn.functional.softplus(x + self.bias) + self.min_val
+
+
+def mappings(key: str) -> Callable:
+    """Given an input string, returns a surjective function f(x): R -> R^+.
+
+    Args:
+        key (str): one of "softplus", "exp", "relu", "expln",
+            or "biased_softplus". If the key beggins with "biased_softplus",
+            then it needs to take the following form:
+            ```"biased_softplus_{bias}"``` where ```bias``` can be converted to a floating point number that will be used to bias the softplus function.
+            Alternatively, the ```"biased_softplus_{bias}_{min_val}"``` syntax can be used. In that case, the additional ```min_val``` term is a floating point
+            number that will be used to encode the minimum value of the softplus transform.
+            In practice, the equation used is softplus(x + bias) + min_val, where bias and min_val are values computed such that the conditions above are met.
+
+    Returns:
+         a Callable
+
+    """
+    _mappings: dict[str, Callable] = {
+        "softplus": torch.nn.functional.softplus,
+        "exp": torch.exp,
+        "relu": torch.relu,
+        "biased_softplus": biased_softplus(1.0),
+    }
+    if key in _mappings:
+        return _mappings[key]
+    elif key.startswith("biased_softplus"):
+        stripped_key = key.split("_")
+        if len(stripped_key) == 3:
+            return biased_softplus(float(stripped_key[-1]))
+        elif len(stripped_key) == 4:
+            return biased_softplus(
+                float(stripped_key[-2]), min_val=float(stripped_key[-1])
+            )
+        else:
+            raise ValueError(f"Invalid number of args in  {key}")
+
+    else:
+        raise NotImplementedError(f"Unknown mapping {key}")
+
+
+class set_skip_existing(_DecoratorContextManager):
+    """A context manager for skipping existing nodes in a TensorDict graph.
+
+    When used as a context manager, it will set the `skip_existing()` value
+    to the ``mode`` indicated, leaving the user able to code up methods that
+    will check the global value and execute the code accordingly.
+
+    When used as a method decorator, it will check the tensordict input keys
+    and if the ``skip_existing()`` call returns ``True``, it will skip the method
+    if all the output keys are already present.
+    This not not expected to be used as a decorator for methods that do not
+    respect the following signature: ``def fun(self, tensordict, *args, **kwargs)``.
+
+    Args:
+        mode (bool, optional):
+            If ``True``, it indicates that existing entries in the graph
+            won't be overwritten, unless they are only partially present. :func:`~.skip_existing`
+            will return ``True``.
+            If ``False``, no check will be performed.
+            If ``None``, the value of :func:`~.skip_existing` will not be
+            changed. This is intended to be used exclusively for decorating
+            methods and allow their behaviour to depend on the same class
+            when used as a context manager (see example below).
+            Defaults to ``True``.
+        in_key_attr (str, optional): the name of the input key list attribute
+            in the module's method being decorated. Defaults to ``in_keys``.
+        out_key_attr (str, optional): the name of the output key list attribute
+            in the module's method being decorated. Defaults to ``out_keys``.
+
+    Examples:
+        >>> with set_skip_existing():
+        ...     if skip_existing():
+        ...         print("True")
+        ...     else:
+        ...         print("False")
+        ...
+        True
+        >>> print("calling from outside:", skip_existing())
+        calling from outside: False
+
+    This class can also be used as a decorator:
+    Examples:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing()
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+        >>> module(TensorDict({}, []))  # prints hello
+        hello
+        TensorDict(
+            fields={
+                out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+            batch_size=torch.Size([]),
+            device=None,
+            is_shared=False)
+
+    Decorating a method with the mode set to ``None`` is useful whenever one
+    wants ot let the context manager take care of skipping things from the outside:
+        >>> from tensordict import TensorDict
+        >>> from tensordict.nn import set_skip_existing, skip_existing, TensorDictModuleBase
+        >>> class MyModule(TensorDictModuleBase):
+        ...     in_keys = []
+        ...     out_keys = ["out"]
+        ...     @set_skip_existing(None)
+        ...     def forward(self, tensordict):
+        ...         print("hello")
+        ...         tensordict.set("out", torch.zeros(()))
+        ...         return tensordict
+        >>> module = MyModule()
+        >>> _ = module(TensorDict({"out": torch.zeros(())}, []))  # prints "hello"
+        hello
+        >>> with set_skip_existing(True):
+        ...     _ = module(TensorDict({"out": torch.zeros(())}, []))  # no print
+
+
+    .. note::
+        To allow for modules to have the same input and output keys and not
+        mistakenly ignoring subgraphs, ``@set_skip_existing(True)`` will be
+        deactivated whenever the output keys are also the input keys:
+
+            >>> class MyModule(TensorDictModuleBase):
+            ...     in_keys = ["out"]
+            ...     out_keys = ["out"]
+            ...     @set_skip_existing()
+            ...     def forward(self, tensordict):
+            ...         print("calling the method!")
+            ...         return tensordict
+            ...
+            >>> module = MyModule()
+            >>> module(TensorDict({"out": torch.zeros(())}, []))  # does not print anything
+            calling the method!
+            TensorDict(
+                fields={
+                    out: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},
+                batch_size=torch.Size([]),
+                device=None,
+                is_shared=False)
+
+
+    """
+
+    def __init__(
+        self, mode: bool | None = True, in_key_attr="in_keys", out_key_attr="out_keys"
+    ):
+        self.mode = mode
+        self.in_key_attr = in_key_attr
+        self.out_key_attr = out_key_attr
+        self._called = False
+
+    def clone(self) -> set_skip_existing:
+        # override this method if your children class takes __init__ parameters
+        out = self.__class__(self.mode)
+        out._called = self._called
+        return out
+
+    def __call__(self, func: Callable):
+
+        self._called = True
+
+        # sanity check
+        for i, key in enumerate(inspect.signature(func).parameters):
+            if i == 0:
+                # skip self
+                continue
+            if key != "tensordict":
+                raise RuntimeError(
+                    "the first argument of the wrapped function must be "
+                    "named 'tensordict'."
+                )
+            break
+
+        @functools.wraps(func)
+        def wrapper(_self, tensordict, *args: Any, **kwargs: Any) -> Any:
+            in_keys = getattr(_self, self.in_key_attr)
+            out_keys = getattr(_self, self.out_key_attr)
+            # we use skip_existing to allow users to override the mode internally
+            if (
+                skip_existing()
+                and all(key in tensordict.keys(True) for key in out_keys)
+                and not any(key in out_keys for key in in_keys)
+            ):
+                return tensordict
+            return func(_self, tensordict, *args, **kwargs)
+
+        return super().__call__(wrapper)
+
+    def __enter__(self) -> None:
+        global _SKIP_EXISTING
+        self.prev = _SKIP_EXISTING
+        if self.mode is not None:
+            _SKIP_EXISTING = self.mode
+        elif not self._called:
+            raise RuntimeError(
+                f"It seems you are using {self.__class__.__name__} as a decorator with ``None`` input. "
+                f"This behaviour is not allowed."
+            )
+
+    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:
+        global _SKIP_EXISTING
+        _SKIP_EXISTING = self.prev
+
+
+def skip_existing():
+    """Returns whether or not existing entries in a tensordict should be re-computed by a module."""
+    return _SKIP_EXISTING
+
+
+def _rebuild_buffer(data, requires_grad, backward_hooks):
+    buffer = Buffer(data, requires_grad)
+    # NB: This line exists only for backwards compatibility; the
+    # general expectation is that backward_hooks is an empty
+    # OrderedDict.  See Note [Don't serialize hooks]
+    buffer._backward_hooks = backward_hooks
+
+    return buffer
+
+
+# For backward compatibility in imports
+from tensordict.utils import Buffer  # noqa
+
+
+def _auto_make_functional():
+    """Returns ``True`` if TensorDictModuleBase subclasses are automatically made functional with the old API."""
+    global AUTO_MAKE_FUNCTIONAL
+    return AUTO_MAKE_FUNCTIONAL
+
+
+class _set_auto_make_functional(_DecoratorContextManager):
+    """Controls if TensorDictModule subclasses should be made functional automatically with the old API."""
+
+    def __init__(self, mode):
+        self.mode = mode
+
+    def clone(self):
+        return self.__class__(self.mode)
+
+    def __enter__(self):
+        global AUTO_MAKE_FUNCTIONAL
+        self._saved_mode = AUTO_MAKE_FUNCTIONAL
+        AUTO_MAKE_FUNCTIONAL = self.mode
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        global AUTO_MAKE_FUNCTIONAL
+        AUTO_MAKE_FUNCTIONAL = self._saved_mode
+
+
+def _dispatch_td_nn_modules():
+    """Returns ``True`` if @dispatch should be used. Not using dispatch is faster and also better compatible with torch.compile."""
+    global DISPATCH_TDNN_MODULES
+    return DISPATCH_TDNN_MODULES
+
+
+class _set_dispatch_td_nn_modules(_DecoratorContextManager):
+    """Controls whether @dispatch should be used. Not using dispatch is faster and also better compatible with torch.compile."""
+
+    def __init__(self, mode):
+        self.mode = mode
+
+    def clone(self):
+        return self.__class__(self.mode)
+
+    def __enter__(self):
+        global DISPATCH_TDNN_MODULES
+        self._saved_mode = DISPATCH_TDNN_MODULES
+        DISPATCH_TDNN_MODULES = self.mode
+
+    def __exit__(self, exc_type, exc_val, exc_tb):
+        global DISPATCH_TDNN_MODULES
+        DISPATCH_TDNN_MODULES = self._saved_mode
```

## tensordict/nn/distributions/__init__.py

 * *Ordering differences only*

```diff
@@ -1,21 +1,21 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from tensordict.nn.distributions import continuous, discrete
-
-from tensordict.nn.distributions.composite import CompositeDistribution
-from tensordict.nn.distributions.continuous import (
-    AddStateIndependentNormalScale,
-    Delta,
-    NormalParamExtractor,
-    NormalParamWrapper,
-)
-from tensordict.nn.distributions.discrete import OneHotCategorical, rand_one_hot
-from tensordict.nn.distributions.truncated_normal import TruncatedNormal
-
-distributions_maps = {
-    distribution_class.lower(): eval(distribution_class)
-    for distribution_class in (*continuous.__all__, *discrete.__all__)
-}
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from tensordict.nn.distributions import continuous, discrete
+
+from tensordict.nn.distributions.composite import CompositeDistribution
+from tensordict.nn.distributions.continuous import (
+    AddStateIndependentNormalScale,
+    Delta,
+    NormalParamExtractor,
+    NormalParamWrapper,
+)
+from tensordict.nn.distributions.discrete import OneHotCategorical, rand_one_hot
+from tensordict.nn.distributions.truncated_normal import TruncatedNormal
+
+distributions_maps = {
+    distribution_class.lower(): eval(distribution_class)
+    for distribution_class in (*continuous.__all__, *discrete.__all__)
+}
```

## tensordict/nn/distributions/composite.py

 * *Ordering differences only*

```diff
@@ -1,160 +1,160 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-import torch
-from tensordict import TensorDict, TensorDictBase
-from tensordict._tensordict import unravel_keys
-from tensordict.utils import NestedKey
-from torch import distributions as d
-
-
-class CompositeDistribution(d.Distribution):
-    """A composition of distributions.
-
-    Groups distributions together with the TensorDict interface. All methods
-    (``log_prob``, ``cdf``, ``icdf``, ``rsample``, ``sample`` etc.) will return a
-    tensordict, possibly modified in-place if the input was a tensordict.
-
-    Args:
-        params (TensorDictBase): a nested key-tensor map where the root entries
-            point to the sample names, and the leaves are the distribution parameters.
-            Entry names must match those of ``distribution_map``.
-
-        distribution_map (Dict[NestedKey, Type[torch.distribution.Distribution]]):
-            indicated the distribution types to be used. The names of the distributions
-            will match the names of the samples in the tensordict.
-
-    Keyword Arguments:
-        extra_kwargs (Dict[NestedKey, Dict]): a possibly incomplete dictionary of
-            extra keyword arguments for the distributions to be built.
-
-    .. note:: In this distribution class, the batch-size of the input tensordict containing the params
-        (``params``) is indicative of the batch_shape of the distribution. For instance,
-        the ``"sample_log_prob"`` entry resulting from a call to ``log_prob``
-        will be of the shape of the params (+ any supplementary batch dimension).
-
-    Examples:
-        >>> params = TensorDict({
-        ...     "cont": {"loc": torch.randn(3, 4), "scale": torch.rand(3, 4)},
-        ...     ("nested", "disc"): {"logits": torch.randn(3, 10)}
-        ... }, [3])
-        >>> dist = CompositeDistribution(params,
-        ...     distribution_map={"cont": d.Normal, ("nested", "disc"): d.Categorical})
-        >>> sample = dist.sample((4,))
-        >>> sample = dist.log_prob(sample)
-        >>> print(sample)
-        TensorDict(
-            fields={
-                cont: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                cont_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
-                nested: TensorDict(
-                    fields={
-                        disc: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.int64, is_shared=False),
-                        disc_log_prob: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
-                    batch_size=torch.Size([4]),
-                    device=None,
-                    is_shared=False)},
-            batch_size=torch.Size([4]),
-            device=None,
-            is_shared=False)
-    """
-
-    def __init__(self, params: TensorDictBase, distribution_map, *, extra_kwargs=None):
-        self._batch_shape = params.shape
-        if extra_kwargs is None:
-            extra_kwargs = {}
-        dists = {}
-        for name, dist_class in distribution_map.items():
-            dist_params = params.get(name, None)
-            kwargs = extra_kwargs.get(name, {})
-            if dist_params is None:
-                raise KeyError
-            dist = dist_class(**dist_params, **kwargs)
-            dists[name] = dist
-        self.dists = dists
-
-    def sample(self, shape=None) -> TensorDictBase:
-        if shape is None:
-            shape = torch.Size([])
-        samples = {name: dist.sample(shape) for name, dist in self.dists.items()}
-        return TensorDict(
-            samples,
-            shape + self.batch_shape,
-        )
-
-    @property
-    def mode(self) -> TensorDictBase:
-        samples = {name: dist.mode for name, dist in self.dists.items()}
-        return TensorDict(
-            samples,
-            self.batch_shape,
-        )
-
-    @property
-    def mean(self) -> TensorDictBase:
-        samples = {name: dist.mean for name, dist in self.dists.items()}
-        return TensorDict(
-            samples,
-            self.batch_shape,
-        )
-
-    def rsample(self, shape=None) -> TensorDictBase:
-        if shape is None:
-            shape = torch.Size([])
-        return TensorDict(
-            {name: dist.rsample(shape) for name, dist in self.dists.items()},
-            shape + self.batch_shape,
-        )
-
-    def log_prob(self, sample: TensorDictBase) -> TensorDictBase:
-        """Writes a ``<sample>_log_prob entry`` for each sample in the input tensordit, along with a ``"sample_log_prob"`` entry with the summed log-prob."""
-        slp = 0.0
-        d = {}
-        for name, dist in self.dists.items():
-            d[_add_suffix(name, "_log_prob")] = lp = dist.log_prob(sample.get(name))
-            while lp.ndim > sample.ndim:
-                lp = lp.sum(-1)
-            slp = slp + lp
-        d["sample_log_prob"] = slp
-        sample.update(d)
-        return sample
-
-    def cdf(self, sample: TensorDictBase) -> TensorDictBase:
-        cdfs = {
-            _add_suffix(name, "_cdf"): dist.cdf(sample.get(name))
-            for name, dist in self.dists.items()
-        }
-        sample.update(cdfs)
-        return sample
-
-    def icdf(self, sample: TensorDictBase) -> TensorDictBase:
-        """Computes the inverse CDF.
-
-        Requires the input tensordict to have one of `<sample_name>+'_cdf'` entry
-        or a `<sample_name>` entry.
-
-        Args:
-            sample (TensorDictBase): a tensordict containing `<sample>_log_prob` where
-                `<sample>` is the name of the sample provided during construction.
-        """
-        for name, dist in self.dists.items():
-            prob = sample.get(_add_suffix(name, "_cdf"), None)
-            if prob is None:
-                try:
-                    prob = self.cdf(sample.get(name))
-                except KeyError:
-                    raise KeyError(
-                        f"Neither {name} nor {name + '_cdf'} could be found in the sampled tensordict. Make sure one of these is available to icdf."
-                    )
-            icdf = dist.icdf(prob)
-            sample.set(_add_suffix(name, "_icdf"), icdf)
-        return sample
-
-
-def _add_suffix(key: NestedKey, suffix: str):
-    key = unravel_keys(key)
-    if isinstance(key, str):
-        return key + suffix
-    return key[:-1] + (key[-1] + suffix,)
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+import torch
+from tensordict import TensorDict, TensorDictBase
+from tensordict._tensordict import unravel_keys
+from tensordict.utils import NestedKey
+from torch import distributions as d
+
+
+class CompositeDistribution(d.Distribution):
+    """A composition of distributions.
+
+    Groups distributions together with the TensorDict interface. All methods
+    (``log_prob``, ``cdf``, ``icdf``, ``rsample``, ``sample`` etc.) will return a
+    tensordict, possibly modified in-place if the input was a tensordict.
+
+    Args:
+        params (TensorDictBase): a nested key-tensor map where the root entries
+            point to the sample names, and the leaves are the distribution parameters.
+            Entry names must match those of ``distribution_map``.
+
+        distribution_map (Dict[NestedKey, Type[torch.distribution.Distribution]]):
+            indicated the distribution types to be used. The names of the distributions
+            will match the names of the samples in the tensordict.
+
+    Keyword Arguments:
+        extra_kwargs (Dict[NestedKey, Dict]): a possibly incomplete dictionary of
+            extra keyword arguments for the distributions to be built.
+
+    .. note:: In this distribution class, the batch-size of the input tensordict containing the params
+        (``params``) is indicative of the batch_shape of the distribution. For instance,
+        the ``"sample_log_prob"`` entry resulting from a call to ``log_prob``
+        will be of the shape of the params (+ any supplementary batch dimension).
+
+    Examples:
+        >>> params = TensorDict({
+        ...     "cont": {"loc": torch.randn(3, 4), "scale": torch.rand(3, 4)},
+        ...     ("nested", "disc"): {"logits": torch.randn(3, 10)}
+        ... }, [3])
+        >>> dist = CompositeDistribution(params,
+        ...     distribution_map={"cont": d.Normal, ("nested", "disc"): d.Categorical})
+        >>> sample = dist.sample((4,))
+        >>> sample = dist.log_prob(sample)
+        >>> print(sample)
+        TensorDict(
+            fields={
+                cont: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                cont_log_prob: Tensor(shape=torch.Size([4, 3, 4]), device=cpu, dtype=torch.float32, is_shared=False),
+                nested: TensorDict(
+                    fields={
+                        disc: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.int64, is_shared=False),
+                        disc_log_prob: Tensor(shape=torch.Size([4, 3]), device=cpu, dtype=torch.float32, is_shared=False)},
+                    batch_size=torch.Size([4]),
+                    device=None,
+                    is_shared=False)},
+            batch_size=torch.Size([4]),
+            device=None,
+            is_shared=False)
+    """
+
+    def __init__(self, params: TensorDictBase, distribution_map, *, extra_kwargs=None):
+        self._batch_shape = params.shape
+        if extra_kwargs is None:
+            extra_kwargs = {}
+        dists = {}
+        for name, dist_class in distribution_map.items():
+            dist_params = params.get(name, None)
+            kwargs = extra_kwargs.get(name, {})
+            if dist_params is None:
+                raise KeyError
+            dist = dist_class(**dist_params, **kwargs)
+            dists[name] = dist
+        self.dists = dists
+
+    def sample(self, shape=None) -> TensorDictBase:
+        if shape is None:
+            shape = torch.Size([])
+        samples = {name: dist.sample(shape) for name, dist in self.dists.items()}
+        return TensorDict(
+            samples,
+            shape + self.batch_shape,
+        )
+
+    @property
+    def mode(self) -> TensorDictBase:
+        samples = {name: dist.mode for name, dist in self.dists.items()}
+        return TensorDict(
+            samples,
+            self.batch_shape,
+        )
+
+    @property
+    def mean(self) -> TensorDictBase:
+        samples = {name: dist.mean for name, dist in self.dists.items()}
+        return TensorDict(
+            samples,
+            self.batch_shape,
+        )
+
+    def rsample(self, shape=None) -> TensorDictBase:
+        if shape is None:
+            shape = torch.Size([])
+        return TensorDict(
+            {name: dist.rsample(shape) for name, dist in self.dists.items()},
+            shape + self.batch_shape,
+        )
+
+    def log_prob(self, sample: TensorDictBase) -> TensorDictBase:
+        """Writes a ``<sample>_log_prob entry`` for each sample in the input tensordit, along with a ``"sample_log_prob"`` entry with the summed log-prob."""
+        slp = 0.0
+        d = {}
+        for name, dist in self.dists.items():
+            d[_add_suffix(name, "_log_prob")] = lp = dist.log_prob(sample.get(name))
+            while lp.ndim > sample.ndim:
+                lp = lp.sum(-1)
+            slp = slp + lp
+        d["sample_log_prob"] = slp
+        sample.update(d)
+        return sample
+
+    def cdf(self, sample: TensorDictBase) -> TensorDictBase:
+        cdfs = {
+            _add_suffix(name, "_cdf"): dist.cdf(sample.get(name))
+            for name, dist in self.dists.items()
+        }
+        sample.update(cdfs)
+        return sample
+
+    def icdf(self, sample: TensorDictBase) -> TensorDictBase:
+        """Computes the inverse CDF.
+
+        Requires the input tensordict to have one of `<sample_name>+'_cdf'` entry
+        or a `<sample_name>` entry.
+
+        Args:
+            sample (TensorDictBase): a tensordict containing `<sample>_log_prob` where
+                `<sample>` is the name of the sample provided during construction.
+        """
+        for name, dist in self.dists.items():
+            prob = sample.get(_add_suffix(name, "_cdf"), None)
+            if prob is None:
+                try:
+                    prob = self.cdf(sample.get(name))
+                except KeyError:
+                    raise KeyError(
+                        f"Neither {name} nor {name + '_cdf'} could be found in the sampled tensordict. Make sure one of these is available to icdf."
+                    )
+            icdf = dist.icdf(prob)
+            sample.set(_add_suffix(name, "_icdf"), icdf)
+        return sample
+
+
+def _add_suffix(key: NestedKey, suffix: str):
+    key = unravel_keys(key)
+    if isinstance(key, str):
+        return key + suffix
+    return key[:-1] + (key[-1] + suffix,)
```

## tensordict/nn/distributions/continuous.py

 * *Ordering differences only*

```diff
@@ -1,266 +1,266 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-from numbers import Number
-from typing import Sequence, Union
-
-import numpy as np
-
-import torch
-from tensordict.nn.utils import mappings
-from torch import distributions as D, nn
-
-__all__ = [
-    "NormalParamExtractor",
-    "NormalParamWrapper",
-    "AddStateIndependentNormalScale",
-    "Delta",
-]
-
-# speeds up distribution construction
-D.Distribution.set_default_validate_args(False)
-
-
-class NormalParamWrapper(nn.Module):
-    """A wrapper for normal distribution parameters.
-
-    Args:
-        operator (nn.Module): operator whose output will be transformed_in in location and scale parameters
-        scale_mapping (str, optional): positive mapping function to be used with the std.
-            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
-            choices: "softplus", "exp", "relu", "biased_softplus_1";
-        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
-
-    Examples:
-        >>> from torch import nn
-        >>> import torch
-        >>> module = nn.Linear(3, 4)
-        >>> module_normal = NormalParamWrapper(module)
-        >>> tensor = torch.randn(3)
-        >>> loc, scale = module_normal(tensor)
-        >>> print(loc.shape, scale.shape)
-        torch.Size([2]) torch.Size([2])
-        >>> assert (scale > 0).all()
-        >>> # with modules that return more than one tensor
-        >>> module = nn.LSTM(3, 4)
-        >>> module_normal = NormalParamWrapper(module)
-        >>> tensor = torch.randn(4, 2, 3)
-        >>> loc, scale, others = module_normal(tensor)
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
-        >>> assert (scale > 0).all()
-
-    """
-
-    def __init__(
-        self,
-        operator: nn.Module,
-        scale_mapping: str = "biased_softplus_1.0",
-        scale_lb: Number = 1e-4,
-    ) -> None:
-        super().__init__()
-        self.operator = operator
-        self.scale_mapping = scale_mapping
-        self.scale_lb = scale_lb
-
-    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
-        net_output = self.operator(*tensors)
-        others = ()
-        if not isinstance(net_output, torch.Tensor):
-            net_output, *others = net_output
-        loc, scale = net_output.chunk(2, -1)
-        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
-        return (loc, scale, *others)
-
-
-class NormalParamExtractor(nn.Module):
-    """A non-parametric nn.Module that splits its input into loc and scale parameters.
-
-    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
-
-    Args:
-        scale_mapping (str, optional): positive mapping function to be used with the std.
-            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
-            choices: "softplus", "exp", "relu", "biased_softplus_1";
-        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
-
-    Examples:
-        >>> import torch
-        >>> from tensordict.nn.distributions import NormalParamExtractor
-        >>> from torch import nn
-        >>> module = nn.Linear(3, 4)
-        >>> normal_params = NormalParamExtractor()
-        >>> tensor = torch.randn(3)
-        >>> loc, scale = normal_params(module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([2]) torch.Size([2])
-        >>> assert (scale > 0).all()
-        >>> # with modules that return more than one tensor
-        >>> module = nn.LSTM(3, 4)
-        >>> tensor = torch.randn(4, 2, 3)
-        >>> loc, scale, others = normal_params(*module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
-        >>> assert (scale > 0).all()
-
-    """
-
-    def __init__(
-        self,
-        scale_mapping: str = "biased_softplus_1.0",
-        scale_lb: Number = 1e-4,
-    ) -> None:
-        super().__init__()
-        self.scale_mapping = scale_mapping
-        self.scale_lb = scale_lb
-
-    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
-        tensor, *others = tensors
-        loc, scale = tensor.chunk(2, -1)
-        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
-        return (loc, scale, *others)
-
-
-class AddStateIndependentNormalScale(torch.nn.Module):
-    """A nn.Module that adds trainable state-independent scale parameters.
-
-    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
-
-    Args:
-        scale_mapping (str, optional): positive mapping function to be used with the std.
-            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
-            choices: "softplus", "exp", "relu", "biased_softplus_1";
-        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
-
-    Examples:
-        >>> from torch import nn
-        >>> import torch
-        >>> num_outputs = 4
-        >>> module = nn.Linear(3, num_outputs)
-        >>> module_normal = AddStateIndependentNormalScale(num_outputs)
-        >>> tensor = torch.randn(3)
-        >>> loc, scale = module_normal(module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4]) torch.Size([4])
-        >>> assert (scale > 0).all()
-        >>> # with modules that return more than one tensor
-        >>> module = nn.LSTM(3, num_outputs)
-        >>> module_normal = AddStateIndependentNormalScale(num_outputs)
-        >>> tensor = torch.randn(4, 2, 3)
-        >>> loc, scale, others = module_normal(*module(tensor))
-        >>> print(loc.shape, scale.shape)
-        torch.Size([4, 2, 4]) torch.Size([4, 2, 4])
-        >>> assert (scale > 0).all()
-    """
-
-    def __init__(
-        self,
-        scale_shape: Union[torch.Size, int, tuple],
-        scale_mapping: str = "exp",
-        scale_lb: Number = 1e-4,
-    ) -> None:
-
-        super().__init__()
-        self.scale_lb = scale_lb
-        if isinstance(scale_shape, int):
-            scale_shape = (scale_shape,)
-        self.scale_shape = scale_shape
-        self.scale_mapping = scale_mapping
-        self.state_independent_scale = torch.nn.Parameter(torch.zeros(scale_shape))
-
-    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
-        loc, *others = tensors
-
-        if self.scale_shape != loc.shape[-len(self.scale_shape) :]:
-            raise RuntimeError(
-                f"Last dimensions of loc ({loc.shape[-len(self.scale_shape):]}) do not match the number of dimensions "
-                f"in scale ({self.state_independent_scale.shape})"
-            )
-
-        scale = torch.zeros_like(loc) + self.state_independent_scale
-        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
-
-        return (loc, scale, *others)
-
-
-class Delta(D.Distribution):
-    """Delta distribution.
-
-    Args:
-        param (torch.Tensor): parameter of the delta distribution;
-        atol (number, optional): absolute tolerance to consider that a tensor matches the distribution parameter;
-            Default is 1e-6
-        rtol (number, optional): relative tolerance to consider that a tensor matches the distribution parameter;
-            Default is 1e-6
-        batch_shape (torch.Size, optional): batch shape;
-        event_shape (torch.Size, optional): shape of the outcome.
-
-    """
-
-    arg_constraints: dict = {}
-
-    def __init__(
-        self,
-        param: torch.Tensor,
-        atol: float = 1e-6,
-        rtol: float = 1e-6,
-        batch_shape: torch.Size | Sequence[int] | None = None,
-        event_shape: torch.Size | Sequence[int] | None = None,
-    ) -> None:
-        if batch_shape is None:
-            batch_shape = torch.Size([])
-        if event_shape is None:
-            event_shape = torch.Size([])
-        self.update(param)
-        self.atol = atol
-        self.rtol = rtol
-        if not len(batch_shape) and not len(event_shape):
-            batch_shape = param.shape[:-1]
-            event_shape = param.shape[-1:]
-        super().__init__(batch_shape=batch_shape, event_shape=event_shape)
-
-    def update(self, param: torch.Tensor) -> None:
-        self.param = param
-
-    def _is_equal(self, value: torch.Tensor) -> torch.Tensor:
-        param = self.param.expand_as(value)
-        is_equal = abs(value - param) < self.atol + self.rtol * abs(param)
-        for i in range(-1, -len(self.event_shape) - 1, -1):
-            is_equal = is_equal.all(i)
-        return is_equal
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        is_equal = self._is_equal(value)
-        out = torch.zeros_like(is_equal, dtype=value.dtype)
-        out.masked_fill_(is_equal, np.inf)
-        out.masked_fill_(~is_equal, -np.inf)
-        return out
-
-    @torch.no_grad()
-    def sample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        return self.param.expand((*sample_shape, *self.param.shape))
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        return self.param.expand((*sample_shape, *self.param.shape))
-
-    @property
-    def mode(self) -> torch.Tensor:
-        return self.param
-
-    @property
-    def mean(self) -> torch.Tensor:
-        return self.param
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+from numbers import Number
+from typing import Sequence, Union
+
+import numpy as np
+
+import torch
+from tensordict.nn.utils import mappings
+from torch import distributions as D, nn
+
+__all__ = [
+    "NormalParamExtractor",
+    "NormalParamWrapper",
+    "AddStateIndependentNormalScale",
+    "Delta",
+]
+
+# speeds up distribution construction
+D.Distribution.set_default_validate_args(False)
+
+
+class NormalParamWrapper(nn.Module):
+    """A wrapper for normal distribution parameters.
+
+    Args:
+        operator (nn.Module): operator whose output will be transformed_in in location and scale parameters
+        scale_mapping (str, optional): positive mapping function to be used with the std.
+            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
+            choices: "softplus", "exp", "relu", "biased_softplus_1";
+        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
+
+    Examples:
+        >>> from torch import nn
+        >>> import torch
+        >>> module = nn.Linear(3, 4)
+        >>> module_normal = NormalParamWrapper(module)
+        >>> tensor = torch.randn(3)
+        >>> loc, scale = module_normal(tensor)
+        >>> print(loc.shape, scale.shape)
+        torch.Size([2]) torch.Size([2])
+        >>> assert (scale > 0).all()
+        >>> # with modules that return more than one tensor
+        >>> module = nn.LSTM(3, 4)
+        >>> module_normal = NormalParamWrapper(module)
+        >>> tensor = torch.randn(4, 2, 3)
+        >>> loc, scale, others = module_normal(tensor)
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
+        >>> assert (scale > 0).all()
+
+    """
+
+    def __init__(
+        self,
+        operator: nn.Module,
+        scale_mapping: str = "biased_softplus_1.0",
+        scale_lb: Number = 1e-4,
+    ) -> None:
+        super().__init__()
+        self.operator = operator
+        self.scale_mapping = scale_mapping
+        self.scale_lb = scale_lb
+
+    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        net_output = self.operator(*tensors)
+        others = ()
+        if not isinstance(net_output, torch.Tensor):
+            net_output, *others = net_output
+        loc, scale = net_output.chunk(2, -1)
+        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
+        return (loc, scale, *others)
+
+
+class NormalParamExtractor(nn.Module):
+    """A non-parametric nn.Module that splits its input into loc and scale parameters.
+
+    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
+
+    Args:
+        scale_mapping (str, optional): positive mapping function to be used with the std.
+            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
+            choices: "softplus", "exp", "relu", "biased_softplus_1";
+        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
+
+    Examples:
+        >>> import torch
+        >>> from tensordict.nn.distributions import NormalParamExtractor
+        >>> from torch import nn
+        >>> module = nn.Linear(3, 4)
+        >>> normal_params = NormalParamExtractor()
+        >>> tensor = torch.randn(3)
+        >>> loc, scale = normal_params(module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([2]) torch.Size([2])
+        >>> assert (scale > 0).all()
+        >>> # with modules that return more than one tensor
+        >>> module = nn.LSTM(3, 4)
+        >>> tensor = torch.randn(4, 2, 3)
+        >>> loc, scale, others = normal_params(*module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4, 2, 2]) torch.Size([4, 2, 2])
+        >>> assert (scale > 0).all()
+
+    """
+
+    def __init__(
+        self,
+        scale_mapping: str = "biased_softplus_1.0",
+        scale_lb: Number = 1e-4,
+    ) -> None:
+        super().__init__()
+        self.scale_mapping = scale_mapping
+        self.scale_lb = scale_lb
+
+    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        tensor, *others = tensors
+        loc, scale = tensor.chunk(2, -1)
+        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
+        return (loc, scale, *others)
+
+
+class AddStateIndependentNormalScale(torch.nn.Module):
+    """A nn.Module that adds trainable state-independent scale parameters.
+
+    The scale parameters are mapped onto positive values using the specified ``scale_mapping``.
+
+    Args:
+        scale_mapping (str, optional): positive mapping function to be used with the std.
+            default = "biased_softplus_1.0" (i.e. softplus map with bias such that fn(0.0) = 1.0)
+            choices: "softplus", "exp", "relu", "biased_softplus_1";
+        scale_lb (Number, optional): The minimum value that the variance can take. Default is 1e-4.
+
+    Examples:
+        >>> from torch import nn
+        >>> import torch
+        >>> num_outputs = 4
+        >>> module = nn.Linear(3, num_outputs)
+        >>> module_normal = AddStateIndependentNormalScale(num_outputs)
+        >>> tensor = torch.randn(3)
+        >>> loc, scale = module_normal(module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4]) torch.Size([4])
+        >>> assert (scale > 0).all()
+        >>> # with modules that return more than one tensor
+        >>> module = nn.LSTM(3, num_outputs)
+        >>> module_normal = AddStateIndependentNormalScale(num_outputs)
+        >>> tensor = torch.randn(4, 2, 3)
+        >>> loc, scale, others = module_normal(*module(tensor))
+        >>> print(loc.shape, scale.shape)
+        torch.Size([4, 2, 4]) torch.Size([4, 2, 4])
+        >>> assert (scale > 0).all()
+    """
+
+    def __init__(
+        self,
+        scale_shape: Union[torch.Size, int, tuple],
+        scale_mapping: str = "exp",
+        scale_lb: Number = 1e-4,
+    ) -> None:
+
+        super().__init__()
+        self.scale_lb = scale_lb
+        if isinstance(scale_shape, int):
+            scale_shape = (scale_shape,)
+        self.scale_shape = scale_shape
+        self.scale_mapping = scale_mapping
+        self.state_independent_scale = torch.nn.Parameter(torch.zeros(scale_shape))
+
+    def forward(self, *tensors: torch.Tensor) -> tuple[torch.Tensor, ...]:
+        loc, *others = tensors
+
+        if self.scale_shape != loc.shape[-len(self.scale_shape) :]:
+            raise RuntimeError(
+                f"Last dimensions of loc ({loc.shape[-len(self.scale_shape):]}) do not match the number of dimensions "
+                f"in scale ({self.state_independent_scale.shape})"
+            )
+
+        scale = torch.zeros_like(loc) + self.state_independent_scale
+        scale = mappings(self.scale_mapping)(scale).clamp_min(self.scale_lb)
+
+        return (loc, scale, *others)
+
+
+class Delta(D.Distribution):
+    """Delta distribution.
+
+    Args:
+        param (torch.Tensor): parameter of the delta distribution;
+        atol (number, optional): absolute tolerance to consider that a tensor matches the distribution parameter;
+            Default is 1e-6
+        rtol (number, optional): relative tolerance to consider that a tensor matches the distribution parameter;
+            Default is 1e-6
+        batch_shape (torch.Size, optional): batch shape;
+        event_shape (torch.Size, optional): shape of the outcome.
+
+    """
+
+    arg_constraints: dict = {}
+
+    def __init__(
+        self,
+        param: torch.Tensor,
+        atol: float = 1e-6,
+        rtol: float = 1e-6,
+        batch_shape: torch.Size | Sequence[int] | None = None,
+        event_shape: torch.Size | Sequence[int] | None = None,
+    ) -> None:
+        if batch_shape is None:
+            batch_shape = torch.Size([])
+        if event_shape is None:
+            event_shape = torch.Size([])
+        self.update(param)
+        self.atol = atol
+        self.rtol = rtol
+        if not len(batch_shape) and not len(event_shape):
+            batch_shape = param.shape[:-1]
+            event_shape = param.shape[-1:]
+        super().__init__(batch_shape=batch_shape, event_shape=event_shape)
+
+    def update(self, param: torch.Tensor) -> None:
+        self.param = param
+
+    def _is_equal(self, value: torch.Tensor) -> torch.Tensor:
+        param = self.param.expand_as(value)
+        is_equal = abs(value - param) < self.atol + self.rtol * abs(param)
+        for i in range(-1, -len(self.event_shape) - 1, -1):
+            is_equal = is_equal.all(i)
+        return is_equal
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        is_equal = self._is_equal(value)
+        out = torch.zeros_like(is_equal, dtype=value.dtype)
+        out.masked_fill_(is_equal, np.inf)
+        out.masked_fill_(~is_equal, -np.inf)
+        return out
+
+    @torch.no_grad()
+    def sample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        return self.param.expand((*sample_shape, *self.param.shape))
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        return self.param.expand((*sample_shape, *self.param.shape))
+
+    @property
+    def mode(self) -> torch.Tensor:
+        return self.param
+
+    @property
+    def mean(self) -> torch.Tensor:
+        return self.param
```

## tensordict/nn/distributions/discrete.py

 * *Ordering differences only*

```diff
@@ -1,87 +1,87 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-from typing import Sequence
-
-import torch
-from torch import distributions as D
-
-__all__ = [
-    "OneHotCategorical",
-]
-
-
-def _treat_categorical_params(
-    params: torch.Tensor | None = None,
-) -> torch.Tensor | None:
-    if params is None:
-        return None
-    if params.shape[-1] == 1:
-        params = params[..., 0]
-    return params
-
-
-def rand_one_hot(values: torch.Tensor, do_softmax: bool = True) -> torch.Tensor:
-    if do_softmax:
-        values = values.softmax(-1)
-    out = values.cumsum(-1) > torch.rand_like(values[..., :1])
-    out = (out.cumsum(-1) == 1).to(torch.long)
-    return out
-
-
-class OneHotCategorical(D.Categorical):
-    """One-hot categorical distribution.
-
-    This class behaves excacly as torch.distributions.Categorical except that it reads and produces one-hot encodings
-    of the discrete tensors.
-
-    """
-
-    num_params: int = 1
-
-    def __init__(
-        self,
-        logits: torch.Tensor | None = None,
-        probs: torch.Tensor | None = None,
-        **kwargs,
-    ) -> None:
-        logits = _treat_categorical_params(logits)
-        probs = _treat_categorical_params(probs)
-        super().__init__(probs=probs, logits=logits, **kwargs)
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        return super().log_prob(value.argmax(dim=-1))
-
-    @property
-    def mode(self) -> torch.Tensor:
-        if hasattr(self, "logits"):
-            return (self.logits == self.logits.max(-1, True)[0]).to(torch.long)
-        else:
-            return (self.probs == self.probs.max(-1, True)[0]).to(torch.long)
-
-    def sample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        out = super().sample(sample_shape=sample_shape)
-        out = torch.nn.functional.one_hot(out, self.logits.shape[-1]).to(torch.long)
-        return out
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        d = D.relaxed_categorical.RelaxedOneHotCategorical(
-            1.0, probs=self.probs, logits=self.logits
-        )
-        out = d.rsample(sample_shape)
-        out.data.copy_((out == out.max(-1)[0].unsqueeze(-1)).to(out.dtype))
-        return out
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+from typing import Sequence
+
+import torch
+from torch import distributions as D
+
+__all__ = [
+    "OneHotCategorical",
+]
+
+
+def _treat_categorical_params(
+    params: torch.Tensor | None = None,
+) -> torch.Tensor | None:
+    if params is None:
+        return None
+    if params.shape[-1] == 1:
+        params = params[..., 0]
+    return params
+
+
+def rand_one_hot(values: torch.Tensor, do_softmax: bool = True) -> torch.Tensor:
+    if do_softmax:
+        values = values.softmax(-1)
+    out = values.cumsum(-1) > torch.rand_like(values[..., :1])
+    out = (out.cumsum(-1) == 1).to(torch.long)
+    return out
+
+
+class OneHotCategorical(D.Categorical):
+    """One-hot categorical distribution.
+
+    This class behaves excacly as torch.distributions.Categorical except that it reads and produces one-hot encodings
+    of the discrete tensors.
+
+    """
+
+    num_params: int = 1
+
+    def __init__(
+        self,
+        logits: torch.Tensor | None = None,
+        probs: torch.Tensor | None = None,
+        **kwargs,
+    ) -> None:
+        logits = _treat_categorical_params(logits)
+        probs = _treat_categorical_params(probs)
+        super().__init__(probs=probs, logits=logits, **kwargs)
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        return super().log_prob(value.argmax(dim=-1))
+
+    @property
+    def mode(self) -> torch.Tensor:
+        if hasattr(self, "logits"):
+            return (self.logits == self.logits.max(-1, True)[0]).to(torch.long)
+        else:
+            return (self.probs == self.probs.max(-1, True)[0]).to(torch.long)
+
+    def sample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        out = super().sample(sample_shape=sample_shape)
+        out = torch.nn.functional.one_hot(out, self.logits.shape[-1]).to(torch.long)
+        return out
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        d = D.relaxed_categorical.RelaxedOneHotCategorical(
+            1.0, probs=self.probs, logits=self.logits
+        )
+        out = d.rsample(sample_shape)
+        out.data.copy_((out == out.max(-1)[0].unsqueeze(-1)).to(out.dtype))
+        return out
```

## tensordict/nn/distributions/truncated_normal.py

 * *Ordering differences only*

```diff
@@ -1,190 +1,190 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-# from https://github.com/toshas/torch_truncnorm
-
-from __future__ import annotations
-
-import math
-from numbers import Number
-from typing import Sequence
-
-import torch
-from torch.distributions import constraints, Distribution
-from torch.distributions.utils import broadcast_all
-
-CONST_SQRT_2 = math.sqrt(2)
-CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)
-CONST_INV_SQRT_2 = 1 / math.sqrt(2)
-CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)
-CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)
-
-
-class TruncatedStandardNormal(Distribution):
-    """Truncated Standard Normal distribution.
-
-    Source: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
-    """
-
-    arg_constraints = {
-        "a": constraints.real,
-        "b": constraints.real,
-    }
-    has_rsample = True
-    eps = 1e-6
-
-    def __init__(
-        self,
-        a: Number | torch.Tensor,
-        b: Number | torch.Tensor,
-        validate_args: bool | None = None,
-    ) -> None:
-        self.a, self.b = broadcast_all(a, b)
-        if isinstance(a, Number) and isinstance(b, Number):
-            batch_shape = torch.Size()
-        else:
-            batch_shape = self.a.size()
-        super().__init__(batch_shape, validate_args=validate_args)
-        if self.a.dtype != self.b.dtype:
-            raise ValueError("Truncation bounds types are different")
-        if any((self.a >= self.b).view(-1).tolist()):
-            raise ValueError("Incorrect truncation range")
-        # eps = torch.finfo(self.a.dtype).eps * 10
-        eps = self.eps
-        self._dtype_min_gt_0 = eps
-        self._dtype_max_lt_1 = 1 - eps
-        self._little_phi_a = self._little_phi(self.a)
-        self._little_phi_b = self._little_phi(self.b)
-        self._big_phi_a = self._big_phi(self.a)
-        self._big_phi_b = self._big_phi(self.b)
-        self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)
-        self._log_Z = self._Z.log()
-        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)
-        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)
-        self._lpbb_m_lpaa_d_Z = (
-            self._little_phi_b * little_phi_coeff_b
-            - self._little_phi_a * little_phi_coeff_a
-        ) / self._Z
-        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z
-        self._variance = (
-            1
-            - self._lpbb_m_lpaa_d_Z
-            - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2
-        )
-        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z
-
-    @constraints.dependent_property
-    def support(self) -> constraints.Constraints:
-        return constraints.interval(self.a, self.b)
-
-    @property
-    def mean(self) -> torch.Tensor:
-        return self._mean
-
-    @property
-    def variance(self) -> torch.Tensor:
-        return self._variance
-
-    @property
-    def entropy(self) -> torch.Tensor:
-        return self._entropy
-
-    @property
-    def auc(self) -> torch.Tensor:
-        return self._Z
-
-    @staticmethod
-    def _little_phi(x: torch.Tensor) -> torch.Tensor:
-        return (-(x**2) * 0.5).exp() * CONST_INV_SQRT_2PI
-
-    def _big_phi(self, x: torch.Tensor) -> torch.Tensor:
-        phi = 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())
-        return phi.clamp(self.eps, 1 - self.eps)
-
-    @staticmethod
-    def _inv_big_phi(x: torch.Tensor) -> torch.Tensor:
-        return CONST_SQRT_2 * (2 * x - 1).erfinv()
-
-    def cdf(self, value: torch.Tensor) -> torch.Tensor:
-        if self._validate_args:
-            self._validate_sample(value)
-        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)
-
-    def icdf(self, value: torch.Tensor) -> torch.Tensor:
-        y = self._big_phi_a + value * self._Z
-        y = y.clamp(self.eps, 1 - self.eps)
-        return self._inv_big_phi(y)
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        if self._validate_args:
-            self._validate_sample(value)
-        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value**2) * 0.5
-
-    def rsample(
-        self,
-        sample_shape: torch.Size | Sequence[int] | None = None,
-    ) -> torch.Tensor:
-        if sample_shape is None:
-            sample_shape = torch.Size([])
-        shape = self._extended_shape(sample_shape)
-        p = torch.empty(shape, device=self.a.device).uniform_(
-            self._dtype_min_gt_0, self._dtype_max_lt_1
-        )
-        return self.icdf(p)
-
-
-class TruncatedNormal(TruncatedStandardNormal):
-    """Truncated Normal distribution.
-
-    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
-    """
-
-    has_rsample = True
-
-    def __init__(
-        self,
-        loc: Number | torch.Tensor,
-        scale: Number | torch.Tensor,
-        a: Number | torch.Tensor,
-        b: Number | torch.Tensor,
-        validate_args: bool | None = None,
-    ) -> None:
-        scale = scale.clamp_min(self.eps)
-        self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)
-        self._non_std_a = a
-        self._non_std_b = b
-        a = (a - self.loc) / self.scale
-        b = (b - self.loc) / self.scale
-        super().__init__(a, b, validate_args=validate_args)
-        self._log_scale = self.scale.log()
-        self._mean = self._mean * self.scale + self.loc
-        self._variance = self._variance * self.scale**2
-        self._entropy += self._log_scale
-
-    def _to_std_rv(self, value: torch.Tensor) -> torch.Tensor:
-        return (value - self.loc) / self.scale
-
-    def _from_std_rv(self, value: torch.Tensor) -> torch.Tensor:
-        return value * self.scale + self.loc
-
-    def cdf(self, value: torch.Tensor) -> torch.Tensor:
-        return super().cdf(self._to_std_rv(value))
-
-    def icdf(self, value: torch.Tensor) -> torch.Tensor:
-        sample = self._from_std_rv(super().icdf(value))
-
-        # clamp data but keep gradients
-        sample_clip = torch.stack(
-            [sample.detach(), self._non_std_a.detach().expand_as(sample)], 0
-        ).max(0)[0]
-        sample_clip = torch.stack(
-            [sample_clip, self._non_std_b.detach().expand_as(sample)], 0
-        ).min(0)[0]
-        sample.data.copy_(sample_clip)
-        return sample
-
-    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
-        value = self._to_std_rv(value)
-        return super().log_prob(value) - self._log_scale
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+# from https://github.com/toshas/torch_truncnorm
+
+from __future__ import annotations
+
+import math
+from numbers import Number
+from typing import Sequence
+
+import torch
+from torch.distributions import constraints, Distribution
+from torch.distributions.utils import broadcast_all
+
+CONST_SQRT_2 = math.sqrt(2)
+CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)
+CONST_INV_SQRT_2 = 1 / math.sqrt(2)
+CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)
+CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)
+
+
+class TruncatedStandardNormal(Distribution):
+    """Truncated Standard Normal distribution.
+
+    Source: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
+    """
+
+    arg_constraints = {
+        "a": constraints.real,
+        "b": constraints.real,
+    }
+    has_rsample = True
+    eps = 1e-6
+
+    def __init__(
+        self,
+        a: Number | torch.Tensor,
+        b: Number | torch.Tensor,
+        validate_args: bool | None = None,
+    ) -> None:
+        self.a, self.b = broadcast_all(a, b)
+        if isinstance(a, Number) and isinstance(b, Number):
+            batch_shape = torch.Size()
+        else:
+            batch_shape = self.a.size()
+        super().__init__(batch_shape, validate_args=validate_args)
+        if self.a.dtype != self.b.dtype:
+            raise ValueError("Truncation bounds types are different")
+        if any((self.a >= self.b).view(-1).tolist()):
+            raise ValueError("Incorrect truncation range")
+        # eps = torch.finfo(self.a.dtype).eps * 10
+        eps = self.eps
+        self._dtype_min_gt_0 = eps
+        self._dtype_max_lt_1 = 1 - eps
+        self._little_phi_a = self._little_phi(self.a)
+        self._little_phi_b = self._little_phi(self.b)
+        self._big_phi_a = self._big_phi(self.a)
+        self._big_phi_b = self._big_phi(self.b)
+        self._Z = (self._big_phi_b - self._big_phi_a).clamp(eps, 1 - eps)
+        self._log_Z = self._Z.log()
+        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)
+        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)
+        self._lpbb_m_lpaa_d_Z = (
+            self._little_phi_b * little_phi_coeff_b
+            - self._little_phi_a * little_phi_coeff_a
+        ) / self._Z
+        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z
+        self._variance = (
+            1
+            - self._lpbb_m_lpaa_d_Z
+            - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2
+        )
+        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z
+
+    @constraints.dependent_property
+    def support(self) -> constraints.Constraints:
+        return constraints.interval(self.a, self.b)
+
+    @property
+    def mean(self) -> torch.Tensor:
+        return self._mean
+
+    @property
+    def variance(self) -> torch.Tensor:
+        return self._variance
+
+    @property
+    def entropy(self) -> torch.Tensor:
+        return self._entropy
+
+    @property
+    def auc(self) -> torch.Tensor:
+        return self._Z
+
+    @staticmethod
+    def _little_phi(x: torch.Tensor) -> torch.Tensor:
+        return (-(x**2) * 0.5).exp() * CONST_INV_SQRT_2PI
+
+    def _big_phi(self, x: torch.Tensor) -> torch.Tensor:
+        phi = 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())
+        return phi.clamp(self.eps, 1 - self.eps)
+
+    @staticmethod
+    def _inv_big_phi(x: torch.Tensor) -> torch.Tensor:
+        return CONST_SQRT_2 * (2 * x - 1).erfinv()
+
+    def cdf(self, value: torch.Tensor) -> torch.Tensor:
+        if self._validate_args:
+            self._validate_sample(value)
+        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)
+
+    def icdf(self, value: torch.Tensor) -> torch.Tensor:
+        y = self._big_phi_a + value * self._Z
+        y = y.clamp(self.eps, 1 - self.eps)
+        return self._inv_big_phi(y)
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        if self._validate_args:
+            self._validate_sample(value)
+        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value**2) * 0.5
+
+    def rsample(
+        self,
+        sample_shape: torch.Size | Sequence[int] | None = None,
+    ) -> torch.Tensor:
+        if sample_shape is None:
+            sample_shape = torch.Size([])
+        shape = self._extended_shape(sample_shape)
+        p = torch.empty(shape, device=self.a.device).uniform_(
+            self._dtype_min_gt_0, self._dtype_max_lt_1
+        )
+        return self.icdf(p)
+
+
+class TruncatedNormal(TruncatedStandardNormal):
+    """Truncated Normal distribution.
+
+    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
+    """
+
+    has_rsample = True
+
+    def __init__(
+        self,
+        loc: Number | torch.Tensor,
+        scale: Number | torch.Tensor,
+        a: Number | torch.Tensor,
+        b: Number | torch.Tensor,
+        validate_args: bool | None = None,
+    ) -> None:
+        scale = scale.clamp_min(self.eps)
+        self.loc, self.scale, a, b = broadcast_all(loc, scale, a, b)
+        self._non_std_a = a
+        self._non_std_b = b
+        a = (a - self.loc) / self.scale
+        b = (b - self.loc) / self.scale
+        super().__init__(a, b, validate_args=validate_args)
+        self._log_scale = self.scale.log()
+        self._mean = self._mean * self.scale + self.loc
+        self._variance = self._variance * self.scale**2
+        self._entropy += self._log_scale
+
+    def _to_std_rv(self, value: torch.Tensor) -> torch.Tensor:
+        return (value - self.loc) / self.scale
+
+    def _from_std_rv(self, value: torch.Tensor) -> torch.Tensor:
+        return value * self.scale + self.loc
+
+    def cdf(self, value: torch.Tensor) -> torch.Tensor:
+        return super().cdf(self._to_std_rv(value))
+
+    def icdf(self, value: torch.Tensor) -> torch.Tensor:
+        sample = self._from_std_rv(super().icdf(value))
+
+        # clamp data but keep gradients
+        sample_clip = torch.stack(
+            [sample.detach(), self._non_std_a.detach().expand_as(sample)], 0
+        ).max(0)[0]
+        sample_clip = torch.stack(
+            [sample_clip, self._non_std_b.detach().expand_as(sample)], 0
+        ).min(0)[0]
+        sample.data.copy_(sample_clip)
+        return sample
+
+    def log_prob(self, value: torch.Tensor) -> torch.Tensor:
+        value = self._to_std_rv(value)
+        return super().log_prob(value) - self._log_scale
```

## tensordict/nn/distributions/utils.py

 * *Ordering differences only*

```diff
@@ -1,40 +1,40 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import torch
-from tensordict.utils import DeviceType
-from torch import distributions as D
-
-
-def _cast_device(
-    elt: torch.Tensor | float,
-    device: DeviceType,
-) -> torch.Tensor | float:
-    if isinstance(elt, torch.Tensor):
-        return elt.to(device)
-    return elt
-
-
-def _cast_transform_device(
-    transform: D.Transform | None,
-    device: DeviceType,
-) -> D.Transform | None:
-    if transform is None:
-        return transform
-    elif isinstance(transform, D.ComposeTransform):
-        for i, t in enumerate(transform.parts):
-            transform.parts[i] = _cast_transform_device(t, device)
-    elif isinstance(transform, D.Transform):
-        for attribute in dir(transform):
-            value = getattr(transform, attribute)
-            if isinstance(value, torch.Tensor):
-                setattr(transform, attribute, value.to(device))
-        return transform
-    else:
-        raise TypeError(
-            f"Cannot perform device casting for transform of type {type(transform)}"
-        )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import torch
+from tensordict.utils import DeviceType
+from torch import distributions as D
+
+
+def _cast_device(
+    elt: torch.Tensor | float,
+    device: DeviceType,
+) -> torch.Tensor | float:
+    if isinstance(elt, torch.Tensor):
+        return elt.to(device)
+    return elt
+
+
+def _cast_transform_device(
+    transform: D.Transform | None,
+    device: DeviceType,
+) -> D.Transform | None:
+    if transform is None:
+        return transform
+    elif isinstance(transform, D.ComposeTransform):
+        for i, t in enumerate(transform.parts):
+            transform.parts[i] = _cast_transform_device(t, device)
+    elif isinstance(transform, D.Transform):
+        for attribute in dir(transform):
+            value = getattr(transform, attribute)
+            if isinstance(value, torch.Tensor):
+                setattr(transform, attribute, value.to(device))
+        return transform
+    else:
+        raise TypeError(
+            f"Cannot perform device casting for transform of type {type(transform)}"
+        )
```

## tensordict/prototype/__init__.py

 * *Ordering differences only*

```diff
@@ -1,12 +1,12 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-from tensordict.prototype.fx import symbolic_trace
-from tensordict.prototype.tensorclass import is_tensorclass, tensorclass
-
-__all__ = [
-    "is_tensorclass",
-    "symbolic_trace",
-    "tensorclass",
-]
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+from tensordict.prototype.fx import symbolic_trace
+from tensordict.prototype.tensorclass import is_tensorclass, tensorclass
+
+__all__ = [
+    "is_tensorclass",
+    "symbolic_trace",
+    "tensorclass",
+]
```

## tensordict/prototype/fx.py

 * *Ordering differences only*

```diff
@@ -1,199 +1,199 @@
-# Copyright (c) Meta Platforms, Inc. and affiliates.
-#
-# This source code is licensed under the MIT license found in the
-# LICENSE file in the root directory of this source tree.
-
-from __future__ import annotations
-
-import operator
-from itertools import filterfalse, tee
-from typing import Any, Callable, Iterable
-
-from tensordict._td import _unravel_key_to_tuple
-
-from tensordict.nn import TensorDictModule, TensorDictSequential
-from tensordict.tensordict import TensorDictBase
-from tensordict.utils import NestedKey
-from torch import fx, nn
-
-__all__ = ["symbolic_trace"]
-
-
-class TDGraphModule(nn.Module):
-    """A graph module for TensorDict."""
-
-    def __init__(
-        self,
-        graph_module: fx.GraphModule,
-        out_keys: list[NestedKey],
-    ) -> None:
-        super().__init__()
-        self.out_keys = [_unravel_key_to_tuple(ok) for ok in out_keys]
-        self._gm = graph_module
-
-    def forward(
-        self,
-        tensordict: TensorDictBase,
-        tensordict_out: TensorDictBase | None = None,
-        **kwargs,
-    ) -> TensorDictBase:
-        outputs = self._gm(tensordict, **kwargs)
-
-        if tensordict_out is None:
-            tensordict_out = tensordict
-
-        for out_key, output in zip(self.out_keys, outputs):
-            if out_key != "_":
-                tensordict_out._set_tuple(
-                    out_key, output, inplace=False, validated=True, non_blocking=False
-                )
-
-        return tensordict_out
-
-    def __getattr__(self, name: str) -> Any:
-        try:
-            return super().__getattr__(name)
-        except AttributeError:
-            return getattr(self._gm, name)
-
-
-def symbolic_trace(td_module: TensorDictModule) -> TDGraphModule:
-    """A symbolic tracer for TensorDictModule."""
-    if isinstance(td_module, TensorDictSequential):
-        return _trace_tensordictsequential(td_module)
-    elif isinstance(td_module, TensorDictModule):
-        return _trace_tensordictmodule(td_module)
-    raise TypeError(f"Unsupported type {type(td_module)}")
-
-
-# cf. https://docs.python.org/3/library/itertools.html#itertools-recipes
-def _partition(
-    pred: Callable[..., bool], iterable: Iterable[Any]
-) -> tuple[Iterable[Any], Iterable[Any]]:
-    """Use a predicate to partition entries into false entries and true entries."""
-    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
-    t1, t2 = tee(iterable)
-    return filterfalse(pred, t1), filter(pred, t2)
-
-
-def _parse_input_nodes(
-    in_keys: list[NestedKey], nodes, td: TensorDictBase, inputs: tuple[Any, ...], env
-):
-    for in_key, node in zip(in_keys, nodes):
-        if in_key in inputs:
-            new_node = inputs[in_key]
-        else:
-            output_proxy = operator.getitem(td, in_key)
-            new_node = output_proxy.node
-            inputs[in_key] = new_node
-        env[node.name] = new_node
-
-
-def _trace_tensordictmodule(td_module: TensorDictModule) -> TDGraphModule:
-    # this graph manipulation is based heavily on example in the PyTorch docs
-    # https://pytorch.org/docs/stable/fx.html#proxy-retracing
-
-    # trace the graph of the underlying module
-    graph = fx.Tracer().trace(td_module.module)
-
-    # create a new graph which we will populate from the old one
-    new_graph = fx.Graph()
-    env = {}
-
-    # create a new placeholder for the input tensordict
-    td = fx.Proxy(new_graph.placeholder("tensordict"))
-
-    node_iter = iter(graph.nodes)
-
-    # the first nodes, in order, are placeholders for the in_keys. We consume them and
-    # convert them to "call_function" nodes with target=operator.getitem.
-    _parse_input_nodes(td_module.in_keys, node_iter, td, {}, env)
-
-    # the remaining nodes we simply clone, pulling any arguments from the env
-    for node in node_iter:
-        new_node = new_graph.node_copy(node, lambda x: env[x.name])
-        env[node.name] = new_node
-
-    return TDGraphModule(
-        fx.GraphModule(td_module.module, new_graph), td_module.out_keys
-    )
-
-
-def _trace_tensordictsequential(td_sequential: TensorDictSequential) -> TDGraphModule:
-    # we track values previously read from / written to the tensordict by storing the
-    # nodes / proxy values in the inputs / outputs dictionaries
-    inputs = {}
-    outputs = {}
-    # env is a lookup for nodes in the new graph using names from the old graph
-    env = {}
-
-    new_graph = fx.Graph()
-    td = fx.Proxy(new_graph.placeholder("tensordict"))
-
-    for i, td_module in enumerate(td_sequential.module):
-        # trace the submodule
-        if isinstance(td_module, TensorDictSequential):
-            graph = _trace_tensordictsequential(td_module).graph
-            node_iter = iter(graph.nodes)
-            _td = next(node_iter)  # tensordict placeholder from submodule graph
-
-            # in the graph of TensorDictSequential, the getitem calls to the tensordict
-            # need not come first, so we partition nodes into getitem calls on the
-            # placeholder tensordict (input_nodes) and the remaining nodes
-            node_iter, input_nodes = _partition(
-                lambda node, _td=_td: (
-                    node.op == "call_function"
-                    and node.target == operator.getitem
-                    and node.args[0] == _td
-                ),
-                node_iter,
-            )
-            _parse_input_nodes(td_module.in_keys, input_nodes, td, inputs, env)
-
-        else:
-            graph = fx.Tracer().trace(td_module.module)
-            # in the trace of a regular nn.Module the placeholder nodes all come first,
-            # so we just consume them in order
-            node_iter = iter(graph.nodes)
-            _parse_input_nodes(td_module.in_keys, node_iter, td, inputs, env)
-
-        # clone the remaining nodes
-        for node in node_iter:
-            if node.op == "output":
-                # capture the outputs but don't clone the output node (this would
-                # result in prematurely returning intermediate values)
-
-                # need to unpack the args in the case that the submodule is itself a
-                # TensorDictSequential that returns a tuple of arguments
-                args = (
-                    node.args[0]
-                    if isinstance(td_module, TensorDictSequential)
-                    else node.args
-                )
-
-                # if the submodule has multiple outputs, args has structure
-                # ((out1, out2,),), so we need to do some extra unpacking
-                args = args[0] if isinstance(args[0], tuple) else args
-
-                for out_key, arg in zip(td_module.out_keys, args):
-                    # any outputs of submodules will need to be returned at the end
-                    outputs[out_key] = env[arg.name]
-                    # we also need to make outputs of submodules available as inputs
-                    # to subsequent submodules
-                    inputs[out_key] = env[arg.name]
-            else:
-                new_node = new_graph.node_copy(node, lambda x: env[x.name])
-                if new_node.op in ("call_module", "get_attr"):
-                    # since we traced the submodule in isolation, we need to patch the
-                    # targets of any calls to methods on the module or attribute access
-                    new_node.target = f"{i}.module.{new_node.target}"
-                    new_node.name = f"_{i}_{new_node.name}"
-                env[node.name] = new_node
-
-    # finally we add a new output node that collects all of the output values from
-    # submodules in the graph and returns them together
-    new_graph.output(tuple(outputs.values()))
-
-    return TDGraphModule(
-        fx.GraphModule(td_sequential.module, new_graph), tuple(outputs.keys())
-    )
+# Copyright (c) Meta Platforms, Inc. and affiliates.
+#
+# This source code is licensed under the MIT license found in the
+# LICENSE file in the root directory of this source tree.
+
+from __future__ import annotations
+
+import operator
+from itertools import filterfalse, tee
+from typing import Any, Callable, Iterable
+
+from tensordict._td import _unravel_key_to_tuple
+
+from tensordict.nn import TensorDictModule, TensorDictSequential
+from tensordict.tensordict import TensorDictBase
+from tensordict.utils import NestedKey
+from torch import fx, nn
+
+__all__ = ["symbolic_trace"]
+
+
+class TDGraphModule(nn.Module):
+    """A graph module for TensorDict."""
+
+    def __init__(
+        self,
+        graph_module: fx.GraphModule,
+        out_keys: list[NestedKey],
+    ) -> None:
+        super().__init__()
+        self.out_keys = [_unravel_key_to_tuple(ok) for ok in out_keys]
+        self._gm = graph_module
+
+    def forward(
+        self,
+        tensordict: TensorDictBase,
+        tensordict_out: TensorDictBase | None = None,
+        **kwargs,
+    ) -> TensorDictBase:
+        outputs = self._gm(tensordict, **kwargs)
+
+        if tensordict_out is None:
+            tensordict_out = tensordict
+
+        for out_key, output in zip(self.out_keys, outputs):
+            if out_key != "_":
+                tensordict_out._set_tuple(
+                    out_key, output, inplace=False, validated=True, non_blocking=False
+                )
+
+        return tensordict_out
+
+    def __getattr__(self, name: str) -> Any:
+        try:
+            return super().__getattr__(name)
+        except AttributeError:
+            return getattr(self._gm, name)
+
+
+def symbolic_trace(td_module: TensorDictModule) -> TDGraphModule:
+    """A symbolic tracer for TensorDictModule."""
+    if isinstance(td_module, TensorDictSequential):
+        return _trace_tensordictsequential(td_module)
+    elif isinstance(td_module, TensorDictModule):
+        return _trace_tensordictmodule(td_module)
+    raise TypeError(f"Unsupported type {type(td_module)}")
+
+
+# cf. https://docs.python.org/3/library/itertools.html#itertools-recipes
+def _partition(
+    pred: Callable[..., bool], iterable: Iterable[Any]
+) -> tuple[Iterable[Any], Iterable[Any]]:
+    """Use a predicate to partition entries into false entries and true entries."""
+    # partition(is_odd, range(10)) --> 0 2 4 6 8   and  1 3 5 7 9
+    t1, t2 = tee(iterable)
+    return filterfalse(pred, t1), filter(pred, t2)
+
+
+def _parse_input_nodes(
+    in_keys: list[NestedKey], nodes, td: TensorDictBase, inputs: tuple[Any, ...], env
+):
+    for in_key, node in zip(in_keys, nodes):
+        if in_key in inputs:
+            new_node = inputs[in_key]
+        else:
+            output_proxy = operator.getitem(td, in_key)
+            new_node = output_proxy.node
+            inputs[in_key] = new_node
+        env[node.name] = new_node
+
+
+def _trace_tensordictmodule(td_module: TensorDictModule) -> TDGraphModule:
+    # this graph manipulation is based heavily on example in the PyTorch docs
+    # https://pytorch.org/docs/stable/fx.html#proxy-retracing
+
+    # trace the graph of the underlying module
+    graph = fx.Tracer().trace(td_module.module)
+
+    # create a new graph which we will populate from the old one
+    new_graph = fx.Graph()
+    env = {}
+
+    # create a new placeholder for the input tensordict
+    td = fx.Proxy(new_graph.placeholder("tensordict"))
+
+    node_iter = iter(graph.nodes)
+
+    # the first nodes, in order, are placeholders for the in_keys. We consume them and
+    # convert them to "call_function" nodes with target=operator.getitem.
+    _parse_input_nodes(td_module.in_keys, node_iter, td, {}, env)
+
+    # the remaining nodes we simply clone, pulling any arguments from the env
+    for node in node_iter:
+        new_node = new_graph.node_copy(node, lambda x: env[x.name])
+        env[node.name] = new_node
+
+    return TDGraphModule(
+        fx.GraphModule(td_module.module, new_graph), td_module.out_keys
+    )
+
+
+def _trace_tensordictsequential(td_sequential: TensorDictSequential) -> TDGraphModule:
+    # we track values previously read from / written to the tensordict by storing the
+    # nodes / proxy values in the inputs / outputs dictionaries
+    inputs = {}
+    outputs = {}
+    # env is a lookup for nodes in the new graph using names from the old graph
+    env = {}
+
+    new_graph = fx.Graph()
+    td = fx.Proxy(new_graph.placeholder("tensordict"))
+
+    for i, td_module in enumerate(td_sequential.module):
+        # trace the submodule
+        if isinstance(td_module, TensorDictSequential):
+            graph = _trace_tensordictsequential(td_module).graph
+            node_iter = iter(graph.nodes)
+            _td = next(node_iter)  # tensordict placeholder from submodule graph
+
+            # in the graph of TensorDictSequential, the getitem calls to the tensordict
+            # need not come first, so we partition nodes into getitem calls on the
+            # placeholder tensordict (input_nodes) and the remaining nodes
+            node_iter, input_nodes = _partition(
+                lambda node, _td=_td: (
+                    node.op == "call_function"
+                    and node.target == operator.getitem
+                    and node.args[0] == _td
+                ),
+                node_iter,
+            )
+            _parse_input_nodes(td_module.in_keys, input_nodes, td, inputs, env)
+
+        else:
+            graph = fx.Tracer().trace(td_module.module)
+            # in the trace of a regular nn.Module the placeholder nodes all come first,
+            # so we just consume them in order
+            node_iter = iter(graph.nodes)
+            _parse_input_nodes(td_module.in_keys, node_iter, td, inputs, env)
+
+        # clone the remaining nodes
+        for node in node_iter:
+            if node.op == "output":
+                # capture the outputs but don't clone the output node (this would
+                # result in prematurely returning intermediate values)
+
+                # need to unpack the args in the case that the submodule is itself a
+                # TensorDictSequential that returns a tuple of arguments
+                args = (
+                    node.args[0]
+                    if isinstance(td_module, TensorDictSequential)
+                    else node.args
+                )
+
+                # if the submodule has multiple outputs, args has structure
+                # ((out1, out2,),), so we need to do some extra unpacking
+                args = args[0] if isinstance(args[0], tuple) else args
+
+                for out_key, arg in zip(td_module.out_keys, args):
+                    # any outputs of submodules will need to be returned at the end
+                    outputs[out_key] = env[arg.name]
+                    # we also need to make outputs of submodules available as inputs
+                    # to subsequent submodules
+                    inputs[out_key] = env[arg.name]
+            else:
+                new_node = new_graph.node_copy(node, lambda x: env[x.name])
+                if new_node.op in ("call_module", "get_attr"):
+                    # since we traced the submodule in isolation, we need to patch the
+                    # targets of any calls to methods on the module or attribute access
+                    new_node.target = f"{i}.module.{new_node.target}"
+                    new_node.name = f"_{i}_{new_node.name}"
+                env[node.name] = new_node
+
+    # finally we add a new output node that collects all of the output values from
+    # submodules in the graph and returns them together
+    new_graph.output(tuple(outputs.values()))
+
+    return TDGraphModule(
+        fx.GraphModule(td_sequential.module, new_graph), tuple(outputs.keys())
+    )
```

## tensordict/prototype/tensorclass.py

 * *Ordering differences only*

```diff
@@ -1,23 +1,23 @@
-import warnings
-from functools import wraps
-
-from tensordict._td import is_tensorclass as is_tensorclass_true  # no_qa
-from tensordict.tensorclass import tensorclass as tensorclass_true  # no_qa
-
-
-@wraps(tensorclass_true)
-def tensorclass(*args, **kwargs):  # noqa: D103
-    warnings.warn(
-        "tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
-        category=DeprecationWarning,
-    )
-    return tensorclass_true(*args, **kwargs)
-
-
-@wraps(is_tensorclass_true)
-def is_tensorclass(*args, **kwargs):  # noqa: D103
-    warnings.warn(
-        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
-        category=DeprecationWarning,
-    )
-    return is_tensorclass_true(*args, **kwargs)
+import warnings
+from functools import wraps
+
+from tensordict._td import is_tensorclass as is_tensorclass_true  # no_qa
+from tensordict.tensorclass import tensorclass as tensorclass_true  # no_qa
+
+
+@wraps(tensorclass_true)
+def tensorclass(*args, **kwargs):  # noqa: D103
+    warnings.warn(
+        "tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
+    )
+    return tensorclass_true(*args, **kwargs)
+
+
+@wraps(is_tensorclass_true)
+def is_tensorclass(*args, **kwargs):  # noqa: D103
+    warnings.warn(
+        "is_tensorclass is not a prototype anymore and can be imported directly from tensordict root.",
+        category=DeprecationWarning,
+    )
+    return is_tensorclass_true(*args, **kwargs)
```

## Comparing `tensordict_nightly-2024.6.2.dist-info/LICENSE` & `tensordict_nightly-2024.6.3.dist-info/LICENSE`

 * *Ordering differences only*

 * *Files 19% similar despite different names*

```diff
@@ -1,21 +1,21 @@
-MIT License
-
-Copyright (c) Meta Platforms, Inc. and affiliates.
-
-Permission is hereby granted, free of charge, to any person obtaining a copy
-of this software and associated documentation files (the "Software"), to deal
-in the Software without restriction, including without limitation the rights
-to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
-copies of the Software, and to permit persons to whom the Software is
-furnished to do so, subject to the following conditions:
-
-The above copyright notice and this permission notice shall be included in all
-copies or substantial portions of the Software.
-
-THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
-IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
-  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
-  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
-  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
-  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
-  SOFTWARE.
+MIT License
+
+Copyright (c) Meta Platforms, Inc. and affiliates.
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+  FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+  OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+  SOFTWARE.
```

## Comparing `tensordict_nightly-2024.6.2.dist-info/METADATA` & `tensordict_nightly-2024.6.3.dist-info/METADATA`

 * *Files 14% similar despite different names*

```diff
@@ -1,525 +1,524 @@
-Metadata-Version: 2.1
-Name: tensordict-nightly
-Version: 2024.6.2
-Summary: UNKNOWN
-Home-page: https://github.com/pytorch/tensordict
-Author: tensordict contributors
-Author-email: vmoens@fb.com
-License: BSD
-Platform: UNKNOWN
-Classifier: Programming Language :: Python :: 3.8
-Classifier: Programming Language :: Python :: 3.9
-Classifier: Programming Language :: Python :: 3.10
-Classifier: Programming Language :: Python :: 3.11
-Classifier: Development Status :: 4 - Beta
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: torch >=2.4.0.dev
-Requires-Dist: numpy
-Requires-Dist: cloudpickle
-Provides-Extra: checkpointing
-Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
-Provides-Extra: h5
-Requires-Dist: h5py >=3.8 ; extra == 'h5'
-Provides-Extra: tests
-Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: pyyaml ; extra == 'tests'
-Requires-Dist: pytest-instafail ; extra == 'tests'
-Requires-Dist: pytest-rerunfailures ; extra == 'tests'
-Requires-Dist: pytest-benchmark ; extra == 'tests'
-
-<!--- BADGES: START --->
-<!---
-[![Documentation](https://img.shields.io/badge/Documentation-blue.svg?style=flat)](https://pytorch.github.io/tensordict/)
---->
-[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-package]
-[![Discord Shield](https://dcbadge.vercel.app/api/server/tz3TgTAe3D)](https://discord.gg/tz3TgTAe3D)
-[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)][#docs-package-benchmark]
-[![Python version](https://img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/)
-[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
-<a href="https://pypi.org/project/tensordict"><img src="https://img.shields.io/pypi/v/tensordict" alt="pypi version"></a>
-<a href="https://pypi.org/project/tensordict-nightly"><img src="https://img.shields.io/pypi/v/tensordict-nightly?label=nightly" alt="pypi nightly version"></a>
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)][#pepy-package]
-[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))][#pepy-package-nightly]
-[![codecov](https://codecov.io/gh/pytorch/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-package]
-[![circleci](https://circleci.com/gh/pytorch/tensordict.svg?style=shield)][#circleci-package]
-[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)][#conda-forge-package]
-[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
-
-[#docs-package]: https://pytorch.github.io/tensordict/
-[#docs-package-benchmark]: https://pytorch.github.io/tensordict/dev/bench/
-[#github-license]: https://github.com/pytorch/tensordict/blob/main/LICENSE
-[#pepy-package]: https://pepy.tech/project/tensordict
-[#pepy-package-nightly]: https://pepy.tech/project/tensordict-nightly
-[#codecov-package]: https://codecov.io/gh/pytorch/tensordict
-[#circleci-package]: https://circleci.com/gh/pytorch/tensordict
-[#conda-forge-package]: https://anaconda.org/conda-forge/tensordict
-
-<!--- BADGES: END --->
-
-# TensorDict
-
-[**Installation**](#installation) | [**General features**](#general-principles) |
-[**Tensor-like features**](#tensor-like-features) |  [**Distributed capabilities**](#distributed-capabilities) |
-[**TensorDict for functional programming**](#tensordict-for-functional-programming) |
-[**TensorDict for parameter serialization](#tensordict-for-parameter-serialization) |
-[**Lazy preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
-
-`TensorDict` is a dictionary-like class that inherits properties from tensors,
-such as indexing, shape operations, casting to device or point-to-point communication
-in distributed settings. Whenever you need to execute an operation over a batch of tensors, 
-TensorDict is there to help you.
-
-The primary goal of TensorDict is to make your code-bases more _readable_, _compact_, and _modular_. 
-It abstracts away tailored operations, making your code less error-prone as it takes care of 
-dispatching the operation on the leaves for you.
-
-Using tensordict primitives, most supervised training loops can be rewritten in a generic way:
-```python
-for i, data in enumerate(dataset):
-    # the model reads and writes tensordicts
-    data = model(data)
-    loss = loss_module(data)
-    loss.backward()
-    optimizer.step()
-    optimizer.zero_grad()
-```
-
-With this level of abstraction, one can recycle a training loop for highly heterogeneous task.
-Each individual step of the training loop (data collection and transform, model prediction, loss computation etc.)
-can be tailored to the use case at hand without impacting the others.
-For instance, the above example can be easily used across classification and segmentation tasks, among many others.
-
-## Features
-
-### General principles
-
-Unlike other [pytrees](https://github.com/pytorch/pytorch/blob/main/torch/utils/_pytree.py), TensorDict
-carries metadata that make it easy to query the state of the container. The main metadata
-are the [``batch_size``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.batch_size) 
-(also referred as ``shape``), 
-the [``device``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.device), 
-the shared status
-([``is_memmap``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase.is_memmap) or 
-[``is_shared``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase.is_shared)), 
-the dimension [``names``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.names) 
-and the [``lock``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.lock_) status.
-
-A tensordict is primarily defined by its `batch_size` (or `shape`) and its key-value pairs:
-```python
->>> from tensordict import TensorDict
->>> import torch
->>> data = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
-```
-The `batch_size` and the first dimensions of each of the tensors must be compliant.
-The tensors can be of any dtype and device. 
-
-Optionally, one can restrict a tensordict to
-live on a dedicated ``device``, which will send each tensor that is written there:
-```python
->>> data = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4], device="cuda:0")
-```
-When a tensordict has a device, all write operations will cast the tensor to the
-TensorDict device:
-```python
->>> data["key 3"] = torch.randn(3, 4, device="cpu")
->>> assert data["key 3"].device is torch.device("cuda:0")
-```
-Once the device is set, it can be cleared with the 
-[``clear_device_``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.clear_device_)
-method. 
-
-### TensorDict as a specialized dictionary
-TensorDict possesses all the basic features of a dictionary such as 
-[``clear``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.clear), 
-[``copy``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.copy), 
-[``fromkeys``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.fromkeys), 
-[``get``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.get), 
-[``items``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.items), 
-[``keys``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.keys), 
-[``pop``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.pop), 
-[``popitem``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.popitem), 
-[``setdefault``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.setdefault), 
-[``update``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.update) and 
-[``values``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.values).
-
-But that is not all, you can also store nested values in a tensordict:
-```python
->>> data["nested", "key"] = torch.zeros(3, 4) # the batch-size must match
-```
-and any nested tuple structure will be unravelled to make it easy to read code and
-write ops programmatically:
-```python
->>> data["nested", ("supernested", ("key",))] = torch.zeros(3, 4) # the batch-size must match
->>> assert (data["nested", "supernested", "key"] == 0).all()
->>> assert (("nested",), "supernested", (("key",),)) in data.keys(include_nested=True)  # this works too!
-```
-
-You can also store non-tensor data in tensordicts:
-
-```python
->>> data = TensorDict({"a-tensor": torch.randn(1, 2)}, batch_size=[1, 2])
->>> data["non-tensor"] = "a string!"
->>> assert data["non-tensor"] == "a string!"
-```
-
-### Tensor-like features
-
-**\[Nightly feature\]** TensorDict supports many common point-wise arithmetic operations such as `==` or `+`, `+=`
-and similar (provided that the underlying tensors support the said operation):
-```python
->>> td = TensorDict.fromkeys(["a", "b", "c"], 0)
->>> td += 1
->>> assert (td==1).all()
-```
-
-TensorDict objects can be indexed exactly like tensors. The resulting of indexing
-a TensorDict is another TensorDict containing tensors indexed along the required dimension:
-```python
->>> data = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
->>> sub_tensordict = data[..., :2]
->>> assert sub_tensordict.shape == torch.Size([3, 2])
->>> assert sub_tensordict["key 1"].shape == torch.Size([3, 2, 5])
-```
-
-Similarly, one can build tensordicts by stacking or concatenating single tensordicts:
-```python
->>> tensordicts = [TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4]) for _ in range(2)]
->>> stack_tensordict = torch.stack(tensordicts, 1)
->>> assert stack_tensordict.shape == torch.Size([3, 2, 4])
->>> assert stack_tensordict["key 1"].shape == torch.Size([3, 2, 4, 5])
->>> cat_tensordict = torch.cat(tensordicts, 0)
->>> assert cat_tensordict.shape == torch.Size([6, 4])
->>> assert cat_tensordict["key 1"].shape == torch.Size([6, 4, 5])
-```
-
-TensorDict instances can also be reshaped, viewed, squeezed and unsqueezed:
-```python
->>> data = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
-... }, batch_size=[3, 4])
->>> print(data.view(-1))
-torch.Size([12])
->>> print(data.reshape(-1))
-torch.Size([12])
->>> print(data.unsqueeze(-1))
-torch.Size([3, 4, 1])
-```
-
-One can also send tensordict from device to device, place them in shared memory,
-clone them, update them in-place or not, split them, unbind them, expand them etc.
-
-If a functionality is missing, it is easy to call it using `apply()` or `apply_()`:
-```python
-tensordict_uniform = data.apply(lambda tensor: tensor.uniform_())
-```
-
-``apply()`` can also be great to filter a tensordict, for instance:
-```python
-data = TensorDict({"a": torch.tensor(1.0, dtype=torch.float), "b": torch.tensor(1, dtype=torch.int64)}, [])
-data_float = data.apply(lambda x: x if x.dtype == torch.float else None) # contains only the "a" key
-assert "b" not in data_float
-```
-
-### Distributed capabilities
-
-Complex data structures can be cumbersome to synchronize in distributed settings.
-`tensordict` solves that problem with synchronous and asynchronous helper methods
-such as `recv`, `irecv`, `send` and `isend` that behave like their `torch.distributed`
-counterparts:
-```python
->>> # on all workers
->>> data = TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(())}, [])
->>> # on worker 1
->>> data.isend(dst=0)
->>> # on worker 0
->>> data.irecv(src=1)
-```
-
-When nodes share a common scratch space, the
-[`MemmapTensor` backend](https://pytorch.github.io/tensordict/tutorials/tensordict_memory.html)
-can be used
-to seamlessly send, receive and read a huge amount of data.
-
-### TensorDict for functional programming
-
-We also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).
-For instance, TensorDict makes it easy to concatenate model weights to do model ensembling:
-```python
->>> from torch import nn
->>> from tensordict import TensorDict
->>> import torch
->>> from torch import vmap
->>> layer1 = nn.Linear(3, 4)
->>> layer2 = nn.Linear(4, 4)
->>> model = nn.Sequential(layer1, layer2)
->>> params = TensorDict.from_module(model)
->>> # we represent the weights hierarchically
->>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".")
->>> weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".")
->>> assert (params == TensorDict({"0": weights1, "1": weights2}, [])).all()
->>> # Let's use our functional module
->>> x = torch.randn(10, 3)
->>> with params.to_module(model):
-...     out = model(x)
->>> # an ensemble of models: we stack params along the first dimension...
->>> params_stack = torch.stack([params, params], 0)
->>> # ... and use it as an input we'd like to pass through the model
->>> def func(x, params):
-...     with params.to_module(model):
-...         return model(x)
->>> y = vmap(func, (None, 0))(x, params_stack)
->>> print(y.shape)
-torch.Size([2, 10, 4])
-```
-
-Moreover, tensordict modules are compatible with `torch.fx` and (soon) `torch.compile`,
-which means that you can get the best of both worlds: a codebase that is
-both readable and future-proof as well as efficient and portable!
-
-### TensorDict for parameter serialization and building datasets
-
-TensorDict offers an API for parameter serialization that can be >3x faster than
-regular calls to `torch.save(state_dict)`. Moreover, because tensors will be saved
-independently on disk, you can deserialize your checkpoint on an arbitrary slice
-of the model.
-
-```python
->>> model = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 3))
->>> params = TensorDict.from_module(model)
->>> params.memmap("/path/to/saved/folder/", num_threads=16)  # adjust num_threads for speed
->>> # load params
->>> params = TensorDict.load_memmap("/path/to/saved/folder/", num_threads=16)
->>> params.to_module(model)  # load onto model
->>> params["0"].to_module(model[0])  # load on a slice of the model
->>> # in the latter case we could also have loaded only the slice we needed
->>> params0 = TensorDict.load_memmap("/path/to/saved/folder/0", num_threads=16)
->>> params0.to_module(model[0])  # load on a slice of the model
-```
-
-The same functionality can be used to access data in a dataset stored on disk.
-Soring a single contiguous tensor on disk accessed through the `tensordict.MemoryMappedTensor`
-primitive and reading slices of it is not only **much** faster than loading
-single files one at a time but it's also easier and safer (because there is no pickling
-or third-party library involved):
-
-```python
-# allocate memory of the dataset on disk
-data = TensorDict({
-    "images": torch.zeros((128, 128, 3), dtype=torch.uint8),
-    "labels": torch.zeros((), dtype=torch.int)}, batch_size=[])
-data = data.expand(1000000)
-data = data.memmap_like("/path/to/dataset")
-# ==> Fill your dataset here
-# Let's get 3 items of our dataset:
-data[torch.tensor([1, 10000, 500000])]  # This is much faster than loading the 3 images independently
-```
-
-### Preprocessing with TensorDict.map
-
-Preprocessing huge contiguous (or not!) datasets can be done via `TensorDict.map`
-which will dispatch a task to various workers:
-
-```python
-import torch
-from tensordict import TensorDict, MemoryMappedTensor
-import tempfile
-
-def process_data(data):
-    images = data.get("images").flip(-2).clone()
-    labels = data.get("labels") // 10
-    # we update the td inplace
-    data.set_("images", images)  # flip image
-    data.set_("labels", labels)  # cluster labels
-
-if __name__ == "__main__":
-    # create data_preproc here
-    data_preproc = data.map(process_data, num_workers=4, chunksize=0, pbar=True)  # process 1 images at a time
-```
-
-### Lazy preallocation
-
-Pre-allocating tensors can be cumbersome and hard to scale if the list of preallocated
-items varies according to the script configuration. TensorDict solves this in an elegant way.
-Assume you are working with a function `foo() -> TensorDict`, e.g.
-```python
-def foo():
-    data = TensorDict({}, batch_size=[])
-    data["a"] = torch.randn(3)
-    data["b"] = TensorDict({"c": torch.zeros(2)}, batch_size=[])
-    return data
-```
-and you would like to call this function repeatedly. You could do this in two ways.
-The first would simply be to stack the calls to the function:
-```python
-data = torch.stack([foo() for _ in range(N)])
-```
-However, you could also choose to preallocate the tensordict:
-```python
-data = TensorDict({}, batch_size=[N])
-for i in range(N):
-    data[i] = foo()
-```
-which also results in a tensordict (when `N = 10`)
-```
-TensorDict(
-    fields={
-        a: Tensor(torch.Size([10, 3]), dtype=torch.float32),
-        b: TensorDict(
-            fields={
-                c: Tensor(torch.Size([10, 2]), dtype=torch.float32)},
-            batch_size=torch.Size([10]),
-            device=None,
-            is_shared=False)},
-    batch_size=torch.Size([10]),
-    device=None,
-    is_shared=False)
-```
-When `i==0`, your empty tensordict will automatically be populated with empty tensors
-of batch-size `N`. After that, updates will be written in-place.
-Note that this would also work with a shuffled series of indices (pre-allocation does
-not require you to go through the tensordict in an ordered fashion).
-
-
-### Nesting TensorDicts
-
-It is possible to nest tensordict. The only requirement is that the sub-tensordict should be indexable
-under the parent tensordict, i.e. its batch size should match (but could be longer than) the parent
-batch size.
-
-We can switch easily between hierarchical and flat representations.
-For instance, the following code will result in a single-level tensordict with keys `"key 1"` and `"key 2.sub-key"`:
-```python
->>> data = TensorDict({
-...     "key 1": torch.ones(3, 4, 5),
-...     "key 2": TensorDict({"sub-key": torch.randn(3, 4, 5, 6)}, batch_size=[3, 4, 5])
-... }, batch_size=[3, 4])
->>> tensordict_flatten = data.flatten_keys(separator=".")
-```
-
-Accessing nested tensordicts can be achieved with a single index:
-```python
->>> sub_value = data["key 2", "sub-key"]
-```
-
-## TensorClass
-
-Content flexibility comes at the cost of predictability.
-In some cases, developers may be looking for data structure with a more explicit behavior.
-`tensordict` provides a `dataclass`-like decorator that allows for the creation of custom dataclasses that support
-the tensordict operations:
-```python
->>> from tensordict.prototype import tensorclass
->>> import torch
->>>
->>> @tensorclass
-... class MyData:
-...    image: torch.Tensor
-...    mask: torch.Tensor
-...    label: torch.Tensor
-...
-...    def mask_image(self):
-...        return self.image[self.mask.expand_as(self.image)].view(*self.batch_size, -1)
-...
-...    def select_label(self, label):
-...        return self[self.label == label]
-...
->>> images = torch.randn(100, 3, 64, 64)
->>> label = torch.randint(10, (100,))
->>> mask = torch.zeros(1, 64, 64, dtype=torch.bool).bernoulli_().expand(100, 1, 64, 64)
->>>
->>> data = MyData(images, mask, label=label, batch_size=[100])
->>>
->>> print(data.select_label(1))
-MyData(
-    image=Tensor(torch.Size([11, 3, 64, 64]), dtype=torch.float32),
-    label=Tensor(torch.Size([11]), dtype=torch.int64),
-    mask=Tensor(torch.Size([11, 1, 64, 64]), dtype=torch.bool),
-    batch_size=torch.Size([11]),
-    device=None,
-    is_shared=False)
->>> print(data.mask_image().shape)
-torch.Size([100, 6117])
->>> print(data.reshape(10, 10))
-MyData(
-    image=Tensor(torch.Size([10, 10, 3, 64, 64]), dtype=torch.float32),
-    label=Tensor(torch.Size([10, 10]), dtype=torch.int64),
-    mask=Tensor(torch.Size([10, 10, 1, 64, 64]), dtype=torch.bool),
-    batch_size=torch.Size([10, 10]),
-    device=None,
-    is_shared=False)
-```
-As this example shows, one can write a specific data structures with dedicated methods while still enjoying the TensorDict
-artifacts such as shape operations (e.g. reshape or permutations), data manipulation (indexing, `cat` and `stack`) or calling
-arbitrary functions through the `apply` method (and many more).
-
-Tensorclasses support nesting and, in fact, all the TensorDict features.
-
-
-## Installation
-
-**With Pip**:
-
-To install the latest stable version of tensordict, simply run
-
-```bash
-pip install tensordict
-```
-
-This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
-
-To enjoy the latest features, one can use
-
-```bash
-pip install tensordict-nightly
-```
-
-**With Conda**:
-
-Install `tensordict` from `conda-forge` channel.
-
-```sh
-conda install -c conda-forge tensordict
-```
-
-
-## Citation
-
-If you're using TensorDict, please refer to this BibTeX entry to cite this work:
-```
-@misc{bou2023torchrl,
-      title={TorchRL: A data-driven decision-making library for PyTorch}, 
-      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
-      year={2023},
-      eprint={2306.00577},
-      archivePrefix={arXiv},
-      primaryClass={cs.LG}
-}
-```
-
-## Disclaimer
-
-TensorDict is at the *beta*-stage, meaning that there may be bc-breaking changes introduced, but 
-they should come with a warranty.
-Hopefully these should not happen too often, as the current roadmap mostly 
-involves adding new features and building compatibility with the broader
-PyTorch ecosystem.
-
-## License
-
-TensorDict is licensed under the MIT License. See [LICENSE](LICENSE) for details.
-
-
+Metadata-Version: 2.1
+Name: tensordict-nightly
+Version: 2024.6.3
+Summary: UNKNOWN
+Home-page: https://github.com/pytorch/tensordict
+Author: tensordict contributors
+Author-email: vmoens@fb.com
+License: BSD
+Platform: UNKNOWN
+Classifier: Programming Language :: Python :: 3.8
+Classifier: Programming Language :: Python :: 3.9
+Classifier: Programming Language :: Python :: 3.10
+Classifier: Programming Language :: Python :: 3.11
+Classifier: Development Status :: 4 - Beta
+Description-Content-Type: text/markdown
+Requires-Dist: torch (>=2.4.0.dev)
+Requires-Dist: numpy
+Requires-Dist: cloudpickle
+Provides-Extra: checkpointing
+Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
+Provides-Extra: h5
+Requires-Dist: h5py (>=3.8) ; extra == 'h5'
+Provides-Extra: tests
+Requires-Dist: pytest ; extra == 'tests'
+Requires-Dist: pyyaml ; extra == 'tests'
+Requires-Dist: pytest-instafail ; extra == 'tests'
+Requires-Dist: pytest-rerunfailures ; extra == 'tests'
+Requires-Dist: pytest-benchmark ; extra == 'tests'
+
+<!--- BADGES: START --->
+<!---
+[![Documentation](https://img.shields.io/badge/Documentation-blue.svg?style=flat)](https://pytorch.github.io/tensordict/)
+--->
+[![Docs - GitHub.io](https://img.shields.io/static/v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-package]
+[![Discord Shield](https://dcbadge.vercel.app/api/server/tz3TgTAe3D)](https://discord.gg/tz3TgTAe3D)
+[![Benchmarks](https://img.shields.io/badge/Benchmarks-blue.svg)][#docs-package-benchmark]
+[![Python version](https://img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/downloads/)
+[![GitHub license](https://img.shields.io/badge/license-MIT-blue.svg)][#github-license]
+<a href="https://pypi.org/project/tensordict"><img src="https://img.shields.io/pypi/v/tensordict" alt="pypi version"></a>
+<a href="https://pypi.org/project/tensordict-nightly"><img src="https://img.shields.io/pypi/v/tensordict-nightly?label=nightly" alt="pypi nightly version"></a>
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads)][#pepy-package]
+[![Downloads](https://static.pepy.tech/personalized-badge/tensordict-nightly?period=total&units=international_system&left_color=blue&right_color=orange&left_text=Downloads%20(nightly))][#pepy-package-nightly]
+[![codecov](https://codecov.io/gh/pytorch/tensordict/branch/main/graph/badge.svg?token=9QTUG6NAGQ)][#codecov-package]
+[![circleci](https://circleci.com/gh/pytorch/tensordict.svg?style=shield)][#circleci-package]
+[![Conda - Platform](https://img.shields.io/conda/pn/conda-forge/tensordict?logo=anaconda&style=flat)][#conda-forge-package]
+[![Conda (channel only)](https://img.shields.io/conda/vn/conda-forge/tensordict?logo=anaconda&style=flat&color=orange)][#conda-forge-package]
+
+[#docs-package]: https://pytorch.github.io/tensordict/
+[#docs-package-benchmark]: https://pytorch.github.io/tensordict/dev/bench/
+[#github-license]: https://github.com/pytorch/tensordict/blob/main/LICENSE
+[#pepy-package]: https://pepy.tech/project/tensordict
+[#pepy-package-nightly]: https://pepy.tech/project/tensordict-nightly
+[#codecov-package]: https://codecov.io/gh/pytorch/tensordict
+[#circleci-package]: https://circleci.com/gh/pytorch/tensordict
+[#conda-forge-package]: https://anaconda.org/conda-forge/tensordict
+
+<!--- BADGES: END --->
+
+# TensorDict
+
+[**Installation**](#installation) | [**General features**](#general-principles) |
+[**Tensor-like features**](#tensor-like-features) |  [**Distributed capabilities**](#distributed-capabilities) |
+[**TensorDict for functional programming**](#tensordict-for-functional-programming) |
+[**TensorDict for parameter serialization](#tensordict-for-parameter-serialization) |
+[**Lazy preallocation**](#lazy-preallocation) | [**Nesting TensorDicts**](#nesting-tensordicts) | [**TensorClass**](#tensorclass)
+
+`TensorDict` is a dictionary-like class that inherits properties from tensors,
+such as indexing, shape operations, casting to device or point-to-point communication
+in distributed settings. Whenever you need to execute an operation over a batch of tensors, 
+TensorDict is there to help you.
+
+The primary goal of TensorDict is to make your code-bases more _readable_, _compact_, and _modular_. 
+It abstracts away tailored operations, making your code less error-prone as it takes care of 
+dispatching the operation on the leaves for you.
+
+Using tensordict primitives, most supervised training loops can be rewritten in a generic way:
+```python
+for i, data in enumerate(dataset):
+    # the model reads and writes tensordicts
+    data = model(data)
+    loss = loss_module(data)
+    loss.backward()
+    optimizer.step()
+    optimizer.zero_grad()
+```
+
+With this level of abstraction, one can recycle a training loop for highly heterogeneous task.
+Each individual step of the training loop (data collection and transform, model prediction, loss computation etc.)
+can be tailored to the use case at hand without impacting the others.
+For instance, the above example can be easily used across classification and segmentation tasks, among many others.
+
+## Features
+
+### General principles
+
+Unlike other [pytrees](https://github.com/pytorch/pytorch/blob/main/torch/utils/_pytree.py), TensorDict
+carries metadata that make it easy to query the state of the container. The main metadata
+are the [``batch_size``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.batch_size) 
+(also referred as ``shape``), 
+the [``device``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.device), 
+the shared status
+([``is_memmap``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase.is_memmap) or 
+[``is_shared``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDictBase.html#tensordict.TensorDictBase.is_shared)), 
+the dimension [``names``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.names) 
+and the [``lock``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.lock_) status.
+
+A tensordict is primarily defined by its `batch_size` (or `shape`) and its key-value pairs:
+```python
+>>> from tensordict import TensorDict
+>>> import torch
+>>> data = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+```
+The `batch_size` and the first dimensions of each of the tensors must be compliant.
+The tensors can be of any dtype and device. 
+
+Optionally, one can restrict a tensordict to
+live on a dedicated ``device``, which will send each tensor that is written there:
+```python
+>>> data = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4], device="cuda:0")
+```
+When a tensordict has a device, all write operations will cast the tensor to the
+TensorDict device:
+```python
+>>> data["key 3"] = torch.randn(3, 4, device="cpu")
+>>> assert data["key 3"].device is torch.device("cuda:0")
+```
+Once the device is set, it can be cleared with the 
+[``clear_device_``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.clear_device_)
+method. 
+
+### TensorDict as a specialized dictionary
+TensorDict possesses all the basic features of a dictionary such as 
+[``clear``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.clear), 
+[``copy``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.copy), 
+[``fromkeys``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.fromkeys), 
+[``get``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.get), 
+[``items``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.items), 
+[``keys``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.keys), 
+[``pop``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.pop), 
+[``popitem``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.popitem), 
+[``setdefault``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.setdefault), 
+[``update``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.update) and 
+[``values``](https://pytorch.org/tensordict/reference/generated/tensordict.TensorDict.html#tensordict.TensorDict.values).
+
+But that is not all, you can also store nested values in a tensordict:
+```python
+>>> data["nested", "key"] = torch.zeros(3, 4) # the batch-size must match
+```
+and any nested tuple structure will be unravelled to make it easy to read code and
+write ops programmatically:
+```python
+>>> data["nested", ("supernested", ("key",))] = torch.zeros(3, 4) # the batch-size must match
+>>> assert (data["nested", "supernested", "key"] == 0).all()
+>>> assert (("nested",), "supernested", (("key",),)) in data.keys(include_nested=True)  # this works too!
+```
+
+You can also store non-tensor data in tensordicts:
+
+```python
+>>> data = TensorDict({"a-tensor": torch.randn(1, 2)}, batch_size=[1, 2])
+>>> data["non-tensor"] = "a string!"
+>>> assert data["non-tensor"] == "a string!"
+```
+
+### Tensor-like features
+
+**\[Nightly feature\]** TensorDict supports many common point-wise arithmetic operations such as `==` or `+`, `+=`
+and similar (provided that the underlying tensors support the said operation):
+```python
+>>> td = TensorDict.fromkeys(["a", "b", "c"], 0)
+>>> td += 1
+>>> assert (td==1).all()
+```
+
+TensorDict objects can be indexed exactly like tensors. The resulting of indexing
+a TensorDict is another TensorDict containing tensors indexed along the required dimension:
+```python
+>>> data = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+>>> sub_tensordict = data[..., :2]
+>>> assert sub_tensordict.shape == torch.Size([3, 2])
+>>> assert sub_tensordict["key 1"].shape == torch.Size([3, 2, 5])
+```
+
+Similarly, one can build tensordicts by stacking or concatenating single tensordicts:
+```python
+>>> tensordicts = [TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4]) for _ in range(2)]
+>>> stack_tensordict = torch.stack(tensordicts, 1)
+>>> assert stack_tensordict.shape == torch.Size([3, 2, 4])
+>>> assert stack_tensordict["key 1"].shape == torch.Size([3, 2, 4, 5])
+>>> cat_tensordict = torch.cat(tensordicts, 0)
+>>> assert cat_tensordict.shape == torch.Size([6, 4])
+>>> assert cat_tensordict["key 1"].shape == torch.Size([6, 4, 5])
+```
+
+TensorDict instances can also be reshaped, viewed, squeezed and unsqueezed:
+```python
+>>> data = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": torch.zeros(3, 4, 5, dtype=torch.bool),
+... }, batch_size=[3, 4])
+>>> print(data.view(-1))
+torch.Size([12])
+>>> print(data.reshape(-1))
+torch.Size([12])
+>>> print(data.unsqueeze(-1))
+torch.Size([3, 4, 1])
+```
+
+One can also send tensordict from device to device, place them in shared memory,
+clone them, update them in-place or not, split them, unbind them, expand them etc.
+
+If a functionality is missing, it is easy to call it using `apply()` or `apply_()`:
+```python
+tensordict_uniform = data.apply(lambda tensor: tensor.uniform_())
+```
+
+``apply()`` can also be great to filter a tensordict, for instance:
+```python
+data = TensorDict({"a": torch.tensor(1.0, dtype=torch.float), "b": torch.tensor(1, dtype=torch.int64)}, [])
+data_float = data.apply(lambda x: x if x.dtype == torch.float else None) # contains only the "a" key
+assert "b" not in data_float
+```
+
+### Distributed capabilities
+
+Complex data structures can be cumbersome to synchronize in distributed settings.
+`tensordict` solves that problem with synchronous and asynchronous helper methods
+such as `recv`, `irecv`, `send` and `isend` that behave like their `torch.distributed`
+counterparts:
+```python
+>>> # on all workers
+>>> data = TensorDict({"a": torch.zeros(()), ("b", "c"): torch.ones(())}, [])
+>>> # on worker 1
+>>> data.isend(dst=0)
+>>> # on worker 0
+>>> data.irecv(src=1)
+```
+
+When nodes share a common scratch space, the
+[`MemmapTensor` backend](https://pytorch.github.io/tensordict/tutorials/tensordict_memory.html)
+can be used
+to seamlessly send, receive and read a huge amount of data.
+
+### TensorDict for functional programming
+
+We also provide an API to use TensorDict in conjunction with [FuncTorch](https://pytorch.org/functorch).
+For instance, TensorDict makes it easy to concatenate model weights to do model ensembling:
+```python
+>>> from torch import nn
+>>> from tensordict import TensorDict
+>>> import torch
+>>> from torch import vmap
+>>> layer1 = nn.Linear(3, 4)
+>>> layer2 = nn.Linear(4, 4)
+>>> model = nn.Sequential(layer1, layer2)
+>>> params = TensorDict.from_module(model)
+>>> # we represent the weights hierarchically
+>>> weights1 = TensorDict(layer1.state_dict(), []).unflatten_keys(".")
+>>> weights2 = TensorDict(layer2.state_dict(), []).unflatten_keys(".")
+>>> assert (params == TensorDict({"0": weights1, "1": weights2}, [])).all()
+>>> # Let's use our functional module
+>>> x = torch.randn(10, 3)
+>>> with params.to_module(model):
+...     out = model(x)
+>>> # an ensemble of models: we stack params along the first dimension...
+>>> params_stack = torch.stack([params, params], 0)
+>>> # ... and use it as an input we'd like to pass through the model
+>>> def func(x, params):
+...     with params.to_module(model):
+...         return model(x)
+>>> y = vmap(func, (None, 0))(x, params_stack)
+>>> print(y.shape)
+torch.Size([2, 10, 4])
+```
+
+Moreover, tensordict modules are compatible with `torch.fx` and (soon) `torch.compile`,
+which means that you can get the best of both worlds: a codebase that is
+both readable and future-proof as well as efficient and portable!
+
+### TensorDict for parameter serialization and building datasets
+
+TensorDict offers an API for parameter serialization that can be >3x faster than
+regular calls to `torch.save(state_dict)`. Moreover, because tensors will be saved
+independently on disk, you can deserialize your checkpoint on an arbitrary slice
+of the model.
+
+```python
+>>> model = nn.Sequential(nn.Linear(3, 4), nn.Linear(4, 3))
+>>> params = TensorDict.from_module(model)
+>>> params.memmap("/path/to/saved/folder/", num_threads=16)  # adjust num_threads for speed
+>>> # load params
+>>> params = TensorDict.load_memmap("/path/to/saved/folder/", num_threads=16)
+>>> params.to_module(model)  # load onto model
+>>> params["0"].to_module(model[0])  # load on a slice of the model
+>>> # in the latter case we could also have loaded only the slice we needed
+>>> params0 = TensorDict.load_memmap("/path/to/saved/folder/0", num_threads=16)
+>>> params0.to_module(model[0])  # load on a slice of the model
+```
+
+The same functionality can be used to access data in a dataset stored on disk.
+Soring a single contiguous tensor on disk accessed through the `tensordict.MemoryMappedTensor`
+primitive and reading slices of it is not only **much** faster than loading
+single files one at a time but it's also easier and safer (because there is no pickling
+or third-party library involved):
+
+```python
+# allocate memory of the dataset on disk
+data = TensorDict({
+    "images": torch.zeros((128, 128, 3), dtype=torch.uint8),
+    "labels": torch.zeros((), dtype=torch.int)}, batch_size=[])
+data = data.expand(1000000)
+data = data.memmap_like("/path/to/dataset")
+# ==> Fill your dataset here
+# Let's get 3 items of our dataset:
+data[torch.tensor([1, 10000, 500000])]  # This is much faster than loading the 3 images independently
+```
+
+### Preprocessing with TensorDict.map
+
+Preprocessing huge contiguous (or not!) datasets can be done via `TensorDict.map`
+which will dispatch a task to various workers:
+
+```python
+import torch
+from tensordict import TensorDict, MemoryMappedTensor
+import tempfile
+
+def process_data(data):
+    images = data.get("images").flip(-2).clone()
+    labels = data.get("labels") // 10
+    # we update the td inplace
+    data.set_("images", images)  # flip image
+    data.set_("labels", labels)  # cluster labels
+
+if __name__ == "__main__":
+    # create data_preproc here
+    data_preproc = data.map(process_data, num_workers=4, chunksize=0, pbar=True)  # process 1 images at a time
+```
+
+### Lazy preallocation
+
+Pre-allocating tensors can be cumbersome and hard to scale if the list of preallocated
+items varies according to the script configuration. TensorDict solves this in an elegant way.
+Assume you are working with a function `foo() -> TensorDict`, e.g.
+```python
+def foo():
+    data = TensorDict({}, batch_size=[])
+    data["a"] = torch.randn(3)
+    data["b"] = TensorDict({"c": torch.zeros(2)}, batch_size=[])
+    return data
+```
+and you would like to call this function repeatedly. You could do this in two ways.
+The first would simply be to stack the calls to the function:
+```python
+data = torch.stack([foo() for _ in range(N)])
+```
+However, you could also choose to preallocate the tensordict:
+```python
+data = TensorDict({}, batch_size=[N])
+for i in range(N):
+    data[i] = foo()
+```
+which also results in a tensordict (when `N = 10`)
+```
+TensorDict(
+    fields={
+        a: Tensor(torch.Size([10, 3]), dtype=torch.float32),
+        b: TensorDict(
+            fields={
+                c: Tensor(torch.Size([10, 2]), dtype=torch.float32)},
+            batch_size=torch.Size([10]),
+            device=None,
+            is_shared=False)},
+    batch_size=torch.Size([10]),
+    device=None,
+    is_shared=False)
+```
+When `i==0`, your empty tensordict will automatically be populated with empty tensors
+of batch-size `N`. After that, updates will be written in-place.
+Note that this would also work with a shuffled series of indices (pre-allocation does
+not require you to go through the tensordict in an ordered fashion).
+
+
+### Nesting TensorDicts
+
+It is possible to nest tensordict. The only requirement is that the sub-tensordict should be indexable
+under the parent tensordict, i.e. its batch size should match (but could be longer than) the parent
+batch size.
+
+We can switch easily between hierarchical and flat representations.
+For instance, the following code will result in a single-level tensordict with keys `"key 1"` and `"key 2.sub-key"`:
+```python
+>>> data = TensorDict({
+...     "key 1": torch.ones(3, 4, 5),
+...     "key 2": TensorDict({"sub-key": torch.randn(3, 4, 5, 6)}, batch_size=[3, 4, 5])
+... }, batch_size=[3, 4])
+>>> tensordict_flatten = data.flatten_keys(separator=".")
+```
+
+Accessing nested tensordicts can be achieved with a single index:
+```python
+>>> sub_value = data["key 2", "sub-key"]
+```
+
+## TensorClass
+
+Content flexibility comes at the cost of predictability.
+In some cases, developers may be looking for data structure with a more explicit behavior.
+`tensordict` provides a `dataclass`-like decorator that allows for the creation of custom dataclasses that support
+the tensordict operations:
+```python
+>>> from tensordict.prototype import tensorclass
+>>> import torch
+>>>
+>>> @tensorclass
+... class MyData:
+...    image: torch.Tensor
+...    mask: torch.Tensor
+...    label: torch.Tensor
+...
+...    def mask_image(self):
+...        return self.image[self.mask.expand_as(self.image)].view(*self.batch_size, -1)
+...
+...    def select_label(self, label):
+...        return self[self.label == label]
+...
+>>> images = torch.randn(100, 3, 64, 64)
+>>> label = torch.randint(10, (100,))
+>>> mask = torch.zeros(1, 64, 64, dtype=torch.bool).bernoulli_().expand(100, 1, 64, 64)
+>>>
+>>> data = MyData(images, mask, label=label, batch_size=[100])
+>>>
+>>> print(data.select_label(1))
+MyData(
+    image=Tensor(torch.Size([11, 3, 64, 64]), dtype=torch.float32),
+    label=Tensor(torch.Size([11]), dtype=torch.int64),
+    mask=Tensor(torch.Size([11, 1, 64, 64]), dtype=torch.bool),
+    batch_size=torch.Size([11]),
+    device=None,
+    is_shared=False)
+>>> print(data.mask_image().shape)
+torch.Size([100, 6117])
+>>> print(data.reshape(10, 10))
+MyData(
+    image=Tensor(torch.Size([10, 10, 3, 64, 64]), dtype=torch.float32),
+    label=Tensor(torch.Size([10, 10]), dtype=torch.int64),
+    mask=Tensor(torch.Size([10, 10, 1, 64, 64]), dtype=torch.bool),
+    batch_size=torch.Size([10, 10]),
+    device=None,
+    is_shared=False)
+```
+As this example shows, one can write a specific data structures with dedicated methods while still enjoying the TensorDict
+artifacts such as shape operations (e.g. reshape or permutations), data manipulation (indexing, `cat` and `stack`) or calling
+arbitrary functions through the `apply` method (and many more).
+
+Tensorclasses support nesting and, in fact, all the TensorDict features.
+
+
+## Installation
+
+**With Pip**:
+
+To install the latest stable version of tensordict, simply run
+
+```bash
+pip install tensordict
+```
+
+This will work with Python 3.7 and upward as well as PyTorch 1.12 and upward.
+
+To enjoy the latest features, one can use
+
+```bash
+pip install tensordict-nightly
+```
+
+**With Conda**:
+
+Install `tensordict` from `conda-forge` channel.
+
+```sh
+conda install -c conda-forge tensordict
+```
+
+
+## Citation
+
+If you're using TensorDict, please refer to this BibTeX entry to cite this work:
+```
+@misc{bou2023torchrl,
+      title={TorchRL: A data-driven decision-making library for PyTorch}, 
+      author={Albert Bou and Matteo Bettini and Sebastian Dittert and Vikash Kumar and Shagun Sodhani and Xiaomeng Yang and Gianni De Fabritiis and Vincent Moens},
+      year={2023},
+      eprint={2306.00577},
+      archivePrefix={arXiv},
+      primaryClass={cs.LG}
+}
+```
+
+## Disclaimer
+
+TensorDict is at the *beta*-stage, meaning that there may be bc-breaking changes introduced, but 
+they should come with a warranty.
+Hopefully these should not happen too often, as the current roadmap mostly 
+involves adding new features and building compatibility with the broader
+PyTorch ecosystem.
+
+## License
+
+TensorDict is licensed under the MIT License. See [LICENSE](LICENSE) for details.
+
+
```

### html2text {}

```diff
@@ -1,22 +1,21 @@
-Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.6.2 Summary:
+Metadata-Version: 2.1 Name: tensordict-nightly Version: 2024.6.3 Summary:
 UNKNOWN Home-page: https://github.com/pytorch/tensordict Author: tensordict
 contributors Author-email: vmoens@fb.com License: BSD Platform: UNKNOWN
 Classifier: Programming Language :: Python :: 3.8 Classifier: Programming
 Language :: Python :: 3.9 Classifier: Programming Language :: Python :: 3.10
 Classifier: Programming Language :: Python :: 3.11 Classifier: Development
-Status :: 4 - Beta Description-Content-Type: text/markdown License-File:
-LICENSE Requires-Dist: torch >=2.4.0.dev Requires-Dist: numpy Requires-Dist:
-cloudpickle Provides-Extra: checkpointing Requires-Dist: torchsnapshot-nightly
-; extra == 'checkpointing' Provides-Extra: h5 Requires-Dist: h5py >=3.8 ; extra
-== 'h5' Provides-Extra: tests Requires-Dist: pytest ; extra == 'tests'
-Requires-Dist: pyyaml ; extra == 'tests' Requires-Dist: pytest-instafail ;
-extra == 'tests' Requires-Dist: pytest-rerunfailures ; extra == 'tests'
-Requires-Dist: pytest-benchmark ; extra == 'tests' [![Docs - GitHub.io](https:/
-/img.shields.io/static/
+Status :: 4 - Beta Description-Content-Type: text/markdown Requires-Dist: torch
+(>=2.4.0.dev) Requires-Dist: numpy Requires-Dist: cloudpickle Provides-Extra:
+checkpointing Requires-Dist: torchsnapshot-nightly ; extra == 'checkpointing'
+Provides-Extra: h5 Requires-Dist: h5py (>=3.8) ; extra == 'h5' Provides-Extra:
+tests Requires-Dist: pytest ; extra == 'tests' Requires-Dist: pyyaml ; extra ==
+'tests' Requires-Dist: pytest-instafail ; extra == 'tests' Requires-Dist:
+pytest-rerunfailures ; extra == 'tests' Requires-Dist: pytest-benchmark ; extra
+== 'tests' [![Docs - GitHub.io](https://img.shields.io/static/
 v1?logo=github&style=flat&color=pink&label=docs&message=tensordict)][#docs-
 package] [![Discord Shield](https://dcbadge.vercel.app/api/server/tz3TgTAe3D)]
 (https://discord.gg/tz3TgTAe3D) [![Benchmarks](https://img.shields.io/badge/
 Benchmarks-blue.svg)][#docs-package-benchmark] [![Python version](https://
 img.shields.io/pypi/pyversions/tensordict.svg)](https://www.python.org/
 downloads/) [![GitHub license](https://img.shields.io/badge/license-MIT-
 blue.svg)][#github-license] _[_p_y_p_i_ _v_e_r_s_i_o_n_]_[_p_y_p_i_ _n_i_g_h_t_l_y_ _v_e_r_s_i_o_n_][![Downloads]
```

## Comparing `tensordict_nightly-2024.6.2.dist-info/RECORD` & `tensordict_nightly-2024.6.3.dist-info/RECORD`

 * *Files 22% similar despite different names*

```diff
@@ -1,37 +1,37 @@
-tensordict/__init__.py,sha256=KnyJ9ME65lcfYfNLZKabmHZOhpuZOCXB0SKPVQ0_F9U,1459
-tensordict/_contextlib.py,sha256=yao_SZSgKUJt6dXHtAc5ZFJzAmm_NQQFYgR1rjDg0k8,6156
-tensordict/_lazy.py,sha256=-p6uqvhndZrKPNszwi-F8pHbWiJHazV-spe4Jr743VI,137304
-tensordict/_pytree.py,sha256=Q6QuFk1aiM7it7SU6ff8XHQqXAAZNWKADkHZA92-Mzc,7844
-tensordict/_td.py,sha256=sCNIZEPRsJOFjYqoj_B00UvwAjsIbWnTlk7-6cyvudE,154656
-tensordict/_tensordict.pyd,sha256=wEDkFBXZqBtfK3evyGBnP-mfd4dYzkdk1YQlPqiisXI,113664
-tensordict/_torch_func.py,sha256=m2pa2-21mDK252nPf1rDsTMH4qhPLAxAWv4W7WoStIs,23597
-tensordict/base.py,sha256=wi6FA9mmY1w2oN4ljwsbaeWmMrH9-lnEizWuPXxPhzk,295790
-tensordict/functional.py,sha256=o6mqb-BYOpeodQbfg8VXgm1JsUvdz3AJxZJAxq8DwG0,17864
-tensordict/memmap.py,sha256=dI2RYp8W2JMjiBML9GOxIwNAxEMQagIeX1tERjGOEOk,40454
-tensordict/persistent.py,sha256=Ta73Qf9XMXSdwbQ3sto2TW4HkahtByyujngUISM4Guo,50308
-tensordict/tensorclass.py,sha256=nvjqTTj6c661H77nHI0SGBpXFogS12Gl-2c3IBfELn4,102288
-tensordict/tensordict.py,sha256=daJ7LcwpVeWKzGLqz8_M9Gti3BLAb6iRLIhz-yqX3jA,1002
-tensordict/utils.py,sha256=lMFJLBk_Vmk6rR2WS3OLB9W6zujuOZ00Y0XLt07FYHI,76842
-tensordict/version.py,sha256=fCyUh3B5kXXO0TFY3Dx-P5IIGBb1RClGukRxb8hmTAU,86
-tensordict/nn/__init__.py,sha256=nWPo4TqDb1hYILbEc73EHVyv3iy2kumYcoUp1MaXS1g,1634
-tensordict/nn/common.py,sha256=RqjFAF4bm5_B9BB0m7xxgAVWjIXdxrUq-Y3imiNkBYk,54882
-tensordict/nn/ensemble.py,sha256=CWyHKGsN7hmr3HjynEPj5xrCvZggeFy_C7iSpZxHlS4,5940
-tensordict/nn/functional_modules.py,sha256=HnJYqzXH59FrxEZa3fGPODcVMTRlK3TlW8icu2a6J5E,25976
-tensordict/nn/params.py,sha256=2uX-Ga8mzqM6-B0FHbSZ8zZIOt7h6hwtpfeeuaZtWT4,38863
-tensordict/nn/probabilistic.py,sha256=2R4cCEtNkZlxnivx5FcjYa0K-51h2QpvfnbUWOSaIpM,26168
-tensordict/nn/sequence.py,sha256=mD0oJplRLlKflEGSopeSjc9MzdOfH00FM8Tm17_T7Sw,19947
-tensordict/nn/utils.py,sha256=UCK2w6QoOCouDoxP08CvokffxfnwTtWv3Wl5Ls8rgxs,13231
-tensordict/nn/distributions/__init__.py,sha256=kfuBq-yJHh9OkaQHaOJUpvam4KjKLIirE8UbpYF3BuM,795
-tensordict/nn/distributions/composite.py,sha256=bsvnghcdoj2Ak3r3eNgs2GpBxWIV9-z8IIwdZiOM7oM,6629
-tensordict/nn/distributions/continuous.py,sha256=Ge1qivY1uB3sWBVTFD6wKQbBrvU3tAa5GV28aqCs9l4,9924
-tensordict/nn/distributions/discrete.py,sha256=VrcrSPr9YyBDKagOphYTAgpiQqlwfpf4-8t3TsrJyPQ,2667
-tensordict/nn/distributions/truncated_normal.py,sha256=f--2ISj15TTUlLcUzMh1e50yADuh-vKMFbrzLqwUv7I,6694
-tensordict/nn/distributions/utils.py,sha256=3vEDATr12hUk9OYYKf4dzPmNMmLzKHQuZPsdWcPVfi4,1266
-tensordict/prototype/__init__.py,sha256=XBECOVFLLCiUsvNwwByWkz-U87nt7oJXPp2iIIA5Ysk,393
-tensordict/prototype/fx.py,sha256=AD6zDHD2g24iuP9v-yzWN_NX1xInK5E8RHvkend0amc,7889
-tensordict/prototype/tensorclass.py,sha256=bzbSJ7uaQVxfUS6uzwyh5SOBrhufxsW_dLzjMbQJLvw,796
-tensordict_nightly-2024.6.2.dist-info/LICENSE,sha256=PGO-oZsq4EzhE1-WQS2xGiEF3UCVb9YawfQ09cIMV_8,1119
-tensordict_nightly-2024.6.2.dist-info/METADATA,sha256=6LO6lJh9pKqSpUZTiRDBcaMQZP8gRI89H8rbFiLbm-A,22933
-tensordict_nightly-2024.6.2.dist-info/WHEEL,sha256=Z6c-bE0pUM47a70GvqO_SvH_XXU0lm62gEAKtoNJ08A,100
-tensordict_nightly-2024.6.2.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
-tensordict_nightly-2024.6.2.dist-info/RECORD,,
+tensordict/__init__.py,sha256=q682cYiVq8A7gkWNDL-70vCF3801OyLEI2DnBe3O8Lg,1406
+tensordict/_contextlib.py,sha256=F4b0KH4FSeKsY9dkRuIXgHEn1dL_aNk7lFs1DonDYn4,6000
+tensordict/_lazy.py,sha256=_C1zNYjPH-T1BgAgs0IuPTcRT4LJBWW3vsocPrpAF9w,133661
+tensordict/_pytree.py,sha256=ElcwjgEENz4B0m6XfxzzPbHJLIE-KwbryiDXJSo9F6g,7568
+tensordict/_td.py,sha256=_geHwZRVs0PugSYQLUlTsmONyLZ8A_3zIz-GoVC3ej4,150542
+tensordict/_tensordict.so,sha256=ay12DmKsRg80gGaFcJxQ6hw1T8KCigRrp0HOkwMy5BA,3286432
+tensordict/_torch_func.py,sha256=_ibLTwmsakyJU2U0uCXTH8z0RTGw2814RJaf5Sc87j8,22955
+tensordict/base.py,sha256=DFqJEAnnFwZseveXEczrVSpWLBhyfY3LUethDnOPdL4,288174
+tensordict/functional.py,sha256=zc40oFsr_b-T1MrjcFHozdOahpUlsenExnYsTtZWZm8,17459
+tensordict/memmap.py,sha256=pa0OkI4hIg0TJJSxVd9yi5AwsaxiW-k59FTrPoU3Uc8,39377
+tensordict/persistent.py,sha256=-Fz-RGh6UjA2nTRfbKRxTw4F0wOUnHop91SZhwx8Vdw,48928
+tensordict/tensorclass.py,sha256=XqJOhxRn4Wf4_Fp0qe-SrSqSKeuRt5s16vSl9QwFhFY,99433
+tensordict/tensordict.py,sha256=mJnJZFdu21M_l7KkLFK1VsmEIeW0K4bhOEUPAsv3PYQ,965
+tensordict/utils.py,sha256=tVA9k_Z2zFP1GIOUmpLtY4e5iV56_AgvGgq4_K6_py8,74581
+tensordict/version.py,sha256=pmZcY9e2lcxdWaOuOLF1Gj5s7WVZWvTZMkjzpZTbwCg,51
+tensordict/nn/__init__.py,sha256=wMy41A-lVGWHRgEd1A5nGjJFIIt0CoHaobRpD-2CphY,1572
+tensordict/nn/common.py,sha256=daOD-CONPNYR6yyivEGhnAWOCmC8v-olRptmSoQDwSg,53582
+tensordict/nn/ensemble.py,sha256=-4Xjo3N82zZ8wiVTfnoDYy_AOYZDQ7m1LGa3niyj7h8,5811
+tensordict/nn/functional_modules.py,sha256=6LR0pcLyaqdzWrVFYiepsiNCKYlNMtLwThMFRuT2n0c,25318
+tensordict/nn/params.py,sha256=01cRfXS7yY3LiGEhV_tvMqAysVmSCpEUQMKcs1cY8tk,37581
+tensordict/nn/probabilistic.py,sha256=JG3VPut-ISHhOdoLFea65NPop9JuVONjgNKuXOA4IFg,25591
+tensordict/nn/sequence.py,sha256=gzU7atTMUOaD9t3q4Q8xiAjjQ05Qadh5KJCsMyaXfTI,19496
+tensordict/nn/utils.py,sha256=e3-Dbw9eS-z9QDI4dXizFBCQdO29Kehx8SrW4kuUUPY,12884
+tensordict/nn/distributions/__init__.py,sha256=TcEi7v5r1fmLcsaDVUlzTQZPIbnFuoqJ5OL07iVb6S4,774
+tensordict/nn/distributions/composite.py,sha256=P6lMyCL9idr8ouOw6TOFT5acNlpGLN_iMdk3ZGnYuQE,6469
+tensordict/nn/distributions/continuous.py,sha256=ZYx6EvShz8urVStOb3L13Bw6x4nygA8yFI4DFOdXhqE,9658
+tensordict/nn/distributions/discrete.py,sha256=gUzTKs3yO8GkSY0ghBWVaZdHdmFyMMLWd5LV52CfrXY,2580
+tensordict/nn/distributions/truncated_normal.py,sha256=d1ontYbG_Q-W5gcVnWLadvZ1OBBeZGmBb5EUmr3ekak,6504
+tensordict/nn/distributions/utils.py,sha256=fX6NUeNnWKK6kDdOp8NTM43Lls3vMCF5h-S2teVZw3E,1226
+tensordict/prototype/__init__.py,sha256=b9Wmi1tbh2Em_wmKa52IXwxuudd6CUqdLgaob0bSWqQ,381
+tensordict/prototype/fx.py,sha256=LrDoMQ2kT_qPs-hGVPuUW6TogaH_GjbLM1yFdVltfw0,7690
+tensordict/prototype/tensorclass.py,sha256=6FcvFgJpnf2ctYUiv5I2lr7eRkM3KtvR2-6YEU_X3lw,773
+tensordict_nightly-2024.6.3.dist-info/LICENSE,sha256=xdjS4_xk-IwnLuIFCvTYTl9Y8aXRejqpmke3dGam_nI,1098
+tensordict_nightly-2024.6.3.dist-info/METADATA,sha256=DF7Hh9-zUnEUMfZk5mOErY7RPaZ9kNfZRxI2XbPB7Lk,22390
+tensordict_nightly-2024.6.3.dist-info/WHEEL,sha256=Fs91Ann61S2A_mZ0z4nJS8CwPq9BU-Prnl2pSvzthmM,103
+tensordict_nightly-2024.6.3.dist-info/top_level.txt,sha256=4EMgKpsnmq04uFLPJXf8tMQb1POT8QqR1pR74y5wycc,11
+tensordict_nightly-2024.6.3.dist-info/RECORD,,
```

